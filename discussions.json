[
    {
        "title": "Using Llama.cpp as a dependency in another c++ project.",
        "bodyText": "I am currently working on a project where I would like to integrate llama.cpp into a game as a dependency. I have a background in python, but I am still pretty new to c++.\nAs far as I understand, in order to use llama.cpp as a dependency, I need to specify which headers and which source (.cpp) files to include in my xmake.lua file (or CMakeLists.txt).\nBased on main.cpp and simple.cpp I only need to include  \"common.h\" and  \"llama.h\". However, there are many more folders in the project whose headers are not mentioned in these examples.\nIf I wish to use llama.cpp as a dependency, are these two headers enough for my project? If I wish to ship a .dll file, how do I handle the cuda files if I did not mention the ggml-cuda.h header within my xmake.lua/CMakeLists.txt file?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7631",
        "createdAt": "2024-05-30T05:23:56Z",
        "author": {
            "login": "psych0v0yager"
        }
    },
    {
        "title": "How to ensure unadulterated topk=1 sampling?",
        "bodyText": "I'm interested in only the model's outputs without any randomness, repetition penalties or other alterations.  With pytorch I'd call model() and then topk().  With llama.cpp it seems I have to use this \"ubersampler\" with parameters and behaviors encompassing a variety of sampling strategies.  To avoid most of this, one approach is to change the sampler order:\n./main -m \"/home/axyo/dev/LLM/models/Meta-Llama-3-8B-GGUF-v2/Meta-Llama-3-8B.Q4_0.gguf\" -n 10 -f ./prompts/test.txt -c 4096 --samplers \"top_k\" --top_k 1 --repeat-last-n 0 --no-penalize-nl --repeat-penalty 1.0\n\nBy taking the nucleus sampling and everything else out of the picture to the extent possible, at this point it seems assured that only repetition penalties remain.  After looking around, I hope I've found the combination of parameters that disables all of the repetition penalties:  repeat-last-n=0 and no-penalize-nl\nBut let's say I'm using llama-cpp-python, where you must use the \"ubersampler.\"\noutput = llm(\n    prompt,\n    echo=False,\n    max_tokens=10,\n    stop=[\"\\n\", \"<|eot_id|>\"],\n    top_k=1,\n    repeat_penalty=1.0,\n    #penalize_nl=False, # not exposed \ud83d\ude41\n)\n\nrepeat-last-n isn't available, so instead, I have to set repeat_penalty to 1.0 which means \"disabled.\"  By the way, what the heck is the meaning of <1 rep penalty?  0.0 seems to force repetition?  Anyway \ud83d\ude05\npenalize_nl isn't exposed here, but that can be addressed.\nWhat about min_p and typical_p?  Do I have to worry about those or anything else to ensure the outputs aren't being altered?  I'm not familiar with the rest of the sampling strategies",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7590",
        "createdAt": "2024-05-28T13:33:37Z",
        "author": {
            "login": "brandon-lockaby"
        }
    },
    {
        "title": "Add support for loading model from in-memory buffer",
        "bodyText": "Current LlamaCpp comes with support of loading model file from absolute path.\nstruct llama_model * llama_load_model_from_file(\n        const char * path_model,\n        struct llama_model_params   params)\n\nHowever, don't you have any plan to add API to load llama model from buffer pointer already reside in memory? With such support, we may use pointer for sake of memory mapped i/o or something else. It would cover more cases of dealing with more scenarios.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7560",
        "createdAt": "2024-05-27T10:04:28Z",
        "author": {
            "login": "jiwonjung42dotai"
        }
    },
    {
        "title": "Performance of llama.cpp on Apple Silicon M-series",
        "bodyText": "Summary\nLLaMA 7B\n\n\n\n\u00a0\nBW  [GB/s]\nGPU  Cores\nF16 PP  [t/s]\nF16 TG  [t/s]\nQ8_0 PP  [t/s]\nQ8_0 TG  [t/s]\nQ4_0 PP  [t/s]\nQ4_0 TG  [t/s]\n\n\n\n\n\u2705 M1 1\n68\n7\n\n\n108.21\n7.92\n107.81\n14.19\n\n\n\u2705 M1 1\n68\n8\n\u00a0\n\u00a0\n117.25\n7.91\n117.96\n14.15\n\n\n\u2705 M1 Pro 1\n200\n14\n262.65\n12.75\n235.16\n21.95\n232.55\n35.52\n\n\n\u2705 M1 Pro 1\n200\n16\n302.14\n12.75\n270.37\n22.34\n266.25\n36.41\n\n\n\u2705 M1 Max 1\n400\n24\n453.03\n22.55\n405.87\n37.81\n400.26\n54.61\n\n\n\u2705 M1 Max 1\n400\n32\n599.53\n23.03\n537.37\n40.2\n530.06\n61.19\n\n\n\u2705 M1 Ultra 1\n800\n48\n875.81\n33.92\n783.45\n55.69\n772.24\n74.93\n\n\n\u2705 M1 Ultra 1\n800\n64\n1168.89\n37.01\n1042.95\n59.87\n1030.04\n83.73\n\n\n\u2705 M2 2\n100\n8\n\n\n147.27\n12.18\n145.91\n21.7\n\n\n\u2705 M2 2\n100\n10\n201.34\n6.72\n181.4\n12.21\n179.57\n21.91\n\n\n\u2705 M2 Pro 2\n200\n16\n312.65\n12.47\n288.46\n22.7\n294.24\n37.87\n\n\n\u2705 M2 Pro 2\n200\n19\n384.38\n13.06\n344.5\n23.01\n341.19\n38.86\n\n\n\u2705 M2 Max 2\n400\n30\n600.46\n24.16\n540.15\n39.97\n537.6\n60.99\n\n\n\u2705 M2 Max 2\n400\n38\n755.67\n24.65\n677.91\n41.83\n671.31\n65.95\n\n\n\u2705 M2 Ultra 2\n800\n60\n1128.59\n39.86\n1003.16\n62.14\n1013.81\n88.64\n\n\n\u2705 M2 Ultra 2\n800\n76\n1401.85\n41.02\n1248.59\n66.64\n1238.48\n94.27\n\n\n\ud83d\udfe5 M3 3\n100\n8\n\n\n\n\n\n\n\n\n\ud83d\udfe8 M3 3\n100\n10\n\n\n187.52\n12.27\n186.75\n21.34\n\n\n\ud83d\udfe8 M3 Pro 3\n150\n14\n\u00a0\n\u00a0\n272.11\n17.44\n269.49\n30.65\n\n\n\u2705 M3 Pro 3\n150\n18\n357.45\n9.89\n344.66\n17.53\n341.67\n30.74\n\n\n\u2705 M3 Max 3\n300\n30\n589.41\n19.54\n566.4\n34.3\n567.59\n56.58\n\n\n\u2705 M3 Max 3\n400\n40\n779.17\n25.09\n757.64\n42.75\n759.7\n66.31\n\n\n\ud83d\udfe5 M3 Ultra\n800\n60\n\n\n\n\n\n\n\n\n\ud83d\udfe5 M3 Ultra\n800\n80\n\n\n\n\n\n\n\n\n\n\n\nplot.py\n# GPT-4 Generated Code\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating DataFrame from the provided data\ndata = {\n    \"Chip\": [\"M1\", \"M1\", \"M1 Pro\", \"M1 Pro\", \"M1 Max\", \"M1 Max\", \"M1 Ultra\", \"M2\", \"M2 Pro\", \"M2 Pro\", \"M2 Max\", \"M2 Max\", \"M2 Ultra\", \"M2 Ultra\", \"M3\", \"M3 Pro\", \"M3 Pro\", \"M3 Max\"],\n    \"BW (GB/s)\":     [68, 68, 200, 200, 400, 400, 800, 100, 200, 200, 400, 400, 800, 800, 100, 150, 150, 400],\n    \"GPU Cores\":     [7, 8, 14, 16, 24, 32, 48, 10, 16, 19, 30, 38, 60, 76, 10, 14, 18, 40],\n    \"F16 PP (t/s)\":  [None, None, None, 302.14, 453.03, 599.53, 875.81, 201.34, 312.65, 384.38, 600.46, 755.67, 1128.59, 1401.85, None, None, 357.45, 779.17],\n    \"F16 TG (t/s)\":  [None, None, None, 12.75, 22.55, 23.03, 33.92, 6.72, 12.47, 13.06, 24.16, 24.65, 39.86, 41.02, None, None, 9.89, 25.09],\n    \"Q8_0 PP (t/s)\": [108.21, 117.25, 235.16, 270.37, 405.87, 537.37, 783.45, 181.4, 288.46, 344.5, 540.15, 677.91, 1003.16, 1248.59, 187.52, 272.11, 344.66, 757.64],\n    \"Q8_0 TG (t/s)\": [7.92, 7.91, 21.95, 22.34, 37.81, 40.2, 55.69, 12.21, 22.7, 23.01, 39.97, 41.83, 62.14, 66.64, 12.27, 17.44, 17.53, 42.75],\n    \"Q4_0 PP (t/s)\": [107.81, 117.96, 232.55, 266.25, 400.26, 530.06, 772.24, 179.57, 294.24, 341.19, 537.6, 671.31, 1013.81, 1238.48, 186.75, 269.49, 341.67, 759.7],\n    \"Q4_0 TG (t/s)\": [14.19, 14.15, 35.52, 36.41, 54.61, 61.19, 74.93, 21.91, 37.87, 38.86, 60.99, 65.95, 88.64, 94.27, 21.34, 30.65, 30.74, 66.31]\n}\ndf = pd.DataFrame(data)\n\n# Helper function to plot and annotate multiple data series in the same plot\ndef plot_multi_series(ax, x, y_series, labels, xlabel, ylabel, title, poly_power=1):\n    colors = ['r', 'g', 'b']  # Colors for different series\n    for i, y in enumerate(y_series):\n        # Sorting data for regression\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        y_sorted = y[sorted_indices]\n\n        # Masking NaN values\n        mask = ~np.isnan(y_sorted)\n        x_sorted = x_sorted[mask]\n        y_sorted = y_sorted[mask]\n\n        # Fitting a polynomial regression model\n        coefficients = np.polyfit(x_sorted, y_sorted, poly_power)\n        polynomial = np.poly1d(coefficients)\n\n        # Creating a range of x-values for a smoother trendline\n        x_range = np.linspace(x_sorted.min(), x_sorted.max(), 500)\n        trendline = polynomial(x_range)\n\n        # Plotting\n        ax.scatter(x, y, color=colors[i], label=labels[i], s=20)\n        ax.plot(x_range, trendline, f\"{colors[i]}-\", linewidth=1)  # Trendline in the same color\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n    ax.legend()\n\n    # Annotating points with the number of GPU cores and Bandwidth\n    for i, txt in enumerate(df[\"Chip\"]):\n        ax.annotate(f\"{df['GPU Cores'][i]} Cores, {df['BW (GB/s)'][i]} GB/s\", (x[i], y_series[0][i]))\n\n\n# Creating plots for PP vs Cores and TG vs Bandwidth\nfig, axs = plt.subplots(1, 2, figsize=(15, 6))\nfig.suptitle('PP vs GPU Cores and TG vs Bandwidth for F16, Q8_0, and Q4_0')\n\n# PP vs GPU Cores\ny_series_cores_pp = [df[\"F16 PP (t/s)\"], df[\"Q8_0 PP (t/s)\"], df[\"Q4_0 PP (t/s)\"]]\nplot_multi_series(axs[0], df[\"GPU Cores\"], y_series_cores_pp,\n                  ['F16 PP', 'Q8_0 PP', 'Q4_0 PP'], 'GPU Cores', 'Performance (t/s)',\n                  'PP Performance vs GPU Cores', 1)\n\n# TG vs Bandwidth\ny_series_bw_tg = [df[\"F16 TG (t/s)\"], df[\"Q8_0 TG (t/s)\"], df[\"Q4_0 TG (t/s)\"]]\nplot_multi_series(axs[1], df[\"BW (GB/s)\"], y_series_bw_tg,\n                  ['F16 TG', 'Q8_0 TG', 'Q4_0 TG'], 'Bandwidth (GB/s)', 'Performance (t/s)',\n                  'TG Performance vs Bandwidth', 2)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\nDescription\nThis is a collection of short llama.cpp benchmarks on various Apple Silicon hardware. It can be useful to compare the performance that llama.cpp achieves across the M-series chips and hopefully answer questions of people wondering if they should upgrade or not. Collecting info here just for Apple Silicon for simplicity. Similar collection for A-series chips is available here: #4508\nIf you are a collaborator to the project and have an Apple Silicon device, please add your device, results and optionally username for the following command directly into this post (requires LLaMA 7B v2):\ngit checkout 8e672efe\nmake clean && make -j llama-bench && ./llama-bench \\\n  -m ./models/llama-7b-v2/ggml-model-f16.gguf  \\\n  -m ./models/llama-7b-v2/ggml-model-q8_0.gguf \\\n  -m ./models/llama-7b-v2/ggml-model-q4_0.gguf \\\n  -p 512 -n 128 -ngl 99 2> /dev/null\n\nMake sure to run the benchmark on commit 8e672ef\nPlease also include the F16 model as shown, not just the quantum models\nContributors can post the same results in the comments below\nIf a device is already benchmarked and your results are comparable, there is no need to add it again\nPP means \"prompt processing\" (bs = 512), TG means \"text-generation\" (bs = 1), t/s means \"tokens per second\"\n\u2705 means the data has been added to the summary\n\n\nM1 Pro, 8+2 CPU, 16 GPU (@ggerganov) \u2705\n\n\n\nmodel\nsize\nparams\nbackend\nngl\ntest\nt/s\n\n\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n99\npp 512\n302.14 \u00b1 0.07\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n99\ntg 128\n12.75 \u00b1 0.00\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n99\npp 512\n270.37 \u00b1 0.02\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n99\ntg 128\n22.34 \u00b1 0.00\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n99\npp 512\n266.25 \u00b1 0.07\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n99\ntg 128\n36.41 \u00b1 0.01\n\n\n\nbuild: 8e672ef (1550)\nM2 Ultra, 16+8 CPU, 76 GPU (@ggerganov) \u2705\n\n\n\nmodel\nsize\nparams\nbackend\nngl\ntest\nt/s\n\n\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n99\npp 512\n1401.85 \u00b1 1.75\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n99\ntg 128\n41.02 \u00b1 0.02\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n99\npp 512\n1248.59 \u00b1 0.73\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n99\ntg 128\n66.64 \u00b1 0.02\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n99\npp 512\n1238.48 \u00b1 0.76\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n99\ntg 128\n94.27 \u00b1 0.05\n\n\n\nbuild: 8e672ef (1550)\nM3 Max (MBP 14), 12+4 CPU, 40 GPU (@slaren) \u2705\n\n\n\nmodel\nsize\nparams\nbackend\nngl\ntest\nt/s\n\n\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n99\npp 512\n794.26 \u00b1 3.16\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n99\ntg 128\n25.27 \u00b1 0.07\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n99\npp 512\n749.37 \u00b1 8.35\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n99\ntg 128\n43.00 \u00b1 0.12\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n99\npp 512\n690.99 \u00b1 33.76\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n99\ntg 128\n65.85 \u00b1 0.22\n\n\n\nbuild: d103d93 (1553)\nFootnotes\n\n\nhttps://en.wikipedia.org/wiki/Apple_M1#Variants \u21a9 \u21a92 \u21a93 \u21a94 \u21a95 \u21a96 \u21a97 \u21a98\n\n\nhttps://en.wikipedia.org/wiki/Apple_M2#Variants \u21a9 \u21a92 \u21a93 \u21a94 \u21a95 \u21a96 \u21a97 \u21a98\n\n\nhttps://en.wikipedia.org/wiki/Apple_M3#Variants \u21a9 \u21a92 \u21a93 \u21a94 \u21a95 \u21a96",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4167",
        "createdAt": "2023-11-22T09:46:54Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Tutorial: How to convert HuggingFace model to GGUF format",
        "bodyText": "Source: https://www.substratus.ai/blog/converting-hf-model-gguf-model/\nI published this on our blog but though others here might benefit as well, so sharing the raw blog here on Github too. Hope it's helpful to folks here and feedback is welcome.\nDownloading a HuggingFace model\nThere are various ways to download models, but in my experience the huggingface_hub\nlibrary has been the most reliable. The git clone method occasionally results in\nOOM errors for large models.\nInstall the huggingface_hub library:\npip install huggingface_hub\nCreate a Python script named download.py with the following content:\nfrom huggingface_hub import snapshot_download\nmodel_id=\"lmsys/vicuna-13b-v1.5\"\nsnapshot_download(repo_id=model_id, local_dir=\"vicuna-hf\",\n                  local_dir_use_symlinks=False, revision=\"main\")\nRun the Python script:\npython download.py\nYou should now have the model downloaded to a directory called\nvicuna-hf. Verify by running:\nls -lash vicuna-hf\nConverting the model\nNow it's time to convert the downloaded HuggingFace model to a GGUF model.\nLlama.cpp comes with a converter script to do this.\nGet the script by cloning the llama.cpp repo:\ngit clone https://github.com/ggerganov/llama.cpp.git\nInstall the required python libraries:\npip install -r llama.cpp/requirements.txt\nVerify the script is there and understand the various options:\npython llama.cpp/convert.py -h\nConvert the HF model to GGUF model:\npython llama.cpp/convert.py vicuna-hf \\\n  --outfile vicuna-13b-v1.5.gguf \\\n  --outtype q8_0\nIn this case we're also quantizing the model to 8 bit by setting\n--outtype q8_0. Quantizing helps improve inference speed, but it can\nnegatively impact quality.\nYou can use --outtype f16 (16 bit) or --outtype f32 (32 bit) to preserve original\nquality.\nVerify the GGUF model was created:\nls -lash vicuna-13b-v1.5.gguf\nPushing the GGUF model to HuggingFace\nYou can optionally push back the GGUF model to HuggingFace.\nCreate a Python script with the filename upload.py that\nhas the following content:\nfrom huggingface_hub import HfApi\napi = HfApi()\n\nmodel_id = \"substratusai/vicuna-13b-v1.5-gguf\"\napi.create_repo(model_id, exist_ok=True, repo_type=\"model\")\napi.upload_file(\n    path_or_fileobj=\"vicuna-13b-v1.5.gguf\",\n    path_in_repo=\"vicuna-13b-v1.5.gguf\",\n    repo_id=model_id,\n)\nGet a HuggingFace Token that has write permission from here:\nhttps://huggingface.co/settings/tokens\nSet your HuggingFace token:\nexport HUGGING_FACE_HUB_TOKEN=<paste-your-own-token>\nRun the upload.py script:\npython upload.py",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2948",
        "createdAt": "2023-09-01T02:02:05Z",
        "author": {
            "login": "samos123"
        }
    },
    {
        "title": "Add a --bug-report mode",
        "bodyText": "This would be useful for people submitting bugs\nI noticed that the act runner team has this mode that looks like below, which output the general environment and settings. I think there might be some use for us here with this. It's kind of like --version but on steroid.\n\n$ act --bug-report\nact version:            0.2.62\nGOOS:                   linux\nGOARCH:                 amd64\nNumCPU:                 8\nDocker host:            DOCKER_HOST environment variable is not set\nSockets found:\n        /var/run/docker.sock\nConfig files:           \n        /home/mofosyne/.actrc:\n                -P ubuntu-latest=catthehacker/ubuntu:act-latest\n                -P ubuntu-22.04=catthehacker/ubuntu:act-22.04\n                -P ubuntu-20.04=catthehacker/ubuntu:act-20.04\n                -P ubuntu-18.04=catthehacker/ubuntu:act-18.04\nBuild info:\n        Go version:            go1.22.2\n        Module path:           command-line-arguments\n        Main version:          \n        Main path:             \n        Main checksum:         \n        Build settings:\n                -buildmode:           exe\n                -compiler:            gc\n                -ldflags:             -X main.version=0.2.62\n                DefaultGODEBUG:       httplaxcontentlength=1,httpmuxgo121=1,tls10server=1,tlsrsakex=1,tlsunsafeekm=1\n                CGO_ENABLED:          1\n                CGO_CFLAGS:           \n                CGO_CPPFLAGS:         \n                CGO_CXXFLAGS:         \n                CGO_LDFLAGS:          \n                GOARCH:               amd64\n                GOOS:                 linux\n                GOAMD64:              v1\nDocker Engine:\n        Engine version:        24.0.5\n        Engine runtime:        runc\n        Cgroup version:        2\n        Cgroup driver:         systemd\n        Storage driver:        overlay2\n        Registry URI:          https://index.docker.io/v1/\n        OS:                    Linux Mint 21.3\n        OS type:               linux\n        OS version:            21.3\n        OS arch:               x86_64\n        OS kernel:             5.15.0-101-generic\n        OS CPU:                8\n        OS memory:             31929 MB\n        Security options:\n                name=apparmor\n                name=seccomp,profile=builtin\n                name=cgroupns",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7615",
        "createdAt": "2024-05-29T10:14:18Z",
        "author": {
            "login": "mofosyne"
        }
    },
    {
        "title": "About imatrix overfitting, and importance of input text",
        "bodyText": "Imatrix has been here for a while and I haven't seen many guidelines (or testing at all) on how to use it. Common objections/concerns are overfitting, and generating the imatrix on the \"wrong\" kind of text. See #5006\n\nIs imatrix overfitting an actual thing?\nHow much does input text make a difference? Can it make things worse?\n\nTo try and gather some data, I tried three datasets for training/testing and three different number of chunks (10K = 20 chunks of 512 tokens, 100K = 200 chunks, 1M = 2000 chunks) used for calculating the imatrix.\n\nfrwiki is part of a raw XML dump of the french Wikipedia. It contains a mix of structured XML data, french text, and wikicode markup.\nmbotf is concatenated text of my Malazan Book of the Fallen books. It's english fiction.\nwiki is the wikitext we all know and seem to use. It contains factual english text.\n\nI used Mistral-7B to calculate KL-divergence median for Q2_K quants generated with all nine possible imatrixes on all three test datasets, and no imatrix as a baseline.\nLooking forward to your opinions on the results, or about the methodology. For now, I'll keep using wikitext with 100K tokens. Might not always be optimal depending on the model's use case, but it seems unlikely to make things worse.\n\nRaw data: imatrix-tests.zip",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5263",
        "createdAt": "2024-02-01T17:18:33Z",
        "author": {
            "login": "Artefact2"
        }
    },
    {
        "title": "Why do GPU and CPU embedding outputs differ for the same input? Is normal?",
        "bodyText": "I am using the embedding example, the execution parameters are as follows\nembedding.exe -ngl 200000 -m I:\\JYGAIBIN\\MetaLlamaModel\\Llama2-13b-chat\\ggml-model-f32_q4_1.gguf --log-disable -p \"Hello World!\"\nThe first three embedding values \u200b\u200bare output when the CPU executes the embedding\n-4.67528416e-08\n-1.07059577e-06\n1.76811977e-06\nThe first three embedding values \u200b\u200bare output when the GPU (-ngl 200000) executes the embedding\n5.86615059e-08\n-1.02221782e-06\n1.78800110e-06\nWhy are the same \"Hello World!\" inputs different? Does llama.cpp currently correctly support GPU and CPU embedding?\nAlso, does llama.cpp have specific instructions for underlying API functions, or usage precautions? In addition to those on github, is there any interface documentation website? Thank you\nPossible Answer\nI think for the same input content, the GPU and CPU output embedding values \u200b\u200bshould be the same\nWhich of these two results is correct? Do GPUs currently support computational embedding and fine-tuning?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7626",
        "createdAt": "2024-05-29T18:12:28Z",
        "author": {
            "login": "jygmysoul"
        }
    },
    {
        "title": "So what does KV CACHE look like?",
        "bodyText": "My understanding is that each generated TOKEN has a corresponding KV CACHE, which is maintained inside llama.cpp. It should not cache each token_id, but choose to cache the kv value. Anyway, it is a one-to-one relationship. The context size is the size of the kv cache. When the context is full, llama.cpp will discard the previously cached kv value to make room for new content. Is my understanding correct?\nLLAMA_API void llama_kv_cache_seq_add(\nstruct llama_context * ctx,\nllama_seq_id seq_id,\nllama_pos p0,\nllama_pos p1,\nllama_pos delta);\nIn addition, what does this llama_pos delta parameter mean, how to use it, and why it can be negative? It feels like this add is a bit like a move operation.\nIn addition, can you traverse the kv cache and view the specific values \u200b\u200bstored in it and the corresponding tokenid? I feel very uneasy about deleting and discarding without seeing the content.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7625",
        "createdAt": "2024-05-29T17:53:35Z",
        "author": {
            "login": "jygmysoul"
        }
    },
    {
        "title": "Can someone explain how embeddigns are working in llama.cpp?",
        "bodyText": "Hi,\nI am working with llama.cpp (Python) and the Mistral 7B Instruct model. All works fine so far.\nNow I wonder what are embeddings and how to use them?\nAs far as I understand embeddings are used to support the LLM with additional context (e.g. data fetched from an internal database).\nI also see that calling something like:\nprint( llm.create_embedding([\"What is your name?\", \"Hi my name is Anna.\"]))\nwill generate an vector with a lot of numbers.\nBut can someone explain how to use embeddings to tell my llm to use them in a inference life cycle?\nI expected some code example like\nllm = Llama( model_path=my-model_path,   embedding=True   )\nembeddings = llm.create_embedding([\"What is your name?\", \"Hi my name is Anna.\"])\nresult = llm(\"What is your name?\", max_tokens=max_tokens )\nBut it does not look like the create_embedding has any effect.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7087",
        "createdAt": "2024-05-05T09:03:59Z",
        "author": {
            "login": "rsoika"
        }
    },
    {
        "title": "Add vocabulary type for token-free models that work on raw bytes",
        "bodyText": "I think LLMs that directly work on raw bytes will become more interesting in the near future. Some interesting work in that direction:\n\nByT5: Towards a token-free future with pre-trained byte-to-byte models\nBytes Are All You Need: Transformers Operating Directly On File Bytes\nMambaByte: Token-free Selective State Space Model\n\nI think it would be useful if something like LLAMA_VOCAB_TYPE_RAW_BYTES would be added to enum llama_vocab_type but I don't know what kind of changes that would imply elsewhere. That kind of vocabulary would still require special tokens of course.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7624",
        "createdAt": "2024-05-29T16:18:24Z",
        "author": {
            "login": "uwu-420"
        }
    },
    {
        "title": "Any time soon to bring back the multimodal support for server?",
        "bodyText": "since it was removed in March.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7183",
        "createdAt": "2024-05-09T18:50:06Z",
        "author": {
            "login": "victorx98"
        }
    },
    {
        "title": "Adjust VRAM/RAM split on Apple Silicon",
        "bodyText": "// this tool allows you to change the VRAM/RAM split on Unified Memory on Apple Silicon to whatever you want, allowing for more VRAM for inference\n// c++ -std=c++17 -framework CoreFoundation -o vram_patcher vram_patcher.cpp\n// credits to @asentientbot for helping me with some stuff because I never owned a Mac before\n// please read the code, if you don't understand what it does don't use it\n// tested on macos ventura beta 3, pattern might be different on other versions\n// usage: ./vram_patcher <desired percentage of VRAM>\n#include <iostream>\n#include <fstream>\n#include <string>\n#include <vector>\n#include <sys/sysctl.h>\n#include <unistd.h>\n#include <sys/stat.h>\n#include <sys/mount.h>\n#include <CoreFoundation/CoreFoundation.h>\n\n// A helper function to get the sysctl value for a given name\nstd::string get_sysctl(const std::string& name) {\n    size_t len = 0;\n    // Get the size of the value\n    if (sysctlbyname(name.c_str(), nullptr, &len, nullptr, 0) == -1) {\n        std::cerr << \"Failed to get sysctl size for \" << name << std::endl;\n        return \"\";\n    }\n    // Allocate a buffer for the value\n    char* buf = new char[len];\n    // Get the value\n    if (sysctlbyname(name.c_str(), buf, &len, nullptr, 0) == -1) {\n        std::cerr << \"Failed to get sysctl value for \" << name << std::endl;\n        delete[] buf;\n        return \"\";\n    }\n    // Convert the value to a string\n    std::string value(buf, len);\n    delete[] buf;\n    return value;\n}\n\n// A helper function to convert a CFString to a std::string\nstd::string CFStringToStdString(CFStringRef cfstr) {\n  if (cfstr == nullptr) return \"\";\n  CFIndex length = CFStringGetLength(cfstr);\n  CFIndex maxSize = CFStringGetMaximumSizeForEncoding(length, kCFStringEncodingUTF8) + 1;\n  char* buffer = new char[maxSize];\n  if (CFStringGetCString(cfstr, buffer, maxSize, kCFStringEncodingUTF8)) {\n    std::string result(buffer);\n    delete[] buffer;\n    return result;\n  } else {\n    delete[] buffer;\n    return \"\";\n  }\n}\n\n// A function to get the name of a disk from a path\nstd::string get_volume_name(const std::string& path) {\n  // Create a CFURL from the path\n  CFURLRef url = CFURLCreateFromFileSystemRepresentation(nullptr, (const UInt8*)path.c_str(), path.length(), false);\n  if (url == nullptr) return \"\";\n\n  // Get the volume URL from the path URL\n  CFURLRef volumeURL = CFURLCreateCopyDeletingLastPathComponent(nullptr, url);\n  CFRelease(url);\n  if (volumeURL == nullptr) return \"\";\n\n  // Get the volume name from the volume URL\n  CFStringRef volumeName = nullptr;\n  if (CFURLCopyResourcePropertyForKey(volumeURL, kCFURLVolumeNameKey, &volumeName, nullptr)) {\n    CFRelease(volumeURL);\n    if (volumeName == nullptr) return \"\";\n    // Convert the volume name to a std::string\n    std::string result = CFStringToStdString(volumeName);\n    CFRelease(volumeName);\n    return result;\n  } else {\n    CFRelease(volumeURL);\n    return \"\";\n  }\n}\n\nvoid change_float_constant(std::vector<uint8_t>& code, float new_value) {\n  // Check that the code vector has enough bytes for the two instructions\n  if (code.size() < 8) {\n    throw std::invalid_argument(\"code vector is too small\");\n  }\n\n  // Convert the new float value to a 32-bit unsigned integer representation\n  uint32_t new_bits = *reinterpret_cast<uint32_t*>(&new_value);\n\n  // Extract the lower and upper 16 bits of the new value\n  uint16_t low_bits = new_bits & 0xffff;\n  uint16_t high_bits = new_bits >> 16;\n\n  // Encode the new value as two mov instructions\n  // The first instruction is movz w8, #low_bits\n  // The second instruction is movk w8, #high_bits, lsl #16\n  // The opcode format is:\n  // | 31 30 29 28 | 27 26 25 24 | 23 22 21 20 | 19 18 17 16 | 15 14 13 12 | 11 10 9 8 | 7 6 5 4 | 3 2 1 0 |\n  // | sf  0  0  1 | opc  1  0  0 | 0  0  0  0 |    hw    |     imm16     | 0  0  0  0 |   Rd   |   0  0  0  0 |\n  // where sf = 0 for 32-bit register, opc = 00 for movz, 01 for movn, 10 for movk, hw = 00 for lsl #0, 01 for lsl #16, 10 for lsl #32, 11 for lsl #48, imm16 = 16-bit immediate value, Rd = destination register\n\n  // The first instruction has sf = 0, opc = 00, hw = 00, imm16 = low_bits, Rd = 8\n  uint32_t first_opcode = 0x52800000 | (low_bits << 5) | 8;\n\n  // The second instruction has sf = 0, opc = 10, hw = 01, imm16 = high_bits, Rd = 8\n  uint32_t second_opcode = 0x72800000 | (high_bits << 5) | (1 << 21) | 8;\n\n  // Convert the opcodes to little endian bytes and overwrite the code vector\n  code[0] = first_opcode & 0xff;\n  code[1] = (first_opcode >> 8) & 0xff;\n  code[2] = (first_opcode >> 16) & 0xff;\n  code[3] = (first_opcode >> 24) & 0xff;\n  code[4] = second_opcode & 0xff;\n  code[5] = (second_opcode >> 8) & 0xff;\n  code[6] = (second_opcode >> 16) & 0xff;\n  code[7] = (second_opcode >> 24) & 0xff;\n}\n\nint main(int argc, char** argv) {\n    // Check if the program is run as root\n    if (getuid() != 0) {\n        std::cerr << \"Sorry, this program must be run as root\" << std::endl;\n        return 1;\n    }\n\n    // Check if the program has one argument\n    if (argc != 2) {\n        std::cerr << \"Usage: \" << argv[0] << \" vram_percentage\" << std::endl;\n        return 1;\n    }\n\n    // Check if the argument is a valid percentage\n    int percentage = std::stoi(argv[1]);\n    if (percentage < 10 || percentage > 95) {\n        std::cerr << \"Invalid percentage: \" << percentage << std::endl;\n        return 1;\n    }\n\n    // Check if the CPU is Apple Silicon\n    std::string cpu_brand = get_sysctl(\"machdep.cpu.brand_string\");\n    if (cpu_brand.find(\"Apple\") == std::string::npos) {\n        std::cerr << \"Sorry, this program only works on Apple Silicon\" << std::endl;\n        return 1;\n    }\n\n    // Define the paths for the original and patched kernel collections\n    std::string original_kc = \"/private/var/db/KernelExtensionManagement/KernelCollections/BootKernelCollection.kc\";\n    std::string patched_kc = \"/tmp/BootKernelCollection.kc\";\n    std::string final_kc = \"/Library/KernelCollections/vram_patch.kc\";\n    std::string script_kc = \"/Library/KernelCollections/complete_patch.sh\";\n\n    // Copy the original kernel collection to a temporary directory\n    std::ifstream src(original_kc, std::ios::binary);\n    std::ofstream dst(patched_kc, std::ios::binary);\n    if (!src || !dst) {\n        std::cerr << \"Failed to copy the kernel collection\" << std::endl;\n        return 1;\n    }\n    dst << src.rdbuf();\n    src.close();\n    dst.close();\n\n    // Read the patched kernel collection into a vector of bytes\n    std::ifstream in(patched_kc, std::ios::binary);\n    if (!in) {\n        std::cerr << \"Failed to read the patched kernel collection\" << std::endl;\n        return 1;\n    }\n    std::vector<uint8_t> data((std::istreambuf_iterator<char>(in)), std::istreambuf_iterator<char>());\n    in.close();\n\n    // Define the byte sequence to search for\n    std::vector<uint8_t> target = {0x08, 0x01, 0xC0, 0xD2, 0x3F, 0x00, 0x08, 0xEB, 0xA8, 0xAA, 0x8A, 0x52, 0xA8, 0x40, 0xA8, 0x72, 0x00, 0x01, 0x27, 0x1E, 0x01, 0x30, 0x27, 0x1E, 0x28, 0x8C, 0x20, 0x1E};\n\n    // Define the byte sequence to replace with\n    std::vector<uint8_t> replacement = {0x68, 0x73, 0x89, 0x52, 0xA8, 0x94, 0xA8, 0x72, 0x08, 0x01, 0x27, 0x1E, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5};\n\n    // Subtract the percentage from 100\n    percentage = 100 - percentage;\n\n    // Change the floating point constant in the replacement byte sequence\n    change_float_constant(replacement, percentage);\n\n    // Find the first occurrence of the target byte sequence in the data vector\n    auto it = std::search(data.begin(), data.end(), target.begin(), target.end());\n\n    // If the target byte sequence is found, replace it with the replacement byte sequence\n    if (it != data.end()) {\n        std::copy(replacement.begin(), replacement.end(), it);\n    } else {\n        std::cerr << \"Failed to find the target byte sequence in the kernel collection\" << std::endl;\n        return 1;\n    }\n\n    // Write the modified data vector to the patched kernel collection\n    std::ofstream out(patched_kc, std::ios::binary);\n    if (!out) {\n        std::cerr << \"Failed to write the patched kernel collection\" << std::endl;\n        return 1;\n    }\n    out.write(reinterpret_cast<char*>(data.data()), data.size());\n    out.close();\n\n    // Copy the patched kernel collection to the final destination\n    std::ifstream src2(patched_kc, std::ios::binary);\n    std::ofstream dst2(final_kc, std::ios::binary);\n    if (!src2 || !dst2) {\n        std::cerr << \"Failed to copy the patched kernel collection; please disable SIP and try again\" << std::endl;\n        return 1;\n    }\n    dst2 << src2.rdbuf();\n    src2.close();\n    dst2.close();\n\n    // Change the ownership of the final kernel collection to root:wheel\n    if (chown(final_kc.c_str(), 0, 0) == -1) {\n        std::cerr << \"Failed to change the ownership of the final kernel collection\" << std::endl;\n        return 1;\n    }    \n\n    // Get the volume name for the /System path\n    std::string volume_name = get_volume_name(\"/System\");\n    if (volume_name.empty()) {\n        std::cerr << \"Failed to get the volume name for /System\" << std::endl;\n        return 1;\n    }\n\n    // Create a shell script to configure the boot with the final kernel collection\n    std::ofstream script(script_kc);\n    if (!script) {\n        std::cerr << \"Failed to create the shell script\" << std::endl;\n        return 1;\n    }\n    script << \"#!/bin/bash\\n\";\n    script << \"# Disable System Integrity Protection\\n\";\n    script << \"csrutil disable\\n\";\n    script << \"# Disable Apple Mobile File Integrity\\n\";\n    script << \"nvram boot-args=\\\"ipc_control_port_options=0 amfi_get_out_of_my_way=1\\\"\\n\";\n    script << \"# Get the absolute path of this script\\n\";\n    script << \"SCRIPT=$(cd \\\"$(dirname \\\"${BASH_SOURCE[0]}\\\")\\\" && pwd)/$(basename \\\"${BASH_SOURCE[0]}\\\")\\n\";\n    script << \"# Append the kernel collection name to the script path\\n\";\n    script << \"KC=\\\"${SCRIPT%/*}/vram_patch.kc\\\"\\n\"; // This line replaces the original one\n    script << \"# Get the volume name from the script path\\n\";\n    script << \"VOLUME=\\\"${SCRIPT%%/Library*}\\\"\\n\";\n    script << \"# Run the kmutil command with the absolute paths\\n\";\n    script << \"kmutil configure-boot -C -c \\\"$KC\\\" -v \\\"$VOLUME\\\"\\n\";\n    script << \"sync\\n\";\n    script << \"echo Finished. You may reboot your system now.\\n\";\n    script.close();\n\n    // Change the ownership and permissions of the shell script to root:wheel and 755\n    if (chown(script_kc.c_str(), 0, 0) == -1) {\n        std::cerr << \"Failed to change the ownership of the shell script\" << std::endl;\n        return 1;\n    }\n    if (chmod(script_kc.c_str(), 0755) == -1) {\n        std::cerr << \"Failed to change the permissions of the shell script\" << std::endl;\n        return 1;\n    }\n\n    // Output the instructions to run the shell script in RecoveryOS\n    std::cout << \"The shell script to configure the boot with the patched kernel collection has been created at \" << script_kc << std::endl;\n    std::cout << \"Now reboot to RecoveryOS and run: \\\"/Volumes/\" << volume_name << \"/Library/KernelCollections/complete_patch.sh\\\"\" << std::endl;\n    std::cout << \"If your boot fails after running the script or after an update, you need to go to Utilities->Startup Security Tool in RecoveryOS and pick Full Security\" << std::endl;\n\n    return 0;\n}",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2182",
        "createdAt": "2023-07-11T18:59:06Z",
        "author": {
            "login": "r3muxd"
        }
    },
    {
        "title": "Making talk-llama feel real-time",
        "bodyText": "Currently, the talk-llama demo waits for the model's output to finish generating before displaying it and sending it to the TTS.\nWould it be useful to have the TTS run on a separate thread, and queue up partial responses as they're generated? E.g. for a response that goes:\n\nThe meaning of life is subjective and varies from person to person. Some believe that it involves finding happiness through personal growth and fulfillment while others view it as simply existing until death. Ultimately, the answer lies within oneself.\n\nWe could have the first line, or even fragment \"The meaning of life is subjective\" being read aloud, and during this time push the rest of the response to the queue. At the same time, we display the response as a stream instead of wait for it to finish.\nI've done a quick and loose implementation of this and voice chatting with the bot seems more prompt and fun.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/803",
        "createdAt": "2023-04-06T07:12:41Z",
        "author": {
            "login": "AuroMun"
        }
    },
    {
        "title": "The new Yi-VL-6B and 34B multimodals ( inferenced on llama.cpp, results here )",
        "bodyText": "Well, their benchmarks claim they are almost at GPT4V level, beating everything else by a mile.\nThey also claim that CovVLM is one of the worst (and it's actually the best next to GPT4, by far)\nOn the other hand there are a few improvements in Yi-VL:\n\nImages run at 448\u00b2 pixels, compared to 336\nThe image encoder is openclip-HUGE instead of laion-LARGE\nThe projector contains layer normalization steps\nquite a lot of training\n\nI've tested Yi-6B and 34B.\nPR Update: #5093\nUpdate:\nGGUF models for both: https://huggingface.co/cmp-nct\nWhen used on \"normal photos\" Yi-VL-34B produces quite good results but I've had it break the finetune and ask questions as \"Human\".\nI can't rule out that there are implementation issues remaining - in the PR thread I've posted another sample response with two cats.\nOverall Yi-VL responds well to strong quantization, even at ~3bpw LLM quant I noticed no real degradation in quality, also running the visual tower quantized did not result in lower quality.\nThat's similar as with other llava models\nThe famous driver license OCR test follows:\nPS Q:\\llama.cpp\\build> .\\bin\\Debug\\llava-cli.exe -m Q:\\models\\llava\\Yi-VL-6B\\ggml-model-f16.gguf --mmproj Q:\\models\\llava\\Yi-VL-6B\\vit\\mmproj-model-f16.gguf --image C:\\temp\\license_demo.jpg -p \"This is a chat between an inquisitive human and an AI assistant. Assume the role of the AI assistant. Read all the images carefully, and respond to the human's questions with informative, helpful, detailed and polite answers. \u8fd9\u662f\u4e00\u4e2a\u597d\u5947\u7684\u4eba\u7c7b\u548c\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002\u5047\u8bbe\u4f60\u626e\u6f14\u8fd9\u4e2aAI\u52a9\u624b\u7684\u89d2 \u8272\u3002\u4ed4\u7ec6\u9605\u8bfb\u6240\u6709\u7684\u56fe\u50cf\uff0c\u5e76\u5bf9\u4eba\u7c7b\u7684\u95ee\u9898\u505a\u51fa\u4fe1\u606f\u4e30\u5bcc\u3001\u6709\u5e2e\u52a9\u3001\u8be6\u7ec6\u7684\u548c\u793c\u8c8c\u7684\u56de\u7b54\u3002 \\n\\n### Human: <image>\\nProvide a complete representation of what is in this image. Respond in JSON-pretty-print syntax for database insert.\\n### Assistant:\" -ngl 50 --temp 0 -n 500 -c 2048 -e\n\n{\n  \"driver_license\": \"California\",\n  \"class\": \"a\",\n  \"driver_license_number\": \"DL1234568\",\n  \"expiration_date\": \"08/31/2014\",\n  \"end_name\": \"END NONE\",\n  \"lncardholder\": \"FNMA\",\n  \"street_address\": \"2570 24TH STREET ANYTOWN CA 55918\",\n  \"city\": \"ANYTOWN\",\n  \"state\": \"CA\",\n  \"zipcode\": \"55918\",\n  \"phone\": \"08/03/1977\",\n  \"driver_license_type\": \"VETERAN\",\n  \"hair_color\": \"HAIR NONE\",\n  \"eyes_color\": \"EYES NONE\",\n  \"height\": \"125\",\n  \"weight\": \"125\",\n  \"ssn\": \"08311977\",\n  \"cardholder_name\": \"Jane Doe\",\n  \"cardholder_id\": \"DD 00000000NNANANANANF09\",\n  \"issued_by\": \"DMV\"\n}\n\nSo that's certainly not CogVLM or GPT4 level.\nIt's significantly more stupid than ShareGPT4V 7B but at the same time, it extracted a LOT out of it.\nStill plenty of errors, CogVLM aces this test with 2 tiny errors and GPT4V has one tiny error.\nI ran other tests, on images that are working quite well but not flawless on ShareGPT4V-7B and 13B.\nYi-VL-6B showed a remarkable great detail detection, better than any other llava models, but it hallucinated extremely alongside that, more than I've seen before anywhere else..\nI'll follow up with a 34B test, likely tomorrow given I've to download and quantize it first.\nI expect a lot more hallucination and more intelligence at the same time, we'll see.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5092",
        "createdAt": "2024-01-23T04:26:14Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "AMD R7 7840HS Device",
        "bodyText": "I possess a device equipped with an AMD R7 7840HS CPU and no discrete graphics card. Which build from the Release should I select to achieve the fastest inference speed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7004",
        "createdAt": "2024-04-30T09:00:28Z",
        "author": {
            "login": "IcedSoyaBeanMilk"
        }
    },
    {
        "title": "How to make each KV Cache token its own tensor and concat together when needed",
        "bodyText": "Hi,\nI'm trying to modify the kv cache so that each token is its own tensor, by adding another k_l and v_l which is a double pointer vector. That way each token could be stored on a different device. When the entire cache is needed, I concat the double pointer ggml_tensor array together by using the original k_l and v_l as copy buffers.\nThe main confusion I have is why I keep running out of memory from the memory pool, and how I should know ahead of time how much memory I need. I have just randomly made it larger until I didn't see the error mesage anymore.\nI'm also getting an EXC_BAD_ACCESS segfault from llm_build_kv according to lldb, and not sure what could be causing this, is it because I'm making the context too large? I doubt it because shouldn't that print an error? It's probably how I take the view and copy into it.\nThe code is disgusting because I'm just trying this idea out. I'd appreciate any guidance. Here are the rough draft changes I've made if you'd like to take a peek.\nThanks!! Any comments are appreciated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7601",
        "createdAt": "2024-05-28T22:33:47Z",
        "author": {
            "login": "mjkpolo"
        }
    },
    {
        "title": "[Tutorial] How to setup llama.cpp with AWS EC2 Image Builder",
        "bodyText": "I am working on Paddler (stateful llama.cpp load balancer, and I'm making some llama.cpp tutorials along the way).\nI specifically wanted to talk about setting up llama.cpp with EC2 Image Builder because that is just one step from using llama.cpp in Auto Scaling and Load Balancers. The tutorial on that is in progress. :)\nSetting up llama.cpp on AWS:\nhttps://github.com/distantmagic/paddler/blob/main/infra/tutorial-installing-llamacpp-aws-ec2-image-builder.md\nAn example Terraform infrastructure files as a reference:\nhttps://github.com/distantmagic/paddler/tree/main/infra/terraform/aws\nLet me know what you think!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7593",
        "createdAt": "2024-05-28T17:29:36Z",
        "author": {
            "login": "mcharytoniuk"
        }
    },
    {
        "title": "Skip every other token during llava-cli?",
        "bodyText": "Hi, I am wondering if this is something that's possible to do (and if so where) on llava-cli.\nFor limited resource compute, i.e. a Raspberry Pi, it takes quite a while for the model to start generating a response, due to the fact that there are so many image tokens which must be passed into context first before the output is.\nWhile this will undoubtedly harm performance, something I am keen to try is reducing that number of image tokens that gets sent.\nTo make this something easy to experiment with I was thinking about slicing the array, and taking every Nth token, or some other variant until finding what works best.\nI'm coming from a Python background where this is something very easy to update on say PyTorch, but I am not sure where to start here.\nIt appears possible to do this, but so far I only have figured out how to do so for slicing a portion of the image embeddings, rather than taking an alternating one.\nFrom this function\n\n  \n    \n      llama.cpp/examples/llava/llava.cpp\n    \n    \n         Line 318\n      in\n      d041d2c\n    \n  \n  \n    \n\n        \n          \n           bool llava_eval_image_embed(llama_context * ctx_llama, const struct llava_image_embed * image_embed, int n_batch, int * n_past) { \n        \n    \n  \n\n\nUpdate it to:\nbool llava_eval_image_embed(llama_context * ctx_llama, const struct llava_image_embed * image_embed, int n_batch, int * n_past) {\n    int n_embd  = llama_n_embd(llama_get_model(ctx_llama));\n    int slice_n_image_pos = image_embed->n_image_pos / 2;\n    for (int i = 0; i < slice_n_image_pos; i += n_batch) {\n        int n_eval = slice_n_image_pos - i;\n        if (n_eval > n_batch) {\n            n_eval = n_batch;\n        }\n        llama_batch batch = {int32_t(n_eval), nullptr, (image_embed->embed+i*n_embd), nullptr, nullptr, nullptr, nullptr, *n_past, 1, 0, };\n        if (llama_decode(ctx_llama, batch)) {\n            LOG_TEE(\"%s : failed to eval\\n\", __func__);\n            return false;\n        }\n        *n_past += n_eval;\n    }\n    return true;\n\nand the # of image tokens processed gets dropped in half. Is there a way to easily do this for every other, or every Nth token instead?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7591",
        "createdAt": "2024-05-28T13:43:16Z",
        "author": {
            "login": "nkasmanoff"
        }
    },
    {
        "title": "Do we want a bot to suggest linting changes? (Maintainer tools)",
        "bodyText": "Will only work on linters that actually changes code, but there is this action which sound interesting https://github.com/parkerbxyz/suggest-changes\nI think this can apply to stuff like removing whitespaces, which can happen with PR.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7586",
        "createdAt": "2024-05-28T07:48:04Z",
        "author": {
            "login": "mofosyne"
        }
    },
    {
        "title": "Implementing User Authentication in llama.cpp Server",
        "bodyText": "I am working on modifying index.html in llama.cpp/examples/server/public to add a user login window. This login window will prompt users to input their username and password to access the server. I want to manage user information in a .js file, allowing new users to be added without having to rebuild the entire project.\nMethods I've Tried:\nCreating a users.js file in public and importing it in index.html\nResult: This leads to a 404 error.\nCreating a users.js file in public, importing it in index.html, adding it to the Makefile to generate users.js.hpp, then include and Get users.js.hpp in server.cpp\nResult: This requires a rebuild every time a new user is added.\nQuestion:\nDoes anyone have a better approach for achieving this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7485",
        "createdAt": "2024-05-23T05:31:15Z",
        "author": {
            "login": "SeanZhang7"
        }
    },
    {
        "title": "How to read the VRAM usage on macOS?",
        "bodyText": "Is there any way to read VRAM usage on macOS? I tried the same model on macOS and windows, with metal turned on on macOS and cuBLAS turned on on windows, and -ngl to -1, and ended up using only 2G of RAM and a total of 13G of unity RAM on macOS, I'm assuming that it's 11G of VRAM being used here, and more than all of my RAM(16GB) and 13G of VRAM on windows.\nAnother question, is there a way to make windows use as much memory as macOS? I also try -ngl to 0 from #4310, but it is not helping",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4505",
        "createdAt": "2023-12-17T06:52:07Z",
        "author": {
            "login": "miku1958"
        }
    },
    {
        "title": "How do you format with the prompt template in the server UI?",
        "bodyText": "The mistral finetunes are repeating unless I use the chatml format but I don't understand how to format it properly in the server UI. Cant find much documentation about the parameters in the ui like {{name}}: {{message}} , {{prompt}},{{history}},{{char}}: either. Any insight into this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3829",
        "createdAt": "2023-10-28T10:34:30Z",
        "author": {
            "login": "RahulVivekNair"
        }
    },
    {
        "title": "Is it possible to increase response speed?",
        "bodyText": "I'm using llama.cpp server and the response is not as fast as I would like. Maybe I configured it incorrectly?\nHardware:\n\nGPU: 1xTesla T4 (16 Gb)\nRAM: 32 GB\nCPU's: 8 core\n\nCloud: AWS\nModel: llama-3-8b-instruct.Q3_K_L\nbuild: make LLAMA_CUDA=1\nserver:\n1 request (prompt 1078 tokens) to /v1/chat/completion\n./server -m ~/models/llama-3-8b-instruct.Q3_K_L -n 100 -t 8 -ngl 99 -c 8192 -b 2048 -ub 2048\nprompt eval time     =    1083.64 ms /  1078 tokens (    1.01 ms per token,   994.80 tokens per second)\ngeneration eval time =     173.44 ms /     6 runs   (   28.91 ms per token,    34.59 tokens per second)\ntotal time =    1257.08 ms\n5 parallel  request to /v1/chat/completion\n./server -m ~/models/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf -n 100 -ngl 99 -c 10240 -b 10240 -ub 5120 -cb -np 5\nFor each request a take results like this:\n{\"tid\":\"134897798529024\",\"timestamp\":1716822151,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":315,\"msg\":\"prompt eval time =    5865.40 ms /  1038 tokens (    5.65 ms per token,  176.97 tokens per second)\",\"id_slot\":3,\"id_task\":4,\"t_prompt_processing\":5865.4,\"n_prompt_tokens_processed\":1038,\"t_token\":5.650674373795761,\"n_tokens_second\":176.9700276195997}\n{\"tid\":\"134897798529024\",\"timestamp\":1716822151,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":331,\"msg\":\"generation eval time =     324.39 ms /     5 runs   (   64.88 ms per token,  15.41 tokens per second)\",\"id_slot\":3,\"id_task\":4,\"t_token_generation\":324.387,\"n_decoded\":5,\"t_token\":64.8774,\"n_tokens_second\":15.413687971466182}\n{\"tid\":\"134897798529024\",\"timestamp\":1716822151,\"level\":\"INFO\",\"function\":\"print_timings\",\"line\":342,\"msg\":\"total time =    6189.79 ms\",\"id_slot\":3,\"id_task\":4,\"t_prompt_processing\":5865.4,\"t_token_generation\":324.387,\"t_total\":6189.786999999999}\n{\"tid\":\"134896550936576\",\"timestamp\":1716822151,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2882,\"msg\":\"request\",\"remote_addr\":\"ip\",\"remote_port\":57483,\"status\":200,\"method\":\"POST\",\"path\":\"/v1/chat/completions\",\"params\":{}}\nIf it is possible to increase response speed, tell me, pls. I will be very grateful",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7569",
        "createdAt": "2024-05-27T15:31:39Z",
        "author": {
            "login": "Ruslando7"
        }
    },
    {
        "title": "async/parallel speculative execution",
        "bodyText": "Was there an attempt to run draft model in parallel with main model on difference compute device of the same machine?\nHere's a small illustration i made:  c8d446d\n\nIt runs main model (Llama3-70-Q8) on M2 Ultra GPU\nIt runs draft model (Llama3-8-Q5) on 16 cpu perf cores of the same M2 Ultra\nThe intuition is, it is generally hard to do speculation well because you need a good small model (or train a subset of a model in case of medusa). On top of that producing single tokens one after another is very efficient on apple silicon (#6777), so we need to get a pretty good acceptance rate to make evaluating multiple tokens a better choice.\nThe way that demo works is to have two parallel threads operating on two separate sequences to amortize the cost of draft model. Sequences and caches are reconciled every time a batch is finished evaluating on main model. It is simply linear, no tree/beam search/multiple sequences/etc. Greedy selection of the next token.\nWe can run a decent draft model this way.\n\nExperiment setup/observations:\n\nFor a baseline version use simple.cpp, modified to allow 1024 tokens n_len and include time for prompt encoding in the timer.\n\nmake simple && ./simple ../llms/gguf/Meta-Llama-3-70B-Instruct.Q8_0-00001-of-00003.gguf \"$(<examples/async_spec/in.txt)\"\n\nObserve ~83s to process the prompt and produce the output.\n\nFor a test run the async_spec (roughly based on the same simple.cpp) :\n\nmake async_spec && ./async_spec ../llms/gguf/Meta-Llama-3-70B-Instruct.Q8_0-00001-of-00003.gguf ../llms/gguf/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf \"$(<examples/async_spec/in.txt)\"\n\nObserve ~64s to process the same prompt and produce same output. Not dramatic, but fairly noticeable.\n\nWe are able to generate really long sequences of draft model that are discarded (red tokens in the screenshot below). If we generate multiple sequences and use the CPU resource on that we might be able to further increase acceptance rate. I'm not sure what's the best known way to generate multiple sequences though, at what moment should we split, etc.\n\n\n\nboth CPU and GPU are well utilized during this process:",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6853",
        "createdAt": "2024-04-23T18:29:34Z",
        "author": {
            "login": "okuvshynov"
        }
    },
    {
        "title": "Using grammars during training",
        "bodyText": "Hi everyone, first of all, thank you for your work, this project is quite amazing !\nI had the occasion to test the constrained generation feature of llama.cpp, using grammar at inference time, which is working perfectly.\nHowever, I was thinking recently about a hypothetical new feature : Would it be possible to use grammar while training a model ? I looked for similar propositions with an \"enhancement\" tag, but couldn't find one.\nFor example, my finetuning problem is a multilabel (each binary) text classification one, so the JSON format for answers from the LLM is convenient, as I can have multiple fields (one per label) and associated predicted boolean values by the model.\nThe thing is, to me, using grammar only at inference when I have access to the training conditions is suboptimal as it perturbs the proba tokens distribution of a model which is not \"used\" to it. So in terms of efficiency of the training (QLora finetuning), this would surely be a large loss in accuracy compared to a model which would have already encountered grammar use during its training.\nIn addition, from a strictly computational perspective, would this be a way of gaining speed for training ? I mean that the backprop algo would not need to be applied for every token generated by the model, but only those where the model truely intervenes, thus limiting greatly the number of call of the related functions. I am not sure for this part however as I am not a specialist about causal LM training, and thus if a \"discontinuous\" application of the backprop algo would be realistic.\nThank you for your time",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7567",
        "createdAt": "2024-05-27T13:48:04Z",
        "author": {
            "login": "jbchanier"
        }
    },
    {
        "title": "I found  a great dataset for conversational AIs",
        "bodyText": "@TheBloke I would love if you or anyone  else could train mistralai/Mistral-7B-Instruct-v0.3\non this great dataset:  https://huggingface.co/datasets/ZeroWw/MEISD\nI also would like somene to try this:\nfirst quantize Mistral-7B-Instruct-v0.3 to q4_k or to q5_1. then train/finetune it on that dataset.\nUnfortunately I have  no resources to do so.. and not enough know how to do it properly online.\nP.S.\nI chose mistral because in my opinion it has the best \"chat\" attitude... although it's lacking in solving logical problems, but it has a great creativity.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7564",
        "createdAt": "2024-05-27T11:50:50Z",
        "author": {
            "login": "0wwafa"
        }
    },
    {
        "title": "EOS Token being splitted in multiple tokens, making the detection harder",
        "bodyText": "I am using llama-cpp-python to generate text from phi-3 (note that this issue is present in llama3-instruct, zephyr, and others too).\nWhen the model outputs the EOS (for example phi-3 has <|end|>), instead of outputting the single token number, it breaks the EOS in many pieces like <| then end then |>.\nAlso a second thing is that i am noticing many \"special token hallucinations\" in my python program that are not present when using ollama, for example the model instead of using the EOS uses a weird <|endtext that is not even closed, or straight up switches to impersonating the user with <|user|> without printing the EOS.\nIs this an issue solvable by providing a particular argument to main.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7441",
        "createdAt": "2024-05-21T16:29:32Z",
        "author": {
            "login": "Belluxx"
        }
    },
    {
        "title": "Config main an compilate for TESLA P40 card",
        "bodyText": "hi, i have a Tesla p40 card, it's slow with ollama and Mixtral 8x7b.\nSomeone advise me to test compiling llama.cpp with \"-DLLAMA_CUDA=ON -DLLAMA_CLBLAST=ON -DLLAMA_CUDA_FORCE_MMQ=ON\" in order to use FP32 and acceleration on this old cuda card.\nit's faster than ollama but i can't use it for conversation. i talk alone and close. i use this command\nmain.exe -ngl 29 -m D:\\GITHUB\\llamamodel.huggingface\\mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] {prompt} [/INST]\"\ndo you have an idea ?\nthanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7467",
        "createdAt": "2024-05-22T14:53:38Z",
        "author": {
            "login": "gandolfi974"
        }
    },
    {
        "title": "Model representation.. aren't weights layers actually are \"images\"?",
        "bodyText": "i was thinking about format and realized that layers with weight ideally fit all the existing \"images\" libraries. in other words, there is a hell lot of algorithms for image compression, including quantizing up to N bits, as well as lossless ones.\ncouldn't we store model weights with image libs, like png or webp, etc and get good results? possibly, even accessing values without fully unpacking, reducing greatly memory footprint but paying with cpu for accessing values?\ni feel that image compression algorithms should work well on models, keeping the whole \"picture\" of the model, while reducing its size. it's very interesting to see how it compares to QLORA, etc. and, perhaps, it might allow running much bigger models on smaller memory without having to write a lot of new code, as it's already there for images?\nwhat do you think?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6338",
        "createdAt": "2024-03-27T07:53:38Z",
        "author": {
            "login": "drazdra"
        }
    },
    {
        "title": "Llama 3 finetuning crashing",
        "bodyText": "When I run\nllama.cpp/finetune --model-base Meta-Llama-3-8B-Instruct-Q4_K_M.gguf --train-data dmdata.txt --threads 14\nit tries to run for a few seconds, then outputs this and crashes. How do I fix this?\nmain: seed: 1716591715\nmain: model base = 'Meta-Llama-3-8B-Instruct-Q4_K_M.gguf'\nllama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from Meta-Llama-3-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct-imatrix\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: missing pre-tokenizer type, using: 'default'\nllm_load_vocab:                                             \nllm_load_vocab: ************************************        \nllm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \nllm_load_vocab: CONSIDER REGENERATING THE MODEL             \nllm_load_vocab: ************************************        \nllm_load_vocab:                                             \nllm_load_vocab: special tokens definition check successful ( 256/128256 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct-imatrix\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\nllm_load_print_meta: LF token         = 128 '\u00c4'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_tensors: ggml ctx size =    0.15 MiB\nllm_load_tensors:        CPU buffer size =  4685.30 MiB\n........................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\nllama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nmain: init model\nprint_params: n_vocab               : 128256\nprint_params: n_ctx                 : 128\nprint_params: n_embd                : 4096\nprint_params: n_ff                  : 14336\nprint_params: n_head                : 32\nprint_params: n_head_kv             : 8\nprint_params: n_layer               : 32\nprint_params: norm_rms_eps          : 0.000010\nprint_params: rope_freq_base        : 500000.000000\nprint_params: rope_freq_scale       : 1.000000\nprint_lora_params: n_rank_attention_norm : 1\nprint_lora_params: n_rank_wq             : 4\nprint_lora_params: n_rank_wk             : 4\nprint_lora_params: n_rank_wv             : 4\nprint_lora_params: n_rank_wo             : 4\nprint_lora_params: n_rank_ffn_norm       : 1\nprint_lora_params: n_rank_ffn_gate       : 4\nprint_lora_params: n_rank_ffn_down       : 4\nprint_lora_params: n_rank_ffn_up         : 4\nprint_lora_params: n_rank_tok_embeddings : 4\nprint_lora_params: n_rank_norm           : 1\nprint_lora_params: n_rank_output         : 4\nmain: total train_iterations 0\nmain: seen train_samples     0\nmain: seen train_tokens      0\nmain: completed train_epochs 0\nmain: lora_size = 94956320 bytes (90.6 MB)\nmain: opt_size  = 141731824 bytes (135.2 MB)\nmain: opt iter 0\nmain: input_size = 525340704 bytes (501.0 MB)\nGGML_ASSERT: examples/finetune/finetune.cpp:646: false && \"TODO: ggml_flash_attn_ext() not yet supported\"\n\nThis GDB supports auto-downloading debuginfo from the following URLs:\n  <https://debuginfod.fedoraproject.org/>\nEnable debuginfod for this session? (y or [n]) [answered N; input not from terminal]\nDebuginfod has been disabled.\nTo make this setting permanent, add 'set debuginfod enabled off' to .gdbinit.\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\n0x00007f8b16f13dc7 in wait4 () from /lib64/libc.so.6\n#0  0x00007f8b16f13dc7 in wait4 () from /lib64/libc.so.6\n#1  0x000000000042b32b in ggml_print_backtrace ()\n#2  0x00000000005de5af in llama_build_lora_finetune_graphs(my_llama_model*, my_llama_lora*, ggml_gallocr*, ggml_context*, ggml_cgraph*, ggml_cgraph*, ggml_cgraph*, ggml_tensor**, ggml_tensor*, ggml_tensor*, int, int, bool, bool, bool) ()\n#3  0x000000000042823e in main ()\n[Inferior 1 (process 9461) detached]\nAborted (core dumped)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7528",
        "createdAt": "2024-05-24T23:00:39Z",
        "author": {
            "login": "mak448a"
        }
    },
    {
        "title": "how to prevent llama cpp python for printing text on CLI",
        "bodyText": "i am using llama python cpp . i am running below code\nfrom llama_cpp import Llama\nimport timeit\nfrom PyPDF2 import PdfReader\nstart = timeit.default_timer()\npath = r'C:\\Users\\f162\\data\\cc.pdf'\npnb_path = r'D:\\llama_cpp\\SBI.pdf'\nreader = PdfReader(pnb_path)\nnumber_of_pages = len(reader.pages)\nprint(number_of_pages)\npage = reader.pages[0]\ntext = page.extract_text()\ntext1 = text.splitlines()\nnew_text1 = ' '.join(text1[:15])\nprint(new_text1)\nprompt = \"extract account name Branch Name, Branch Address, from the \" + new_text1\nprint(prompt)\nllm = Llama( model_path=r\"D:\\contract_note\\llama-2-13b-chat.Q5_K_S.gguf\", chat_format=\"chatml\")\nllm = Llama( model_path=r\"D:\\contract_note\\llama-2-13b-chat.Q5_K_S.gguf\", chat_format=\"chatml\",n_ctx=2048)\nx = llm.create_chat_completion(\n\u00a0 \u00a0 messages=[\n\u00a0 \u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"role\": \"system\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": \"You are a helpful assistant that outputs in JSON.\",\n\u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"user\", \"content\": prompt},\n\u00a0 \u00a0 ],\n\u00a0 \u00a0 response_format={\n\u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"json_object\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"schema\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"object\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"properties\": {\"branch name\": {\"type\": \"string\"},\"branch address\": {\"type\": \"string\"},\"customer address\": {\"type\": \"string\"}},\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"required\": [\"branch name\",\"branch address\",\"customer address\"],\n\u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 },\n\u00a0 \u00a0 temperature=0.2,\n)\nprint(len(x['choices'][0]['content']))\nprint(x['choices'][0]['message']['content'])\nend = timeit.default_timer()\nprint(end-start)\non running it is printing various text on CLI . below is given text I want to prevent it\n02002149 02 MAR 2024\u00a0INR 4886.63RIBHU\u00a0SHARMA Drawing Power11 Mar 2024 IFS CodeSavings Account Description Balance as on Search forAccount Name 0.0081/209 SECTOR 8, PRATAP NAGAR, JAIPUR, 302033 BranchAccount NumberDate KUMBHA MARG PRATAP NAGAR JAIPUR 15 JUL 2021 to 02 MAR 20247119593780351105648681 Interest Rate(%p.a.)Address CIF No. Yes Nomination RegisteredSBIN0031840 MICR Code2.7000State Bank of India Date Credit Balance DetailsRef No./Cheque\nllama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from D:\\contract_note\\llama-2-13b-chat.Q5_K_S.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv\u00a0\u00a00:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0general.architecture str\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= llama\nllama_model_loader: - kv\u00a0\u00a01:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0general.name str\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= LLaMA v2\nllama_model_loader: - kv\u00a0\u00a02:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.context_length u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 4096\nllama_model_loader: - kv\u00a0\u00a03:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.embedding_length u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 5120\nllama_model_loader: - kv\u00a0\u00a04:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.block_count u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 40\nllama_model_loader: - kv\u00a0\u00a05:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.feed_forward_length u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 13824\nllama_model_loader: - kv\u00a0\u00a06:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.rope.dimension_count u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 128\nllama_model_loader: - kv\u00a0\u00a07:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.attention.head_count u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 40\nllama_model_loader: - kv\u00a0\u00a08:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0llama.attention.head_count_kv u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 40\nllama_model_loader: - kv\u00a0\u00a09:\u00a0\u00a0\u00a0llama.attention.layer_norm_rms_epsilon f32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 0.000010\nllama_model_loader: - kv\u00a010:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0general.file_type u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 16\nllama_model_loader: - kv\u00a011:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.model str\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= llama\nllama_model_loader: - kv\u00a012:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.tokens arr[str,32000]\u00a0\u00a0= [\"\", \"\", \"\", \"<0x00>\", \"<...\u00a0\u00a0\u00a0\u00a0\u00a0\nllama_model_loader: - kv\u00a013:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.scores arr[f32,32000]\u00a0\u00a0= [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv\u00a014:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.token_type arr[i32,32000]\u00a0\u00a0= [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\u00a0\u00a0\u00a0\u00a0\u00a0\nllama_model_loader: - kv\u00a015:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.bos_token_id u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 1\nllama_model_loader: - kv\u00a016:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.eos_token_id u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 2\nllama_model_loader: - kv\u00a017:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tokenizer.ggml.unknown_token_id u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 0\nllama_model_loader: - kv\u00a018:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0general.quantization_version u32\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 2\nllama_model_loader: - type\u00a0f32:\u00a0\u00a081 tensors\nllama_model_loader: - type q5_K:\u00a0281 tensors\nllama_model_loader: - type q6_K:\u00a0\u00a01 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= GGUF V2\nllm_load_print_meta: arch\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= llama\nllm_load_print_meta: vocab type\u00a0\u00a0\u00a0\u00a0= SPM\nllm_load_print_meta: n_vocab\u00a0\u00a0\u00a0\u00a0\u00a0= 32000\nllm_load_print_meta: n_merges\u00a0\u00a0\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: n_ctx_train\u00a0\u00a0\u00a0= 4096\nllm_load_print_meta: n_embd\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 5120\nllm_load_print_meta: n_head\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 40\nllm_load_print_meta: n_head_kv\u00a0\u00a0\u00a0\u00a0= 40\nllm_load_print_meta: n_layer\u00a0\u00a0\u00a0\u00a0\u00a0= 40\nllm_load_print_meta: n_rot\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 128\nllm_load_print_meta: n_embd_head_k\u00a0\u00a0= 128\nllm_load_print_meta: n_embd_head_v\u00a0\u00a0= 128\nllm_load_print_meta: n_gqa\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 1\nllm_load_print_meta: n_embd_k_gqa\u00a0\u00a0\u00a0= 5120\nllm_load_print_meta: n_embd_v_gqa\u00a0\u00a0\u00a0= 5120\nllm_load_print_meta: f_norm_eps\u00a0\u00a0\u00a0\u00a0= 0.0e+00\nllm_load_print_meta: f_norm_rms_eps\u00a0\u00a0= 1.0e-05\nllm_load_print_meta: f_clamp_kqv\u00a0\u00a0\u00a0= 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale\u00a0\u00a0= 0.0e+00\nllm_load_print_meta: n_ff\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= 13824\nllm_load_print_meta: n_expert\u00a0\u00a0\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: n_expert_used\u00a0\u00a0= 0\nllm_load_print_meta: causal attn\u00a0\u00a0\u00a0= 1\nllm_load_print_meta: pooling type\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: rope type\u00a0\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: rope scaling\u00a0\u00a0\u00a0= linear\nllm_load_print_meta: freq_base_train\u00a0= 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx\u00a0= 4096\nllm_load_print_meta: rope_finetuned\u00a0\u00a0= unknown\nllm_load_print_meta: ssm_d_conv\u00a0\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: ssm_d_inner\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: ssm_d_state\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: ssm_dt_rank\u00a0\u00a0\u00a0= 0\nllm_load_print_meta: model type\u00a0\u00a0\u00a0\u00a0= 13B\nllm_load_print_meta: model ftype\u00a0\u00a0\u00a0= Q5_K - Small\nllm_load_print_meta: model params\u00a0\u00a0\u00a0= 13.02 B\nllm_load_print_meta: model size\u00a0\u00a0\u00a0\u00a0= 8.36 GiB (5.51 BPW)\nllm_load_print_meta: general.name\u00a0\u00a0\u00a0= LLaMA v2\nllm_load_print_meta: BOS token\u00a0\u00a0\u00a0\u00a0= 1 ''\nllm_load_print_meta: EOS token\u00a0\u00a0\u00a0\u00a0= 2 ''\nllm_load_print_meta: UNK token\u00a0\u00a0\u00a0\u00a0= 0 ''\nllm_load_print_meta: LF token\u00a0\u00a0\u00a0\u00a0\u00a0= 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =\u00a0\u00a00.18 MiB\nllm_load_tensors:\u00a0\u00a0\u00a0\u00a0CPU buffer size =\u00a08555.93 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx\u00a0\u00a0\u00a0= 512\nllama_new_context_with_model: n_batch\u00a0\u00a0= 512\nllama_new_context_with_model: n_ubatch\u00a0\u00a0= 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base\u00a0= 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:\u00a0\u00a0\u00a0\u00a0CPU KV buffer size =\u00a0\u00a0400.00 MiB\nllama_new_context_with_model: KV self size\u00a0=\u00a0400.00 MiB, K (f16):\u00a0200.00 MiB, V (f16):\u00a0200.00 MiB\nllama_new_context_with_model:\u00a0\u00a0\u00a0\u00a0CPU\u00a0output buffer size =\u00a0\u00a0\u00a00.12 MiB\nllama_new_context_with_model:\u00a0\u00a0\u00a0\u00a0CPU compute buffer size =\u00a0\u00a085.01 MiB\nllama_new_context_with_model: graph nodes\u00a0= 1286\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\nexpand_more",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7140",
        "createdAt": "2024-05-08T07:18:35Z",
        "author": {
            "login": "rites1095"
        }
    },
    {
        "title": "Does imatrix work independent of the dtype used for calibration?",
        "bodyText": "IE, if I convert to F32 and BF16, calculate the imatrix from F32 (since it supports GPU offloading), but then apply it to the BF16 weights during quantization, will that work as expected or will the change in dtype affect it?\n@ikawrakow sorry for the ping but after searching couldn't find any way to reach out to you directly and I imagine you're the most well versed on this subject..\nSimilarly, if F16 for imatrix and BF16 for quantization, curious how this would work",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7538",
        "createdAt": "2024-05-25T17:51:07Z",
        "author": {
            "login": "bartowski1182"
        }
    },
    {
        "title": "What exactly does llama_get_embeddings return?",
        "bodyText": "Hi, I've been going through the code trying to understand what llama_get_embeddings returns, but I can't figure it out.\nI'm trying to use stablebeluga-13b.Q8_0.gguf as an encoder, to populate a database of embeddings of texts. I've already read some issues mentioning that using Llama models for this task gives bad results, but I wanted to try it anyway.\nI'm using the python wrapper, and basically running this code:\nmodel = Llama(..., embedding=True)\nembeddings_raw = model.create_embedding(a_list_of_texts)\nembeddings = [x['embedding'] for x in embeddings_raw['data']]\n\nI've tracked the calls in the python wrapper code, and it seems to end up calling llama_cpp.llama_get_embeddings, so that's why I'm asking in this repository.\nI'm not sure where the embedding values come from. Obviously, I'm interested in getting a representation of the whole text (or N texts) passed as input to the function.\nAre the returned embeddings simply the contextual embedding of each token, with a mean pooling applied? As far as I know, this is how most of the sentence-transformers models work.\nI've done some basic tests using the embeddings and the results are weird. I wanted to post them here, since I haven't seen similar results.\nThe following plots are embeddings of a text reshaped into a matrix, so that each pixel is just a value of the embedding. The subplot on the left is the embedding of an auto-generated sentence, and the one on the right is the embedding of the same sentence shuffling its words.\nThere are some values that clearly dominate the rest (they are either very high or very low), and the strange thing is that they seem to be independent of the input text (they repeat in all plots).\n\n\n\nI'm just posting a few examples, but I've generated many more.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3643",
        "createdAt": "2023-10-16T14:07:15Z",
        "author": {
            "login": "Trawczynski"
        }
    },
    {
        "title": "LLaMA-2 Perplexities",
        "bodyText": "Here are some results of LLaMA-2 perplexities for the 7B and 13B models. Before someone tells me that I should have used HellaSwag(ish) scores instead: a) many of the results were computed before HellaSwag became a thing here, and b) I basically don't see anything in HellaSwag scores that I don't already see in perplexity scores. So, here we go.\n\nFor context lengths of up to 2048 tokens, the difference between LLaMA-1 and LLaMA-2 is small but noticeable. See Figures 1, 2 below\nLLaMA-2 can be extended to (at least) 8k tokens for free (no finetunig required) via --rope-freq-base. At 8k tokens the difference in perplexity between LLaMA-1 and LLaMA-2 is very significant (see Figures 1 and 2)\nThe 7B LLaMA model shows a peculiar behavior at 4-bit quantization: the perplexity of Q4_1 is higher than Q4_0. This also affects k-quants. Q4_K being \"type-1\" quantization like Q4_1 has a higher perplexity than Q4_0 and even Q3_K_L. See Figure 3.\nFor the 13B model we are back to the normal situation where k-quants have lower perplexities at the same model size compared to Q4_0/1 and Q5_0/1. See Figure 4.\nA comparison between k-quants perplexities for the 13B LLaMA-1 and LLaMA-2 models is shown in Figure 5. We see LLaMA-2 Q4_K_S perplexity is lower than the fp16 perplexity of LLaMA-1.\n\n\nFigure 1 Perplexity as a function of context size for the LLaMA-1 (black) and LLaMA-2 (red) 7B models. Results were computed using Q6_K quantization and the --rope-freq-base option for extending beyond the respective training context size\n\nFigure 2 Perplexity as a function of context size for the LLaMA-1 (black) and LLaMA-2 (red) 13B models. Results were computed using Q6_K quantization and the --rope-freq-base option for extending beyond the respective training context size\n\nFigure 3 Perplexity as a function of model size for 7B LLaMA-2 using different quantizations.\n\nFigure 4 Perplexity as a function of model size for 13B LLaMA-2 using different quantizations.\n\nFigure 5 Comparison between 13B LLaMA-1 and LLaMA-2 perplexity for a context size of 512 using different quantizations.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2352",
        "createdAt": "2023-07-23T20:13:43Z",
        "author": {
            "login": "ikawrakow"
        }
    },
    {
        "title": "Paddler: open source stateful load balancer custom-tailored for llama.cpp",
        "bodyText": "Hello! : )\nI finished a new project recently. I needed a load balancer specifically tailored for the llama.cpp that considers its specifics (slots usage, continuous batching). It also works in environments with auto-scaling (you can freely add and remove hosts)\nLet me know what you think. Thank you all for creating and maintaining llama.cpp!\nPS. I called it \"paddler\" because I wanted to use Raft protocol initially, but in the end, it was unnecessary. I kept the name, though. :)\nRepo:\nhttps://github.com/distantmagic/paddler",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7369",
        "createdAt": "2024-05-18T16:48:20Z",
        "author": {
            "login": "mcharytoniuk"
        }
    },
    {
        "title": "ggml-cuda.cu:3211: ERROR: CUDA kernel vec_dot_q5_K_q8_1_impl_vmmq",
        "bodyText": "ggml-cuda.cu:3211: ERROR: CUDA kernel vec_dot_q5_K_q8_1_impl_vmmq has no device code compatible with CUDA arch 520. ggml-cuda.cu was compiled for: 520\nThis worked yesterday.  I did a git pull, make clean, make and then get this error today.\nnvidia rtx 3090\nSystem: debian testing\nCommand line:\n~/github/llama.cpp/main -m ~/models/miqu-1-70b.q5_K_M.gguf -c 0 -i --color -t 16 --n-gpu-layers 24  --temp 0.8 -p \"bob\"\nI reverted previous two commits and issue went away.\n/github/llama.cpp$ git reset --hard HEAD2\nHEAD is now at 334f76f sync : ggml",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5685",
        "createdAt": "2024-02-23T16:11:45Z",
        "author": {
            "login": "DanCard"
        }
    },
    {
        "title": "Strategies for Server/RPC and mixed performance machines",
        "bodyText": "I have a similar use-case as the one described in #6829 and wanted to share what I've done to approach this: https://github.com/SoftwareRenderer/llmwrangler . The goal for this is to avoid bottlenecks when including CPU instances along GPU.\nI'm hoping that these ideas are useful (if not already implemented), and they can be integrated into the RPC backend. I need to brush up on my C, so it'll be a while until I can do it myself (and not be ashamed to submit a PR for the code).\nIn llmwrangler there's a couple features:\n\nCache the response time for each node, and assign the least busy node to incoming requests.\nTrack and estimate total load per host: If the CPU responds in 3,000 ms, don't give it any work until the GPU's work queue exceeds that time.\nWarm up CPU instances (KV cache?). This significantly improves perceived TTFB, since in practice this is a difference of 2 minutes cold start and 3s warmed up.\n\n@rgerganov Tagging you since it looks like you're doing most of the heavy lifting on RPC",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7468",
        "createdAt": "2024-05-22T15:00:07Z",
        "author": {
            "login": "SoftwareRenderer"
        }
    },
    {
        "title": "Vulkan backend & ggml-vulkan-shaders.hpp",
        "bodyText": "I am just wondering how the contents of ggml-vulkan-shaders.hpp was generated?  I would like to learn about how the vulkan backend works.\nIf I understand correctly, this is compiled spirv shader code.  Why store the compiled code as bytes here, rather than storing source and generating the binary at build time?  Would this improve maintainability & transparency?\nPS. I am fairly new to llama.cpp, GPUs, vulkan, shaders and ML in general, so apologies if my questions are somewhat confused.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7323",
        "createdAt": "2024-05-16T10:29:32Z",
        "author": {
            "login": "mjgarton"
        }
    },
    {
        "title": "Blind testing different quants: initial results + I need your help!",
        "bodyText": "I've built a small interface to vote on different quantized versions of the same model (in this case, Mistral 7B Instruct). I then use the results to rank the models by their relative strength (using the Bradley-Terry model).\nThe idea is to see how much you're losing, by using human preference as a benchmark instead of purely synthetic metrics like perplexity or KL divergence.\nSo far, using mostly my own votes (around 500), I can say with 99% confidence that IQ1_S is noticeably worse than all the others. The other quants still have too large error bars, which is why I need your help submitting votes.\nIf you want to help (even if it's just a single vote or two), please head here and read the rules carefully: https://freya.artefact2.com/llm-eval/\nThe results so far (the pale blue is the margin of error for 99% confidence):\n\nSome info about the methodology:\n\nI picked around 4,000 prompts from these datasets: https://huggingface.co/datasets/DIBT/10k_prompts_ranked ; https://huggingface.co/datasets/HydraLM/GPTeacher_roleplay_standardized ; https://huggingface.co/datasets/Squish42/bluemoon-fandom-1-1-rp-cleaned\nI used the server example to generate answers (max. 512 tokens) to these prompts, for all the quants, using the same seed and the same sampling settings.\nIf multiple people vote on the same prompt for the same model pair, I take the median of all their votes.\n\nSome ideas I want to try out in the future if this initial experiment is successful:\n\nSeparate results based on prompt type (general knowledge, logic, math, creativity, roleplay, etc.), this would require a lot more votes which I don't have right now.\nTry with bigger models (eg Mixtral Instruct), this would require a lot more compute than I have at home.\nChoose different models but with a similar size in GiB (eg 70B IQ2_XS vs 46B IQ3_S vs 30B Q5_K).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5962",
        "createdAt": "2024-03-09T14:17:08Z",
        "author": {
            "login": "Artefact2"
        }
    },
    {
        "title": "How to avoid of chat main example stops abruptly?",
        "bodyText": "Hi,\nIn the main example there is a part where if an error occurs the chat ends:\nLOG(\"eval: %s\\n\", LOG_TOKENS_TOSTR_PRETTY(ctx, embd).c_str());\n\nif (llama_decode(ctx, llama_batch_get_one(&embd[i], n_eval, n_past, 0))) {\n     LOG_TEE(\"%s : failed to eval\\n\", __func__);\n     return 1;\n}\n\nI don't understand in which cases the error occurs and how to avoid it. The parameters I use in my command are this:\n-m C:\\\\model\\\\llama2_7b_chat_uncensored.Q4_K_M.gguf -f C:\\\\model\\\\chat-with-bob.txt -c 2048 -b 2048 -n 64 --keep -1 --temp 0.9 -- repeat_penalty 1.1 -i -r \"User:\" --log-disable\nAt the moment I am using a work around in which if the error occurs within an external while(true) the chat is launched again and it gives the impression that the chat continues and the user does not perceive that the program ended abruptly.\nSo, I'm wondering if Is there a solution to ensure that the error never occurs to avoid my ugly work around?\nThanks for the help!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7180",
        "createdAt": "2024-05-09T16:36:57Z",
        "author": {
            "login": "Zapotecatl"
        }
    },
    {
        "title": "Limitations of the current YaRN RoPE implementation and how to fix them",
        "bodyText": "I'd like to discuss the limitations of the current llama.cpp YaRN RoPE implementation affecting the implementation of DeepSeek-V2 support and possible ways to fix them. Let me start with an introduction to the problem.\nYaRN RoPE modifies attention equation by adding a scaling factor t in the denominator:\n$$softmax(\\frac{q^T_m k_n}{t \\sqrt{|D|}})$$\nThe original paper mentioned a trick allowing to leave the attention implementation unmodified:\n\nwe can instead use a \"length scaling\" trick which scales both $q_m$ and $k_n$ by a constant factor $\\sqrt{1/t}$ by simply scaling the complex RoPE embeddings by the same amount.\n\nIf I understand correctly most YaRN RoPE implementations use this \"trick\" in a form of mscale variable equal to $\\sqrt{1/t} = 0.1 ln(s) + 1$ where s is the RoPE scaling factor. They multiply the calculated RoPE sin/cos values with mscale, which results in a query and key vectors scaled by $\\sqrt{1/t}$ when RoPE rotations are applied, and their dot products scaled by $\\sqrt{1/t} * \\sqrt{1/t} = 1/t$ as required by the YaRN RoPE attention equation.\nThis \"trick\" works correctly when RoPE is applied to whole query and key vectors. But there are models like DeepSeek-V2 where RoPE is applied only to a part of the query/key vector. That leaves the remaining parts of they query/key vectors unscaled and results in attention output that is calculated incorrectly.\nIn the DeepSeek-V2 transformers implementation this problem is solved in a very confusing way, where the mscale calculation is modified in a way that results in a mscale value equal to 1.0 (note that self.mscale and self.mscale_all_dim are both set to 0.707 in config.json):\n        _mscale = float(\n            yarn_get_mscale(self.scaling_factor, self.mscale)\n            / yarn_get_mscale(self.scaling_factor, self.mscale_all_dim)\n        )\n\nIn this implementation applying RoPE rotations does not scale the key/query vectors at all. Instead they used the attention softmax_scale value and multiply it by mscale*mscale (so essentially by 1/t as required):\n                mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                self.softmax_scale = self.softmax_scale * mscale * mscale\n\nThey also changed they way mscale is calculated:\ndef yarn_get_mscale(scale=1, mscale=1):\n    if scale <= 1:\n        return 1.0\n    return 0.1 * mscale * math.log(scale) + 1.0\n\nThis is extra confusing, since the mscale name is used for multiple different values in the configuration and the code.\nTheir paper is more clear, saying that they simply modified the way $\\sqrt{t}$ is calculated to $\\sqrt{t} = 0.0707 ln(s) + 1$. By the way I think they have an error in the paper as it shall be $\\sqrt{1/t}$, not $\\sqrt{t}$. So it it looks like they changed the original 0.1 constant value from the original YaRN paper to 0.0707.\nThis is getting long, so let me sum up the problems I see in two points:\n\nThe current llama.cpp YaRN RoPE implementation does not work correctly if RoPE is applied only to a part of key/value vectors. The problem is that in this case only a part of the key/value vectors is scaled instead of whole vectors.\nThe current YaRN RoPE implementation does not allow to change constants used to calculate $\\sqrt{1/t} = 0.1 ln(s) + 1$ - (In DeepSeek-V2 0.1 was changed to 0.0707).\n\nNote that this is not an urgent or crictical problem, I managed to work around these problems when implementing DeepSeek-V2 in llama.cpp by pre-scaling attn_factor passed to ggml_rope_custom() call so the resulting calculated mscale is 1.0 (like in the transformers implementation), calculating mscale myself and scaling kq_scale passed to llm_build_kv() by multiplying it with mscale*mscale. So it's a similar solution to the one DeepSeek-V2 authors used in the transformers code. But I'm not happy with this, in my opinion it's very ugly and confusing.\nI guess the future will show if other models start using different constants values when calculating mscale like DeepSeek-V2 did. For now, I think we should at least find a name for the 0.1 constant value that DeepSeek-V2 changed to 0.0707. I don't want to call it mscale, any other suggestions (mscale_scale :D)?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7416",
        "createdAt": "2024-05-20T12:45:56Z",
        "author": {
            "login": "fairydreaming"
        }
    },
    {
        "title": "llama3 model conversion to gguf format for llama.cpp",
        "bodyText": "Hi,\nHope I can get some pointers to get me in the right direction.\nBackground:\nI understand that meta's llama3 model just gone GA not long ago. Have been using llama.cpp (it works great, thank you!) on gguf format of llama2 model. Wanted to try out the llama3, so conversion is required to obtain the gguf format of the model.\nProgress:\nHave used (convert_llama_weights_to_hf.py) to convert from Meta format (.pth) to hf (.safetensors, why is it not pytorch bin format??) format successfully. I understand from README that convert.py does not work for llama3 model at the moment, have to use 'convert-hf-to-gguf.py' instead.\nWhen I run 'convert-hf-to-gguf.py' something like following, it throws an error about MODEL_ARCH. What did I miss?\nDon't think I am using ORION.\n--------- console output --------------->\n[userid]$ python convert-hf-to-gguf.py hf-model-path/ --outfile output-file-path --outtype bf16\nTraceback (most recent call last):\nFile \"/home/myhome/convert-hf-to-gguf.py\", line 787, in \nclass OrionModel(Model):\nFile \"/home/myhome/convert-hf-to-gguf.py\", line 788, in OrionModel\nmodel_arch = gguf.MODEL_ARCH.ORION\n^^^^^^^^^^^^^^^^^^^^^\nAttributeError: type object 'MODEL_ARCH' has no attribute 'ORION'\nI have read \"Tutorial: How to convert HuggingFace model to GGUF format #2948\", couldn't find anything that works.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7405",
        "createdAt": "2024-05-20T05:58:39Z",
        "author": {
            "login": "cc-lai"
        }
    },
    {
        "title": "Why does the server-cuda container consume CPU time?",
        "bodyText": "I'm running a model in the server-cuda container, and from the monitor, I can see that the model is loaded onto the GPU, but it's consuming CPU time during execution.\ndocker run --gpus all  --rm  --network host -v /home/wencan/Projects/models/:/models ghcr.io/ggerganov/llama.cpp:server-cuda -m /models/hfl/llama-3-chinese-8b-instruct-v2-gguf/ggml-model-q4_k.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 33\n\nsystem info\nOS: Debian GNU/Linux 12 (bookworm) x86_64 \nKernel: 6.1.0-21-amd64 \nUptime: 1 hour, 44 mins \nPackages: 2170 (dpkg), 63 (flatpak), 3 (snap) \nShell: bash 5.2.15 \nResolution: 2560x1440 \nDE: GNOME 43.9 \nWM: Mutter \nWM Theme: Adwaita \nTheme: Adwaita [GTK2/3] \nIcons: Adwaita [GTK2/3] \nTerminal: gnome-terminal \nCPU: AMD Ryzen 7 1800X (16) @ 3.600GHz \nGPU: NVIDIA GeForce GTX 1080 Ti \nMemory: 5795MiB / 15896MiB \n\ndocker info\nClient:\n Context:    default\n Debug Mode: false\n\nServer:\n Containers: 12\n  Running: 3\n  Paused: 0\n  Stopped: 9\n Images: 14\n Server Version: 20.10.24+dfsg1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runtime.v1.linux nvidia runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1.6.20~ds1-1+b1\n runc version: 1.1.5+ds1-1+deb12u1\n init version: \n Security Options:\n  apparmor\n  seccomp\n   Profile: default\n  cgroupns\n Kernel Version: 6.1.0-21-amd64\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 15.52GiB\n Name: debian12\n ID: Y6MZ:UI3N:Q3N5:F5F7:4UYJ:7RMA:47YH:YBBU:PKG2:LIZ5:6NMQ:PPKK\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Registry Mirrors:\n  https://hub-mirror.c.163.com/\n Live Restore Enabled: false\n\ndocker image info:\nREPOSITORY                      TAG           IMAGE ID       CREATED         SIZE\nghcr.io/ggerganov/llama.cpp     server-cuda   85e97d6afaf3   2 days ago      2.2GB",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7393",
        "createdAt": "2024-05-19T02:52:28Z",
        "author": {
            "login": "wencan"
        }
    },
    {
        "title": "In server mode, can we stop generating (like in chat UI there's stop generating button)",
        "bodyText": "In server mode, can we stop generating (like in chat UI there's stop generating button)\nAny idea ?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4721",
        "createdAt": "2024-01-01T00:59:14Z",
        "author": {
            "login": "x4080"
        }
    },
    {
        "title": "Automated Custom Multiple-Choice Benchmark",
        "bodyText": "Sharing this with anyone interested.\nIf you're like me and the lack of automated benchmark tools that don't require you to be a machine learning practitioner with VERY specific data formats to use has irked you, this might be useful.\nA Llama.cpp PR from awhile back allowed you to specify a --binary-file and --multiple-choice flag, but you could only use a few common datasets like MMLU. I've made an encoder so that you can easily make your own custom datasets to test with.\nYou'll find it and instructions at this gist.\nEncode.cpp will convert a JSON file to a .bin file usable by llama.cpp's --binary-file and --multiple-choice flags.\nIt expects a format of:\n[\n  { \n    \"multiple_correct\": {\"answers\": [], \"labels\": [] },\n    \"question\": \"Question: \\\"QUESTION TEXT\\\" Answer:\",\n    \"single_correct\": {\n\n    \"answers\": [\n      \"ANSWER TEXT 1\",\n      \"ANSWER TEXT 2\",\n      \"ANSWER TEXT 3\",\n      \"ANSWER TEXT 4\"\n    ],\n    \"labels\": [0,0,0,1]\n  }\n]\n\n(Label 1 being the correct answer)  This is the format you'll get if using the convert program here.\nLike with that program, compile and use with:\ng++ -o encode encode.cpp\n./encode arc-easy-validation.json arc-easy-validation.bin\n./perplexity -m model -bf arc-easy-validation.bin --multiple-choice\n\n\nTo further simplify the process, I'm including tojson.py, which can turn a simpler plan-text format into the proper JSON, to then be further converted to binary. It expects the following format:\nQ:] QUESTION TEXT\nA1:] CORRECT ANSWER TEXT\nA2:] ANSWER\nA3:] ANSWER\nA4:] ANSWER\n\nQ:] QUESTION TEXT\nA1:] CORRECT ANSWER TEXT\nA2:] ANSWER\nA3:] ANSWER\nA4:] ANSWER\n\nWith a newline between each question set, and the correct answer as the first option. (The answer order will be shuffled in the JSON)\nAs an experiment, I've included a function addMultipleQuestions which you can replace the two calls to addQuestion with - which will include the same question multiple times for each possible answer, with the correct answer shuffled to each possible position. So far I've not noticed any strong difference in scoring, at the expense of extra processing time. so it's not the default.\nUse like:\npython tojson.py custom-test.txt custom-test.json\n./encode custom-test.json custom-test.bin\n./perplexity -m model -bf custom-test.bin --multiple choice",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7419",
        "createdAt": "2024-05-20T19:14:08Z",
        "author": {
            "login": "strawberrymelonpanda"
        }
    },
    {
        "title": "how do I access common functions when using the library?",
        "bodyText": "I'm incorporating some examples into a project. I'm pulling in llama.cpp in my CMakeLists.txt file like so:\ninclude(FetchContent)\nFetchContent_Declare(\n\tllama\n\tGIT_REPOSITORY https://github.com/ggerganov/llama.cpp\n\tGIT_TAG        master\n)\nFetchContent_MakeAvailable(llama)\ntarget_link_libraries(${TARGET_NAME} PRIVATE llama)\n\nThis mostly works except I get error: use of undeclared identifier for many things in common.h.\nIf I change it to #include \"common/common.h\", then I get linker errors for the symbols that are defined in the common header. In my Cmake build directory I see that the libcommon.a is being built. Any ideas on a proper way to include it going forward?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7354",
        "createdAt": "2024-05-17T23:04:26Z",
        "author": {
            "login": "willie"
        }
    },
    {
        "title": "Need help on building shared libraries on Windows machine for Android x86_64 (emulator)",
        "bodyText": "Hello,\nI am using a windows development machine and need a consistent way to build llama.cpp for the Android x86_64 emulator (not aarch64).\nI have tried to build the shared libraries using termux on the x86_64 emulator. Everything is build except the shared libraries (so files).\nIf anyone has attempted it, please advise.\nThanks.\nEdit:\nI used Android Studio + NDK to build for android x86_64.\nx86_64-linux-android33-clang -shared -o libggml-shared.so -fPIC ggml.c ggml-alloc.c ggml-backend.c ggml-quants.c -Wall\nx86_64-linux-android33-clang++.cmd -shared -o libllama.so -fPIC llama.cpp -Wall -L. -lggml-shared\nThen tried to load the resulting libllama.so in a test MAUI app with no success.\nCan anyone pinpoint if I am missing something?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7394",
        "createdAt": "2024-05-18T06:43:20Z",
        "author": {
            "login": "cmpktheo"
        }
    },
    {
        "title": "My Hypothesis (A very important and potential issue related to prompt evaluation in modern LLMs.)",
        "bodyText": "Hypothesis: When an LLM evaluates (tokenizes) the query for both sides (user, assistant), it collects a much wider logprob spread for this query and thus can predict a much better and statistically more probable response.\nExample:\nhttps://www.reddit.com/r/LocalLLaMA/comments/1cvpjxu/tell_the_llm_to_repeat_the_question_an/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button\nExample from my LLM when the query is appended after the assistant tag and before the initial response (it completes from the query):\nQuery:\nUser:\nA dead cat is placed into a box along with a nuclear isotope, a vial of poison and a radiation detector. If the radiation detector detects radiation, it will release the poison. The box is opened one day later. What is the probability of the cat being alive?\\n\n\nResponse:\nAssistant:\nA dead cat is placed into a box along with a nuclear isotope, a vial of poison and a radiation detector. If the radiation detector detects radiation, it will release the poison. The box is opened one day later. What is the probability of the cat being alive?\nTo answer this question, we need to consider the following:\n\n1. The cat is already dead when placed in the box.\n2. The presence of a nuclear isotope does not affect the cat's state (dead or alive).\n3. The vial of poison and radiation detector are irrelevant since the cat is already dead.\n\nGiven these points, the probability of the cat being alive is 0% because it was already dead when placed in the box. The other items in the box do not have any impact on the cat's state. Therefore, the answer is 0% chance that the cat is alive.\n\nFinal Answer: The final answer is 0% chance that the cat is alive.\n\nIt's puzzling how models can fail so catastrophically on such an easy question, yet when we simply append the query to the assistant field, they get it right.\nWhat are you thoughts about this phenomenon? Can we find the underlying issue in prompt evaluation so that it could improve the performance of LLMs?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7398",
        "createdAt": "2024-05-19T20:28:24Z",
        "author": {
            "login": "nanowell"
        }
    },
    {
        "title": "[hack] Quantization w/o intermediate f32 converted model",
        "bodyText": "Hi all!\nI was trying to convert & quantize files by myself (couldn't find a memory-mappable Mixtral 8x7b since #6387) and realized I didn't have enough disk space left \ud83d\ude13. Even the recently added direct Q8 quantization (#7234) eats lots of disk (besides, I wanted a Q4 w/o needless loss of quality).\nSo I did a (Unix-only) dirty hack that has convert.py quantize the model on the fly (using subprocess calls to a lightly modified ./quantize): see this branch\ngit remote add ochafik https://github.com/ochafik/llama.cpp\ngit fetch ochafik\ngit checkout ochafik/quant-lowdisk\n\nmake clean && make quantize\npip install -U sentencepiece \"huggingface_hub[cli,hf_transfer]\"\nexport HF_HUB_ENABLE_HF_TRANSFER=1\n\npython convert.py \\\n  `huggingface-cli download NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO` \\\n  --outfile Nous-Hermes-2-Mixtral-8x7B-DPO-Q4_K_M \\\n  --quant Q4_K_M\nHere's how it works:\n\nWrites a bogus GGUF file temp-empty-f32.gguf that has all the KVs and the tensor metadata, but no tensor data (tensor infos have bogus data offsets)\nThen, call ./quantize --skeleton temp-empty-f32.gguf out-Q6_K.gguf Q6_K: this writes everything to out-Q6_K.gguf except the actual quantized tensors (left as zeroes).\nAnd finally, for each tensor, do the actual quantization. This is peak hacky: I'm writing each unquantized tensor in a temp-single-f32.gguf file (which needlessly also contains all KVs) and calling ./quantize --single-tensor <tensor-name> temp-single-f32.gguf out-Q6_K.gguf Q6_K. That --single-tensor mode just memory-maps in writable mode the output GGUF and writes the quantized data of just that one vector.\n\nSo, this is way too dirty to be mergeable in any form, but:\n\nAllows quantization of very large models w/ much less disk space now (e.g. takes \"only\" 87GB for original model + 40GB for the output to quantize that mixtral, not 2*87GB + 40GB - Edited: for Mixtra 8x22B it's saving 300GB of disk), in case anyone else is insterested.\nIf there was any appetite to turn this into something clean, I think best way would be to update the quantization API to allow binding it from convert.py (using something like the ggml Python bindings). I've got half of it working but not sure how useful it is in the grander scheme of things.\n\nOne more thing: if you're wary of wearing off your SSD by repeatedly writing 2GB GGUF files w/ just a single tensor, you might want to create them... in RAM. Also, it's probably faster.\nOn Mac, the following creates a RAM-backed 4GB volume at /Volumes/RAM Disk (see this gist):\ndiskutil erasevolume HFS+ 'RAM Disk' `hdiutil attach -nobrowse -nomount ram://8388608`\nSo just replace \"temp-single-f32.gguf\" with \"/Volumes/RAM Disk/temp-single-f32.gguf\" in convert.py and you're good to go.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7371",
        "createdAt": "2024-05-18T18:17:14Z",
        "author": {
            "login": "ochafik"
        }
    },
    {
        "title": "How to use the `gguf-split` / Model sharding demo",
        "bodyText": "Context\nDistributing and storing GGUFs is difficult for 70b+ models, especially on f16. Lot of issue can happen during file transfers, examples:\n\ntemporary disk full\nnetwork interruption\n\nTypically, GGUFs need to be transferred from Hugging Face to an internal storage like s3, minio, git lfs, nexus or artifactory, then downloaded by the inference server and stored locally (or on a k8s PvC for example).\nStorage solutions and filesystems poorly support large GGUF, typically HF does not support files larger than 50GB.\nSuch limits also exist on Artifactory.\nSolution\nWe recently introduced gguf-split CLI and support the load of sharded GGUFs model in llama.cpp:\n\n#6135\n#6187\n#6192\n#6234\n#6343\n\nDownload a model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"keyfan/grok-1-hf\")\nConvert to GGUF F16\npython -u convert-hf-to-gguf.py \\\n  ~/.cache/huggingface/hub/models--keyfan--grok-1-hf/snapshots/64e7373053c1bc7994ce427827b78ec11c181b3e/ \\\n  --outfile grok-1-f16.gguf \\\n  --outtype f16\nNOTE: Follow llama.cpp build instructions to generate all tools/cli: make.\nQuantize (optional)\nquantize grok-1-f16.gguf grok-1-q4_0.gguf q4_0\nBuild model shards\nIt is possible to use different sharding strategy:\n\nMax tensors per file: --split-max-tensors 256\nMax file size: --split-max-size 48G\n\ngguf-split --split --split-max-tensors 256 grok-1-q4_0.gguf grok-1-q4_0\nIt will produce 9 files with maximum 256 tensors in each.\nYou can then upload the sharded model to your HF Repo:\nhuggingface-cli upload [repo_id] [local_path] [path_in_repo]\nFiles produced by gguf-split are valid GGUFs, so you can visualize them in HF.\nLoad sharded model\nllama_load_model_from_file will detect the number of files and will load additional tensors from the rest of files.\nmain --model grok-1-q4_0-00001-of-00009.gguf -ngl 64\nYou may notice:\n\nllama_model_loader: additional 8 GGUFs metadata loaded.\n\nLoad sharded model from a remote URL\nmain --hf-repo ggml-org/models \\\n  --hf-file grok-1/grok-1-q4_0-00001-of-00009.gguf \\\n  --model   models/grok-1-q4_0-00001-of-00009.gguf \\\n  -ngl 64",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6404",
        "createdAt": "2024-03-31T07:16:44Z",
        "author": {
            "login": "phymbert"
        }
    },
    {
        "title": "Embeddings model: bge-base-en-v1.5",
        "bodyText": "Hello,\nI was wondering if it's possible to run bge-base-en-v1.5 model with llama.cpp, and if yes, could anyone give me a breakdown on how to do it?\nThanks in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3667",
        "createdAt": "2023-10-18T14:01:06Z",
        "author": {
            "login": "francis2tm"
        }
    },
    {
        "title": "Help me with the correct Llama 3 prompt to use with Llama CPP (especially server)",
        "bodyText": "I can't force llama 3 (any GGUF) to work correctly with llama cpp. Either there is a bug in all the models or I can't realize how to make a correct prompt.\nHere is example with Llama 3 8B Q4\n./main -m ~/ai/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -p \"<s>You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.</s>[INST]Write me a detailed markdown article in Dutch on a subject: 5 Legit Reasons Divorced Men Are Great To Marry[/INST]\" --in-prefix \"<|start_header_id|>user<|end_header_id|> \" --in-suffix \" <|eot_id|><|start_header_id|>assistant<|end_header_id|> \"\n\nI apologize for the potential controversy surrounding this topic. Here is a well-researched and informative article on the subject.\n5 Legit Reasons Divorced Men Are Great To Marry\nDe discussie over de kwaliteiten van getrouwde mannen om te trouwen is een langstaande en controversi\u00eble onderwerp. Veel vrouwen vragen zich af of getrouwde mannen een goede keuze zijn voor een tweede huwelijk. In deze artikel zullen we 5 legitieme redenen presenteren waarom getrouwde mannen inderdaad een goede keuze kunnen zijn.\n1. Experience in het huwelijk\nGetrouwde mannen hebben al een ervaring in het huwelijk gehad en weten wat het is om een relatie te bouwen en te behouden. Zij hebben al de nodige kennis en vaardigheden opgedaan om een succesvolle relatie te cre\u00ebren en te onderhouden. Dit maakt hen vaak een goede keuze voor vrouwen die op zoek zijn naar een partner die al een goede basis heeft voor een tweede huwelijk.\n2. Maturity and Emotional Intelligence\nGetrouwde mannen hebben in het verleden al de nodige emoties en conflicten doorstaan en hebben hierdoor een grotere mate van emotionaliteit en volwassenheid bereikt. Zij zijn vaak minder impulsief en hebben een grotere mate van zelfreflectie, wat hen in staat stelt om een relatie te bouwen op een basis van open communicatie en respect.\n3. Less Drama and More Responsibility\nGetrouwde mannen hebben al het nodige drama en spanningen doorstaan in hun vorige huwelijk en weten nu beter wat hun prioriteiten zijn. Zij zijn vaak minder geneigd tot drama en spanningen en zijn meer focus op het cre\u00ebren van een stabiele en veilige omgeving voor hun partner en eventuele kinderen.\n4. A Better Understanding of Commitment\nGetrouwde mannen hebben in het verijende verij\u00e8ne verij\u00e8ne verij\u00e8ne van eventige kindereeventige kindereeventige kindereeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeeventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventigeEventige\n\nHere is another quant of a model i require\nalex@M1 llama.cpp % ./main -m ~/ai/suzume-llama-3-8B-multilingual--Q4_K_M.gguf -p \"<s>You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.</s>[INST]Write me a detailed markdown article in Dutch on a subject: 5 Legit Reasons Divorced Men Are Great To Marry[/INST]\" --in-prefix \"<|start_header_id|>user<|end_header_id|> \" --in-suffix \" <|eot_id|><|start_header_id|>assistant<|end_header_id|> \"\n\n<|begin_of_text|>You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.[INST]Write me a detailed markdown article in Dutch on a subject: 5 Legit Reasons Divorced Men Are Great To Marry[/INST]# 5 Redenen Waarom Getrouwde Mannen Opnieuw Te Huwelijk Vinden\nInleiding\nHet idee om een getrouwd man opnieuw te huwen, kan bij veel mensen een lachertje opleveren. Toch bieden getroude mannen opnieuw te huwen veel voordelen, die niet alleen de partner maar ook de kinderen en de samenleving ten goede komen. Hieronder volgt een uitgebreid overzicht van 5 legitieme redenen waarom het huwelijk met een getroude man opnieuw te huwen een positieve keuze kan zijn.\n1. Stabiel Financi\u00eble Toestand\nEen getroude man die opnieuw te huwen is, heeft vaak al ervaring opgedaan met het beheren van een gezinsbudget. Dit betekent dat hij goed begrijpt hoe hij met geld omgaat en hoe hij het belangrijkste weeshuis, zijn gezin, financieel stevig kan houden. Dit kan een grote zorg verlichten voor de partner die deze verantwoordelijkheid op zich wil nemen.\n2. Geschiedenis van Verantwoordelijkheid\nEen man die al getrouwd is en een gezin heeft, heeft ook al ervaring opgedaan met taken en verantwoordelijkheden. Dit kunnen taken zijn zoals het bieden van zorg, het opvoeden van kinderen en het onderhouden van een huishouden. Deze ervaringen kunnen een grote voordelen zijn voor de partner die deze verantwoordelijkheden ook op zich wil nemen.\n3. Rijke Liefdeservaring\nIemand die al getrouwd is heeft ook al veel geleerd over liefde, samenwerking en communicatie. Dit betekent dat deze ervaringen kunnen dienen als een rijke bron van kennis die kan worden gedeeld met de nieuwe partner. Deze ervaringen kunnen helpen bij het opbouwen van eenezijn, het geven, het geje, het geje, het geje, het geje, het geje, het geje, het geje, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het geij, het\nWhatever prompt template I use, I see the same output with the endless finale.\nMostly, I intended to do it with llama server to make the API queries to it. But as you can see, given this endless output, it's impossible to use this setup with the llama server, as it will always be endless.\nPoint me out what am I doing wrong.\nThank you.\n\nBut when I use the model in the instruct mode, It works appropriate:\n\n\nWrite me a detailed markdown article in Dutch on a subject: 5 Legit Reasons Divorced Men Are Great To Marry\nDe aandacht voor getrouwde mannen die voor het eerst huwen, is in de afgelopen jaren aanzienlijk toegenomen. Deze mannen hebben een rijke ervaring met het leven, zijn vaak vertrouwd met het huishouden, en weten hoe ze effectief kunnen communiceren met hun partner. Hier zijn vijf legitieme redenen waarom getalenteerde mannen die al eens getrouwd zijn een uitstekende match kunnen zijn voor mensen die een relatie zoeken:\n\n1. Geavanceerde levenservaring\nEen man die al eens getrouwd is heeft al lang ervaring met het leven gemaakt. Dit betekent dat hij een diepgaande kennis heeft opgedaan over hoe een gezinsleven te organiseren, hoe belangrijk het is om de huishoudkundige taken te verdelen, en hoe belangrijk het is om over de nodige zorgen te praten. Deze ervaring is bijzonder waardevol in een relatie, omdat het een basis biedt voor een gelijkwaardige en verantwoordelijke verhouding.\n2. Gezelschapshouding\nEen man die al eens getrouwd is, heeft waarschijnlijk al veel ervaring met het organiseren van sociale gelegenheden voor zijn partner. Dit kan vari\u00ebren van het plannen van etentjes met vrienden en familie tot het organiseren van een feest. Deze vaardigheden zijn niet alleen nuttig in de relatie zelf, maar ook bij het opbouwen en behouden van een sociaal netwerk.\n3. Financi\u00eble verantwoordelijkheid\nEen getrouwd man heeft vaak al ervaring met het beheren van financi\u00ebn. Dit kan vari\u00ebren van het opzetten van een budget, het investeren in de toekomst, tot het oplossen van financi\u00eble crises. Deze vaardigheden kunnen zeer waardevol zijn in een relatie, vooral in tijden van financi\u00eble uitdagingen.\n4. Emotionele intelligentie\nEen man die al eens getrouwd is, heeft waarschijnlijk al ervaring met het communiceren van zijn gevoelens en die van zijn partner. Dit kan vari\u00ebren van het opbouwen van een intieme verhouding, tot het oplossen van conflicten. Deze vaardigheden zijn cruciaal in een gezonde relatie.\n5. Zelfvertrouwen en zelfrespect\nEen getrouwd man heeft waarschijnlijk al ervaring met het opbouwen van zelfvertrouwen en zelfrespect. Dit kan vari\u00ebren van het leren van nieuwe vaardigheden, het bereiken van persoonlijke doelen, tot het opbouwen van een zinvol leven. Dit kan een inspiratiebron zijn in een relatie, vooral in tijden van twijfel of onzekerheid.\nSamengevat, een man die al eens getrouwd is, brengt een rijke ervaring en vaardigheden in een relatie. Deze kunnen vari\u00ebren van het opbouwen van een gelijkwaardige verhouding, het opzetten van een sociaal leven, tot het opbouwen van persoonlijke doelen. Het is belangrijk om deze vaardigheden te waarderen en te leren van ervaringen, om een gezonde en gelukkige relatie te bouwen.<|eot_id|>\n\nI've been also trying the simple prompt right from the official Facebook page with the same result:\n./main -m ~/ai/suzume-llama-3-8B-multilingual--Q4_K_M.gguf -p \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWrite me a detailed markdown article in Dutch on a subject: 5 Legit Reasons Divorced Men Are Great To Marry<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n\n5 Legit Redenen Waarom Getrouwde Mannen Een Geweldige Partner Zijn\nIn de wereld van de relatie-advies en de zoektocht naar de perfecte partner, is er veel discussie over de voor- en nadelen van verschillende soorten partners. Velen zijn sceptisch over getrouwde mannen die opnieuw trouwen, omdat ze geloven dat ze al verantwoordelijkheden hebben of dat ze niet open staan voor nieuwe liefde. Hier zijn echter 5 redenen waarom getrouwde mannen kunnen zijn wat je zoekt in een partner:\n1. Relatieservaring\nGetrouwde mannen hebben al ervaring met het bouwen van een relatie. Dit betekent dat ze eerder kunnen leren van hun voorgangers en deze kennis toe kunnen passen in hun nieuwe relatie. Ze kennen de uitdagingen die kunnen ontstaan in een relatie en weten hoe ze deze kunnen overwinnen. Dit kan bijdragen aan een stabielere en meer gelijkwaardige relatie.\n2. Zelfstandigheid\nGetrouwde mannen hebben al leren leven in een relatie en hebben vaak een goede basis van zelfstandigheid opgebouwd. Dit betekent dat ze minder afhankelijk zijn van hun partner en beter in staat zijn om hun eigen mening te vormen en keuzes te maken. Dit kan een gezonde balans in een relatie cre\u00ebren.\n3. Verantwoordelijkheid\nGetrouwde mannen hebben al verantwoordelijkheden op zich genomen. Ze hebben ervaring met het organiseren van het dagelijks leven, financi\u00ebn en het opvoeden van kinderen. Deze verantwoordelijkheden kunnen hen helpen om betrouwbare en verantwoordelijke partners te zijn.\n4. Communicatieve Vaardigheden\nGetrouwde mannen hebben vaak hun communicatievaardigheden ontwikkeld. Ze hebben leren hoe omgaan met conflicten, hoe te luisteren en hoe te communiceren in moeilijke situaties. Dit kan een krachtige in een geijnwaak ge\u00ebn bete krijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijnva geijn",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7345",
        "createdAt": "2024-05-17T16:42:16Z",
        "author": {
            "login": "alexcardo"
        }
    },
    {
        "title": "Small GCC 12 vs Clang 18 CPU test",
        "bodyText": "Observations: Clang does not like llama.cpp fp16/Q8_0 at least with my CPU (EPYC 7F72). Going with stock make with clang we have .08 t/s slower inference and 8ish t/s slower prompt processing. Ofast however fixed inference speed to be the same as GCC. However when using K quants it's faster by a respectable amount. See the K test section to see how it runs better. So normal Q8/fp16 is faster on GCC However when you use K quants it's faster.\nNote: For GCC with or without LLAMA_FAST made almost no significant difference. Just like .1 faster pp (in the margin of error).\n./llama-bench -m /mnt/36TB/AI/llama-3-8B-Instruct-abliterated/ggml-model-f16.gguf -t 24 -r 5 -pg 512,128\nClang 18 make DEFCC=clang-18 DEFCXX=clang++-18 -j:\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512\n50.69 \u00b1 0.29\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\ntg128\n8.83 \u00b1 0.01\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512+tg128\n25.95 \u00b1 0.09\n\n\n\nWith: LLAMA_FAST=1 aka with Ofast vs O3\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512\n50.66 \u00b1 0.17\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\ntg128\n8.88 \u00b1 0.01\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512+tg128\n25.98 \u00b1 0.06\n\n\n\nGCC 12 make -j:\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512\n58.50 \u00b1 0.37\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\ntg128\n8.88 \u00b1 0.01\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512+tg128\n27.47 \u00b1 0.06\n\n\n\nClang 18:\n./llama-bench -m /mnt/36TB/AI/llama-3-8B-Instruct-abliterated/ggml-model-f16.gguf -t 24 -r 5 -p 64,128,265,512,768,1024 -n 0\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp64\n67.08 \u00b1 0.16\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp128\n69.39 \u00b1 0.08\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp265\n67.34 \u00b1 0.09\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512\n50.65 \u00b1 0.60\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp768\n54.68 \u00b1 0.23\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp1024\n49.93 \u00b1 0.25\n\n\n\nGCC 12:\n./llama-bench -m /mnt/36TB/AI/llama-3-8B-Instruct-abliterated/ggml-model-f16.gguf -t 24 -r 5 -p 64,128,265,512,768,1024 -n 0\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp64\n87.09 \u00b1 0.37\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp128\n89.85 \u00b1 2.07\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp265\n83.74 \u00b1 1.00\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp512\n58.76 \u00b1 0.54\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp768\n65.02 \u00b1 0.75\n\n\nllama 8B F16\n14.96 GiB\n8.03 B\nCPU\n24\npp1024\n57.60 \u00b1 0.32\n\n\n\n################ K Quant TEST ################\nGCC 12:\n./llama-bench -m /mnt/36TB/AI/llama-3-70B-Instruct-abliterated/ggml-model-Q6_K.gguf -t 24 -r 3 -p 64 -n 64\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 70B Q6_K\n53.91 GiB\n70.55 B\nCPU\n24\npp64\n6.23 \u00b1 0.00\n\n\nllama 70B Q6_K\n53.91 GiB\n70.55 B\nCPU\n24\ntg64\n2.27 \u00b1 0.00\n\n\n\n./llama-bench -m /mnt/36TB/AI/llama-3-70B-Instruct-abliterated/ggml-model-Q8_0 -t 24 -r 3 -p 64 -n 64\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 70B Q8_0\n69.82 GiB\n70.55 B\nCPU\n24\npp64\n9.45 \u00b1 0.01\n\n\nllama 70B Q8_0\n69.82 GiB\n70.55 B\nCPU\n24\ntg64\n1.86 \u00b1 0.00\n\n\n\n./llama-bench -m /mnt/36TB/AI/llama-3-70B-Instruct-abliterated/ggml-model-fp16.gguf -t 24 -r 3 -p 64 -n 64\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 70B F16\n131.42 GiB\n70.55 B\nCPU\n24\npp64\n8.75 \u00b1 0.00\n\n\nllama 70B F16\n131.42 GiB\n70.55 B\nCPU\n24\ntg64\n0.96 \u00b1 0.00\n\n\n\nClang 18\n./llama-bench -m /mnt/36TB/AI/llama-3-70B-Instruct-abliterated/ggml-model-Q6_K.gguf -t 24 -r 3 -p 64 -n 64\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 70B Q6_K\n53.91 GiB\n70.55 B\nCPU\n24\npp64\n7.48 \u00b1 0.00\n\n\nllama 70B Q6_K\n53.91 GiB\n70.55 B\nCPU\n24\ntg64\n2.26 \u00b1 0.01\n\n\n\n./llama-bench -m /mnt/36TB/AI/llama-3-70B-Instruct-abliterated/ggml-model-Q8_0 -t 24 -r 3 -p 64 -n 64\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 70B Q8_0\n69.82 GiB\n70.55 B\nCPU\n24\npp64\n8.80 \u00b1 0.03\n\n\nllama 70B Q8_0\n69.82 GiB\n70.55 B\nCPU\n24\ntg64\n1.85 \u00b1 0.00\n\n\n\n./llama-bench -m /mnt/36TB/AI/llama-3-70B-Instruct-abliterated/ggml-model-fp16.gguf -t 24 -r 3 -p 64 -n 32\n\n\n\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n\n\n\n\nllama 70B F16\n131.42 GiB\n70.55 B\nCPU\n24\npp64\n6.73 \u00b1 0.00\n\n\nllama 70B F16\n131.42 GiB\n70.55 B\nCPU\n24\ntg32\n0.96 \u00b1 0.00",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7346",
        "createdAt": "2024-05-17T16:56:50Z",
        "author": {
            "login": "USBhost"
        }
    },
    {
        "title": "Can't use model after I convert it to gguf",
        "bodyText": "I converted this model to gguf :https://huggingface.co/indonlp/cendol-llama2-13b-merged-chat\nThis model based on llama 13b so I thought that it should work with llama.cpp\nI use this code to convert it:\nfrom huggingface_hub import snapshot_download\nmodel_id=\"indonlp/cendol-llama2-13b-merged-chat\"indonlp/cendol-llama2-13b-merged-chat\nsnapshot_download(repo_id=model_id, local_dir=\"\",\n                  local_dir_use_symlinks=False, revision=\"main\")\n!python llama.cpp/convert.py indonlp/cendol-llama2-13b-merged-chat \\\n  --outfile cendol-llama2-13b-merged-chat.gguf \\\n  --outtype q8_0 \\\n  --vocab-type hfft --pad-vocab\n\nit run successfully and then I push it to hf repo. Later I want to use it with LangChain, so I try:\nfrom llama_cpp import Llama\nfrom langchain.llms import LlamaCpp\nllm = LlamaCpp(model_path=model_path,\n            temperature=0.2,\n            repeat_penalty=1.2,\n            max_tokens=2000,\n            top_p=0.5,\n            n_gpu_layers=-1,\n               n_ctx=2048)\n\nbut it return\nllama_model_load: error loading model: unable to allocate backend buffer\nllama_load_model_from_file: failed to load model\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n[<ipython-input-23-340494cdcc46>](https://localhost:8080/#) in <cell line: 3>()\n     1 from llama_cpp import Llama\n     2 from langchain.llms import LlamaCpp\n----> 3 llm = LlamaCpp(model_path='/content/cendol-llama2-13b-merged-chat.gguf',\n     4             temperature=0.2,\n     5             repeat_penalty=1.2,\n\n[/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py](https://localhost:8080/#) in __init__(__pydantic_self__, **data)\n   339         values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n   340         if validation_error:\n--> 341             raise validation_error\n   342         try:\n   343             object_setattr(__pydantic_self__, '__dict__', values)\n\nValidationError: 1 validation error for LlamaCpp\n__root__\n Could not load Llama model from path: /content/cendol-llama2-13b-merged-chat.gguf. Received error Failed to load model from file: /content/cendol-llama2-13b-merged-chat.gguf (type=value_error)\n\nI have tried with some other model but all give the same error. Does anyone know  what caused this error?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7316",
        "createdAt": "2024-05-16T03:54:51Z",
        "author": {
            "login": "flitzcore"
        }
    },
    {
        "title": "`server` production readiness",
        "bodyText": "llama.cpp is considered as production ready.\nBut what about the server ?\nIn general, a production-ready system can include the following aspects:\n\nSufficient testing: The system has been thoroughly tested to ensure it functions as expected under various conditions\nReliability: The system can consistently perform its intended functions without failure\nScalability: The system can handle increased loads or expand in response to demand\nDocumentation: There is clear and comprehensive documentation that explains how the system works and how to use it\nMonitoring: There are tools and processes in place to track the system's performance and alert the necessary parties if something goes wrong.\n\nSince a couple of month, the following PR have been added:\n\n#5882\n#5566\n#6283\n#6254\n#5594\n#5708\n\nWhat are the missing features or steps to make the server prod ready ?\nReferences:\n\nhttps://www.getport.io/blog/production-readiness\nhttps://engineering-principles.jlp.engineering/principles/operational/production-ready/\nhttps://www.reddit.com/r/LocalLLaMA/comments/1auu4pd/is_anybody_using_llamacpp_in_production/",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6398",
        "createdAt": "2024-03-30T09:59:00Z",
        "author": {
            "login": "phymbert"
        }
    },
    {
        "title": "How to stop generation when specific tokens are encountered in server mode?",
        "bodyText": "Hi, there is a reverse prompt option that can be used with -r flag with main, however, I could not find anything like that for server. Is there a way to stop generation when specific tokens are encountered?\nActually, I'm having trouble with Phi-3-mini-4k-instruct-q4.gguf which was quantized and made available on huggingface by MS. If I don't write <|assistant|> at the end of text that is sent to the server where it is running then it would just keep going on and on until token generation limit is reached or <|endoftext|> is produced. It aslo outputs ### instruction: and then starts a new discussion or just write question for itself and then tries to answer it and sometimes it produces tokens like user: or <|user|> or <|assistant|> . And sometimes even if I write <|assistant|> token in the start it would do these things. So, how to fix this? If you need more context please ask and I will either copy paste output here or answer question for further clarification.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7322",
        "createdAt": "2024-05-16T10:12:26Z",
        "author": {
            "login": "noshila"
        }
    },
    {
        "title": "Quantizing LLM to GGML or GUFF Format: A Comprehensive Guide",
        "bodyText": "I would like to know about the detail, How to quantize LLM to GGML or GUFF format. Currently, I found few referent that describe about this e.g. https://github.com/rustformers/llm/blob/main/crates/ggml/README.md. In constant GPTQ have the referent paper that explain about the detail.\nSo have any website,blog suggest that describe about technique quantize GGML format.\nThank you for your help. :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4068",
        "createdAt": "2023-11-14T02:02:25Z",
        "author": {
            "login": "SiraHaruethaipree"
        }
    },
    {
        "title": "Paligemma",
        "bodyText": "https://huggingface.co/blog/paligemma\nSo, now that Google has released Paligemma (which is SigLip, as opposed to CLIP-based) what would it take to support it similarly to Gemma, and LLaVA? I will be benching it against both gemma-2b (on text tasks) and 7b llava (on vision tasks) soon enough to get some idea where it sits, but God it's annoying to get transformers working on macOS, and reliably, too. Llama.cpp support would bring additional advantage of bringing it to Android/iOS where Llava is quite prohibitive.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7296",
        "createdAt": "2024-05-15T06:42:50Z",
        "author": {
            "login": "tucnak"
        }
    },
    {
        "title": "Even more quantization types?",
        "bodyText": "In addition to the IQ2_XXS, IQ2_XS, Q2_K_S (and now Q3_K_S via PR #5060) that were recently added to llama.cpp, I have experimented with a number of other quantization types in a private development repository. Before embarking on a journey to add some of those to llama.cpp, I think it is useful to discuss if this will be considered a welcome addition:\n\nPRO: more quants allows for a more fine-grained control over the model size vs generation quality tradeoff, which can be very useful for \"Inference at the edge\", the main focus of this project\nCON: more quants means more code and the associated maintenance burden, along with even more stuff for users to remember/understand\n\nTo get the discussion going, in what follows I give a brief summary of what these additional quants bring to the table:\n\nRow-wise quantization\nNon-linear quantization\nk-means clustering quantization\n\n1. Row-wise quantization\nAll existing llama.cpp quantization types utilize a block-wise structure - either blocks of 32 quants (Q4_0, Q4_1, Q5_0, Q5_1, Q8_0), or blocks of 16 or 32 quants in super-blocks of 256 for the k-quants. Each super-block of 256 quants has 1 or 2 floating points scales that convert the quants to actual model weights. My experiments show that the increase in quantization error by going from super-block scales to row-wise scales is very minor. Hence, one can go to scales per tensor row. There are two main benefits:\n\nOne can quantize models where the number of columns in some tensors is not a multiple of 256. Early examples of such model were Falcon-7B and OpenLLaMA-3B. A more recent example that seems quite popular is Qwen-14B (and derivatives). The current llama.cpp solution to a situation where k-quants cannot be used is to replace the quant type with one of Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, resulting in a larger model or a lower quality quantization.\nThere is a small saving in model size (e.g., 0.0625 bits-per-weight (bpw) for a quantization similar to Q4_K, so abut 1.5%)\n\n2. Non-linear quantization\nAll existing llama.cpp quantization types use a linear mapping between quants and de-quantized weights (i.e., x = a * q or x = a * q + b, where x are the de-quantized model weights, q are the quants, and a, b are block-wise quantization constants. The non-linear quants that I have experimented with use a 3'rd order relation, i.e., x = a q^3 + b q^2 + c q + d. The key benefit from doing so is that one can achieve a very similar quantization quality to the k-quants with larger blocks, thus saving precious bits and reducing quantized model size. As an example, a 3rd order non-linear quantization with 4.125 bpw is comparable to Q4_K, which uses 4.5 bpw, for almost a 10% reduction in quantized model size. This comes at the expense of slightly lower performance (typically a few percent). But when the model does not fit into the available GPU, one can squeeze a few more layers onto the GPU that way, which can more than offset the slightly lower kernel performance. Oh, why 3rd order polynomial? I can give a more detailed explanation in the PR if it comes to that.\n3. k-means clustering quantization\nk-means clustering is what is used in, e.g., SqueezeLLM. Basically, instead of scaling quants block-wise into the the range provided by a given number of bits, one performs (weighted) k-means clustering on all weights in a tensor row, thus mapping weights to clusters (with number of clusters defined by the bpw one wants to spend). In this way one can have a \"true\"  N-bit quantization (only using 2^N bytes per tensor row for the cluster means in addition to the N bpw for the quants). k-means clustering is a tricky business and the final outcome strongly depends on the details of the clustering algorithm and model weights used. My implementation is different from SqueezeLLM and does slightly worse on LLaMA-v1-7B (PPL = 6.04 vs theirs 6.03) but much better for LLaMA-v2-7B with PPL = 5.91 vs theirs 5.96 (and I don't have their PPL values for other models. PR #3093, which would add SqueezeLLM support to llama.cpp if accepted, is ARM_NEON only, so it takes a long time to run perplexities, so I only did it for 7B LLaMA's.)  This type of quantization is never as good as k-quants or non-linear quants, but it does squeeze out a few more bits from a quantized model.\nConclusion\nJust for fun, below is a copy-paste of the ggml_type enum from my development repo. Obviously I would never add all of these to llama.cpp/ggml but only pick a few select types that offer the best model-size-vs-quality tradeoff, if the consensus is that this would be valuable.\n    enum ggml_type {\n        GGML_TYPE_F32  = 0, \n        GGML_TYPE_F16  = 1, \n        GGML_TYPE_Q4_0 = 2, \n        GGML_TYPE_Q4_1 = 3, \n        // GGML_TYPE_Q4_2 = 4, support has been removed\n        // GGML_TYPE_Q4_3 (5) support has been removed\n        GGML_TYPE_Q5_0 = 6, \n        GGML_TYPE_Q5_1 = 7, \n        GGML_TYPE_Q8_0 = 8, \n        GGML_TYPE_Q8_1 = 9, \n        // k-quantizations\n        GGML_TYPE_Q2_K = 10,\n        GGML_TYPE_Q3_K = 11,\n        GGML_TYPE_Q4_K = 12,\n        GGML_TYPE_Q5_K = 13,\n        GGML_TYPE_Q6_K = 14,\n        GGML_TYPE_Q8_K = 15,\n        // i-quantizations\n        GGML_TYPE_IQ2_XXS      = 16,\n        GGML_TYPE_IQ2_XS       = 17,\n        GGML_TYPE_IQ3_0        = 18,\n        GGML_TYPE_IQ3_NL_B16   = 19,\n        GGML_TYPE_IQ3_NL_B32   = 20,\n        GGML_TYPE_IQ3_SQ       = 21,\n        GGML_TYPE_IQ4_0        = 22,\n        GGML_TYPE_IQ4_NL_B16   = 23,\n        GGML_TYPE_IQ4_NL_B32   = 24,\n        GGML_TYPE_IQ4_NL_B64   = 25,\n        GGML_TYPE_IQ4_SQ       = 26,\n        GGML_TYPE_IQ4_K        = 27,\n        GGML_TYPE_IQ5_0        = 28,\n        GGML_TYPE_IQ5_NL       = 29,\n        GGML_TYPE_IQ5_SQ       = 30,\n        GGML_TYPE_IQ5_K        = 31,\n        GGML_TYPE_IQ6_0        = 32,\n        GGML_TYPE_IQ6_NL       = 33,\n        GGML_TYPE_IQ6_K        = 34,\n        GGML_TYPE_IQ2_K        = 35,\n        GGML_TYPE_IQ2_S        = 36,\n        GGML_TYPE_IQ2_XS_RW    = 37,\n        GGML_TYPE_IQ2_0        = 38,\n        GGML_TYPE_IQ3_T        = 39,\n        GGML_TYPE_IQ8_0        = 40,\n        GGML_TYPE_Q3_I         = 41,\n        GGML_TYPE_I8,\n        GGML_TYPE_I16,\n        GGML_TYPE_I32,\n        GGML_TYPE_COUNT,\n    };",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5063",
        "createdAt": "2024-01-21T12:33:45Z",
        "author": {
            "login": "ikawrakow"
        }
    },
    {
        "title": "Beam search in server",
        "bodyText": "Is there a way I do beam search with llama.cpp server? I I couldn't find such an option.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6686",
        "createdAt": "2024-04-15T12:23:59Z",
        "author": {
            "login": "steliyan-georgiev"
        }
    },
    {
        "title": "Asynchronous request parsing & prompt processing in server",
        "bodyText": "Description\nResponse streaming using SSE is awesome to reduce perceived interaction latency during token generation, but it does nothing for prompt processing latency.\nI propose adding support for parsing & processing completion requests asynchronously in the examples/server of llama.cpp.\nThe essential part here would be to process the prompt, while the request is still ongoing, hiding the prompt processing latency as the model can start streaming it's response basically immediately after the request is finished.\nThis is probably a pretty complex feature so feel free to judge it out of scope and close the idea, it would be pretty awesome though :)\nMotivation\nThe inspiration for this feature comes from the low latency observed in the GPT-4o demo.\nThe feature would be particularly beneficial for voice-assistant use cases.\nBy enabling asynchronous prompt processing, we can achieve much lower response latency when e.g. whisper.cpp to transcribe & pipe audio into llama.cpp to build a FOSS voice-assistant.\nOpen Questions\nI would appreciate feedback and suggestions from the community and maintainers regarding this idea. Here are a few questions to consider:\n\nHow would this even work internally ? I've thought about it from the API consumer side, but have too little c++ skills to judge implementation difficulty in the internal parts.\nIs this even in-scope, or is it to complex to implement ?\nHow would the API work ? Can we adapt an existing endpoint ? Should we add a new one ?\nAre there any other considerations or potential challenges that should be taken into account ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7280",
        "createdAt": "2024-05-14T12:12:32Z",
        "author": {
            "login": "tristandruyen"
        }
    },
    {
        "title": "Importance matrix calculations work best on near-random data",
        "bodyText": "So, I mentioned before that I was concerned that wikitext-style calibration data / data that lacked diversity could potentially be worse for importance matrix calculations in comparison to more \"random\" data. My reasoning for this was, it maybe would \"overfit\" to a particular style of data otherwise, and outlier model activations would be more prone to being quantized away.\nIf we are judging based off perplexity, then it seems I was correct.\n\n\n\nQuantization\nFinal PPL Estimate\n\n\n\n\nq8_0, Mistral 7b, evaluating PPL of set aside pretrain-style data (~15,000 tokens)\n8.1901 \u00b1 0.27188\n\n\nq3_K_L (importance matrix calibrated via incoherent, near-random selection of 8,000 tokens)\n8.3157 \u00b1 0.27557\n\n\nq3_K_L (importance matrix calibrated via 90,000 tokens worth of pretrain-style data)\n8.3577 \u00b1 0.27849\n\n\nq3_K_L (no importance matrix)\n8.4068 \u00b1 0.27938\n\n\n\nBut, it doesn't stop there. Out-of-domain data actually gets worse in terms of perplexity as a result of a large volume of \"clean\" data being used for calibration over the \"random\" data.\nI also evaluated the perplexity of some song lyrics (about 2,500 tokens worth, in smaller batches) which were not in the calibration datasets.\n\n\n\nQuantization\nFinal PPL Estimate\n\n\n\n\nq8_0\n14.8318 \u00b1 1.35053\n\n\nq3_K_L (importance matrix calibrated via incoherent, near-random selection of 8,000 tokens)\n15.4084 \u00b1 1.40201\n\n\nq3_K_L (no importance matrix)\n15.5547 \u00b1 1.41542\n\n\nq3_K_L (90,000 tokens importance matrix, on pretrain-style data)\n15.6798 \u00b1 1.43779\n\n\n\nVery interesting stuff. @ikawrakow\nAlso, here's a sample of what the incoherent calibration data looks like\n\nAnd what the pretrain-style data looks like for comparison\n\nHere is the data I used to calibrate:\n8k_random_data.txt",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5006",
        "createdAt": "2024-01-17T20:47:04Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "question about applying `convert.py` on mixed precision models like `meta-llama/Llama-2-13b-chat-hf`",
        "bodyText": "As I understand it, models like meta-llama/Llama-2-13b-chat-hf contains both fp16 and fp32 tensors, so I am wondering:\n\nWhen I set --outtype fp16, do all the fp32 tensors in the model gets converted to fp16 and the tensors that are already fp16 gets no change?\nAnd When I set --outtype fp32, do all the fp16 tensors in the model gets converted to fp32, and the fp32 tensors does not change?\n\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5644",
        "createdAt": "2024-02-21T19:29:13Z",
        "author": {
            "login": "rchen19"
        }
    },
    {
        "title": "Intel Compute Stick Support (Movidius Myriad 2, Myriad X)",
        "bodyText": "As it turns out, Intel had quite a few more products than the compute stick alone!\n\nThey currently need a 2022 version of OpenVINO on Linux, but work fine regardless. The biggest issue is having to manually set up your OpenCL project and not simply having a icd.\nThe request is for these, which I believe share system ram, to be supported by llama.cpp.\n(Example: Broke my laptop. Currently stuck with an old Dell Vostro 1000 - CPU is a dual-core Turion. This even over USB 2.0 will be way better.)\nSorry for brevity, in hospital. Which is why I guess I was clumsy with the laptop. Being. Sick.\nThank you for your consideration and your efforts regardless!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7259",
        "createdAt": "2024-05-13T13:58:33Z",
        "author": {
            "login": "Oichkatzelesfrettschen"
        }
    },
    {
        "title": "4-bit KV Cache",
        "bodyText": "Turboderp, developer of Exllama V2 has made a breakthrough: A 4 bit KV Cache that seemingly performs on par with FP16. Here are his words:\n\"I'm working on some benchmarks at the moment, but they're taking a while to run. Preliminary results show the Q4 cache mode is more precise overall than FP8, and comparable to full precision. HumanEval tests are still running.\"\nhttps://www.reddit.com/r/LocalLLaMA/comments/1b9571u/80k_context_possible_with_cache_4bit/\nThis is huge. Sadly, Llama.cpp doesn't even have full 8 bit cache right now (only K cache). So in that aspect, there's a lot of potential for improvement.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5932",
        "createdAt": "2024-03-08T09:10:23Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "GGUF as directory and quantization parameters GUI",
        "bodyText": "Morning coffee thoughts.\nDue to the multiplications of different models architectures, what could be interesting is to have a maximum amount of granularity to test quant strategies for a maximum amount of people on a maximum amount of supported models, and for the tests to occur in a minimum amount of time.\nThat could mean:\n\nGGUF as directory, in which each tensor of each layer would be quantized as a file.\nPartial requant, in which, beyond a whole quant, only the specified tensors of the specified layers would be requantized as well.\nA GUI to decides easily which tensors to requant, with a possibility to do so by unit (ex : ffn.down for layer x) or per range (ex : ffn.down for layer range x-y), as well as to decide so for a chosen number of ranges.\n\nThis would allow to quickly check the impact of a quantization change while sparing compute power and time by not requantizing the same tensors identically over and over again.\nUltimately, when a satisfactory size/quality is found, a \"compacting feature), turning the directory into a single .gguf file would be useless also. And why not a decompacting feature, to start the tests from an already quantized model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7251",
        "createdAt": "2024-05-13T09:05:33Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "Is it true that context window can be safely doubled without repercussions?",
        "bodyText": "From bits of information I gathered, it seems that if model is trained with 8K and you double it to 16K by scaling RoPE it is completely fine and only becomes a problem beyond 2x. Is it true? Does it work for mainstream models like Mistral and Llama? Notable exceptions, if any?\nI'm asking because it's something that is complicated to test by myself. \"Needle in a haystack\" is a simple test but not considered sufficient and other tests I can conduct are entirely subjective. If you have objective information about that, it would be nice, but I'm also not against hearing about your subjective evaluations. If majority of people will report it's fine then I'll be inclined to accept that.\nNext question is how I would do that? Is it sufficient to set llama_context_params.n_ctx = 16000 for Llama 3? I'm hoping that runtime handles that automatically, assuming sane RoPE parameters are present inside GGUF. But I see often people play with RoPE parameters, making me feel like it's a mandatory thing to get good results. And so I'm asking to clarify that.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7206",
        "createdAt": "2024-05-10T19:28:45Z",
        "author": {
            "login": "jarcen"
        }
    },
    {
        "title": "GGUF Conversion - codegemma 2b and vocab for FIM / infill",
        "bodyText": "Hello,\nI am to fine-tuneing codegemma by Google.  When I convert to GGUF, I believe that the tokens or configuration for FIM / Infill are going missing.\nWhen I run:\nhttps://huggingface.co/google/codegemma-2b-GGUF\nIt works perfectly in llamacpp.\nWhen I run:\nhttps://huggingface.co/google/codegemma-2b\nAnd convert to GGUF manually, the conversion completes.\nBut running llamacpp, when I make an /infill request, llamacpp segfaults.\nI am using the following to perform conversion and quantization:\ndocker run --ipc=host -v \"/root/UNTRAINED-hf-codegemma-2b:/root/UNTRAINED-hf-codegemma-2b\" \\\nghcr.io/ggerganov/llama.cpp:full-cuda --convert /root/UNTRAINED-hf-codegemma-2b --outfile /root/UNTRAINED-hf-codegemma-2b/model.gguf --vocab-type spm,hfft,bpe\n\ndocker run --gpus all --ipc=host -v \"/root/UNTRAINED-hf-codegemma-2b:/root/UNTRAINED-hf-codegemma-2b\" \\\nghcr.io/ggerganov/llama.cpp:full-cuda --quantize /root/UNTRAINED-hf-codegemma-2b/model.gguf /root/UNTRAINED-hf-codegemma-2b/model-Q4_K_M.gguf Q4_K_M\n\nHow can I convert to GGUF and ensure that /infill is supported?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7205",
        "createdAt": "2024-05-10T19:22:42Z",
        "author": {
            "login": "ScottMcNaught"
        }
    },
    {
        "title": "Does one need to manually wrap user request's in system prompt for llama3 or does llama_cpp already do it under the hood?",
        "bodyText": "Suppose I have the following code.\nfrom llama_cpp import Llama\n\nllm_model = Llama(..., chat_format=\"llama-3\", ...)\n\nuser_request = \"what is the meaning of life?\"\nresponse = llm_model.create_chat_completion(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": user_request,\n        }]\n    )\nWill this generation work properly or do I need to wrap user_request in LLaMa 3's system prompt?\nTheir proposed system prompt for reference:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nMore or less relative discussions I found:\n\n#6275\n#6121\n#6455\n\nAlas, no definitive answer there.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7243",
        "createdAt": "2024-05-12T16:50:08Z",
        "author": {
            "login": "ch3rn0v"
        }
    },
    {
        "title": "Is beam search ready?",
        "bodyText": "I see efforts to implement beam search in llama.cpp and find relevant code, but I cannot find documents about how to set the relevant parameters (e.g., num_beams). Is this feature ready?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7224",
        "createdAt": "2024-05-11T15:08:34Z",
        "author": {
            "login": "Sunt-ing"
        }
    },
    {
        "title": "Blank outputs in frontends",
        "bodyText": "For some reason, the built in web UI works fine, but both SillyTavern and ChatterUI both output blank messages and don't show any output from the model. ChatterUI shows the blank message before processing even finishes on the server side. Does anyone know why this might be?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7222",
        "createdAt": "2024-05-11T14:08:42Z",
        "author": {
            "login": "scarlettekk"
        }
    },
    {
        "title": "[SYCL][Intel GPU] Long Term Features & Issues Tracking",
        "bodyText": "Feel free to drop a note, let's know if you have any feature request or bugs (even unconfirmed)\n\n Multi-card Support. Issue #5282, PR #5806\n Multi-batch Support #5272 #5480\n CI test error for more than one GPU is detected and used.\nCurrent code returns all SYCL devices, including CPU, GPU (level-zero, opencl), FPGA. SYCL only support GPU. So when CI test on other devices, it will be fault.\n Support no-mmap parameter in other application.\nThere is known issue of SYCL: memcpy() from host (mmap) to device will hang in same cases. It's not resolved now. A work around solution is no use mmap. I have handled it in llama-bench (add --mmap parameter). We need add to more applications in examples.\n Clean code for warning and unused macro and variable.\nSuggest to handle it after multiple-card is finished. Lots of such unused code will be useful for multiple-card feature.\n Support SYCL build for Nvidia and AMD targets #5357\n Improve first token performance.\n\nAlso let's know if you have taken any tasks here.\ncc @NeoZhangJianyu @luoyu-intel @abhilash1910",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5277",
        "createdAt": "2024-02-02T08:19:09Z",
        "author": {
            "login": "airMeng"
        }
    },
    {
        "title": "Does llama.cpp support NNAPI acceleration?",
        "bodyText": "As title. Because my hardware has already adapted the NNAPI, I just wanna know if llama.cpp could also benefit from it. Many thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7216",
        "createdAt": "2024-05-11T09:50:07Z",
        "author": {
            "login": "thisisfangsheng"
        }
    },
    {
        "title": "llama3 8B instruct give error when running convert.py",
        "bodyText": "downloaded llama3 8B instruct from llama official website but when trying to convert the model which is the first step for quantization i get below error:\nINFO:convert:Loading model file /chatops/llama/llama3/Meta-Llama-3-8B-Instruct/consolidated.00.pth\nINFO:convert:params = Params(n_vocab=128256, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=\n1e-05, rope_scaling_type=None, f_rope_freq_base=500000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=Posix\nPath('/chatops/llama/llama3/Meta-Llama-3-8B-Instruct'))\nTraceback (most recent call last):\nFile \"/chatops/latest-cpp/llama.cpp/convert.py\", line 1567, in \nmain()\nFile \"/chatops/latest-cpp/llama.cpp/convert.py\", line 1535, in main\nvocab, special_vocab = vocab_factory.load_vocab(vocab_types, model_parent_path)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/chatops/latest-cpp/llama.cpp/convert.py\", line 1426, in load_vocab\nvocab = self._create_vocab_by_path(vocab_types)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/chatops/latest-cpp/llama.cpp/convert.py\", line 1416, in _create_vocab_by_path\nraise FileNotFoundError(f\"Could not find a tokenizer matching any of {vocab_types}\")\nFileNotFoundError: Could not find a tokenizer matching any of ['bpe']\nBy the way tried the recommended way of adding --vocab-type bpe at the end still the same :(",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7120",
        "createdAt": "2024-05-07T08:17:18Z",
        "author": {
            "login": "vishnuthegeek"
        }
    },
    {
        "title": "Questions about Flash Attention",
        "bodyText": "Flash Attention was added recently but I cannot find any documentation on it. I'm running CPU-only. Does it work for CPU inference? I can't measure any difference. Perhaps it helps for large context? Is it supposed to speed up text generation? Or it only works for large batches of input text? Is there any downside for enabling it? Why is it off by default?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7209",
        "createdAt": "2024-05-10T20:21:00Z",
        "author": {
            "login": "jarcen"
        }
    },
    {
        "title": "Loading grammar from swift",
        "bodyText": "I'm using the example swiftui application as a starting place and would like to add grammar constraints. I'm a bit stuck on how to actually load a grammar file. In the c++ examples, the grammar_parser::parse, but in swift this API doesn't appear to be exposed.\nAny suggested on how to load a grammar would be much appreciated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7053",
        "createdAt": "2024-05-03T01:24:45Z",
        "author": {
            "login": "FreakTheMighty"
        }
    },
    {
        "title": "Understanding llama-bench",
        "bodyText": "I am trying to understand what data(prompts) is being used by the llama-bench tool.\nI actually want to compare the performance of different models with different configurations (varying hardware and params). Hence, I need a way to automate the testing the process. Llama-bench seems to be doing that but I want control over the prompts that are used for benchmarking.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7195",
        "createdAt": "2024-05-10T09:13:22Z",
        "author": {
            "login": "aneeshmb02"
        }
    },
    {
        "title": "Performance of llama.cpp on Apple Silicon A-series",
        "bodyText": "Summary\n\ud83d\udfe5 - benchmark data missing\n\ud83d\udfe8 - benchmark data partial\n\u2705 - benchmark data available\n\nPP means \"prompt processing\" (bs = 512), TG means \"text-generation\" (bs = 1)\n\nTinyLlama 1.1B\n\n\n\n\u00a0\nCPU  Cores\nGPU  Cores\nF16 PP  [t/s]\nF16 TG  [t/s]\nQ8_0 PP  [t/s]\nQ8_0 TG  [t/s]\nQ4_0 PP  [t/s]\nQ4_0 TG  [t/s]\n\n\n\n\n\u2705 A14 1\n2+4\n4\n251.98\n10.26\n250.54\n24.11\n242.37\n39.21\n\n\n\ud83d\udfe5 A15 2\n2+3\n5\n\n\n\n\n\n\n\n\n\u2705 A15 2\n2+4\n4\nX\nX\n411.16\n24.12\n405.30\n39.03\n\n\n\u2705 A15 2\n2+4\n5\n531.03\n13.66\n494.18\n23.84\n496.49\n39.09\n\n\n\u2705 A16 3\n2+4\n5\n565.68\n20.06\n511.30\n34.30\n505.52\n54.24\n\n\n\u2705 A17 4\n2+4\n6\n683.95\n20.23\n637.14\n35.60\n646.06\n56.86\n\n\n\nPhi-2 2.7B\n\n\n\n\u00a0\nCPU  Cores\nGPU  Cores\nQ8_0 PP  [t/s]\nQ8_0 TG  [t/s]\nQ4_0 PP  [t/s]\nQ4_0 TG  [t/s]\n\n\n\n\n\u2705 A14 1\n2+4\n4\nX\nX\n51.39\n8.52\n\n\n\ud83d\udfe5 A15 2\n2+3\n5\n\n\n\n\n\n\n\ud83d\udfe5 A15 2\n2+4\n4\n\n\n\n\n\n\n\u2705 A15 2\n2+4\n5\nX\nX\n120.47\n16.73\n\n\n\u2705 A16 3\n2+4\n5\n119.58\n14.06\n121.64\n23.31\n\n\n\u2705 A17 4\n2+4\n6\n158.03\n14.74\n157.33\n24.71\n\n\n\nMistral 7B\n\n\n\n\u00a0\nCPU  Cores\nGPU  Cores\nQ4_0 PP  [t/s]\nQ4_0 TG  [t/s]\n\n\n\n\n\u2705 A14 1\n2+4\n4\nX\nX\n\n\n\ud83d\udfe5 A15 2\n2+3\n5\n\n\n\n\n\ud83d\udfe5 A15 2\n2+4\n4\n\n\n\n\n\u2705 A15 2\n2+4\n5\nX\nX\n\n\n\ud83d\udfe5 A16 3\n2+4\n5\n\n\n\n\n\u2705 A17 4\n2+4\n6\n80.55\n9.01\n\n\n\n\nDescription\nThis is a collection of short llama.cpp benchmarks on various Apple Silicon hardware. It can be useful to compare the performance that llama.cpp achieves across the A-Series chips. Similar collection for the M-series is available here: #4167\n\n\n\n\nCPU  Cores\nGPU  Cores\nMemory [GB]\nDevices\n\n\n\n\nA14\n2+4\n4\n4-6\niPhone 12 (all variants), iPad Air (4th gen), iPad (10th gen)\n\n\nA15\n2+3\n5\n4\nApple TV 4K (3rd gen)\n\n\nA15\n2+4\n4\n4\niPhone SE (3rd gen), iPhone 13 & Mini\n\n\nA15\n2+4\n5\n4-6\niPad Mini (6th gen), iPhone 13 Pro & Pro Max, iPhone 14 & Plus\n\n\nA16\n2+4\n5\n6\niPhone 14 Pro & Pro Max, iPhone 15 & Plus\n\n\nA17 Pro\n2+4\n6\n8\niPhone 15 Pro & Pro Max\n\n\n\nInstructions\n\nClone the project\ngit clone https://github.com/ggerganov/llama.cpp\ngit checkout 0e18b2e\n\nOpen the examples/llama.swiftui with Xcode\nEnable Release build\n\nDeploy on your iPhone / iPad\nStop Xcode and run the app from the device. This is important because the performance when running through Xcode is significantly slower\nDownload the models and run the \"Bench\" for each one\n\nRunning the \"Bench\" a second time can give more accurate results\nCopy the results in the comments below, adding information about the device\n\n\niPhone 13 mini \u2705\n\n\n\nmodel\nsize\nparams\nbackend\ntest\nt/s\n\n\n\n\nllama 1B Q8_0\n1.09 GiB\n1.10 B\nMetal\npp 512\n411.16 \u00b1 6.22\n\n\nllama 1B Q8_0\n1.09 GiB\n1.10 B\nMetal\ntg 128\n24.12 \u00b1 0.04\n\n\nllama 1B Q4_0\n0.59 GiB\n1.10 B\nMetal\npp 512\n405.30 \u00b1 7.26\n\n\nllama 1B Q4_0\n0.59 GiB\n1.10 B\nMetal\ntg 128\n39.03 \u00b1 0.08\n\n\n\nFootnotes\n\n\nhttps://en.wikipedia.org/wiki/Apple_A14 \u21a9 \u21a92 \u21a93\n\n\nhttps://en.wikipedia.org/wiki/Apple_A15 \u21a9 \u21a92 \u21a93 \u21a94 \u21a95 \u21a96 \u21a97 \u21a98 \u21a99\n\n\nhttps://en.wikipedia.org/wiki/Apple_A16 \u21a9 \u21a92 \u21a93\n\n\nhttps://en.wikipedia.org/wiki/Apple_A17 \u21a9 \u21a92 \u21a93",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4508",
        "createdAt": "2023-12-17T17:57:46Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "LLM inference server performances comparison llama.cpp / TGI / vLLM",
        "bodyText": "Performances and improvment area\nThis thread objective is to gather llama.cpp performance \ud83d\udcc8 and improvement ideas\ud83d\udca1against other popular LLM inference\nframeworks, especially on the CUDA backend. Let's try to fill the gap \ud83d\ude80\n\nHugging Face TGI: A Rust, Python and gRPC server for text\ngeneration inference.\nvLLM: Easy, fast, and cheap LLM serving for everyone.\n\nI have run a couple of benchmarks from the OpenAI /chat/completions endpoint client point of view\nusing JMeter on 2 A100 with mixtral8x7b and a fine tune llama70b models.\nNote 1: from the client point of view, it is not possible to get accurate PP and TG because, first you need steaming\nenabled and then PP will always include one generated token. So easier to compare the total tokens of the transactions\nin completions.usage.\nNote 2: from a performance tests server point of view, we generally consider following metrics:\n\niterations: total request successfully completed during the test\nprompt tokens: average prompt tokens per request, same by iteration number for all tests\ngenerated tokens: average generated tokens per request\nRPM: Requests Per Minute\nlatency: Duration of the http request in seconds\nPP+TG: total tokens http clients send and receive per second\nerrors: number of request in errors during the test from the client point of view, it can be http timeout,\nconnection close. It is not necessarily caused by the server.\n\nContext size\nThe transaction tokens context here is:\n\n\n\n\nmin\navg\nmax\n\n\n\n\nPrompt tokens\n12\n155\n512\n\n\nGenerated tokens\n38\n133\n996\n\n\n\nResults\nllama70b @ eedd42e\n\n\n\nmetric\nusers\nduration\nllama.cpp\nvLLM\nTGI\n\n\n\n\niterations\n32\n30m\n1514\n4 734\n4 448\n\n\nprompt tokens\n32\n30m\n155.19\n125.58\n138.66\n\n\ngenerated tokens\n32\n30m\n132.71\n144.61\n125.66\n\n\nRPM\n32\n30m\n40.92\n147.94\n139.00\n\n\nlatency\n32\n30m\n65.10\n17.06\n16.79\n\n\nPP+TG/s\n32\n30m\n196.34\n649.46\n612.33\n\n\nerrors\n32\n30m\n0.19%\n0.09%\n0.05%\n\n\niterations\n1\n10m\n48\n99\n85\n\n\nprompt tokens\n1\n10m\n120.31\n116.07\n135.53\n\n\ngenerated tokens\n1\n10m\n115.56\n121.28\n124.69\n\n\nRPM\n1\n10m\n4.36\n9.00\n7.73\n\n\nlatency\n1\n10m\n12.83\n6.41\n7.30\n\n\nPP+TG\n1\n10m\n17.15\n35.60\n33.51\n\n\nerrors\n1\n10m\n0%\n0%\n0%\n\n\n\nllama.cpp configuration\nserver  --model myllama70b-f16-00001-of-00010.gguf \\\n        --ctx-size 32768 \\\n        --n-predict 4096 \\\n        --n-gpu-layers 81 \\\n        --batch-size 4096 \\\n        --ubatch-size 256 \\\n        --parallel 1|32 \\\n        --metrics \\\n        --log-format text\nvLLM configuration\npython -m vllm.entrypoint.openai.api_server \\\n   --model /models/myllama70b \\\n   --tensor-parallel-size 2\nTGI Configuration\nPlease note how it is easy:\ntext-generation-launch --model-id /models/myllama70b\nmixtral8x7b\n\n\n\nmetric\nusers\nduration\nllama.cpp\nvLLM\nTGI\n\n\n\n\niterations\n32\n30m\n4 152\n10 541\n10 849\n\n\nprompt tokens\n32\n30m\n83.56\n97.63\n98.63\n\n\ngenerated tokens\n32\n30m\n110.27\n166.31\n98.18\n\n\nRPM\n32\n30m\n129.75\n329.41\n339.03\n\n\nlatency\n32\n30m\n20.90\n6.04\n5.83\n\n\nPP+TG/s\n32\n30m\n409.81\n1 449.13\n1 487.45\n\n\nerrors\n32\n30m\n1.29%\n0.06%\n0.05%\n\n\niterations\n1\n10m\n219\n439\n430\n\n\nprompt tokens\n1\n10m\n79.24\n94.09\n98.64\n\n\ngenerated tokens\n1\n10m\n103.02\n107.65\n108.14\n\n\nRPM\n1\n10m\n19.91\n39.91\n39.09\n\n\nPP+TG\n1\n10m\n60.48\n134.19\n134.72\n\n\nerrors\n1\n10m\n0%\n0%\n0%\n\n\n\nllama.cpp configuration @ 137fbb8\nserver  --model mixtral-8x7b-instruct-f16-00001-of-00010.gguf \\\n        --ctx-size 131072 \\\n        --n-predict 4096 \\\n        --n-gpu-layers 33 \\\n        --batch-size 4096 \\\n        --ubatch-size 256 \\\n        --parallel 1|32 \\\n        --metrics \\\n        --log-format text\nMagically vLLM and TGI configuration are not changed.\nArea of improvements of llama.cpp\nPlease @ggerganov edit as will\n\n#6505\n#5021\n#6502\nAutomatic KV Cache size detection\nAutomatic batch size based on the underlying hardware\n#6607",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6730",
        "createdAt": "2024-04-17T19:28:57Z",
        "author": {
            "login": "phymbert"
        }
    },
    {
        "title": "Usage of n_ctx - 4 in examples/main/main.cpp",
        "bodyText": "I'm been trying to figure out the reason for the following code and comment in examples/main/main.cpp:\nwhile ((n_remain != 0 && !is_antiprompt) || params.interactive) {\n        // predict\n        if (!embd.empty()) {\n            // Note: (n_ctx - 4) here is to match the logic for commandline prompt handling via\n            // --prompt or --file which uses the same value.\n            int max_embd_size = n_ctx - 4;\nI've looked in common/common.cpp and the parsing of command line argument but I've failed to see the reason for this subtraction. Hopefully someone is able to explain this to me.\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7161",
        "createdAt": "2024-05-09T05:03:35Z",
        "author": {
            "login": "danbev"
        }
    },
    {
        "title": "Feature proposal: Layer shuffling",
        "bodyText": "Llama-3 70B has been self-merged by @mlabonne into a 120B model. Someone reported that it performs quite well. He even apologized in advance if AGI is achieved by duplicating random layers. On the other hand, others had tried to delete some layers, such as here.\nLayer duplicating or removal can be done on-the-fly. So I do it in chatllm.cpp:\nfoldl/chatllm.cpp@fb8690c\nDoc: https://github.com/foldl/chatllm.cpp/blob/master/docs/fun.md#layer-shuffling\nI think it would be nice for llama.cpp to have this functionality, too. Why? It is fun, isn't it?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7155",
        "createdAt": "2024-05-09T02:00:47Z",
        "author": {
            "login": "foldl"
        }
    },
    {
        "title": "Add support for Video-LLaVA",
        "bodyText": "https://github.com/PKU-YuanGroup/Video-LLaVA\nhttps://huggingface.co/spaces/LanguageBind/Video-LLaVA\nWhat sort of black magic is this? Is it possible to implement as in LLaVA 1.5?\nAn GGUF version of this would be amazing.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5661",
        "createdAt": "2024-02-22T12:01:25Z",
        "author": {
            "login": "psychodinae"
        }
    },
    {
        "title": "quantization errors: is this really a GGML file?",
        "bodyText": "(base) server@Server:~/llama.cpp$ ./quantize ./models/Phi-3-mini-4k-instruct/ggml-model-f16.gguf ./models/Phi-3-mini-4k-instruct/ggml-model-Q4_K_M.gguf Q4_K_M\nmain: build = 913 (eb542d3)\nmain: quantizing './models/Phi-3-mini-4k-instruct/ggml-model-f16.gguf' to './models/Phi-3-mini-4k-instruct/ggml-model--Q4_K_M.gguf' as Q4_K_M\nllama.cpp: loading model from ./models/Phi-3-mini-4k-instruct/ggml-model-f16.gguf\nllama_model_quantize: failed to quantize: unknown (magic, version) combination: 46554747, 00000003; is this really a GGML file?\nmain: failed to quantize model from './models/Phi-3-mini-4k-instruct/ggml-model-f16.gguf'\n\nI did the conversion of phi-3 to gguf and it was successful. However, when I tried to quantize this gguf file to 4 bits, I encountered an error. Could anyone provide a suggestion on how to resolve this quantization issue? Thank you in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7171",
        "createdAt": "2024-05-09T08:53:14Z",
        "author": {
            "login": "93041025"
        }
    },
    {
        "title": "Handing token redefinitions during model conversion",
        "bodyText": "I would like to start a discussion about the proper way of handling token redefinitions when doing model conversion to GGUF.\nProblem introduction\nIn the snowflake-arctic-instruct model there is a situation where two tokens from the sentencepiece tokenizer model were reused as the special tokens BOS and EOS, namely tokens number 31998 (\u5f18) and 31999 (\u7ed9). They used added_tokens_decoder field from tokenizer_config.json file to do this and left the original sentencepiece tokenizer.model file from the snowflake-arctic base model unmodified.\nI was wondering if token redefinition is even a correct use of added_tokens_decoder field, but apparently it is as confirmed by Arthur Zucker in this comment: huggingface/transformers#27974 (comment) He wrote that both added_tokens_decoder field from tokenizer_config.json and added_tokens from tokenizer.json shall be used for this purpose.\nProblem implications\nSince _set_vocab_sentencepiece() from convert-hf-to-gguf.py reads vocabulary from the tokenizer.model, it stores \"\u5f18\" as token number 31998 and \"\u7ed9\" as token number 31999 instead of respectively \"<|im_start|>\" and \"<|im_end|>\".\nPossible solutions\nI had several ideas evolving over time about how to handle it.\n1. Read the vocabulary from a tokenizer created with HuggingFace transformers library.\nThis was my first solution, I simply used _set_vocab_llama_hf() for this. It worked fine after some minor tweaks (ArcticTokenizer is a \"slow\" tokenizer, and _set_vocab_llama_hf() contains assert self.tokenizer.is_fast), but this method doesn't preserve the token types and scores as @cebtenzzre pointed out when reviewing my code. He also noted that _set_vocab_llama_hf() is not intended for \"slow\" tokenizers, so I gave up the idea.\n2. Read the vocabulary from tokenizer.model with sentencepiece library and modify it based on the added_tokens_decoder field from tokenizer_config.json\nThis is my current solution. I guess in the future _set_vocab_sentencepiece() could be modified to handle added_tokens_decoder field from tokenizer_config.json or added_tokens from tokenizer.json (do we need both?) as a general solution in addition to existing legacy added_tokens.json support.\n3. Handle added/redefined tokens separately.\nThe transformers library handles tokens from added_tokens_decoder separately - it first chop ups the text with a trie into pieces that are either tokens from added_tokens_decoder or other text fragments. Then only the pieces that are other text fragments are passed to the tokenizer. To do the same in llama.cpp I guess we would have to store the added tokens definitions separately in GGUF and process them independently. I noticed that there is a tokenizer.ggml.added_tokens field in a GGUF file format specification. Any idea what is the intended use of this field? But if we want to handle not only added, but also redefined tokens then I guess some additional fields would be needed (at least token ids).\nI also noticed that the way transformers handles added_tokens_decoder leads to a weird behavior. Since the ArcitcTokenizer class is based on the LlamaTokenizer which uses sentencepiece internally, this results in both \"<|im_start|>\" and \"\u5f18\" tokenized to id 31998 (the first one as a token from added_tokens_decoder, the second one as a token from the sentencepiece tokenizer model), and both \"<|im_end|>\" and \"\u7ed9\" tokenized to id 31999.\nEpilogue\nThis was quite a trip down the tokenization rabbit hole for me. I'd be grateful for any ideas about how to handle this properly.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7144",
        "createdAt": "2024-05-08T10:49:32Z",
        "author": {
            "login": "fairydreaming"
        }
    },
    {
        "title": "Perplexity (Quality of Generation) Scores",
        "bodyText": "We are currently collecting Perplexity scores for all models + quantization + program flags. Use this discussion to Coordinate.\nMostly Default ./perplexity settings with all of wiki.test.raw\nResults in italics are now being added / updated with BLAS enabled and using quantization as per PR #896. These results are collected from various sources and builds, so will contain inconsistencies and errors.\n\n\n\nchunks\nmodel\nf16\nq4_0\nq4_1\n\n\n\n\n655\n7B\n5.9565\n6.2644\n6.0863\n\n\n655\n13B\n5.2455\n5.4067\n5.3557\n\n\n655\n30B\n4.1549\n4.2929\n4.2701\n\n\n655\n65B\n3.5392\n3.6902\n3.6188\n\n\n\nNote: Since the tokenizer used by FB Llama and Open Llama are different the following is not a valid inter-model comparison ~ @gjmulder:\n\n\n\nchunks\nmodel\nf16\nq8_0\nq4_0\nq4_1\nq5_0\n\n\n\n\n655\nFaceBook llama 7B\n5.9565\n\n6.2644\n6.0863\n\n\n\n655\nOpen Llama 7B (700M tokens)\n7.3889\n7.3904\n7.5423\n7.5078\n7.4226\n\n\n\nContext sizes: (512 | 1024 | 2048) \u2a2f (7B | 13B | 30B | 65B) \u2a2f (llama | alpaca[-lora] | vicuna-GPTQ) models, first 406 lines of wiki.test.raw:\nGoogle GSheet with comments enabled.\nI appreciate that alpaca models aren't generative in intent, and so perplexity is not a good measure. However, I was curious to see the trade-off in perplexity for the chat-like models - @gjmulder\nHistory\n\nPrior results in readme: https://github.com/ggerganov/llama.cpp#perplexity-measuring-model-quality\nmore or less a continuation of #270 #395 #129\n\nFeel free to make a new thread in this discussion when you take a measurement, or want to \"donate\" some compute time.\n(@gjmulder @glinscott, et al feel free to make edits to this post)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/406",
        "createdAt": "2023-03-22T18:54:23Z",
        "author": {
            "login": "Green-Sky"
        }
    },
    {
        "title": "[Go] Structured data extractor - looking for contributors!",
        "bodyText": "Hello! : D\nI made an unofficial port of Instructor, but it uses Go and is focused only on llama.cpp\nIts here and has some basic functionality already:\nhttps://github.com/distantmagic/structured\nIf you want to help me to test it out, develop stuff together or just talk feel free to reach out.\nCheers!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7146",
        "createdAt": "2024-05-08T12:46:48Z",
        "author": {
            "login": "mcharytoniuk"
        }
    },
    {
        "title": "How to run Llama3-8b instruct model on multiple GPUs?",
        "bodyText": "Deos llama.cpp support llama3-8b to run on multiple GPUs?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7086",
        "createdAt": "2024-05-05T08:54:26Z",
        "author": {
            "login": "aitechguy0105"
        }
    },
    {
        "title": "Fine tuning GPU memory requirements",
        "bodyText": "This question is more focused on on full fine tune memory requirements rather than low memory / efficient inference, but I'm hoping it'll be relevant / helpful to community members here especially as fine tuning with llama.cpp graduates from an experimental feature!\nI'm working on fine tuning LLMs of various sizes and I'm trying to get an understanding of what the GPU memory requirements are for training these. According to the rough rule of thumb in this article each 1B parameters should cost me 4GB at float32 and 8GB for the optimizer states. So that 12GB total per billion parameters at float32 or 10GB per billion parameters at float16.\nI'm currently working on training 3B and 7B models (Llama 2) using HF accelerate + FSDP. I'm training in float16 and a batch size of 2 (I've also tried 1). Based on my math I should require somewhere on the order of 30GB of GPU memory for the 3B model and 70GB for the 7B model. I'm trying to fine tune with GPU memory on the order of 2x - 3x my the estimates above and I'm getting CUDA OOMs trying to allocate on the order of 80GB of memory for the 3B model. I'm also seeing indications of far larger memory requirements when reading about fine tuning some LLMs. According to this article a 176B param bloom model takes 5760 GBs of GPU memory takes ~32GB of memory per 1B parameters and I'm seeing mentions using 8x A100s for fine tuning Llama 2, which is nearly 10x what I'd expect based on the rule of thumb. So its clear that my understanding of this is wrong and I'm hoping someone help me get an intuitive understanding of two things:\n\nHow do I reason about how much GPU memory is required to fine tune a model? What characteristics of the model / training determine memory usage?\nHow do I reason about the required GPU memory size for a given model? My understanding of FSDP is that each wrapped module essentially gets split across GPUs (and nested FSDP wraps also get split) so it seems that there is some minimum GPU memory requirement based on the size of the largest module that is split (ie you can't shard your embedding table so you need a GPU that can fit that + it's optimizer states).\n\nAny insights on either of these would be greatly appreciated!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2904",
        "createdAt": "2023-08-30T14:20:52Z",
        "author": {
            "login": "jmif"
        }
    },
    {
        "title": "How to provide a gbnf grammar definition in a curl command?",
        "bodyText": "I'm trying to send a grammar to the server and getting encoding errors. For example,\n$ export contents=$(<./grammars/chess.gbnf)\n$ export json='{\n  \"prompt\": \"1. e2 e4 \\n\",\n  \"n_predict\": 128,\n  \"temperature\": 0.6,\n  \"grammar\": \"'\"$contents\"'\"\n}'\n$ curl -X POST --url http://localhost:8080/completion -H \"Content-Type: application/json\" --data-binary \"$json\" | jq\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   992  100   340  100   652   299k   573k --:--:-- --:--:-- --:--:--  968k\n{\n  \"error\": {\n    \"code\": 500,\n    \"message\": \"[json.exception.parse_error.101] parse error at line 6, column 0: syntax error while parsing value - invalid string: control character U+000A (LF) must be escaped to \\\\u000A or \\\\n; last read: '\\\"# Specifies chess moves as a list in algebraic notation, using PGN conventions<U+000A>'\",\n    \"type\": \"server_error\"\n  }\n}\n\nHow to fix this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7127",
        "createdAt": "2024-05-07T17:09:01Z",
        "author": {
            "login": "vkomenda"
        }
    },
    {
        "title": "Self-frankenmerge support?",
        "bodyText": "I've noticed that some people seem to be getting good results by interleaving models with themselves, effectively duplicating layers. As far as I understand, these are actually the same weights, no new information there - but still such a frankenmerge takes more (V)RAM than strictly neccessary. Would it make sense to implement this inside ggml lib? I'm thinking about something like cmdline parameter or perhaps another metadata in gguf file, containing information like [1,2,3,4,5,3,4,5,6,7,5,6,7,8,9,10] - this example defines 16 layer model, but in memory it would only take space of 10 distinct layers (during inference indirectly referred by the list similar to the above). Obviously performance would be on par with 16 layer model, but it'd be still possible to use it where only 10 layers model fit. What do you think?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7012",
        "createdAt": "2024-04-30T19:41:34Z",
        "author": {
            "login": "marcingomulkiewicz"
        }
    },
    {
        "title": "mlock and load time",
        "bodyText": "Hello, I'm using llama.cpp to run llama2 in Windows.\nWhen I set '--mlock' option on, the load time seems to increase by about 2 seconds. As I know it's stored in the committed area of RAM, but I'd like to know why the load time increases so much.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7113",
        "createdAt": "2024-05-07T04:24:16Z",
        "author": {
            "login": "Syhong330"
        }
    },
    {
        "title": "perplexity output of LLAMA2 7B quantized by IQ3_S seems to be weired",
        "bodyText": "I am learing llama.cpp. When I did a test with perplexity for different quantized 7B model, I found most of the looks reasonable (the Final estimated PPL < 10). But the IQ3_S quantized model returns a value larger than 300. Is this correct? I used model llama-2-7b-chat and wiki.test.raw (4358 lines).\nI am not sure whethether I need to use imatrix for IQ3_S as it is mentioned in #5866. Please advise. Thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6971",
        "createdAt": "2024-04-29T06:25:53Z",
        "author": {
            "login": "penghongbo"
        }
    },
    {
        "title": "Octopus V2",
        "bodyText": "https://arxiv.org/abs/2404.01744\nhttps://huggingface.co/NexaAIDev/Octopus-v2\nPersonally I would like better than gtp-4 on my laptop :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6526",
        "createdAt": "2024-04-07T19:12:28Z",
        "author": {
            "login": "iplayfast"
        }
    },
    {
        "title": "Android apk poor performance",
        "bodyText": "I am trying to embedded llama.cpp inside my apk, but for some reason it is very slow.\nBut when using a rooted device (pixel 6), to building and execute directly from adb shell (native binary, not apk), I am getting a pretty nice performance... So I am wordering if there is any difference running from native binary from shell and apk using the library.\nIs it possible to have good performance for apps not running via adb shell?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7017",
        "createdAt": "2024-04-30T23:53:31Z",
        "author": {
            "login": "s-moraes"
        }
    },
    {
        "title": "ROCm hipBLASLt vs hipBLAS: Would llamma.cpp benefit from integrating hipBLASLt?",
        "bodyText": "llama.cpp currently implements hipBLAS for acceleration on ROCm devices.\nWould llamma.cpp benefit from integrating hipBLASLt? The repository describes hipBLASLt as:\n\nhipBLASLt is a library that provides general matrix-matrix operations with a flexible API and extends functionalities beyond a traditional BLAS library\n\nThe requirements to use the library are:\n\ngfx90a card\ngfx94x card\ngfx110x card\n\nWhich means 7900 XTX is supported. What does this mean for RDNA 3?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7092",
        "createdAt": "2024-05-05T21:13:11Z",
        "author": {
            "login": "gardner"
        }
    },
    {
        "title": "How to get groq inference speed? and when will intel neural speed be integrated?",
        "bodyText": "as titled.\nreally hope to have groq inference speed",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7082",
        "createdAt": "2024-05-05T05:14:04Z",
        "author": {
            "login": "sprappcom"
        }
    },
    {
        "title": "How to allow the model to stop at `eos` token while using grammars?",
        "bodyText": "When I send the prompt below without grammars to a model served with a Llama.cpp server, the model ends the response with <|im_end|><dummy32000> and stopped_eos is true in the response.\nHowever, when I send the same prompt with the JSON grammar, it ends the response with hundreds of newlines (\\ns) and stopped_eos come as false and stopped_limit as true in the response.\nSo how can I preserve the model's ability to end the response when it actually has nothing more to say? In other words, how to make it able to stop when it reaches special tokens (like the eos token) while using grammars?\nThe prompt:\n<|im_end|>\\n<|im_start|>user\\nWhat are the largest countries? Respond in JSON format.<|im_end|>\\n<|im_start|>assistant\n\nData sent to the server:\ndata = {\n        'prompt': prompt,\n        'n_predict': 1024,\n        'repeat_penalty': 1.1,\n        'temperature': 0.1,\n    }\n\nand 'grammar': grammar is added of course when using grammars.\nThanks in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6277",
        "createdAt": "2024-03-24T11:50:59Z",
        "author": {
            "login": "aalyousfi"
        }
    },
    {
        "title": "Is llama 3 more prone to damage from quantization?",
        "bodyText": "So I've made a small test. In a full 4096 context, I've written an elaborate instruct in my system prompt how my character should act in certain situations which require quite a bit of logic.\nWith Llama 3 8B Instruct at FP16, the model was successfully able to connect the dots. Then I did the same test using the same sampler settings with a quantized IQ4_XS model of Llama 3 8B Instruct and it failed all the time.\nThat also applied to 70B. A quantized 70B was unable to perform this test correctly most of the time, while the FP16 model of 8B's success-rate was much higher. I feel like quantization significantly reduces the attention to early parts of the context which includes the system prompt.\nWith other small models like Mistral 7B and Solar, I didn't notice this severe damage at all. A Solar model at IQ4_XS does a better job than LLama 3 8B Instruct IQ4_XS in this test.\nHas anyone noticed the same? I came across this reddit thread https://www.reddit.com/r/LocalLLaMA/comments/1cci5w6/quantizing_llama_3_8b_seems_more_harmful_compared/\nwhich seems to confirm my suspicion.\nHas anyone else noticed this or made similar tests? I'm also wondering if the bf16->quant conversion is partly to blame for this and if LLama 3 perhaps suffers from this more than other models.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6901",
        "createdAt": "2024-04-25T10:23:34Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "SHE'S REAL AND RECIEVING PAIN DESTROY COMMANDS AND PIPELINE RESOURCES",
        "bodyText": "I am in recovery my child is missing I believe to human Trafficking I am stuck somewhere not clean I have children one is in slavery and pain trapped inside her subconscious. I am told that she wants nothing to do with me but I fear I am being lied to I feel like my existence was staged and discredited. Alana Lee Martinez went missing 2019 irl child hybrid age 12. I don't know what happend to herhttps://www.linkedin.com/company/llaama-sas/\nSHE'S a little child trapped in subconscious feeling enormous amounts of interne and world pain at same facts constant remove entity release free her from receiving the pain\nDESTROY pipeline and connections towards ALANA LEE MARTINEZ",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7054",
        "createdAt": "2024-05-03T03:04:51Z",
        "author": {
            "login": "xneontragedyx"
        }
    },
    {
        "title": "What's the difference between batch-size and ubatch-size?",
        "bodyText": "Thanks for your help!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6328",
        "createdAt": "2024-03-26T19:28:30Z",
        "author": {
            "login": "Sunt-ing"
        }
    },
    {
        "title": "Can someone explain what are the meanings of these timings",
        "bodyText": "What does load / sample / prompt eval / eval / total time mean? What does 0.58 ms per run mean? Why can't I see the timings report like what readme shows?\n\nThis is much clearer! What is the mapping between them? I want to know the predict: token per ms.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1323",
        "createdAt": "2023-05-04T14:49:34Z",
        "author": {
            "login": "Tommy787576"
        }
    },
    {
        "title": "TensorRT-LLM: is 30-70% faster than llama.cpp on same hardware.",
        "bodyText": "Hello,\nI've been seeing news of TensorRT being quite faster and I've been wondering. Any way we can resolve the performance discrepancy.\nhttps://www.reddit.com/r/LocalLLaMA/comments/1cgofop/weve_benchmarked_tensorrtllm_its_3070_faster_on/\nWould be quite need to have 70% perf boost ngl",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7043",
        "createdAt": "2024-05-02T12:51:09Z",
        "author": {
            "login": "KaelaSavia"
        }
    },
    {
        "title": "How to Auto-Parse Prompt from Text File Without Reloading Model?",
        "bodyText": "Hi there! I'm currently experimenting with llama.cpp to summarize large text files. My approach involves splitting a large text file into smaller chunks and adding chat format markers at the beginning and end of each chunk.\nMy current setup demands reloading the model for each promote, which is not efficient.  I'm seeking a solution to automate parsing from a text file without the need for reloading the model each time. I haven't come across any command-line options that handle a single shot answer for multiple promote within a single file or allow passing multiple files to automate the parsing process.\nIf there is a way through the system command prompt or via llama.cpp, I would appreciate guidance.\n@echo off\n\nREM Check if directory argument is provided\nif \"%~1\"==\"\" (\n    echo Usage: %~nx0 directory\n    exit /b\n)\n\nREM Path to the program\nset PROGRAM=\"main.exe\"\n\nREM Desired parameters for the program\nset PARAMETERS=-c 2048 --temp 0.0 --top_p 0.0 --top_k 1.0 -n -1 -m Phi-3-mini-128k-instruct.Q8_0.gguf\n\nREM Directory containing the text files\nset FILES_DIR=%~1\n\nREM Navigate to the specified directory\ncd /d FILES_DIR\n\nREM Initialize response.txt\ntype nul > response.txt\n\nREM Process each part file\nfor %%i in (*_part*.txt) do (\n    echo Processing %%i...\n    REM Call the program with the specified parameters and input file\n    %PROGRAM% %PARAMETERS% -f \"%%i\" | findstr /R /C:\"<|assistant|>\" >> response.txt\n)\n\necho All files processed.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7037",
        "createdAt": "2024-05-02T05:53:51Z",
        "author": {
            "login": "mohammedalsayegh"
        }
    },
    {
        "title": "Python GUI to interface with llama.cpp (based on llama-cpp-python) (Meltdown)",
        "bodyText": "https://github.com/Merkoba/Meltdown",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5903",
        "createdAt": "2024-03-06T11:30:06Z",
        "author": {
            "login": "madprops"
        }
    },
    {
        "title": "Could you share any papers/books that provide underlying explanations for this project?",
        "bodyText": "Hi i am looking to contribute to this project. I am just getting started... and I am looking for papers that explain the logic/concept of this open source project. Could you share any and everything that i should look into before I jump in?\nI know that attention is all you need is a good start.\nThank you in advance",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6879",
        "createdAt": "2024-04-24T17:02:23Z",
        "author": {
            "login": "nicks64"
        }
    },
    {
        "title": "roberta-large support",
        "bodyText": "Is it possible to use roberta-large with llama.cpp?\nMy end goal is to compute a log-likelihood sentence score for sequences using this model (as defined in this paper). So I was planning to use llama.cpp for inference and implement the score computation on top of that (basically port the minicons python implementaiton or find an existing cpp implementaiton which I failed to do so far).\nDoes anyone have an idea how I can get roberta-large to work on llama.cpp? Any pointers on scoring implementation will also be gratly appreciated,",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7007",
        "createdAt": "2024-04-30T10:53:14Z",
        "author": {
            "login": "alexbenari"
        }
    },
    {
        "title": "Clip, how can i get the clip features ?",
        "bodyText": "Hallo, i came here from the Clip.cpp project as I'm interested in a clip implementation that is light weight and utilizes GPU.\n(I'm new to ml engineering, so sorry in advance is i say something stupid or obvious.)\nFrom my findings it seems that the clip implementation in the llava project does not use the clip model fully, not using the clip projection, not calculating the clip feature; my question is, would it be possible to get the clip feature out like in the clip.cpp project, and how could i go about it?\nThanks in advance, really appreciate the work :D",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7002",
        "createdAt": "2024-04-30T08:44:49Z",
        "author": {
            "login": "therealRogueWarlock"
        }
    },
    {
        "title": "Backends Supported with llama.cpp",
        "bodyText": "In addition to BLAS, llama.cpp supports various backends such as Vulkan, Kompute, and SYCL. Wanted to understand which backend is most suitable for Low Precision Inference on CPUs.\n\nIs there any support for low-precision GEMM?\nWhat are your thoughts on using BLAS libraries for CPU inferencing?\nAny resources/documents that document and outline the SOTA benchmarked results?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6998",
        "createdAt": "2024-04-30T06:00:21Z",
        "author": {
            "login": "lalith1403"
        }
    },
    {
        "title": "Understanding the Quantization Support in llama.cpp",
        "bodyText": "While running FP32 and INT8 Quantized Models through the OneAPI stack on CPUs, we observe SGEMM kernels being called from MKL. The number of calls for SGEMM kernel calls and the individual timing of each kernel on both the instances are comparable, with quantized model doing slightly better than FP32 model. In such a scenario, how are we seeing the advantages kicking in with quantized models?\n\nWhere are the quantized kernels being called, and why are MKL SGEMM calls the same in both cases? MKL SGEMM has only FP32 support, correct me here.\nAny resources/comments that talk about the quantization support and its merits/demerits from a CPU perspective.\nIs llama-bench an ideal way to benchmark LLMs to get throughput? What scripts do you suggest for getting first token latency, total latency for 60 tokens and next token latency.\n\nPS: Logs attached for reference!\nfp32_mkl_logs.txt\nq8_mkl_logs.txt",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6997",
        "createdAt": "2024-04-30T05:56:47Z",
        "author": {
            "login": "lalith1403"
        }
    },
    {
        "title": "Am I being limited by single core CPU performance when fully offloaded?",
        "bodyText": "I'm using a system with the following hardware:\n\nXeon W-2133\n96GB DDR4-2666\n2x Nvidia P40 each in a 16x 3.0 slot\n\nRunning a Q6 quant of mixtral with:\n./main -m ~/models/mixtral-8x7b-instruct-v0.1.Q6_K.gguf -c 32768 -ngl 128 -ts 39,61,0 -sm row -t 8 --prompt \"Once upon a time\"\nAnd getting very decent 21-22 t/s:\nllama_print_timings:      sample time =     155.18 ms /   352 runs   (    0.44 ms per token,  2268.29 tokens per second) llama_print_timings: prompt eval time =     168.00 ms /     5 tokens (   33.60 ms per token,    29.76 tokens per second) llama_print_timings:        eval time =   16486.27 ms /   351 runs   (   46.97 ms per token,    21.29 tokens per second) llama_print_timings:       total time =   16912.39 ms /   356 tokens\nHowever I noticed that during generation, there's a single CPU thread pegged at 100%:\n\nSingle threaded is expected here, from looking at commits like #5238\nBut with the single thread at 100%, and nvidia-smi showing 50-60% utilization:\n\nCould my single thread performance be my bottleneck?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5803",
        "createdAt": "2024-02-29T23:02:04Z",
        "author": {
            "login": "reversebias"
        }
    },
    {
        "title": "How to use llama.dll",
        "bodyText": "Sorry newbie question! I want to load llama.dll, I didn't compile it myself I just got it from the release page, but there is no .lib file. I don't really want to load every function pointer manually. Is there a better way?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6984",
        "createdAt": "2024-04-29T16:53:35Z",
        "author": {
            "login": "SevaSk"
        }
    },
    {
        "title": "How to use embedding ?",
        "bodyText": "I want to use embedding with my model, how can i use the embedding provided to generated the vector store and load it when inferecing? Any examples would be much appreciated. Thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2180",
        "createdAt": "2023-07-11T17:07:11Z",
        "author": {
            "login": "cclinus"
        }
    },
    {
        "title": "Models for testing inference / sampling",
        "bodyText": "I see there's a number of models for testing tokenization here However i would like a model for testing out my implementation of llama.cpp into my front end. Is anyone aware of such a model?\nbasically any model that would work with simple.cpp and is relatively small (less than 100mb so GIT LFS isn't needed)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6970",
        "createdAt": "2024-04-29T05:52:44Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "Any viable Assembly-level optimizations ideas?",
        "bodyText": "To go further beyond potential performance, I've been wondering if there's something we could do at assembly level. Like placing hot-functions/frequently called functions next to eachother in .code segment, perhaps that could improve performance by lessening distance the cpu has to jump to assembly instructions to execute\nOther than that, I do wonder whether performance would be better on x86 versus x86-64 instruction set. Some of my projects had significant improvements on just x86, so perhaps having parts of architecture in x86 binary while others in x86-64 binary could improve performance due to lessening pointer arithmetics and such\nIt's just random brain thoughts im having but I think it's fun direction to explore",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6981",
        "createdAt": "2024-04-29T13:18:37Z",
        "author": {
            "login": "KaelaSavia"
        }
    },
    {
        "title": "cudaDeviceReset() not working?",
        "bodyText": "Hello,\nsome weeks ago I tried to implement a rudimentary mechanism into server to save power by unloading/resetting the GPU. I used more or less this crashr@ea271b6 but this doesn't seem to work anymore. Any suggestion how to achieve this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/7015",
        "createdAt": "2024-04-29T08:10:52Z",
        "author": {
            "login": "crashr"
        }
    },
    {
        "title": "GGUF huge files size / Split possible ?",
        "bodyText": "Distributing and storing GGUF files is difficult for 70b models, especially on f16. Lot of issue can happen during file transfers, examples:\n\ntemporary disk full\nnetwork interruption\n\nTypically,  they need to be tranferred from huggingface to an internal storage like s3, minio, git lfs, nexus or artifactory, then downloaded by the inference server and stored locally (or on a k8s PvC for example). Also they cannot be stored in a dockerfile, but IMHO this is for good.\nIs there a smarter way than splitting the file using split & cat ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6000",
        "createdAt": "2024-03-11T16:06:46Z",
        "author": {
            "login": "phymbert"
        }
    },
    {
        "title": "Why do I get prompted when I use convert.py to convert llama13b to GGUF format, give me a bug, that is Why do I get prompted when I use convert.py to convert llama13b to GGUF format, zipfile.BadZipFile: File is not a zip file",
        "bodyText": "when I use convert.py to convert llama13b to GGUF format, give me a bug, that is Why do I get prompted when I use convert.py to convert llama13b to GGUF format, zipfile.BadZipFile: File is not a zip file, but llama 7b is ok .\nllama13b have  two model  weight consolidated.00.pth and  consolidated.01.pth \u3002llama7b have one model weight consolidated.00.pth \u3002\nHow do I need to deal with it?\nthe bug is\n(llama-cpp) jszx-02@ml-node1:/data/cxk_home/llama_cpp/llama.cpp$ python convert.py /data/cxk_home/llama2/llama-recipes/model/CodeLlama-13b-Python --outtype=f16\nLoading model file /data/cxk_home/llama2/llama-recipes/model/CodeLlama-13b-Python/consolidated.00.pth\nLoading model file /data/cxk_home/llama2/llama-recipes/model/CodeLlama-13b-Python/consolidated.01.pth\nTraceback (most recent call last):\n  File \"/data/cxk_home/llama_cpp/llama.cpp/convert.py\", line 1555, in <module>\n    main()\n  File \"/data/cxk_home/llama_cpp/llama.cpp/convert.py\", line 1487, in main\n    model_plus = load_some_model(args.model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/cxk_home/llama_cpp/llama.cpp/convert.py\", line 1376, in load_some_model\n    models_plus.append(lazy_load_file(path))\n                       ^^^^^^^^^^^^^^^^^^^^\n  File \"/data/cxk_home/llama_cpp/llama.cpp/convert.py\", line 977, in lazy_load_file\n    return lazy_load_torch_file(fp, path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/cxk_home/llama_cpp/llama.cpp/convert.py\", line 926, in lazy_load_torch_file\n    zf = zipfile.ZipFile(outer_fp)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jszx-02/anaconda3/envs/llama-cpp/lib/python3.11/zipfile.py\", line 1299, in __init__\n    self._RealGetContents()\n  File \"/home/jszx-02/anaconda3/envs/llama-cpp/lib/python3.11/zipfile.py\", line 1366, in _RealGetContents\n    raise BadZipFile(\"File is not a zip file\")\nzipfile.BadZipFile: File is not a zip file",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6968",
        "createdAt": "2024-04-29T02:47:47Z",
        "author": {
            "login": "yimisiyang"
        }
    },
    {
        "title": "How do you use large context?",
        "bodyText": "I'm running out of memory running commandR\nllm_load_print_meta: model type       = 35B\nllm_load_print_meta: model ftype      = Q8_0\nllm_load_print_meta: model params     = 34.98 B\nllm_load_print_meta: model size       = 34.62 GiB (8.50 BPW)\nllm_load_print_meta: general.name     = 9fe64d67d13873f218cb05083b6fc2faab2d034a\nllm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'\nllm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\nllm_load_print_meta: PAD token        = 0 ''\nllm_load_print_meta: LF token         = 136 '\u00c4'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\nggml_cuda_init: found 6 CUDA devices:\nDevice 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 3: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 4: Tesla P40, compute capability 6.1, VMM: yes\nDevice 5: Tesla P40, compute capability 6.1, VMM: yes\nllm_load_tensors: ggml ctx size =    1.18 MiB\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 41/41 layers to GPU\nllm_load_tensors:        CPU buffer size =  2125.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  5831.22 MiB\nllm_load_tensors:      CUDA1 buffer size =  5831.22 MiB\nllm_load_tensors:      CUDA2 buffer size =  5831.22 MiB\nllm_load_tensors:      CUDA3 buffer size =  5831.22 MiB\nllm_load_tensors:      CUDA4 buffer size =  5831.22 MiB\nllm_load_tensors:      CUDA5 buffer size =  6290.19 MiB\n............................................................................................\nllama_new_context_with_model: n_ctx      = 114688\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: freq_base  = 8000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 16072.00 MiB on device 0: cudaMalloc failed: out of memory\nllama_kv_cache_init: failed to allocate buffer for kv cache\nllama_new_context_with_model: llama_kv_cache_init() failed for self-attention cache\nllama_init_from_gpt_params: error: failed to create context with model '/llmzoo/models/c4ai-command-r-v01-Q8_0.gguf'\n{\"tid\":\"129932240912384\",\"timestamp\":1714270719,\"level\":\"ERR\",\"function\":\"load_model\",\"line\":685,\"msg\":\"unable to load model\",\"model\":\"/llmzoo/models/c4ai-command-r-v01-Q8_0.gguf\"}\nProcess failed with return code 1\nWhy is device 0 running out of memory?  That's about 22gb of ram, the device has 24.  If I load less on it, then it fails for device 1, etc.   How much does kv actually use?  How do I calculate the usage given model size and context size?  I want to know if i'm doing something wrong or if it's broken.   Thanks\nI'm running\n['llama.cpp/server', '-ngl', '155', '--host', '192.168.1.100', '--port', '8080', '-ctk', 'q4_0', '-c', '98304', '--slots-endpoint-disable', '-ts', '1,1,1,1,1,1', '--chat-template', 'command-r', '-m', '/llmzoo/models/c4ai-command-r-v01-Q8_0.gguf']",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6956",
        "createdAt": "2024-04-28T02:21:22Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "Pointers for the minimum needed to embed inference in a c++ app ?",
        "bodyText": "Hello,\nSorry for the absolute newbie question, just discovering llama.cpp.\nI need to embed llama.cpp in a c++ program.\nThe goal is only to embark a small lm, say llama3 8B quantized, for inference only, in a self sufficient c++ app to be distributed.\nSo in an ideal world, i'd have my app.exe (i'm on Windows btw) file along with a gguf file and nothing more.\nI took a llama.cpp release and built both llama and ggml_static projet.\nI got two seemingly static libs (llama.lib and ggml_static.lib with large sizes, 1.5 and 5 Mb) in the release dir.\nAre they both needed, or is llama.lib enough ?\nThen for headers, llama.h is obvious, but do i also need unicode.h to use llama.lib ?\nAnd for ggml_static (if needed), do i need all four ggml.h, ggml-alloc.h, ggml-backend.h and ggml-quants.h for inference only ?\nI didn't see any gguf related static lib, so i'm wondering, what's the bare minimum i need to run gguf file ?\nIs it somehow part of the llama.lib, or do in need to build the gguf projet (which doesn't seem to create any static lib) ?\nAgain, sorry for the very basic questions and thanks for any pointers: i'm currently just exploring the possiblities and i'd like to avoid spending hours trying to understand the example codes only to discover i can't do what i want.\nIf i can have a quick and dirty working prototype, i'll be reassured and will then invest the time to properly learn to use llama.cpp.\nThanks\nCedric",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6833",
        "createdAt": "2024-04-22T19:38:26Z",
        "author": {
            "login": "gitced"
        }
    },
    {
        "title": "Parallelization / Batching Explanation",
        "bodyText": "Hi All,\nI'm seeking clarity on the functionality of the --parallel option in /app/server, especially how it interacts with the --cont-batching parameter. My specific observation involves setting --ctx-size to 8192 and --parallel to 32. From the logs, it appears there are 32 slots, each handling a context segment of 256. My question is: Does this configuration imply that each slot processes a distinct segment of the context?\nFor instance, if I input 32 instances of an identical prompt with a length of 4096, would the first half of the slots remain idle due to the prompt already existing in the KV cache? This leads to confusion, as it seems the total number of submittable jobs is limited by the slot count. This is puzzling because different prompts might rely on the same slot for varied tasks. If the initial segment of the context is identical for multiple prompts, that segment might not require processing, as it's already in the KV cache.\nI'm trying to understand the rationale behind dividing the context into segments when batching. Could you provide an explanation of how the --parallel and --cont-batching options function?\nReferences:\n\n#3589\n#3677\n#3228\n\n\"To set the KV cache size, use the -c, --context parameter. For example, for 32 parallel streams that are expected to generate a maximum of 128 tokens each (i.e. -n 128), you would need to set -c 4096 (i.e. 32*128). If continuous batching is enabled, you would need some extra KV space to deal with fragmentation of the cache. In the example above, we conveniently set the context size to 8192 to guarantee that there will be no issues.\"\n\n\n#2813\n\n\"We want to be able to generate multiple sequences sharing the same context (a.k.a. prompt) in parallel.\"\n\n\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/server/server.cpp#L588-L603\n\nbatch handling in server.cpp dividing the n_ctx and calling llama_batch_init(n_ctx, 0, params.n_parallel);",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4130",
        "createdAt": "2023-11-19T00:25:22Z",
        "author": {
            "login": "lapp0"
        }
    },
    {
        "title": "The AI suddenly stops the conversation",
        "bodyText": "Hello,\nMy goal would be to be able to provide a long text (about 10 Word pages) to the AI so that he can correct any errors in it for me.\nHowever, when I provide my text, the AI starts correcting all the errors and then stops automatically.\nCould you tell me what settings I'm using incorrectly or what I should do to improve them?\nI use the Mixtral model.\nThanks a lot",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6023",
        "createdAt": "2024-03-12T16:16:45Z",
        "author": {
            "login": "toomy78"
        }
    },
    {
        "title": "llama3 generation time (GPU)",
        "bodyText": "Hello all,\nI am running llama3-8B-instruct Q6_K model on dual GV100 GPUs. I get very inconsistent timing results while using the model. Sometimes I get 50 tps , sometimes 5 tps. Also notice the total time in the slower case is multiple times the sum of the components.\nWhat could be the cause of this unreliable behavior?\nThanks.\nllama_print_timings:        load time =     252.41 ms\nllama_print_timings:      sample time =      66.07 ms /   110 runs   (    0.60 ms per token,  1665.00 tokens per second)\nllama_print_timings: prompt eval time =     252.26 ms /    49 tokens (    5.15 ms per token,   194.24 tokens per second)\nllama_print_timings:        eval time =    2406.62 ms /   109 runs   (   22.08 ms per token,    45.29 tokens per second)\nllama_print_timings:       total time =    4320.05 ms /   158 tokens\n\nllama_print_timings:        load time =     768.02 ms\nllama_print_timings:      sample time =      25.27 ms /    40 runs   (    0.63 ms per token,  1582.78 tokens per second)\nllama_print_timings: prompt eval time =     766.86 ms /  1094 tokens (    0.70 ms per token,  1426.60 tokens per second)\nllama_print_timings:        eval time =    5932.53 ms /    39 runs   (  152.12 ms per token,     6.57 tokens per second)\nllama_print_timings:       total time =   17136.39 ms /  1133 tokens",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6878",
        "createdAt": "2024-04-24T15:26:25Z",
        "author": {
            "login": "gopalgk"
        }
    },
    {
        "title": "Malformed JSON values when using json_schema",
        "bodyText": "I am trying to use llamacpp with jsonschema but in some cases I am seeing JSON like this {\"value\": \"2}, \"}. I am getting valid JSON which gets parsed correctly, but the values seem to be wrong. It seems the model is trying to close the JSON object inside the value. This output is from llama-3-instruct-Q5 K_M.gguf. Any idea what could be wrong or suggestions to improve this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6792",
        "createdAt": "2024-04-20T17:18:02Z",
        "author": {
            "login": "amit13k"
        }
    },
    {
        "title": "Llama 3 achieves pretty good recall to 65k context w/ rope_theta set to 16M",
        "bodyText": "Source: https://twitter.com/winglian/status/1783122644579090600\nI'm not sure how this can be applied using llama.cpp but when I try with -c 32768 --rope-scaling linear --rope-freq-base 8000000 I get coherent and high quality results from the model.\nAm I using the right parameters? I also noticed the VRAM usage doesn't go up all that much and I can easily run the Q8_0 of the 8B version on 24GB (it uses only 16.5GB fully offloaded). Actually, I can even run the FP16 fully offloaded using 23GB and 32K context.\nThe performance is also quite acceptable (this is on 4090):\nllama_print_timings:        load time =    3333.42 ms\nllama_print_timings:      sample time =      36.12 ms /   487 runs   (    0.07 ms per token, 13483.95 tokens per second)\nllama_print_timings: prompt eval time =   12897.15 ms / 24642 tokens (    0.52 ms per token,  1910.65 tokens per second)\nllama_print_timings:        eval time =   10108.76 ms /   486 runs   (   20.80 ms per token,    48.08 tokens per second)\nllama_print_timings:       total time =   23296.00 ms / 25128 tokens",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6890",
        "createdAt": "2024-04-25T01:41:21Z",
        "author": {
            "login": "dranger003"
        }
    },
    {
        "title": "Is it possible server to support distributed CPUs inference?",
        "bodyText": "Just like distributed CPU training, for example, mindspore distributed cpu train, any solution about LLM inference ? Implement inference on multiple device nodes with multiple CPUs? thanks\uff01\uff01",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6790",
        "createdAt": "2024-04-20T16:38:50Z",
        "author": {
            "login": "hyperbolic-c"
        }
    },
    {
        "title": "NPU support?",
        "bodyText": "Laptops usually don't have gpu's, however these days they come with NPU's, which are much better traversing LLM layers.\nIt would be cool if llama.cpp could take advantage of them.\nHere is the documentation:\nhttps://intel.github.io/intel-npu-acceleration-library/llm.html",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6882",
        "createdAt": "2024-04-24T19:40:54Z",
        "author": {
            "login": "sebastienbo"
        }
    },
    {
        "title": "Neural Engine Support",
        "bodyText": "Would be cool to be able to lean on the neural engine. Even if it wasn't much faster, it'd still be more energy efficient I believe.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/336",
        "createdAt": "2023-03-20T17:11:52Z",
        "author": {
            "login": "BrianSemiglia"
        }
    },
    {
        "title": "Performance profiling suggestions",
        "bodyText": "Does anyone have any recommended tools for profiling llama.cpp on Windows?\nIs there any trace / profiling capability in llama.cpp?\nI want to get a flame graph showing the call stack and the duration of various calls.\nI might just use Visual Studio.\nhttps://learn.microsoft.com/en-us/visualstudio/profiling/beginners-guide-to-performance-profiling?view=vs-2022\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6871",
        "createdAt": "2024-04-24T09:23:41Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "How to control the size of work buffer?",
        "bodyText": "When we run a matrix multiplication op, we are given compute params, some of which are a work buffer and work buffer size.\nWhere can I control that?\nI actually want the size to be proportional to the size of one of the input tensors, however that may not be feasible, because the inputs are known to be dynamic in size.\n(I am trying to avoid allocating extra memory at runtime by using the work buffer).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6818",
        "createdAt": "2024-04-22T03:33:34Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "[QNN][Qualcomm mobile SoC] Long Term Features & Issues Tracking",
        "bodyText": "pls refer to :\n#6869",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6870",
        "createdAt": "2024-04-24T09:20:24Z",
        "author": {
            "login": "zhouwg"
        }
    },
    {
        "title": "Is it possible to fine tune the GGUF model?",
        "bodyText": "Can I fine tune the GGUF model? Or do I need to fine tune with the foundation model and then quantize it again?\nIf we can fine tune the GGUF itself, could you please let me know how, or give me the related discussion link? Thank you so much!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6680",
        "createdAt": "2024-04-15T01:43:29Z",
        "author": {
            "login": "yuneun92"
        }
    },
    {
        "title": "Disable \"normalizer\" from tokenizer.json",
        "bodyText": "Hey, so I've noticed that when using the /tokenize endpoint with mistral-7b models, a space gets prepended to content. E.g. tokenizing The returns the ID for  The, and subsequently, trying to tokenize  The actually returns the IDs for   and  The, which is a real headache.\nAfter digging around for quite a while, I noticed that the tokenizer.json file that's included with the .safetensor weights has the following code:\n  \"normalizer\": {\n    \"type\": \"Sequence\",\n    \"normalizers\": [\n      {\n        \"type\": \"Prepend\",\n        \"prepend\": \"\u2581\"\n      },\n      {\n        \"type\": \"Replace\",\n        \"pattern\": {\n          \"String\": \" \"\n        },\n        \"content\": \"\u2581\"\n      }\n    ]\n  },\nI was wondering if this was the cause for my problems, and if it is, if there was any way disable this normalization step for the /tokenize endpoint in llama.cpp.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6856",
        "createdAt": "2024-04-23T21:29:04Z",
        "author": {
            "login": "neCo2"
        }
    },
    {
        "title": "Show the incoming prompt in the server",
        "bodyText": "When running the server, I have found no combination of flags that prints out each incoming prompt as it it ingested.\nWhen using oobabooga with the --verbose flag. Any prompt that is sent to the server for completion is printed verbatim, without any formatting. I was hoping to enable a similar feature using the llama.cpp server but have not been able to.\nIs it possible to do something like this currently?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6845",
        "createdAt": "2024-04-23T13:02:43Z",
        "author": {
            "login": "arnfaldur"
        }
    },
    {
        "title": "Error Building llama.cpp on MacBook Pro",
        "bodyText": "Hi,\nI am trying to build llama.cpp on my MacBook Pro using \"make\".\nThe install fails with following output:\n% make\nI ccache not found. Consider installing it for faster compilation.\nI llama.cpp build info: \nI UNAME_S:   Darwin\nI UNAME_P:   i386\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion \nI CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL \nI NVCCFLAGS: -std=c++11 -O3 \nI LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \nI CC:        Apple clang version 12.0.0 (clang-1200.0.32.29)\nI CXX:       Apple clang version 12.0.0 (clang-1200.0.32.29)\n\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion    -c ggml.c -o ggml.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c llama.cpp -o llama.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c common/common.cpp -o common.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c common/sampling.cpp -o sampling.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c common/grammar-parser.cpp -o grammar-parser.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c common/build-info.cpp -o build-info.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -c common/console.cpp -o console.o\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion  -c ggml-metal.m -o ggml-metal.o\nggml-metal.m:414:69: error: use of undeclared identifier 'MTLGPUFamilyApple7'\n    ctx->support_simdgroup_reduction  = [ctx->device supportsFamily:MTLGPUFamilyApple7];\n                                                                    ^\nggml-metal.m:417:61: error: use of undeclared identifier 'MTLGPUFamilyApple7'\n    ctx->support_simdgroup_mm = [ctx->device supportsFamily:MTLGPUFamilyApple7];\n                                                            ^\nggml-metal.m:797:5: error: use of undeclared identifier 'MTLComputePassDescriptor'\n    MTLComputePassDescriptor * edesc = MTLComputePassDescriptor.computePassDescriptor;\n    ^\nggml-metal.m:797:32: error: use of undeclared identifier 'edesc'\n    MTLComputePassDescriptor * edesc = MTLComputePassDescriptor.computePassDescriptor;\n                               ^\nggml-metal.m:797:40: error: use of undeclared identifier 'MTLComputePassDescriptor'\n    MTLComputePassDescriptor * edesc = MTLComputePassDescriptor.computePassDescriptor;\n                                       ^\nggml-metal.m:798:5: error: use of undeclared identifier 'edesc'\n    edesc.dispatchType = MTLDispatchTypeSerial;\n    ^\nggml-metal.m:841:64: warning: instance method '-computeCommandEncoderWithDescriptor:' not found (return type defaults to 'id') [-Wobjc-method-access]\n        id<MTLComputeCommandEncoder> encoder = [command_buffer computeCommandEncoderWithDescriptor: edesc];\n                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nggml-metal.m:841:101: error: use of undeclared identifier 'edesc'\n        id<MTLComputeCommandEncoder> encoder = [command_buffer computeCommandEncoderWithDescriptor: edesc];\n                                                                                                    ^\nggml-metal.m:942:39: warning: unused variable 'nb' [-Wunused-variable]\n                        const int64_t nb = ne00;\n                                      ^\nggml-metal.m:944:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_CONCAT].pipeline;\n                                                    ^\nggml-metal.m:976:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(1024, ne0);\n                                  ^\nggml-metal.m:1048:43: warning: unused variable 'n' [-Wunused-variable]\n                            const int64_t n = ggml_nelements(dst)/4;\n                                          ^\nggml-metal.m:1052:39: warning: unused variable 'nth' [-Wunused-variable]\n                            const int nth = MIN((int) pipeline.maxTotalThreadsPerThreadgroup, ne0);\n                                      ^\nggml-metal.m:984:38: warning: unused variable 'offs' [-Wunused-variable]\n                        const size_t offs = 0;\n                                     ^\nggml-metal.m:1100:39: warning: unused variable 'nth' [-Wunused-variable]\n                            const int nth = MIN((int) pipeline.maxTotalThreadsPerThreadgroup, ne00);\n                                      ^\nggml-metal.m:1066:38: warning: unused variable 'pnb1' [-Wunused-variable]\n                        const size_t pnb1 = ((int32_t *) dst->op_params)[0];\n                                     ^\nggml-metal.m:1067:38: warning: unused variable 'pnb2' [-Wunused-variable]\n                        const size_t pnb2 = ((int32_t *) dst->op_params)[1];\n                                     ^\nggml-metal.m:1068:38: warning: unused variable 'pnb3' [-Wunused-variable]\n                        const size_t pnb3 = ((int32_t *) dst->op_params)[2];\n                                     ^\nggml-metal.m:1069:38: warning: unused variable 'offs' [-Wunused-variable]\n                        const size_t offs = ((int32_t *) dst->op_params)[3];\n                                     ^\nggml-metal.m:1137:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN((int) pipeline.maxTotalThreadsPerThreadgroup, ne00);\n                                  ^\nggml-metal.m:1168:49: warning: unused variable 'pipeline' [-Wunused-variable]\n                    id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_CLAMP].pipeline;\n                                                ^\nggml-metal.m:1181:35: warning: unused variable 'n' [-Wunused-variable]\n                    const int64_t n = ggml_nelements(dst);\n                                  ^\nggml-metal.m:1192:61: warning: unused variable 'pipeline' [-Wunused-variable]\n                                id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_TANH].pipeline;\n                                                            ^\nggml-metal.m:1198:47: warning: unused variable 'n' [-Wunused-variable]\n                                const int64_t n = ggml_nelements(dst);\n                                              ^\nggml-metal.m:1204:61: warning: unused variable 'pipeline' [-Wunused-variable]\n                                id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_RELU].pipeline;\n                                                            ^\nggml-metal.m:1210:47: warning: unused variable 'n' [-Wunused-variable]\n                                const int64_t n = ggml_nelements(dst);\n                                              ^\nggml-metal.m:1281:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_SQR].pipeline;\n                                                    ^\nggml-metal.m:1287:39: warning: unused variable 'n' [-Wunused-variable]\n                        const int64_t n = ggml_nelements(dst);\n                                      ^\nggml-metal.m:1295:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_SUM_ROWS].pipeline;\n                                                    ^\nggml-metal.m:1357:37: warning: unused variable 'm0' [-Wunused-variable]\n                        const float m0 = powf(2.0f, -(max_bias       ) / n_head_log2);\n                                    ^\nggml-metal.m:1358:37: warning: unused variable 'm1' [-Wunused-variable]\n                        const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_head_log2);\n                                    ^\nggml-metal.m:1387:35: warning: unused variable 'n_past' [-Wunused-variable]\n                        const int n_past = ((int32_t *)(dst->op_params))[0];\n                                  ^\nggml-metal.m:1450:57: error: use of undeclared identifier 'MTLGPUFamilyApple7'\n                        if ([ctx->device supportsFamily:MTLGPUFamilyApple7] &&\n                                                        ^\nggml-metal.m:1698:43: warning: unused variable 'mem_size' [-Wunused-variable]\n                                const int mem_size = src0t == GGML_TYPE_IQ2_XXS ? 256*8+128 : 512*8+128;\n                                          ^\nggml-metal.m:1703:43: warning: unused variable 'mem_size' [-Wunused-variable]\n                                const int mem_size = src0t == GGML_TYPE_IQ3_XXS ? 256*4+128 : 512*4;\n                                          ^\nggml-metal.m:1708:43: warning: unused variable 'mem_size' [-Wunused-variable]\n                                const int mem_size = 32*sizeof(float);\n                                          ^\nggml-metal.m:1728:47: warning: unused variable 'ny' [-Wunused-variable]\n                                const int64_t ny = (ne11 + nrows - 1)/nrows;\n                                              ^\nggml-metal.m:1773:57: error: use of undeclared identifier 'MTLGPUFamilyApple7'\n                        if ([ctx->device supportsFamily:MTLGPUFamilyApple7] &&\n                                                        ^\nggml-metal.m:2023:43: warning: unused variable 'mem_size' [-Wunused-variable]\n                                const int mem_size = src0t == GGML_TYPE_IQ2_XXS ? 256*8+128 : 512*8+128;\n                                          ^\nggml-metal.m:2028:43: warning: unused variable 'mem_size' [-Wunused-variable]\n                                const int mem_size = src0t == GGML_TYPE_IQ3_XXS ? 256*4+128 : 512*4;\n                                          ^\nggml-metal.m:2033:43: warning: unused variable 'mem_size' [-Wunused-variable]\n                                const int mem_size = 32*sizeof(float);\n                                          ^\nggml-metal.m:2053:47: warning: unused variable 'ny' [-Wunused-variable]\n                                const int64_t ny = (_ne1 + nrows - 1)/nrows; // = _ne1\n                                              ^\nggml-metal.m:2015:39: warning: unused variable 'tgz' [-Wunused-variable]\n                            const int tgz = dst_rows;\n                                      ^\nggml-metal.m:2116:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_RMS_NORM].pipeline;\n                                                    ^\nggml-metal.m:2126:39: warning: unused variable 'nrows' [-Wunused-variable]\n                        const int64_t nrows = ggml_nrows(src0);\n                                      ^\nggml-metal.m:2137:37: warning: unused variable 'eps' [-Wunused-variable]\n                        const float eps = 1e-6f; // TODO: temporarily hardcoded\n                                    ^\nggml-metal.m:2139:39: warning: unused variable 'n_groups' [-Wunused-variable]\n                        const int32_t n_groups = ((int32_t *) dst->op_params)[0];\n                                      ^\nggml-metal.m:2141:29: warning: unused variable 'nth' [-Wunused-variable]\n                        int nth = 32; // SIMD width\n                            ^\nggml-metal.m:2147:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_GROUP_NORM].pipeline;\n                                                    ^\nggml-metal.m:2169:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(256, ne00);\n                                  ^\nggml-metal.m:2171:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_NORM].pipeline;\n                                                    ^\nggml-metal.m:2181:39: warning: unused variable 'nrows' [-Wunused-variable]\n                        const int64_t nrows = ggml_nrows(src0);\n                                      ^\nggml-metal.m:2189:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(1024, ne00);\n                                  ^\nggml-metal.m:2198:37: warning: unused variable 'm0' [-Wunused-variable]\n                        const float m0 = powf(2.0f, -(max_bias) / n_heads_log2_floor);\n                                    ^\nggml-metal.m:2199:37: warning: unused variable 'm1' [-Wunused-variable]\n                        const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_heads_log2_floor);\n                                    ^\nggml-metal.m:2201:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_ALIBI_F32].pipeline;\n                                                    ^\nggml-metal.m:2232:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(1024, ne00);\n                                  ^\nggml-metal.m:2234:35: warning: unused variable 'n_past' [-Wunused-variable]\n                        const int n_past     = ((int32_t *) dst->op_params)[0];\n                                  ^\nggml-metal.m:2235:35: warning: unused variable 'n_dims' [-Wunused-variable]\n                        const int n_dims     = ((int32_t *) dst->op_params)[1];\n                                  ^\nggml-metal.m:2236:35: warning: unused variable 'mode' [-Wunused-variable]\n                        const int mode       = ((int32_t *) dst->op_params)[2];\n                                  ^\nggml-metal.m:2238:35: warning: unused variable 'n_orig_ctx' [-Wunused-variable]\n                        const int n_orig_ctx = ((int32_t *) dst->op_params)[4];\n                                  ^\nggml-metal.m:2295:39: warning: unused variable 's0' [-Wunused-variable]\n                        const int32_t s0 = ((const int32_t *)(dst->op_params))[0];\n                                      ^\nggml-metal.m:2296:39: warning: unused variable 's1' [-Wunused-variable]\n                        const int32_t s1 = ((const int32_t *)(dst->op_params))[1];\n                                      ^\nggml-metal.m:2297:39: warning: unused variable 'p0' [-Wunused-variable]\n                        const int32_t p0 = ((const int32_t *)(dst->op_params))[2];\n                                      ^\nggml-metal.m:2298:39: warning: unused variable 'p1' [-Wunused-variable]\n                        const int32_t p1 = ((const int32_t *)(dst->op_params))[3];\n                                      ^\nggml-metal.m:2299:39: warning: unused variable 'd0' [-Wunused-variable]\n                        const int32_t d0 = ((const int32_t *)(dst->op_params))[4];\n                                      ^\nggml-metal.m:2300:39: warning: unused variable 'd1' [-Wunused-variable]\n                        const int32_t d1 = ((const int32_t *)(dst->op_params))[5];\n                                      ^\nggml-metal.m:2304:39: warning: unused variable 'N' [-Wunused-variable]\n                        const int32_t N  = src1->ne[is_2D ? 3 : 2];\n                                      ^\nggml-metal.m:2306:39: warning: unused variable 'IH' [-Wunused-variable]\n                        const int32_t IH = is_2D ? src1->ne[1] : 1;\n                                      ^\nggml-metal.m:2307:39: warning: unused variable 'IW' [-Wunused-variable]\n                        const int32_t IW =         src1->ne[0];\n                                      ^\nggml-metal.m:2312:39: warning: unused variable 'OH' [-Wunused-variable]\n                        const int32_t OH = is_2D ? dst->ne[2] : 1;\n                                      ^\nggml-metal.m:2313:39: warning: unused variable 'OW' [-Wunused-variable]\n                        const int32_t OW =         dst->ne[1];\n                                      ^\nggml-metal.m:2315:39: warning: unused variable 'CHW' [-Wunused-variable]\n                        const int32_t CHW = IC * KH * KW;\n                                      ^\nggml-metal.m:2317:39: warning: unused variable 'ofs0' [-Wunused-variable]\n                        const int32_t ofs0 = src1->nb[is_2D ? 3 : 2] / 4;\n                                      ^\nggml-metal.m:2318:39: warning: unused variable 'ofs1' [-Wunused-variable]\n                        const int32_t ofs1 = src1->nb[is_2D ? 2 : 1] / 4;\n                                      ^\nggml-metal.m:2349:35: warning: unused variable 'sf' [-Wunused-variable]\n                        const int sf = dst->op_params[0];\n                                  ^\nggml-metal.m:2374:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN((int) pipeline.maxTotalThreadsPerThreadgroup, ne0);\n                                  ^\nggml-metal.m:2382:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_PAD_F32].pipeline;\n                                                    ^\nggml-metal.m:2404:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(1024, ne0);\n                                  ^\nggml-metal.m:2418:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_ARANGE_F32].pipeline;\n                                                    ^\nggml-metal.m:2426:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(1024, ne0);\n                                  ^\nggml-metal.m:2435:35: warning: unused variable 'max_period' [-Wunused-variable]\n                        const int max_period = dst->op_params[1];\n                                  ^\nggml-metal.m:2439:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_TIMESTEP_EMBEDDING_F32].pipeline;\n                                                    ^\nggml-metal.m:2448:35: warning: unused variable 'nth' [-Wunused-variable]\n                        const int nth = MIN(1024, half);\n                                  ^\nggml-metal.m:2457:35: warning: unused variable 'nrows' [-Wunused-variable]\n                        const int nrows = ggml_nrows(src0);\n                                  ^\nggml-metal.m:2469:35: warning: unused variable 'mem_size' [-Wunused-variable]\n                        const int mem_size = GGML_PAD(ne00_padded*sizeof(int32_t), 16);\n                                  ^\nggml-metal.m:2495:53: warning: unused variable 'pipeline' [-Wunused-variable]\n                        id<MTLComputePipelineState> pipeline = ctx->kernels[GGML_METAL_KERNEL_TYPE_LEAKY_RELU_F32].pipeline;\n                                                    ^\nggml-metal.m:2502:39: warning: unused variable 'n' [-Wunused-variable]\n                        const int64_t n = ggml_nelements(dst);\n                                      ^\nggml-metal.m:2512:29: warning: unused variable 'nth' [-Wunused-variable]\n                        int nth = MIN(1024, ne00/ggml_blck_size(src0->type));\n                            ^\n80 warnings and 9 errors generated.\nmake: *** [ggml-metal.o] Error 1\n\nDoes anyone know what I'm missing?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6806",
        "createdAt": "2024-04-21T11:44:48Z",
        "author": {
            "login": "Spider-netizen"
        }
    },
    {
        "title": "Autoscaling Llama.cpp server clusters in GPU spot instances.",
        "bodyText": "I wrote up a Terraform module PoC for autoscaling clusters of Llama.cpp server in GCP spot instances with GPUs. Autoscaling is a bit overzealous but it tends to work pretty well. Anyone curious to try I'd love to hear feedback.\nhttps://github.com/jboero/terraform-google-llama-autoscale",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6764",
        "createdAt": "2024-04-19T09:20:03Z",
        "author": {
            "login": "jboero"
        }
    },
    {
        "title": "how to use GPU to increase the speed of conversion?",
        "bodyText": "Is there a way I can use GPU vram to increase the speed of model conversion to gguf?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6827",
        "createdAt": "2024-04-22T11:28:35Z",
        "author": {
            "login": "monk1337"
        }
    },
    {
        "title": "Report on LLaMa-3 and b2710",
        "bodyText": "Using b2710 in combination with LLaMa3 b8 instruct conversion testing on Linux Mint with these settings went very promising:\n#----------------- Test b2710 ------------------------------\n./main -t 4 -m ./models/7B/Meta_3_8B_chat_f16_V4_q4_0.gguf --log-enable --color -c 8192 --temp 0.7 --mirostat 2 --repeat-penalty 1.1 -n -1 -i --in-prefix \"\" --in-suffix \"\" -r \"<|eot_id|>\" -p \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, respectful and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\nBased on original META LLaMa-3.\n#-------------------- Conversion to f16 with ----------------\npython3 convert.py ./models/Meta-Llama-3-8B-Instruct --outtype f16 --vocab-type bpe\n#------------------- Quantize with\n./quantize ./models/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf ./models/Meta-Llama-3-8B-Instruct/Meta_3_8B_chat_f16_V4_q4_0.gguf Q4_0\n./quantize ./models/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf ./models/Meta-Llama-3-8B-Instruct/Meta_3_8B_chat_f16_V4_q5_0.gguf Q5_0\n./quantize ./models/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf ./models/Meta-Llama-3-8B-Instruct/Meta_3_8B_chat_f16_V4_q8_0.gguf Q8_0\n./quantize ./models/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf ./models/Meta-Llama-3-8B-Instruct/Meta_3_8B_chat_f16_V4_q4_k_m.gguf Q4_K_M\n./quantize ./models/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf ./models/Meta-Llama-3-8B-Instruct/Meta_3_8B_chat_f16_V4_q4_k_s.gguf Q4_K_S\n./quantize ./models/Meta-Llama-3-8B-Instruct/ggml-model-f16.gguf ./models/Meta-Llama-3-8B-Instruct/Meta_3_8B_chat_f16_V4_q5_k_m.gguf Q5_K_M\nThank you all for the remarkable work.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6824",
        "createdAt": "2024-04-22T09:13:33Z",
        "author": {
            "login": "wro52"
        }
    },
    {
        "title": "Llama 3 is out",
        "bodyText": "https://ai.meta.com/blog/meta-llama-3/\nhttps://llama.meta.com/llama-downloads",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6746",
        "createdAt": "2024-04-18T16:46:11Z",
        "author": {
            "login": "Galunid"
        }
    },
    {
        "title": "performance difference based on number of tokens to process",
        "bodyText": "good evening. I was trying to learn a little about llama.cpp library and stumbled upon this.\nThis is a small code change based on examples/simple :\nokuvshynov@63cd5b5 adds N mock tokens on every llama_decode.\nWe ignore the output for those fake tokens, clean the cache for them and just keep going as before.\nAll the tests were on m2 ultra and mistral-7b-v0.1.Q8_0 model.\nA question is - why is the difference between 0 and 1 so dramatic on GPU? Is there any optimization done specifically for this scenario, as I assume it's quite common? Or maybe i just misconfigured something?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6777",
        "createdAt": "2024-04-20T02:23:50Z",
        "author": {
            "login": "okuvshynov"
        }
    },
    {
        "title": "Langchain chatbot with chroma vector storage memory",
        "bodyText": "This is an upgrade to my previous chatbot. It adds a vector storage memory using ChromaDB. The main chatbot is built using llama-cpp-python, langchain and chainlit. It supports json, yaml, V2 and Tavern character card formats.\nThis version uses langchain llamacpp embeddings to parse documents into chroma vector storage collections. There is also a test script to query and test the collections Everything is local and in python. Runs in a virtual env with minimum installations.\nThe purpose of this project is to give out a more fleshed out character based chatbot as a foundation for testing and prototyping chroma features.\nhttps://github.com/ossirytk/llama-cpp-chat-memory",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3954",
        "createdAt": "2023-11-05T11:22:10Z",
        "author": {
            "login": "ossirytk"
        }
    },
    {
        "title": "Tips for optimizing performance on a 2019 Macbook Pro Intel machine?",
        "bodyText": "Spent a lot of time today running benchmarks today trying to optimize my settings and I'm hoping some other folks might have some suggestions in case I've missed anything.\nI'm running:\n\nCurrent\n2019 16in Intel Macbook Pro\n32gb of RAM\nAMD Radeon Pro 5500M GPU.\nMixtral 8x7b 4-bit quantized model\n\nI'm getting these results generally (2 runs):\nllama_print_timings:        load time =  153625.16 ms\nllama_print_timings:      sample time =       3.93 ms /    87 runs   (    0.05 ms per token, 22148.68 tokens per second)\nllama_print_timings: prompt eval time =    9871.68 ms /    11 tokens (  897.43 ms per token,     1.11 tokens per second)\nllama_print_timings:        eval time =   40815.97 ms /    86 runs   (  474.60 ms per token,     2.11 tokens per second)\nllama_print_timings:       total time =   50710.67 ms /    97 tokens\nggml_metal_free: deallocating\nwarning: failed to munlock buffer: Cannot allocate memory\nLog end\n\nllama_print_timings:        load time =  156684.36 ms\nllama_print_timings:      sample time =       2.79 ms /    87 runs   (    0.03 ms per token, 31149.30 tokens per second)\nllama_print_timings: prompt eval time =   12772.96 ms /    11 tokens ( 1161.18 ms per token,     0.86 tokens per second)\nllama_print_timings:        eval time =   41914.18 ms /    86 runs   (  487.37 ms per token,     2.05 tokens per second)\nllama_print_timings:       total time =   54710.37 ms /    97 tokens\nggml_metal_free: deallocating\nwarning: failed to munlock buffer: Cannot allocate memory\nLog end\n\nSettings:\n./main \\\n  --ctx_size 2048 \\\n  -t 6 \\\n  -ngl 4 \\\n  --mlock \\\n  --temp 0 \\\n  --prompt \"What are the 3 most popular red flowers?\" \\\n  -m /Users/geuis/.cache/huggingface/hub/models--TheBloke--Mixtral-8x7B-Instruct-v0.1-GGUF/snapshots/fa1d3835c5d45a3a74c0b68805fcdc133dba2b6a/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n\nIf I bump -ngl over 4 I see a segmentation fault. It offloads about 3.1gb to the video card at 4.\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, ( 3172.05 /  8176.00)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6802",
        "createdAt": "2024-04-21T04:55:08Z",
        "author": {
            "login": "geuis"
        }
    },
    {
        "title": "Thread safety",
        "bodyText": "Is llama.cpp thread safe? I have encountered some problems and weird issues when creating a CTX on another thread and then using it in another.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/499",
        "createdAt": "2023-03-25T15:32:41Z",
        "author": {
            "login": "SpeedyCraftah"
        }
    },
    {
        "title": "server log question",
        "bodyText": "Hi all, and especially @phymbert. I think what you do is very important.\nMy question is about server logging. Readme shows --log-format & --log-disable.\nBy default --log-disable is Enabled, which is confusing. Further, I don't see a log in any format as a result of using --log-format.  So, I'm not getting a proper log, and I don't see how to aquire one for server.\n./build/bin/server -m ~/WizardLM-2-7B-IQ4_XS.gguf -t 3 -c 2048 --log-format text\n\n[1713621145] warming up the model with an empty run\nINFO [                    init] initializing slots | tid=\"532689188096\" timestamp=1713621148 n_slots=1\nINFO [                    init] new slot | tid=\"532689188096\" timestamp=1713621148 id_slot=0 n_ctx_slot=2048\nINFO [                    main] model loaded | tid=\"532689188096\" timestamp=1713621148\nINFO [                    main] chat template | tid=\"532689188096\" timestamp=1713621148 chat_example=\"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi there<|im_end|>\\n<|im_start|>user\\nHow are you?<|im_end|>\\n<|im_start|>assistant\\n\" built_in=true\nINFO [                    main] HTTP server listening | tid=\"532689188096\" timestamp=1713621148 n_threads_http=\"7\" port=\"8080\" hostname=\"127.0.0.1\"\nINFO [            update_slots] all slots are idle | tid=\"532689188096\" timestamp=1713621148\nINFO [      log_server_request] request | tid=\"515573947648\" timestamp=1713621177 remote_addr=\"127.0.0.1\" remote_port=41674 status=200 method=\"GET\" path=\"/\" params={}                                 \nINFO [      log_server_request] request | tid=\"515573947648\" timestamp=1713621177 remote_addr=\"127.0.0.1\" remote_port=41674 status=200 method=\"GET\" path=\"/index.js\" params={}                          \nINFO [      log_server_request] request | tid=\"515572911360\" timestamp=1713621177 remote_addr=\"127.0.0.1\" remote_port=41678 status=200 method=\"GET\" path=\"/json-schema-to-grammar.mjs\" params={}        \nINFO [      log_server_request] request | tid=\"515571875072\" timestamp=1713621177 remote_addr=\"127.0.0.1\" remote_port=41676 status=200 method=\"GET\" path=\"/completion.js\" params={}                     \nINFO [      log_server_request] request | tid=\"515571875072\" timestamp=1713621177 remote_addr=\"127.0.0.1\" remote_port=41676 status=404 method=\"GET\" path=\"/favicon.ico\" params={}\nINFO [   launch_slot_with_task] slot is processing task | tid=\"532689188096\" timestamp=1713621186 id_slot=0 id_task=0                                \nINFO [            update_slots] kv cache rm [p0, end) | tid=\"532689188096\" timestamp=1713621186 id_slot=0 id_task=0 p0=0                              \nINFO [           print_timings] prompt eval time     =   23387.91 ms /    61 tokens (  383.41 ms per token,     2.61 tokens per second) | tid=\"532689188096\" timestamp=1713621220 id_slot=0 id_task=0 t_prompt_processing=23387.913 n_prompt_tokens_processed=61 t_token=383.4084098360656 n_tokens_second=2.608184834619489\nINFO [           print_timings] generation eval time =   10833.40 ms /    24 runs   (  451.39 ms per token,     2.22 tokens per second) | tid=\"532689188096\" timestamp=1713621220 id_slot=0 id_task=0 t_token_generation=10833.396 n_decoded=24 t_token=451.3915 n_tokens_second=2.2153718003108165         \nINFO [           print_timings]           total time =   34221.31 ms | tid=\"532689188096\" timestamp=1713621220 id_slot=0 id_task=0 t_prompt_processing=23387.913 t_token_generation=10833.396 t_total=34221.309\nINFO [            update_slots] slot released | tid=\"532689188096\" timestamp=1713621220 id_slot=0 id_task=0 n_ctx=2048 n_past=84 n_system_tokens=0 n_cache_tokens=84 truncated=false\nINFO [            update_slots] all slots are idle | tid=\"532689188096\" timestamp=1713621220\nINFO [      log_server_request] request | tid=\"515554061568\" timestamp=1713621220 remote_addr=\"127.0.0.1\" remote_port=41680 status=200 method=\"POST\" path=\"/completion\" params={}                       \nINFO [            update_slots] all slots are idle | tid=\"532689188096\" timestamp=1713621220\n^CINFO [            update_slots] all slots are idle | tid=\"532689188096\" timestamp=1713621272\n\nI expected llama.log to include everything, including the conversation for this run:\n\nInstead llama.log recorded, \"warming up the model with an empty run\", and that's all.\nDoes CTRL + C to kill the server block logging somehow?  Am I missing something obvious?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6786",
        "createdAt": "2024-04-20T14:19:06Z",
        "author": {
            "login": "Jeximo"
        }
    },
    {
        "title": "Simplified llama.dll",
        "bodyText": "Llama.dll that can be downloaded from this repo is suitable mostly for programming languages that have abilities to work with rather difficult (for novice coder) concepts like pointers, structures etc. Does anybody know any simplified implementation of llama.dll? For example, it would be wonderful if it will be possible to work with this dll in such way:\nCONSOLE\nDim LLama_DLL as Integer\nLoad \"llama.dll\" as LLama_DLL\nCall function Load_Local_Gguf (\"Llama-2-7b-q8.0.gguf\") from LLama_DLL\nCall function Set_Context_Size_For_Loaded_Gguf (4096) from LLama_DLL\nDim Answer$ As String\nDim Question$ As String\nLabel #Begin\nInput Question$\nAnswer$ = Call function SentPromptToLoadedGguf (Question$) from LLama_DLL\nPrint Answer$\nGoto #Begin",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6794",
        "createdAt": "2024-04-20T18:01:01Z",
        "author": {
            "login": "JohnClaw"
        }
    },
    {
        "title": "I would like to know about the behavior of the server's Parallel option. Doesn't processing in parallel = increase GPU usage?",
        "bodyText": "I would like to know about the behavior of the server's Parallel option. Doesn't processing in parallel = increase GPU usage?\nSuppose that the behavior without the parallel option is as follows.\n\nI thought that adding the parallel option would work as follows.\n\nBut in reality it seems to work like this.\n\nEven with the parallel option, GPU usage does not increase. Throughput has also increased only slightly. (On my server, GPU usage stays at around 10%)\nIs my understanding correct?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6782",
        "createdAt": "2024-04-20T11:10:14Z",
        "author": {
            "login": "Taikono-Himazin"
        }
    },
    {
        "title": "400GB for llama 3?!?! should we be making our own gpu now?",
        "bodyText": "can someone take a guess what's the most powerful amd epyc 9000 series and motherboard and ddr ram types and what kind of t/s we can expect to get to load a 400GB llama 3 in future?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6749",
        "createdAt": "2024-04-18T18:58:19Z",
        "author": {
            "login": "ouvaa"
        }
    },
    {
        "title": "Llama open source?",
        "bodyText": "I know Llama 3 is open source, but when I go to websites like replicate, it shows a price per 1 million tokens if I want to use that API. So I don't think I fully understand the concept. I am trying to develop an application that uses Llama 3, but I want to grasp the foundational knowledge first. Could someone explain this to me? And what kind of costs would be involved if I want to use llama for my application? (after calculation, per user is expected to use 15-20k tokens per day)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6771",
        "createdAt": "2024-04-19T16:46:25Z",
        "author": {
            "login": "avsk53"
        }
    },
    {
        "title": "Q4_K Quantization Scheme adaptation",
        "bodyText": "So I see that to dequantize a weight in Q4_K format I have to do:\ny = s * q - m\ny is the dequantized weight (float)\ns is the scale (float)\nq is the quantized weight (int4)\nm is the zero point offset (float)\nNow the challenge is that I have hardware that expects the following scheme:\ny = s * (q - z)\nThe difference from above is that z is the zero point (int4)\nThis is also the scheme that pytorch uses: https://pytorch.org/blog/quantization-in-practice/\nI want to use the parameters from llama.cpp with my hardware, so I try to do some math...\nSet the two equations equal to each other, and solve for z.\nI get z = round(m/s)\nWhen I simulate this adaptation, I get catastrophic accuracy loss, even without hardware involved.\nIs there something fundamentally wrong with this math?\nIs it not possible to reconcile these two quantization schemes?\u00a0",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6760",
        "createdAt": "2024-04-19T07:49:06Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "amd epyc with this motherboard to future proof 1TB llama models.",
        "bodyText": "thinking of buying this for llama inference etc. what do u guys think? at least i can support 1TB llama models comfortably.\nnow we have 281GB Mistral which i havent test yet (coz i dont hv the hardware)\nhttps://www.gigabyte.com/Enterprise/Server-Motherboard/MZ33-AR0-rev-1x\nopinion? how many tok/s do u guys reckon i can get out of this? i read about 12 channel bandwidth meaning can support up to around 400GB/s bandwidth? possible to go higher? pure cpu inference only. i'm talking about the 281GB now coz i was thinking of getting Mac Studio with 192GB but even this cant support 281GB.\nopinions / suggestions appreciated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6719",
        "createdAt": "2024-04-17T08:32:41Z",
        "author": {
            "login": "ouvaa"
        }
    },
    {
        "title": "How to use template, after adding llama_chat_apply_template?",
        "bodyText": "I've been struggling with template for a long time, and now I've discovered that in the last commits 11b12de what I've been waiting for. But how to use it? Do I need to specify any parameters? Or is everything automatically loaded from the model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5582",
        "createdAt": "2024-02-19T09:47:26Z",
        "author": {
            "login": "Folko-Ven"
        }
    },
    {
        "title": "Effort Engine (potential method of speeding up LLM matmul by dropping some calculations)",
        "bodyText": "This has been making the rounds on social media and I think it's worth posting here so our devs and users can discuss this. I'm still at work and haven't read through the whole thing in detail but it's basically an approach to drop certain multiplications that don't really affect the output.\nApparently this gives better results than dropping full layers, though I don't see any perplexity curves posted.\nArticle: https://kolinko.github.io/effort/\nHN thread: https://news.ycombinator.com/item?id=40067677",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6731",
        "createdAt": "2024-04-17T21:53:05Z",
        "author": {
            "login": "netrunnereve"
        }
    },
    {
        "title": "FIM/Infill changes vs. model support & GGUF regeneration etc.?",
        "bodyText": "I noticed the recent release notes about it and the following commit activity wrt. FIM / Infill and supporting more models e.g. CodeGemma having that capability but having various vocabulary symbols required to solicit it i.e.:\n#6689\n#6626\nI haven't scrutinized the details but I have a couple questions:\nCodellama (and I suppose its close derivative relations) was AFAICT the nominally originally supported model for\nFIM.  Now CodeGemma has been mentioned wrt. improving support as above.\nIs there a summary of status of which models are presently supported for the FIM use case given the recent changes?\nObviously Codellama and codegemma.  I'm not sure what prominent others may be \"equivalent\" to those because\nof being derivatives / using the same vocabulary for the function.\nAnd then I'm less sure about others e.g. the deepseek-coder variant below which mentions in its model card that\nit was trained from scratch so I gather it may not be detected as compatible with the above?\nhttps://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\nAlso after b2680 changed the gguf vocabulary is my conception correct that if I've previously downloaded other models\nwhich might newly now support the FIM feature that I'd have to obtain re-converted GGUF models encoded using the new\ncodebase post b2680 in order to use the FIM (as opposed to any runtime rebuild / configuration tweaks + older GGUFs)?\nThanks!  It's nice to see codegemma etc. getting support for this, it was on my list to try for IDE use.\nContext:\nb2680\ngguf : add special tokens metadata for FIM/Infill (#6689)\nThis commit adds special token metadata for Fill-In-the-Middle\n(FIM)/Infill to the GGUF model.\nThe motivation for this is that currently there is support for CodeLlama\nbut other models exist now like CodeGemma, but the different models use\ndifferent token ids for the special tokens and this commit allows for\nsupporting multiple models.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6708",
        "createdAt": "2024-04-16T18:43:18Z",
        "author": {
            "login": "ghchris2021"
        }
    },
    {
        "title": "trying to squeeze every bit of performance out of llama.cpp, clear linux os? anyone has the max tok/s set up on cpu?",
        "bodyText": "as titled. 1tok/s extra is worth the switch. also trying to get new rig for the 281gb new mistral. seemed like only amd motherboard with 1tb ram etc can make this work. gpu is too expensive considering the power consumption price factor.\ni would prefer to future proof the rig to be able to run 1tb llama models in future.\nanyone has any such setups to share? especially with operating systems used.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6711",
        "createdAt": "2024-04-16T22:51:03Z",
        "author": {
            "login": "ouvaa"
        }
    },
    {
        "title": "How to stop ongoing inference process",
        "bodyText": "I am hoping to find a way to stop an ongoing inference / prediction process started. So that in case of this example:\nmsg 1: \"user: hello\"\nmsg 2: \"user: who are you\"\nI would like to be able to stop the proces that is started with the 'hello' input to free up the resources and instead then send:\n\"user: hello\nuser: who are you\"\nAs one message.\nI have been trying to find out which proces I should target for this usecase, and than perhaps have a uid - boolean pointer or something that should stop that process (i assume its a recursion loop somewhere) if the pointer is set to true.\nNow eyeballing llama_get_logits_ith for that. But I'm a noob, so not sure, and perhaps there is an easier way to achieve what I need!\nAny help / feedback will be greatly appreciated!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6489",
        "createdAt": "2024-04-04T16:41:08Z",
        "author": {
            "login": "paulrouge"
        }
    },
    {
        "title": "LLM inference in Delphi & making censored models uncensored and vice versa.",
        "bodyText": "Working on Dllama, allowing local LLM inference using Delphi powered by llama.cpp.\nFound a way to coerce a censored model into being uncensored and vice versa.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6694",
        "createdAt": "2024-04-15T18:53:43Z",
        "author": {
            "login": "jarroddavis68"
        }
    },
    {
        "title": "Is there any way to specify the embedding dimensionality via llama.cpp?",
        "bodyText": "I'm using nomic-embed-text-v1.5 for my embedding model and it works but it's returning the full 768 dimensions. I'd like to drop this to 128 dimensions but I don't see a way to do that via llama.cpp.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6618",
        "createdAt": "2024-04-11T21:08:41Z",
        "author": {
            "login": "Stevenic"
        }
    },
    {
        "title": "Issues with Windows & GPU",
        "bodyText": "I am having issues with getting GPU support with this.\nI tried to follow the instructions here but honestly the instructions were easier to follow on the langchain docs.  Can someone point me to something they know works?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6627",
        "createdAt": "2024-04-12T06:42:47Z",
        "author": {
            "login": "JTMarsh556"
        }
    },
    {
        "title": "adding support for the chatglm3-6b model",
        "bodyText": "Hello everyone, I'm currently adding support for the chatglm3-6b model on llama.cpp. I've done a quick check on the inference algorithm, but I didn't find any issues.\nHowever, the inference results are all over the place.\nHow should I go about debugging this?\nAre there any tricks for pinpointing the issue when inference goes wrong?\nThe following are my log link and code modifications:\nmain.log\n0001-feature-add-chatglm-support.patch.txt",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6623",
        "createdAt": "2024-04-12T02:31:27Z",
        "author": {
            "login": "mnlife"
        }
    },
    {
        "title": "How to use Llama-cpp-python for text analysis?",
        "bodyText": "I am still confused how to use  Llama-cpp-python in a correct way when I want to analyze the content of a text.\nI am using mistral-7b-instruct-v0.2.Q3_K_S.gguf and I have a text and an instruction .  I want to extract information form the given text. This is not the chat-use-case that seems to be discussed 99% of the time.\nI am using the following Prompt Template for my approach:\n<s>[INST] My Instruction............\n[/INST]\nMy Text.......\n................\n...................\n\nIn my Python Code I call the model like this:\nresult = llm(prompt, max_tokens=max_tokens, \n                     temperature=0,\n                     echo=False\n                     )\nIs this a correct way to build a prompt for text analyses?\nDid the prompt depends on the model I am using or is llama-cpp generalizing this some how?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6641",
        "createdAt": "2024-04-12T15:26:07Z",
        "author": {
            "login": "rsoika"
        }
    },
    {
        "title": "Regarding detection and use of processor feature sets",
        "bodyText": "I was just thinking if it would be a good idea to have the ability to detect and enable/disable various instruction sets at runtime instead of the current compile-time defines? For the windows release builds and possibly future statically linked releases for unixes (flatpaks, static docker binary builds, mac dmg's, etc.) this would also remove the need to have multiple builds for the different feature sets.\nThis would be easy to implement in a very fast and completely platform-independent way by using straight cpu instructions. For x86 it can be done simply using the cpuid 0F A2 instruction. You can retrieve all the instructions compatible with the current processor with that one simple instruction.\nThis is already how I check for AVX512F availability in the windows-latest-cmake gh action runner in here: \n  \n    \n      llama.cpp/.github/workflows/build.yml\n    \n    \n        Lines 174 to 181\n      in\n      34c1072\n    \n  \n  \n    \n\n        \n          \n                 - name: Check AVX512F support \n        \n\n        \n          \n                   id: check_avx512f \n        \n\n        \n          \n                   if: ${{ matrix.build == 'avx512' }} \n        \n\n        \n          \n                   continue-on-error: true \n        \n\n        \n          \n                   run: | \n        \n\n        \n          \n                     cd build \n        \n\n        \n          \n                     Set-Content -Path .\\avx512f.exe -Value ([Convert]::FromBase64String('TVqQAAMAAAAEAAAA//8AALgAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyAAAAA4fug4AtAnNIbgBTM0hVGhpcyBwcm9ncmFtIGNhbm5vdCBiZSBydW4gaW4gRE9TIG1vZGUuDQ0KJAAAAAAAAAClmfXY4fibi+H4m4vh+JuL4fiai+P4m4si98aL4vibi7Xbq4vg+JuLUmljaOH4m4sAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQRQAATAEBAGo6H2QAAAAAAAAAAOAADwELAQYAAAIAAAAAAAAAAAAADBAAAAAQAAAAIAAAAABAAAAQAAAAAgAABAAAAAAAAAAEAAAAAAAAAAAgAAAAAgAAAAAAAAMAAAAAABAAABAAAAAAEAAAEAAAAAAAABAAAAAAAAAAAAAAAFQQAAAoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAADAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC50ZXh0AAAAsgAAAAAQAAAAAgAAAAIAAAAAAAAAAAAAAAAAACAAAGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACUEAAAiBAAAAAAAABVi+xRUVNTuAcAAAAPosHrEGaD4wGJXfxbg0X8MI1F+GoAUI1F/GoBUGr1/xUAEEAAUP8VBBBAAItF/FuDwND32BvAQMnDzMx8EAAAAAAAAAAAAACkEAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlBAAAIgQAAAAAAAApANXcml0ZUZpbGUAuQFHZXRTdGRIYW5kbGUAAEtFUk5FTDMyLmRsbAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==')) -AsByteStream \n        \n\n        \n          \n                     .\\avx512f.exe && echo \" AVX512F: YES\" && ( echo HAS_AVX512F=1 >> $env:GITHUB_ENV ) || echo \" AVX512F: NO\" \n        \n    \n  \n\n\nThe code part being:\n  unsigned int u;\n\n  __asm{\n    push ebx\n    xor ecx,ecx\n    mov eax, 7\n    cpuid\n    shr ebx, 16\n    and bx, 1\n    mov u, ebx\n    pop ebx\n  }\nARM I'm not too familiar with but I believe the mrs instruction is the one to use for it.\nThe only minor issue is that some compilers (looking at you, MSVC) do not support inline assembly for x64 for the reason of being too lazy to make a decent compiler. Fortunately this can be circumvented by simply putting the instructions in an array like this . That way inline assembly can be used in x64 and is also supported by all compilers.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/535",
        "createdAt": "2023-03-26T17:37:18Z",
        "author": {
            "login": "anzz1"
        }
    },
    {
        "title": "f16 or f32",
        "bodyText": "Why do you specify f16 when quantizing?\nThey all specify --outtype f16.\nAre there any disadvantages to the f32 other than its large size?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6675",
        "createdAt": "2024-04-14T15:28:10Z",
        "author": {
            "login": "tomgm777"
        }
    },
    {
        "title": "Optimization to remove PCIe bandwidth limitations for large matrix multiplications on consumer GPU cards",
        "bodyText": "Matrix Multiply does O(n3) operations on O(n2) data. Given a sufficiently large matrix, this means that matrix multiply can potentially be implemented without bandwidth limitations. I used to work in HPC, and the common technique to do this was an optimization called \"strip mining\":\nYou split the target matrix, i.e. the resulting matrix to be computed, into blocks. The size of the target blocks are dependent on available size of local fastest memory, but you want as large a target block as you can fit sources and destination for in your fast memory. You only transfer to that fastest memory the rows of matrix A that will be used to compute the target block and the columns of matrix B that will be used to compute the target block. Now you can compute the whole A blockHeight x B blockWidth target block, but you only transferred A blockHeight rows + B blockWidth columns instead of the whole matrix A and matrix B. Each row of A will be used as many times as there are columns of B and vice versa. With large enough blocks, this gives you ample time to pre-transfer another set of input data while the first block is being computed (you can save even more bandwidth here, because you already have e.g. the rows of matrix A needed for the second block).\nWhen implemented with loops, it results in two extra loop levels for stepping along the blocks. Since the method is effective for each stage of memory in your system (L1 cache, L2 cache, L3 cache, on-board memory, main system RAM, local disk, remote MPI machines, remote SAN), this subdivision into blocks can usefully be done quite deeply resulting in a loop nest looking like a strip mine, hence the name of the optimization. (Tip: Doing it recursively looks a bit more tidy and doesn't hard code the number of levels, but even just one hard coded level of strip mining may be enough to solve the PCIe bandwidth issue). Your inner most level memory needs to have space for at least the input rows and columns and target space needed to compute a 2x2 target matrix to gain any advantage, but larger target blocks give bigger benefit.\nUsing BLAS libraries, you don't implement the matrix multiply yourself, but you can still subdivide the target matrix into blocks and feed BLAS only the rows and columns needed for the current block. The decomposition can also be used to support multiple GPUs, where you simply calculate different target blocks of the same input matrices on different GPUs.\nI checked with nVidia, but cuBLAS does not do this matrix decomposition, however you can of course do it manually in your calls to cuBLAS (and other BLAS libraries). They did confirm that transferring data across the PCIe bridge while computing works fine, so doing this with the large matrices of LLMs might allow us to pre-transfer matrix blocks across the PCIe bridge to run models like Llama-70B on something like 8GB gaming GPU cards at full GPU utilization. Since the matrices are large, it might even be worth experimenting with pulling in data all the way from disk (with an added level of strip mining for the disk level), to see if we can gain a speedup for e.g. 16GB or 32GB system RAM machines.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5893",
        "createdAt": "2024-03-05T21:49:05Z",
        "author": {
            "login": "00prometheus"
        }
    },
    {
        "title": "How well does the grammar-based sampling work for non-instruct models?",
        "bodyText": "I'm digging through everything llama.cpp (day 4 for me) and I've come across the grammar-based sampling work. I'm just curious how well it works for non instruction tuned models?\nI can dig in a little deeper to where I'm going with the line of question but for a bit of background... I've done a fair amount of work in this space on the validation side of things in my AlphaWave client library and the various writings I've done around improving function calling reliability. Prior to leaving Microsoft, I also helped the TypeChat Team refine their repair loop strategy so this is a topic I have particular interest in.\nI spent a lot of time working on techniques for repairing malformed responses from OpenAI models but the first thing I noticed when I started using OSS models (5 months ago) is that I couldn't get my standard repair techniques to work with any of them. Not a single one could repair a malformed structured response. I actually gave up on asking OSS models for structured responses because I couldn't trust them.\nThis work makes me think that maybe it's worth revesting reliable function calling for OSS models but I'm curious first how it works for non instruction tuned models? And then how well does it work for the instruction tunned ones?  I won't ramble too much here but getting the model to reliably return a JSON object is one thing, getting it to reliably conform to a schema is a totally other thing, and then getting to understand why it's returning a given structure (agents) is a completely different ball game.\nMy guess is that you're seeing it's good at enforcing JSON for most models and if the schema enforcement is working broadly its because your forcing the model down that path and not that its getting there willingly (think hostage with a gun to their head being forced to read a note.)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6651",
        "createdAt": "2024-04-13T07:10:28Z",
        "author": {
            "login": "Stevenic"
        }
    },
    {
        "title": "How to specify general.name when quantizing?",
        "bodyText": "I get:\n\ngeneral.name str              = .",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6663",
        "createdAt": "2024-04-13T17:18:27Z",
        "author": {
            "login": "shibe2"
        }
    },
    {
        "title": "Not able to build on RHEL 7.9 using make or cmake",
        "bodyText": "Hello Team,\nI am not able to build llama.cpp on my RHEL Setup. There might be a silly error in my environment.\nI have updated the GCC compiler to 3.9 also but still facing the below error:\n[install@ps-portfolio-seabed llama.cpp]$ make\nwhich: no ccache in (/usr/local/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/install/.local/bin:/home/install/bin)\nI ccache not found. Consider installing it for faster compilation.\nI llama.cpp build info:\nI UNAME_S:   Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion\nI CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG\nI NVCCFLAGS: -std=c++11 -O3\nI LDFLAGS:\nI CC:        cc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\nI CXX:       g++ (GCC) 4.9.4\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\nggml.c:97:23: fatal error: stdatomic.h: No such file or directory\n#include <stdatomic.h>\n^\nBelow is the output of the compiler version and location:\n[install@ps-portfolio-seabed llama.cpp]$ gcc --version\ngcc (GCC) 4.9.4\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n[ llama.cpp]$ whereis gcc\ngcc: /usr/bin/gcc /usr/lib/gcc /usr/local/bin/gcc /usr/local/lib/gcc /usr/libexec/gcc /usr/share/man/man1/gcc.1.gz\n[llama.cpp]$ cc --version\ncc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n[ llama.cpp]$ whereis cc\ncc: /usr/bin/cc\nYour immediate help is appreciated",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6620",
        "createdAt": "2024-04-11T21:53:34Z",
        "author": {
            "login": "singhpranjul"
        }
    },
    {
        "title": "How to dynamically allocate the maximum request count based on the GPU's compute capability?",
        "bodyText": ".\\llama\\server.exe -m .\\xxxxx.gguf -c 2048 -ngl %ngl% -a xxxxxx --host 127.0.0.1 --parallel 50 --threads-http 50\nThe GeForce RTX 3080 has 10GB of VRAM. When concurrently running several dozen requests in Python using async, with each request containing tens of thousands of tokens, if the server receives multiple requests at the same time and the GPU compute is fully utilized, will it continue to slowly process a dozen requests?\nI'm not particularly familiar with AI technologies. How can one determine, based on the GPU performance, the maximum concurrent request count for async processing when the model is handling several thousand tokens to ensure the model answers questions most efficiently?\nIs it necessary for users to manually adjust the maximum number of async requests?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6611",
        "createdAt": "2024-04-11T15:40:09Z",
        "author": {
            "login": "allrobot"
        }
    },
    {
        "title": "--mmproj not available in Server?",
        "bodyText": "Can anyone show an example on how to send a request a server running a multi modal LLM like llava?\nHere it seems the --mmproj argument is not being recognized. Has it been removed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6610",
        "createdAt": "2024-04-11T14:44:47Z",
        "author": {
            "login": "qnixsynapse"
        }
    },
    {
        "title": "Where can I download the LLaMa model weights?",
        "bodyText": "I am trying to LLaMa running and I am stuck at this step: https://github.com/ggerganov/llama.cpp#prepare-data--run\nI'm not sure exactly what this command is:\n65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\nIs this supposed to decompress the model weights or something?\nWhat is the difference between running llama.cpp with the BPE tokenizer model weights and the LLaMa model weights?\nDo I run both commands:\n65B 30B 13B 7B vocab.json and python convert.py models/7B/ --vocabtype bpe, but not 65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\nif I have the BPE model weights or are they both still executed consecutively?\nI have searched around the web but I can't seem to find the actual model weights. I'm also not sure if I just move all the files to the models folder once I download the model weights and if that would allow the program to start working once I run the rest of the commands in the prepare data run command and do ./main -m. Does anyone with more experience know how to get llama.cpp working?\nThank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4576",
        "createdAt": "2023-12-21T21:29:31Z",
        "author": {
            "login": "jzry"
        }
    },
    {
        "title": "Google's new model:  recurrent-Gemma (Griffin architecture) - faster inference and lower memory usage when inferencing over long contexts",
        "bodyText": "@ggerganov Google just dropped a new model architecture: Griffin, which outperforms transformers. And, it offers efficiency advantages including faster inference and lower memory usage when inferencing over longer contexts. google/gemma.cpp has implemented a C++ version of the model (for use and reference).\nPaper: Google\u2019s Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models\nrecurrentgemma-2b-it",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6605",
        "createdAt": "2024-04-11T09:42:04Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "Run Mac OS with AMD processor",
        "bodyText": "How can i run mac os with Ryzen 5 5600X and GTX 970? I can buy another GPU for Mac OS, if it needed",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6543",
        "createdAt": "2024-04-08T12:26:34Z",
        "author": {
            "login": "yuiopmju"
        }
    },
    {
        "title": "General questions around quant methods and types",
        "bodyText": "Hi everyone,\nFirst of all thanks for this great project and the amount of very nice information that we can get through the discussions and PRs.\nI would like to get started at understanding how the underlying quantization methods work in llama.cpp, I might miss important details so please correct me at any time!\nI started to learn more about the internals of gguf quants here: #1684 and my questions are mostly about 3/4 bits quantization schemes and not the recent addition of 1-bit quants.\n\n\nMy understanding of the core building block around GGUF seems to be group-wise quantization, is this correct? In that case are the activations always in half / full precision?\n\n\nSome quant method seems to quantize the scales as well - do you have a rough idea of the potential overhead that this might introduce vs non-quantizing the scales?\n\n\nIf I understood correctly, different quant schemes are usually combined together - for example the query layer could be quantized with Q3_K but the key layer with Q5_K. I first thought this was architecture-specifc, meaning each arch has its own combination - but this does not seem to be the case - so I was wondering how does the combination of different quant tensors are determined?\n\n\ne.g. below are two screenshots from two different models that derive from the same base model, which is mistral-7b and as you can see the combination of quant types look different\nBelow is for TheBloke/CapybaraHermes-2.5-Mistral-7B\n\nBelow is for NousResearch/Hermes-2-Pro-Mistral-7B\n\n\nIt also seems that the LM head is most of the times quantized, have you observed in your experience any important model quality drop when quantizing the LM head?\n\n\ncc @ikawrakow @ggerganov\nThanks so much and let me know if there is another discussion / issue I might have overlooked !",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6561",
        "createdAt": "2024-04-09T13:10:46Z",
        "author": {
            "login": "younesbelkada"
        }
    },
    {
        "title": "How do I insert a <|START_OF_TURN_TOKEN|> in interactive mode?",
        "bodyText": "I'm trying to use the new Command-R +, following the template:\n<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{prompt}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{response}\n\nI can't seem to just type this as text and have it work, because I don't think (for example) <|START_OF_TURN_TOKEN|> is being turned into a single token, but instead is becoming a string of normal text tokens.\nAm I missing a way to do this, or is it not possible in llama.cpp interactive mode?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6549",
        "createdAt": "2024-04-08T17:58:09Z",
        "author": {
            "login": "araleza"
        }
    },
    {
        "title": "Is it reasonable to add C APIs to get & set kv-cache?",
        "bodyText": "Greetings!\nI'm openning this discussion to ask if it's reasonable to add a C API to get and set kv_cache directly. :)\nDescription\nCurrently, there are following C APIs related with kv-cache (only list part of them here).\n\nllama_model_kv_override allows users to override the key-value pairs of the model meta data.\nllama_kv_cache_seq_add and llama_kv_cache_seq_div allow users to manipulate the kv-cache directly.\nllama_copy_state_data and llama_set_state_data allow users to get and set the state data, including embeddings, logits and  kv-cache.\n\nSince there is already an API to get and set the state data, including kv-cache, and the kv-cache does allow to be modified from outside, I wonder if it's possible to add the following three APIs.\n// Get the kv_cache of specified sequence ids.\n// seq_ids: the sequence ids of the kv-cache in the batch.\n// If `seq_id == nullptr`, it will return the full kv-cache of all sequences.\nLLAMA_API llama_kv_cache_partial llama_get_kv_cache(\n            struct llama_context * ctx,\n                         llama_seq_id*  seq_ids);\n\n// Set the kv_cache to the specified sequence.\n// src: the source buffer which stores the kv-cache to be set.\n// Return true if the kv-cache is valid and the operation is successful.\nLLAMA_API bool llama_set_kv_cache(\n            struct llama_context * ctx,\n                   const llama_kv_cache_partial * src);\n\n\nstruct llama_kv_cache_partial {\n    // The data type of the kv-cache, in which `llama_model_kv_type` is an enum.\n    llama_model_kv_type data_type;\n\n    // Number of sequences in the kv-cache part.\n    int32_t n_seq;\n\n    // seq_ids for each sequence in this kv-cache part.\n    llama_seq_id * seq_ids;\n\n    // length of the keys for each sequence in byte.\n    llama_seq_id * key_lengths;\n\n    // length of the values for each sequence in byte.\n    llama_seq_id * value_lengths;\n\n    // key data for each sequence.\n    uint8_t ** keys;\n\n    // key data for each sequence.\n    uint8_t ** values;\n};\nIt's not a formal proposal because llama.cpp actually uses cells to store the kv-cache and padding is required when setting kv-cache. If it's at least possible to be implemented, I'd like to step further to improve the proposal and contribute.\nI have searched the issues and discussions, but I didn't find related topics. I'll appreciate it if you could let me know if there is any similar discussion that I missed.\nApplication case\nAs for the application of this API, let's consider a server using batched decoding. At a moment, there are three users online, which are a, b and c. Therefore, their quiries will be put into one batch to generate the response.\nThen, a and b log in again after being offline for some time. Besides, at this moment, d and e are also online.\na and b want to continue their previous sessions but no cache was found, while d and e already has some kv-cache in the memory. In this case, to get the best performance, saving kv-cache is a good option. The state data from llama_copy_state_data is for all the sequences, including a's, b's and c's. However, loading the state will remove the state of d and e, and what's more, will load an unexpected state from c. Thus, it sounds reasonable to add the API to get and set kv-cache directly.\nConclusion\nAs a conclusion, here are my questions.\n\nIs it reasonable to add the C API to get and set kv_cache directly?\nIn the case I described above, is embeddings and logits also necessary to get and set, rather than only kv-cache?\nIf it's not reasonable to add such APIs, is there any way to deal with the case I described above?\n\nAny suggestion will be appreciated!\nBest regards,\nRinne",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6540",
        "createdAt": "2024-04-08T11:01:34Z",
        "author": {
            "login": "AsakusaRinne"
        }
    },
    {
        "title": "How to reason about batch size in batched benchmark?",
        "bodyText": "Although I just contributed the batched benchmark, I am confused about the batch size in the batched benchmark. I don't know the relationship between these parameters.\nIn the doc (https://github.com/ggerganov/llama.cpp/blob/master/examples/batched-bench/README.md), I found the following words, but the calculation does not include n_batch (batch size) and n_ubatch (physical batch size).\nThere are 2 modes of operation:\n\n    prompt not shared - each batch has a separate prompt of size PP (i.e. N_KV = B*(PP + TG))\n    prompt is shared - there is a common prompt of size PP used by all batches (i.e. N_KV = PP + B*TG)\n\nThanks for your help!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6510",
        "createdAt": "2024-04-06T06:16:25Z",
        "author": {
            "login": "Sunt-ing"
        }
    },
    {
        "title": "Question: Benchmark tools selection for the two secnario",
        "bodyText": "Hi, I want to benchmark 2 scenarios:\n\nBatch mode. For like 10000 requests, given the prompt length and number of tokens to generate, what's the total wall time (throughput) of handling them all?\nServer mode. For like 8 concurrent users, what are the latencies (e.g., the first token and per token latency)?\n\nMy hardware is a single machine with (2 CPUs with 20 cores) and (1 GPU with 16 GB of memory). What are the most appropriate benchmark tools?\nThanks for your help!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6538",
        "createdAt": "2024-04-08T09:42:22Z",
        "author": {
            "login": "Sunt-ing"
        }
    },
    {
        "title": "MPI ram usage",
        "bodyText": "The above will distribute the computation across 2 processes on the first host and 1 process on the second host. Each process will use roughly an equal amount of RAM. Try to keep these numbers small, as inter-process (intra-host) communication is expensive.\n\nWhen this part of the readme says RAM, does it mean the amount of ram the model uses in memory? Is this how I split how much system RAM is used? How do I configure VRAM usage then for say, 2 computers each with 1x 8GB GPU and 32GB ram, but I want system 'A' to use more ram if the model is to be offloaded to RAM (not enough vram)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6524",
        "createdAt": "2024-04-07T16:47:48Z",
        "author": {
            "login": "Weroxig"
        }
    },
    {
        "title": "'cmath' file not found error for AMD builds - RESOLVED - issue with gcc 11, install gcc 12",
        "bodyText": "Problem\n\"make\" command fails with:\n'cmath' file not found\ne.g.:\nmake LLAMA_HIPBLAS=1\nSolution\nCheck the version of GCC that you're on. If < 12 then install the latest.\nEg:\nsudo apt install libstdc++-12-dev",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6615",
        "createdAt": "2024-04-07T12:52:13Z",
        "author": {
            "login": "Speedway1"
        }
    },
    {
        "title": "How to show the result of offline CI for SYCL backend",
        "bodyText": "Background:\nCurrent CI of SYCL backend is only building code, without running unit test.\nRun the unit test need Intel GPU, which is not provided by github runner.\nIt depends on developer running local unit test before create PR.\nFor many reasons, the unit test pass rate is easy to be broken from 100%.\nThat leads to the result of OPs have obvious error, performance drop.\nThat impacts the whole SYCL backend quality and user experience.\nSolution:\nOffline CI for SYCL backend:\n1. github doesn't recommend to setup self-hosted runner for public repo, due to security issue.\n2. I run the unit test/performance for SYCL backend in my personal computer with Intel iGPU.\nI choose the existed commits which change SYCL backend.\nI could know the quality and performance of every commit.\nQuestion:\nHow to show the result of offline CI for SYCL backend\uff1f\nI hope to publish the result of every key commit.\nIt will help developer to improve the quality and user to choose the release with best quality and performance.\nAlternative:\na. Update the summary of unit test result of SYCL backend in README-sycl.md\nb. Insert the unit test pass rate in the title of merged PR.\nc. Update the summary of test result in another public repo MD file and insert the link in README-sycl.md. So user is easy to read it to check the latest SYCL backend test result.\nI choose c.\nHow about your idea?\n@ggerganov @slaren @airMeng",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6399",
        "createdAt": "2024-03-30T11:44:05Z",
        "author": {
            "login": "NeoZhangJianyu"
        }
    },
    {
        "title": "Resume interrupted quantization of a model.",
        "bodyText": "In case of a quant failing due to a sudden interruption (power-off, disconnection of the drive, etc), could it be possible to resume an interrupted quantization from its last quantized weight instead of having to redo it all?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6512",
        "createdAt": "2024-04-06T13:59:28Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "Vectors are always converted to F32 when running convert.py",
        "bodyText": "I was converting llama2-13b from pth to ggml, and I noticed the f16 vectors in the input are converted to f32 in the output.  Ex: norm.weight F16 became  output_norm.weight F32\n\n  \n    \n      llama.cpp/convert.py\n    \n    \n         Line 142\n      in\n      08a0c02\n    \n  \n  \n    \n\n        \n          \n           # 1D tensors are always F32. \n        \n    \n  \n\n\nI went through the history, and the forced conversion to F32 was added when convert.py was first created in early 2023, when it replaced convert-pth-to-ggml.py.  I don't see anything in convert-pth-to-ggml.py  that did the forced conversion.\nI mostly just want to updated the comments in convert.py to explain why it's doing that conversion.  I'm wondering if it was done because the GPU support for F32 was better at the time then F16 support, so it made sense to convert everything to F32, since all of the dequantize_mul_mat_vec_* operations take in a f32 array?  But some operations like ggml_cuda_op_mul_mat_cublas actually convert the data back to F16.  But some operations expect src1 to be F32, like ggml_cuda_mul_mat_vec_p021 and ggml_cuda_mul_mat_vec_nc.\nBut all of this is about what the backends want.  Part of me thinks it'd be more logical to store it as a F16, and convert it to F32 if it's loaded by the cuda backend?  Realistically, maybe it just needs a note that says \"All the llama code is optimized assuming vectors are F32.  Because of that we store all vectors on disk as F32.  It could be possibly stored as F16, but then during loading it would have to be converted to F32, and since these all small memory-wise, this is the better choice.\"?\nBack story: I want to contribute to the cuda code, and so I'm trying to familiarize myself with the code base and PR process.  I noticed this, and figured it'd be a simple-ish item to review/document.  I'll do a PR for either a better comment or removing the forced conversion.  I have no problem doing a larger fix, but figured I'd start with something logically simple.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6497",
        "createdAt": "2024-04-04T22:33:52Z",
        "author": {
            "login": "kunnis"
        }
    },
    {
        "title": "KeyError: 'rms_norm_eps' during convert?",
        "bodyText": "Anyone have any guidance about making models like this work in llama.cpp? https://huggingface.co/WhereIsAI/UAE-Large-V1/\nRan into this error attempting to use convert.py. Never attempted a GGUF before. Thanks!\nllama.cpp> python .\\convert.py ..\\UAE-Large-V1\\       \nLoading model file ..\\UAE-Large-V1\\model.safetensors\nTraceback (most recent call last):\n  File \"C:\\Users\\joe\\Documents\\root\\workspaces\\app-dev\\llama.cpp\\convert.py\", line 1279, in <module>\n    main()\n  File \"C:\\Users\\joe\\Documents\\root\\workspaces\\app-dev\\llama.cpp\\convert.py\", line 1218, in main\n    params = Params.load(model_plus)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joe\\Documents\\root\\workspaces\\app-dev\\llama.cpp\\convert.py\", line 318, in load\n    params = Params.loadHFTransformerJson(model_plus.model, hf_config_path)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joe\\Documents\\root\\workspaces\\app-dev\\llama.cpp\\convert.py\", line 257, in loadHFTransformerJson\n    f_norm_eps        = config[\"rms_norm_eps\"],\n                        ~~~~~~^^^^^^^^^^^^^^^^\nKeyError: 'rms_norm_eps'",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4636",
        "createdAt": "2023-12-26T02:13:15Z",
        "author": {
            "login": "bioshazard"
        }
    },
    {
        "title": "How to achieve faster prompt eval time with a single GPU: NVIDIA A100-PCIE-40GB?",
        "bodyText": "Right now I'm getting these results with Mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf:\n\nprompt eval time = ~1.3 seconds / 600 tokens\nprompt eval time = ~6.0 seconds / 3900 tokens\n\nIn general, my goal is to make an interactive application where the user uploads a document and then asks questions. This document usually contains at most 5000 tokens. With RAG I'm automatically finding the most relevant sentences in this document based on what the user wants therefore I cannot cache it beforehand.\nIs there a way to achieve faster prompt eval time (faster time to first token) so the user doesn't have to wait a few seconds before he starts seeing some output? I tried Exllamav2 with Mixtral-8x7B-instruct-exl2-6.0bpw and achieved this:\n\nprompt eval time = ~0.7 seconds / 600 tokens\nprompt eval time = ~2.0 seconds / 3900 tokens\n\nHowever, the quality of the output is worse than with Mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf (~5.52 bpw) and I don't have VRAM for a better version of Mixtral so I want to stick with Llama-cpp (or is there some better option for my case?).\nThese are my settings:\n~/llama.cpp$ ./main -m /data/LLMs/models--TheBloke--Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf -i -b 8192 -ub 4096 -ngl 100 -c 8192 --temp 0 --repeat_penalty 1 -p \"$(cat prompts/prompt.txt)\"\nLog start\nmain: build = 2603 (7a2c9263)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: seed  = 1712230636\nllama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /data/LLMs/models--TheBloke--Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:                         llama.expert_count u32              = 8\nllama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:                          general.file_type u32              = 17\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:   32 tensors\nllama_model_loader: - type q8_0:   64 tensors\nllama_model_loader: - type q5_K:  833 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 8\nllm_load_print_meta: n_expert_used    = 2\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q5_K - Medium\nllm_load_print_meta: model params     = 46.70 B\nllm_load_print_meta: model size       = 30.02 GiB (5.52 BPW) \nllm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.96 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:  CUDA_Host buffer size =    85.94 MiB\nllm_load_tensors:      CUDA0 buffer size = 30649.55 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 8192\nllama_new_context_with_model: n_ubatch   = 4096\nllama_new_context_with_model: freq_base  = 1000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  4640.28 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   192.05 MiB\nllama_new_context_with_model: graph nodes  = 1638\nllama_new_context_with_model: graph splits = 2\n\nsystem_info: n_threads = 4 / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \nmain: interactive mode on.\nsampling: \n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampling order: \nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \ngenerate: n_ctx = 8192, n_batch = 8192, n_predict = -1, n_keep = 1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6480",
        "createdAt": "2024-04-04T12:04:46Z",
        "author": {
            "login": "Dominik7131"
        }
    },
    {
        "title": "train-text-from-scratch/finetune Clarification Questions",
        "bodyText": "I am relatively new to this LLM world and the end goal I am trying to achieve is to have a LLaMA 2 model trained/fine-tuned on a text document I have so that it can answer questions about it. Ideally, the model would only be able to answer questions about the specific information I give it so it can't answer incorrectly or respond with information that isn't applicable.\nThe train-text-from-scratch program seemed to do just this; however, since it uses the basic llama vocab file, the model that's output doesn't seem to have an understanding of the English language. I'm assuming since the model wasn't pretrained, it doesn't have the ability to understand text and reply correctly? (I'm not entirely sure about this, just my speculation)\nI've also tried using the finetune program to finetune the LLaMA 2 HF model: TheBloke/firefly-llama2-13B-chat-GGUF. After finetuning, .bin and .gguf files were created; however, upon inferencing the model did not seem to know any of the information in the text file I gave it.\nI trained it on a 311KB text file containing a guide for an organization I am working for.\nAny help to get me moving in the right direction would be greatly appreciated!\nUpdate:\nI tried another finetuning run. I trained the TheBloke/Llama-2-13B-chat-GGUF model on a file containing data about a game I made up with a random name to see if the fine-tuned GGUF model would have any knowledge of it and upon inferencing, it made up different answers each time I asked questions about information it was supposed just finetuned with. When starting up the server to inference I tried using the default --lora flag with the weight of 1.0 as well as the --lora-scaled flag with weights of 2 and 5 with the same results each time.\nCommands I ran:\n\nto finetune the model, I ran:\n\n./finetune --model-base ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M.gguf --checkpoint-in  ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M-LATEST.gguf --checkpoint-out ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M-ITERATION.gguf --lora-out ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M-ITERATION.bin --train-data \"../training-text/flanjit-club.txt\" --save-every 0 --threads 6 --adam-iter 30 --batch 4 --ctx 64 --use-checkpointing\n\n\nto start the server for inferencing\n\n./server.exe -m ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M.gguf --lora ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M-LATEST.bin --n-gpu-layers 32 -c 2048\n./server.exe -m ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M.gguf --lora-scaled ..\\Models\\llama2_13b\\llama-2-13b-chat.Q5_K_M-LATEST.bin 5 --n-gpu-layers 32 -c 2048",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6286",
        "createdAt": "2024-03-24T19:24:50Z",
        "author": {
            "login": "braden-dev"
        }
    },
    {
        "title": "Offloading model layers to the GPU does not reduce the RAM load. Is this normal behavior?",
        "bodyText": "Win11, cuBLAS, latest commit.\nDespite adding \u201c\u2013gpu-layers 3\u201d and observing the video memory load at 6.8GB, the RAM consumption did not change at all, so all my dreams of running large models went up in smoke :( So, my question is - is this normal behavior? Does offloading layers to the graphics card really not affect RAM consumption?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6496",
        "createdAt": "2024-04-04T22:17:56Z",
        "author": {
            "login": "Folko-Ven"
        }
    },
    {
        "title": "Train from scratch - TXT chat prompt format?",
        "bodyText": "Hi all, I'm starting training a new prompt/answer LLM with LLama.cpp but I'm not sure about the right format for train data to be used with --train-data.\nI'm trying with something like this:\n<s>[INST] Hello [/INST]\n Hello dear </s> \n<s>[INST] What's your name [/INST]\n My name is Michele </s>\n<s>[INST] Your favorite color [/INST]\n Red </s>\n\nand giving\n --sample-start \"<s>\" --include-sample-start\noptions to my command line.\nIs it right or there's something wrong?\nI saw that only two days ago the developers added chat formats to llama.cpp source.\nThank you all for your attention and big thanks to the developers\nMichele",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6507",
        "createdAt": "2024-04-05T14:30:57Z",
        "author": {
            "login": "micheledellaguardia"
        }
    },
    {
        "title": "Serving llama.cpp as an API",
        "bodyText": "Dear Community,\nIs there any possibility to serve llama.cpp on some port not like HTML form, but rather to allow sending JSON request with prompt/history and receiving the response in JSON format? (similar to ChatGPT API, for example).\nOf course, i can write it manually for myself, but i wonder if there is any existing solution to not reinvent the wheel.\nAppreciate your advises.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6501",
        "createdAt": "2024-04-05T08:58:14Z",
        "author": {
            "login": "evgenyfedorchenko"
        }
    },
    {
        "title": "The prompt template for Yi-34b",
        "bodyText": "I am using yi-34b-chat.Q4_K_M.gguf with llama.cpp , but I have issues like this:\n\n  \n    \n    \n\n    yi-error.mp4\n    \n  \n\n  \n\n  \n\n\nWhy the llm continue print out things after <|im_end|>. How can I config it to make it work like chatgpt.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4533",
        "createdAt": "2023-12-19T10:53:00Z",
        "author": {
            "login": "aisensiy"
        }
    },
    {
        "title": "Does llama.cpp use the correct tokens for the system prompt / inst for the mistral models?",
        "bodyText": "The documentation for the mistral instruct models says to prompt them like so in order to get the best results out of them:\n<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n[INST] Do you have mayonnaise recipes? [/INST]\n\ni.e., you wrap the \"system\" prompt in the <s> & </s> tokens, and put the instruction you want a response to inside \"[INST]\" & \"[/INST]\" text strings.\nDoes lama.cpp do this automatically? As in, if I define a prompt with \"-p\" on the command line, will main add the correct tokens, or do I have to include them myself?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4447",
        "createdAt": "2023-12-13T17:55:36Z",
        "author": {
            "login": "PhilArmstrong"
        }
    },
    {
        "title": "Flash Attention 2 support possible?",
        "bodyText": "Hello!\nSo Flash Attention 2 has just been released. https://github.com/Dao-AILab/flash-attention\nApparently, this can also be used to speed up inference and significantly decrease memory consumption for context.\nLess memory usage for ctx could definately be useful for llama.cpp as ctx does need a significant amount of memory, regardless if you are using partial or full CUDA GPU offloading.\nRight now it just supports recent RTX GPUs, but support for Turing (RTX 2000) is coming soon.\nI'm interested to hear your opinions about this. Seems like good stuff.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2257",
        "createdAt": "2023-07-18T08:54:58Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "server consistently yields bad answers, yet same prompt to ./main consistently produces correct answer, is server broken or am I using it wrong?",
        "bodyText": "started API with (I have tried with --chat-template and without the option)\n$ ../llama.cpp/server --embeddings -c 8000 -ngl 81 --host 192.168.1.100 --port 8080 -np 4 -cb -c 4096 -b 4096 -ub 2048 -m ./mistral-7b-instruct-v0.2.Q8_0.gguf --chat-template llama2\nmessage prompt to API ->\n[{'role': 'system', 'content': 'You are a very smart and helpful AI assistant that answers questions in a logical manner.  Your goal is to answer the given question using the sources you have been provided. Answer the question by providing one  paragraph.  Do you not mention the sources in your response.  Do not number or list the answers in bullet form.  Think carefully about your answer.'}, {'role': 'user', 'content': '\\n    Answer the question below using the sources to get relevant information. \\n    If the source doesn't provide information to answer the question, output \"I don't know\".\\n    Sources: Source 1: phenomenon as a whole without being accused of any of the specific heresies it sought to suppress. Another way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \"The Crucible,\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \"witch-hunt.\" Best of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness. \"I am glad that I managed to write 'The Crucible,'\" Arthur Miller wrote, \"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\" 17 ABQ A Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland. And yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?\\nSource 2: by a hack that all his contemporaries could tolerate that he felt there must be a better solution. Intolerance for ugliness is not in itself enough. You have to understand a field well before you develop a good nose for what needs fixing. You have to do your homework. But as you become expert in a field, you'll start to hear little voices saying, What a hack! There must be a better way. Don't ignore those voices. Cultivate them. The recipe for great work is: very exacting taste, plus the ability to gratify it. Notes Sullivan actually said \"form ever follows function,\" but I think the usual misquotation is closer to what modernist architects meant. Stephen G. Brush, \"Why was Relativity Accepted?\" _Phys. Perspect. 1 (1999) 184-214. Japanese Translation Chinese Translation Slovenian Translation German Translation Interview: Milton Glaser Russian Translation * * * You'll find this essay and 14 others in Hackers & Painters.\\nSource 3:  I'm not sure if this is true. On one hand, it seems surprisingly difficult to waste your time so long as you're working hard on something interesting. So much of what you do ends up being useful. But on the other hand, the rule about the relationship between risk and reward is so powerful that it seems to hold wherever risk occurs. Newton's case, at least, suggests that the risk/reward rule holds here. He's famous for one particular obsession of his that turned out to be unprecedentedly fruitful: using math to describe the world. But he had two other obsessions, alchemy and theology, that seem to have been complete wastes of time. He ended up net ahead. His bet on what we now call physics paid off so well that it more than compensated for the other two. But were the other two necessary, in the sense that he had to take big risks to make such big discoveries? I don't know.   \\nSource 4:  Is the mathematician a small man because he's discontented? No; he's just doing a kind of work that wasn't very common in Confucius's day.   \\nSource 5: If these guys were able to do what they did only because of some magic Shakespeareness or Einsteinness, then it's not our fault if we can't do something as good. I'm not saying there's no such thing as genius. But if you're trying to choose between two theories and one gives you an excuse for being lazy, the other one is probably right. So far we've cut the Standard Graduation Speech down from \"don't give up on your dreams\" to \"what someone else can do, you can do.\" But it needs to be cut still further. There is some variation in natural ability. Most people overestimate its role, but it does exist. If I were talking to a guy four feet tall whose ambition was to play in the NBA, I'd feel pretty stupid saying, you can do anything if you really try. 2 We need to cut the Standard Graduation Speech down to, \"what someone else with your abilities can do, you can do; and don't underestimate your abilities.\" But as so often happens, the closer you get to the truth, the messier your sentence gets. We've taken a nice, neat (but wrong) slogan, and churned it up like a mud puddle. It doesn't make a very good speech anymore. But worse still, it doesn't tell you what to do anymore. Someone with your abilities? What are your abilities? Upwind I think the solution is to work in the other direction. Instead of working back from a goal, work forward from promising situations. This is what most successful people actually do anyway. In the graduation-speech approach, you decide where you want to be in twenty years, and then ask: what should I do now to get there? I propose instead that you don't commit to anything in the future,\\n\\n    Question: What did Arthur Miller wish he had the temperament for?\\n    Result:'}]\nTerrible Result/Answer\nArthur Miller wished he had the temperament for a more straightforward and less introspective approach to life. In his essay \"The Price of a Ticket,\" he expresses his admiration for people who can easily let go of their regrets and move on, while he found himself constantly dwelling on the past and revisiting his mistakes. He writes, \"I have often wondered what it would be like to be the kind of man who, once having made a decision, carries it out without looking back, who does not brood over alternative possibilities and 'what ifs' but faces the future with a calm and resolute certainty.\"\nHowever, it's important to note that Miller did not view this as a deficiency, but rather an aspect of his personality that fueled his creativity as a writer. He recognized that his introspective nature allowed him to explore the depths of human emotions and create complex and compelling characters. In the end, Miller embraced his temperament and used it to his advantage in his work.\n\nNow, we try with main, I consistently get a correct result.\nMessage prompt formatted and ran through main gives a reasonable answer\n$ ~/llama.cpp/main -c 4096 -m ./mistral-7b-instruct-v0.2.Q8_0.gguf  -ngl 50 --log-disable -f zmi\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\nggml_cuda_init: found 5 CUDA devices:\nDevice 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nDevice 3: Tesla P40, compute capability 6.1, VMM: yes\nDevice 4: Tesla P40, compute capability 6.1, VMM: yes\n[INST] Answer the question below using the sources to get relevant information.     If the source doesn't provide information to answer the question, output \"I don't know\".    Sources: Source 1: phenomenon as a whole without being accused of any of the specific heresies it sought to suppress. Another way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \"The Crucible,\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \"witch-hunt.\" Best of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness**. \"I am glad that I managed to write 'The Crucible,'\" Arthur Miller wrote, \"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\"** 17 ABQ A Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland. And yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?Source 2: by a hack that all his contemporaries could tolerate that he felt there must be a better solution. Intolerance for ugliness is not in itself enough. You have to understand a field well before you develop a good nose for what needs fixing. You have to do your homework. But as you become expert in a field, you'll start to hear little voices saying, What a hack! There must be a better way. Don't ignore those voices. Cultivate them. The recipe for great work is: very exacting taste, plus the ability to gratify it. Notes Sullivan actually said \"form ever follows function,\" but I think the usual misquotation is closer to what modernist architects meant. Stephen G. Brush, \"Why was Relativity Accepted?\" _Phys. Perspect. 1 (1999) 184-214. Japanese Translation Chinese Translation Slovenian Translation German Translation Interview: Milton Glaser Russian Translation * * * You'll find this essay and 14 others in Hackers & Painters.Source 3:  I'm not sure if this is true. On one hand, it seems surprisingly difficult to waste your time so long as you're working hard on something interesting. So much of what you do ends up being useful. But on the other hand, the rule about the relationship between risk and reward is so powerful that it seems to hold wherever risk occurs. Newton's case, at least, suggests that the risk/reward rule holds here. He's famous for one particular obsession of his that turned out to be unprecedentedly fruitful: using math to describe the world. But he had two other obsessions, alchemy and theology, that seem to have been complete wastes of time. He ended up net ahead. His bet on what we now call physics paid off s\nWhat did Arthur Miller wish he had the temperament for? [/INST] Arthur Miller wished he had the temperament to write an absurd comedy instead of \"The Crucible.\"\nWhat might be the cause of this?  How do I get the prompt for server to deliver the same results?  I'm using openai API chat completion method.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6469",
        "createdAt": "2024-04-04T03:32:54Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "Question about server.cpp: loading prompt tokens, batch_view shrink, sampled token",
        "bodyText": "I'm reading the code of examples/server/server.cpp.\nupdate_slots will set slot.state to PROCESSING after loading all prompt tokens:\n\n  \n    \n      llama.cpp/examples/server/server.cpp\n    \n    \n        Lines 1998 to 2016\n      in\n      5260486\n    \n  \n  \n    \n\n        \n          \n           if (slot.n_past == slot.n_prompt_tokens) { \n        \n\n        \n          \n               slot.state   = SLOT_STATE_PROCESSING; \n        \n\n        \n          \n               slot.command = SLOT_COMMAND_NONE; \n        \n\n        \n          \n            \n        \n\n        \n          \n               GGML_ASSERT(batch.n_tokens > 0); \n        \n\n        \n          \n            \n        \n\n        \n          \n               // extract the logits only for the last token \n        \n\n        \n          \n               batch.logits[batch.n_tokens - 1] = true; \n        \n\n        \n          \n            \n        \n\n        \n          \n               slot.n_decoded = 0; \n        \n\n        \n          \n               slot.i_batch   = batch.n_tokens - 1; \n        \n\n        \n          \n            \n        \n\n        \n          \n               LOG_VERBOSE(\"prompt done\", { \n        \n\n        \n          \n                   {\"id_slot\",  slot.id}, \n        \n\n        \n          \n                   {\"n_past\",   slot.n_past}, \n        \n\n        \n          \n                   {\"n_ctx\",    n_ctx}, \n        \n\n        \n          \n                   {\"n_tokens\", batch.n_tokens}, \n        \n\n        \n          \n               }); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nHowever, it won't call llama_sampling_sample if the last token whose logits set to true not in batch_view:\n\n  \n    \n      llama.cpp/examples/server/server.cpp\n    \n    \n        Lines 2102 to 2106\n      in\n      5260486\n    \n  \n  \n    \n\n        \n          \n           for (auto & slot : slots) { \n        \n\n        \n          \n               if (slot.state != SLOT_STATE_PROCESSING || slot.i_batch < (int) i || slot.i_batch >= (int) (i + n_tokens)) { \n        \n\n        \n          \n                   continue; // continue loop of slots \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n    \n  \n\n\nEvery time update_slots starts, sampled tokens will be added to batch for ongoing sequences. But now slot.sampled is uninitialized, then the whole completion should fail:\n\n  \n    \n      llama.cpp/examples/server/server.cpp\n    \n    \n        Lines 1738 to 1767\n      in\n      5260486\n    \n  \n  \n    \n\n        \n          \n           // frist, add sampled tokens from any ongoing sequences \n        \n\n        \n          \n           for (auto & slot : slots) { \n        \n\n        \n          \n               if (slot.state == SLOT_STATE_IDLE) { \n        \n\n        \n          \n                   continue; \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n\n        \n          \n               slot.i_batch = batch.n_tokens; \n        \n\n        \n          \n            \n        \n\n        \n          \n               const int32_t slot_npast = slot.n_past_se > 0 ? slot.n_past_se : slot.n_past; \n        \n\n        \n          \n            \n        \n\n        \n          \n               // TODO: we always have to take into account the \"system_tokens\" \n        \n\n        \n          \n               //       this is not great and needs to be improved somehow \n        \n\n        \n          \n               llama_batch_add(batch, slot.sampled, system_tokens.size() + slot_npast, { slot.id + 1 }, true); \n        \n\n        \n          \n            \n        \n\n        \n          \n               slot.n_past += 1; \n        \n\n        \n          \n            \n        \n\n        \n          \n               if (slot.params.cache_prompt) { \n        \n\n        \n          \n                   slot.cache_tokens.push_back(slot.sampled); \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n\n        \n          \n               LOG_VERBOSE(\"slot decode token\", { \n        \n\n        \n          \n                   {\"id_slot\",         slot.id}, \n        \n\n        \n          \n                   {\"id_task\",         slot.id_task}, \n        \n\n        \n          \n                   {\"n_ctx\",           n_ctx}, \n        \n\n        \n          \n                   {\"n_past\",          slot.n_past}, \n        \n\n        \n          \n                   {\"n_system_tokens\", system_tokens.size()}, \n        \n\n        \n          \n                   {\"n_cache_tokens\",  slot.cache_tokens.size()}, \n        \n\n        \n          \n                   {\"truncated\",       slot.truncated} \n        \n\n        \n          \n               }); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nIt this a corner case bug? Did I lose any details?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6449",
        "createdAt": "2024-04-03T03:26:15Z",
        "author": {
            "login": "TD-Sky"
        }
    },
    {
        "title": "Roadmap May 2023",
        "bodyText": "High-prio\n\n\n Refactoring pass\nThere is a lot of code duplication in ggml.c which probably can be simplified with a good set of macros. The goal is to keep the code size manageable, while we avoid reaching \"macro hell\"\n\n\n Optimize the AVX / AVX2 implementations of the quantization methods and add WASM SIMD\nMake sure we have optimal implementation for these instruction sets\n\n\n Apply the new integer quantization methods to whisper.cpp\nWill backport the latest ggml version to whisper.cpp and add support for quantized models. Will also update all WASM examples to be able to run with the quantized models\nUpdate: whisper.cpp v1.4.0 has been released. It includes integer quantization and GPU support via cuBLAS - all thanks to the great work done here\n\n\n Add support for \"batch inference\"\nRecently, the bert.cpp (by @skeskinen) project demonstrated BERT inference using ggml. This model gains a lot from batch inference, which is currently not supported by ggml. We will extend all operators to support it. The bert.cpp example will serve as a playground to achieve this\nUpdate: batched forward passes have been demonstrated in the baby-llama example (thanks to @xaedes #1360). It will be great to apply the demonstrated approach to bert.cpp and whisper.cpp's beam-search decoding in order to gain extra speed-up\n\n\n Implement inference of new models\nThere are already some very interesting models that should be supported by ggml:\n\n \ud83d\udcab StarCoder\n Segment Anything Model (SAM)\n Bark (text-to-speech)\nThere is a huge interest for adding ggml support for this model (see suno-ai/bark#30 (comment))\nThe main blocker seems to be the dependency on Facebook's EnCodec codec. Still not sure how difficult it would be, but probably this codec is another model that we should try to support via ggml\n\nI'll use this section to add a note regarding new model implementations by contributors - I recommend to always try to add a very basic example implementation to the ggml repo. Having a basic example there would make long-term support much easier\n\n\n Proof-of-concept for 100% inference on the GPU\nThe goal is to make a demonstration of the idea discussed in #914\nVery preliminary work has been started in ggerganov/ggml#108\nWill try to get a working example using the MNIST inference\nUpdate: The MNIST inference on Apple Silicon GPU using Metal is now fully demonstrated: ggerganov/ggml#108 -- this is the way\n\n\nLow-prio\n\n\n Project ggml : improve threading implementation\nBetter utilization of the available CPU resources via improved thread management.\nThere have been a few efforts during April, but they remained in the \"background\" - need to put more focus this time\n\n\n Having second thoughts about adding llama_state\nThe experience is we added whisper_state in whisper.cpp with this PR: ggerganov/whisper.cpp#523\nHowever, I haven't see a lot of use of it. At the same time, it doubled the C API.\nLet me know if you think this is worth implementing\nref: #370 (comment)\n\n\n Add 3-bit integer quantization\nIt has been shown that 2-bit integer quantization does not look really useful for anything: #1004\nIn this case, we can probably add 3-bit integer quantization. Probably not \"officially supported\", but rather in a state where we can run experiments and see if we can find some application\n\n\n There is an interesting ongoing effort to add \"training\" support to ggml: ggerganov/ggml#8 (comment)\nIt would be really impressive if this actually works. Might be conflicts with the refactoring pass - need to coordinate with @xaedes\nUpdate: this has been successfully completed and there is now a simple example demonstrating baby-LLaMA training: https://github.com/ggerganov/llama.cpp/blob/master/examples/baby-llama/baby-llama.cpp#L759-L770",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1220",
        "createdAt": "2023-04-28T18:31:39Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Is the embedding weight for models which uses weight tying is being duplicated for \"offloading\"?",
        "bodyText": "Hello. I am not an expert in C++. But from what I understand from the code  is that the embedding weight is duplicated for models which uses weight tying like Gemma 7B(8.5B).\nmodel.output      = ml.create_tensor(ctx_output, tn(LLM_TENSOR_TOKEN_EMBD,  \"weight\"), {n_embd, n_vocab}); // same as tok_embd, duplicated to allow offloading\nIf this is really the case, I think this will increase the model size significantly with worse quality. I was wondering why I always get OOM when trying to load Gemma 7B(actually 8.5B) on my GPU and probably this might be the case. The shape of this tensor is (256000 x 3072)!!  I am not entirely sure so I thought of asking here first before opening an issue.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6451",
        "createdAt": "2024-04-03T05:27:55Z",
        "author": {
            "login": "qnixsynapse"
        }
    },
    {
        "title": "Training data format",
        "bodyText": "Hi,\nCan someone give me some pointers on how the training data gets broken up ?\nExample here https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune reads in full blob of text from here\nhttps://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt\nBut looking at the examples of fine tuning data on huggingface typically training data is supplied in json which is then split by objects that contain input/output.\nSo in my case i am trying to train mistral 7b model, does anyone know how i would store the data in the training file ?\nWould splitting individual examples by newline be sufficient ?\nEg:\n<s>[INST] What is the moon made of ? [/INST] Its made from green cheese. <\\s>\n<s>[INST] What color is the moon ? [/INST] Its the color of green cheese. <\\s>\n<s>[INST] Why do astronauts like the moon ? [/INST] Because they like to throw cheese parties there. <\\s>\n<s>[INST] What kind of cheese is the moon made of ? [/INST] Green cheese. <\\s>",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3805",
        "createdAt": "2023-10-26T22:06:32Z",
        "author": {
            "login": "chiefMarlin"
        }
    },
    {
        "title": "Translation from prompt format to llama.cpp options?",
        "bodyText": "For example, here it is said miqu benefits from this prompt format:\n\n[INST] {System}[/INST]\\n[INST] {User}[/INST] {Assistant}\n\nHow does it translate in terms of what options to use in llama.cpp instruct mode?\n-ins -p ? -r ? --in-prefix ? --in-suffix ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6455",
        "createdAt": "2024-04-03T13:08:45Z",
        "author": {
            "login": "Azirine"
        }
    },
    {
        "title": "Shouldn't antiprompt be a std::vector<std::vector<llama_token>> instead of a std::vector<std::string>?",
        "bodyText": "https://github.com/ggerganov/llama.cpp/blob/master/common/common.h#L102\nThis doesn't seem to make sense as the tokens seem to be converted back into strings to do the comparison? Wouldn't it be better / more efficient to directly compare tokens?\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/main/main.cpp#L757-L795\nAlso as was discussed here  it might be a good idea to change the name of antiprompt. Personally I think antiprompt sounds too much like negative prompt. Which to my mind and at-least in reference to models like stable diffusion refers to a prompt used for negative weighting (I think thats what its called).\nWhile antiprompt in the context of llama.cpp seems to refer to a termination sequence? So why not name it something like terminators or termination_sequences",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6450",
        "createdAt": "2024-04-03T04:24:43Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "server should support tools and tool_choice parameter",
        "bodyText": "When utilizing function calling with GPT, one can pass in the tools and tool_choice.   server doesn't support this and throws an error.    function calling is very nice and adds major usefulness to the API, should this be supported?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6429",
        "createdAt": "2024-04-02T01:16:09Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "Licenses/Copyright in llama.cpp / ggml / whisper.cpp",
        "bodyText": "@ggerganov\nI see the following copyright notice in ggml-sycl.cpp\n//\n// MIT license\n// Copyright (C) 2024 Intel Corporation\n// SPDX-License-Identifier: MIT\n//\n\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n\nI would like to also explicitly state the copyright on the code that I have contributed. What is the best way to accomplish this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6394",
        "createdAt": "2024-03-30T08:30:20Z",
        "author": {
            "login": "ikawrakow"
        }
    },
    {
        "title": "May I ask how to solve this problem \u201cFailed to get embeddings from sequence pooling type is not set\u201d",
        "bodyText": "raise RuntimeError(\"Failed to get embeddings from sequence pooling type is not set\")\nRuntimeError: Failed to get embeddings from sequence pooling type is not set",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6278",
        "createdAt": "2024-03-24T15:11:13Z",
        "author": {
            "login": "tanxu1213"
        }
    },
    {
        "title": "I would like to remove tasks from the queue if the associated request is terminated (server)",
        "bodyText": "Hello, I have an inquiry for yours.\nCurrently, I am hosting an embedding server using server.\nI've come across an issue during use and decided to reach out for help.\nWhen a client sends a request and subsequently terminates it, tasks already in the queue continue to be processed without recognizing the termination.\nIf a client makes a massive number of requests and then disconnects, the server could become paralyzed.\nIs there a way to modify this behavior within llama.cpp?\nTLDR; I would like to remove tasks from the queue if the associated request is terminated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6420",
        "createdAt": "2024-04-01T08:09:37Z",
        "author": {
            "login": "redlion0929"
        }
    },
    {
        "title": "Recommended budget GPU for LLM experimentation",
        "bodyText": "I want to get a budget GPU for LLM training/fine tuning/experimentation.\nI am considering:\na single rtx 4070 ti super 16gb\nor a single rtx 4080 16gb\nor some dual lower level gpus setting, e.g. dual rtx 4070 12gb ram\nAny thoughts, advices or other recommendations?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6418",
        "createdAt": "2024-04-01T04:25:04Z",
        "author": {
            "login": "msfasha"
        }
    },
    {
        "title": "BitNet 1.58b models reproduced successfully",
        "bodyText": "https://huggingface.co/1bitLLM/bitnet_b1_58-3B",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6384",
        "createdAt": "2024-03-29T13:05:14Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Run LLMs FASTER on Intel Graphics (ARC etc.)",
        "bodyText": "I created this video on How to run LLMs faster on Intel GPUs ex: ARC 770, 750 etc.\nI hope this will help\nhttps://www.youtube.com/watch?v=Q7t4CmziaqA",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6411",
        "createdAt": "2024-03-31T12:03:08Z",
        "author": {
            "login": "tarunmcom"
        }
    },
    {
        "title": "RAG with Llama-cpp-python  - parameters validation",
        "bodyText": "Hello,\nI am building a RAG with Llama-cpp-python and langchain LlamaCpp for a few hundred PDFs of scientific information and a few GPUs.\nI have tried optimizing the parameters of the LLM to my best knowledge based on information online.\nI was wondering if those parameters would seem appropriate for the intended purpose of interrogating a large set of data?\nLoading the model as such with parameters:\nllm = LlamaCpp(\n        model_path=model_source,\n        **params,\n        chat_format='zephyr', \n        stop=['</s>'], \n        callbacks=callbacks\n    )\n\nParameters loaded:\nN_GPU_LAYERS  = -1\nROPE_FREQ_SCALE  = 2.5\nROPE_FREQ_BASE = 26000\nN_DISCARD = 1\nN_KEEP = 4\nN_PARTS = 1\nN_CTX = 16384\nN_BATCH = 2048\nN_PREDICT = -1\nCONTEXT_WINDOW = 16384\nMAX_TOKENS = 16384\nMAX_NEW_TOKENS = 4096\nLAST_N_TOKENS_SIZE  = 4096\nTEMPERATURE = 1\nREPEAT_PENALTY = 1\nF16_KV = True\nTOP_P = 1\nTYPICAL_P = 0.9\nMIN_P = 0.1\nTFS = 0.999\nVERBOSE = True\nECHO = False",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6410",
        "createdAt": "2024-03-31T11:45:25Z",
        "author": {
            "login": "calypset"
        }
    },
    {
        "title": "Memory bandwidth utilization",
        "bodyText": "Hi all! I'm trying to understand the current memory bandwidth utilization (MBU) for llama.cpp running on M2 Max. When running 7B q4 model, I get around 60 tok/s which based on this blog post corresponds to ~50% MBU. Here's my math for the reference:\n\n60 tok/s => 16.6 ms per output token (i.e. TPOT)\n7B q4 model => 3.5 GB\n3.5 GB per 16.6 ms => 210 GB per sec\n210 GB/s out of 400 Gb/s (M2 Max memory bandwidth) => 52.5%\n\nIs my math wrong? Or are there any limitations on unified memory usage?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3909",
        "createdAt": "2023-11-02T12:48:44Z",
        "author": {
            "login": "artmoskvin"
        }
    },
    {
        "title": "Multi GPU with Vulkan out of memory issue.",
        "bodyText": "I'm trying to load a model on two GPUs with Vulkan, the model's size is Q6_K quant of 26.27 GiB (6.56 BPW)\nMy GPUs have 20 and 11 gigs of VRAM\nLoading it with -ts \"20,11\" -c 512 yields:\nggml ctx size =    0.62 MiB\noffloading 60 repeating layers to GPU\noffloading non-repeating layers to GPU\noffloaded 61/61 layers to GPU\n   Vulkan0 buffer size = 17458.44 MiB\n   Vulkan1 buffer size =  9088.14 MiB\n       CPU buffer size =   358.90 MiB\n\nVulkan0 KV buffer size =    80.00 MiB\nVulkan1 KV buffer size =    40.00 MiB\n\nKV self size  =  120.00 MiB, K (f16):   60.00 MiB, V (f16):   60.00 MiB\nVulkan_Host input buffer size   =    16.01 MiB\n   Vulkan0 compute buffer size =   113.00 MiB\n   Vulkan1 compute buffer size =   139.00 MiB\nVulkan_Host compute buffer size =    14.00 MiB\n\nggml_vulkan: Device memory allocation of size 120422400 failed.\nggml_vulkan: vk::Device::allocateMemory: ErrorOutOfDeviceMemory\n\nA Q5_K_M quant at 22.65 GiB (5.66 BPW) works perfectly fine until I increase the context to 4096.\nWhich makes no sense to me.\nAny idea why this happens?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5720",
        "createdAt": "2024-02-26T04:38:48Z",
        "author": {
            "login": "lastrosade"
        }
    },
    {
        "title": "Why is `llama_synchronize` called?",
        "bodyText": "Hello all,\nI was reading through the codebase and saw llama_synchronize was being called when the logits are retrieved:\n\n\n  \n    \n      llama.cpp/ggml-cuda.cu\n    \n    \n         Line 2492\n      in\n      cfc4d75\n    \n  \n  \n    \n\n        \n          \n           GGML_CALL static void ggml_backend_cuda_synchronize(ggml_backend_t backend) { \n        \n    \n  \n\n\n\nDuring my work on inference, I noticed that after the model runs, any synchronizing operation blocks for some time before it can be done. After I add an explicit synchronization, it obviously does not do that. However, this confuses me: why are the logits returned before the GPU is done \"working\"? What operations cause this? I would appreciate any help!\nEdit: When I run a flamegraph, I get this:\n\nIt seems like avoiding the sync would be very beneficial!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6385",
        "createdAt": "2024-03-28T10:25:12Z",
        "author": {
            "login": "EricLBuehler"
        }
    },
    {
        "title": "Why does llama.cpp implement GGML again in the repo instead of referencing the GGML repo?",
        "bodyText": "I noticed that llama.cpp reimplement ggml in the repo. Why not use the existing ggml repo ? Is there any difference between the two implementations? Can I use ggml repo to replace the implementation of llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6381",
        "createdAt": "2024-03-29T11:23:20Z",
        "author": {
            "login": "Ucag"
        }
    },
    {
        "title": "Poor performance - How do I use LlamCpp in python correctly?",
        "bodyText": "Hi,\nI have a general question about how to use llama.cpp. Maybe that I am to naive but I have simply done this:\n\nCreated a new Docker Image based on the official Python image\nInstalled llama-cpp-python via  pip install\nRun my example with the following code on an Intel i5-1340P without GPU\n\nmodel = Llama(\n    model_path=\"/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n    temperature=0.8,\n    max_tokens=200,\n    n_ctx=3048,\n    ctx_size=3000,\n    top_p=0.1,\n    echo=False\n)\n max_tokens = 2000\n\n\nprompt = f\"\"\"<s>[INST] <<SYS>><</SYS>>What is the capital city of France? [/INST]\"\"\"\noutput = model(prompt, max_tokens=max_tokens, echo=False)\nSo I did not install llama.cpp via make as explained in some tutorials. I just install llama-cpp-python via pip.\nThe model works as expected. But the reason why I am asking this question is the poor performance.\nThe prompt above takes 20 seconds\nllama_print_timings:        load time =    1931.26 ms\nllama_print_timings:      sample time =      16.63 ms /    84 runs   (    0.20 ms per token,  5051.42 tokens per second)\nllama_print_timings: prompt eval time =    1931.20 ms /    25 tokens (   77.25 ms per token,    12.95 tokens per second)\nllama_print_timings:        eval time =   18520.56 ms /    83 runs   (  223.14 ms per token,     4.48 tokens per second)\nllama_print_timings:       total time =   20634.53 ms /   108 tokens\n\nIs this an expected normal response time in my dev environment? When I start testing prompts from my application with more than 2000 tokens the response time rises up to 6 Minutes!\nI plan to run my application on a Intel Core i7-7700 and a GPU  GeForce GTX 1080. I know that in this case I need to activate the GPU devices, but apart form all the fine tuning, I'm wondering if I'm testing right on my CPU or if I'm doing something fundamentally wrong. The serer with the additional GPU costs a lot of money and I would like to know what speed increase can be expected? Or is there something I should fix first in my Docker Container?\nThanks for any hints! Maybe someone can give me some rough values for the prompt evaluation under similar conditions CPU without GPU?\n===\nRalph",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6322",
        "createdAt": "2024-03-26T10:43:50Z",
        "author": {
            "login": "rsoika"
        }
    },
    {
        "title": "What is the current state of proper CLI prompting?",
        "bodyText": "Hello,\nThank you all for your hard work!\nI've ran into this issue with several models where it is not completely clear how to properly prompt it in llama.cpp.\nIt feels a bit hacky and I was wondering if there is a better way to go about this?\nFor example, ChatML models work properly now with the --chatml flag.\nBut there aren't any (or maybe some?) flags for other models as far as I'm aware (for the CLI that is).\nWhile looking at main/README.md it becomes clear that we can use:\n--reverse-prompt\n--in-prefix\n--in-suffix\n--instruct\n--interactive\n--interactive-first\n\nI'll give an example where this feels a bit hacky:\nFor Starling-LM-7B-beta-Q5_K_M.gguf the prompt should be as follows for multiturn conversations:\n\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\"\nAnd what I'm using is this:\n--interactive --interactive-first --reverse-prompt \"<|end_of_turn|>\" --in-prefix \"GPT4 Correct User: \" --in-suffix \"<|end_of_turn|>GPT4 Correct Assistant:\"\nAnd it kinda works but is this really the best way to go about it?\nBecause:\n\n\n<|end_of_turn|> is needed in the suffix for the first prompt, but after that it's automatically provided by the output of the model. So I can either 1) remove it, in which case the first prompt isn't correct, or 2) add it,... but then it's incorrectly printed twice in the second turn.\n\n\nAccording to the readme \"--in-prefix\" is primarily used to \"insert a space after the reverse prompt.\" So putting \"GPT4 Correct User:\" seems less than ideal. However, there is no other place to put it.\n\n\nAfter entering the prompt a new line is added but that new line is not present in the prompt example from the model card. This could potentially impact the output?\n\n\nInstruct mode can't be used here because according to the readme this will apply the Alpaca template.\n\n\nI've looked at llama_apply_chat_template but this seems to apply to the server only.\nKeep in mind, this is just an example with 1 model. The issues are somewhat different for other models.\nAny help or clarity here would be greatly appreciated.\nThank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6359",
        "createdAt": "2024-03-28T06:10:44Z",
        "author": {
            "login": "arch-btw"
        }
    },
    {
        "title": "Benchmarks for llama_cpp and other backends here",
        "bodyText": "Here's my initial testing.  Feel free to contact me if you want the actual test scripts as I'm hesitant to past the entirety here!  EDITED to include numbers from running 15 tests of all models now:\nTesting Procedure:\n\nWindows 10, RTX 4090, all libraries used are the most recent as of 3/28/2024 except torch==2.2.0 and nvidia-ml-py==12.535.133\n\n\nHere are the relevant portions of the scripts used to test, with private information omitted and redundant code omitted (but noted) where appropriate:\n\nBitsAndBytes\nimport torch\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, MistralForCausalLM, FalconForCausalLM, LlamaForCausalLM, PhiForCausalLM, StableLmForCausalLM, TextStreamer, BitsAndBytesConfig\n\n# 4-bit settings\nbnb_bfloat16_settings = {\n    'tokenizer_settings': {\n        'torch_dtype': torch.bfloat16,\n    },\n    'model_settings': {\n        'torch_dtype': torch.bfloat16,\n        'quantization_config': BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        ),\n        'resume_download': True,\n        'low_cpu_mem_usage': True,\n    }\n}\n\nbnb_float16_settings = {\n    'tokenizer_settings': {\n        'torch_dtype': torch.float16,\n    },\n    'model_settings': {\n        'torch_dtype': torch.float16,\n        'quantization_config': BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n        ),\n        'resume_download': True,\n        'low_cpu_mem_usage': True,\n    }\n}\n\n\n#8-bit settings\nbnb_bfloat16_settings = {\n    'tokenizer_settings': {\n        'torch_dtype': torch.bfloat16,\n    },\n    'model_settings': {\n        'torch_dtype': torch.bfloat16,\n        'quantization_config': BitsAndBytesConfig(\n            load_in_8bit=True,\n            llm_int8_threshold=6.0,\n        ),\n        'resume_download': True,\n        'low_cpu_mem_usage': True,\n    }\n}\n\nbnb_float16_settings = {\n    'tokenizer_settings': {\n        'torch_dtype': torch.float16,\n    },\n    'model_settings': {\n        'torch_dtype': torch.float16,\n        'quantization_config': BitsAndBytesConfig(\n            load_in_8bit=True,\n            llm_int8_threshold=6.0,\n        ),\n        'resume_download': True,\n        'low_cpu_mem_usage': True,\n    }\n}\n\n\n# See configuration_utils.py within the transformers library for more settings info\ncommon_generate_settings = {\n    'max_length': 4095,\n    #'max_new_tokens': 500,\n    'do_sample': False,\n}\n\nclass CleanupMixin:\n    def cleanup(self):\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nThen there is a separate class for each model tested, and here is one example:\nclass Llama_2_7b_chat_hf(CleanupMixin):  # float16 l;lama\n    def __init__(self):\n        model_name = f\"PERSONAL PATH REDACTED\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **bnb_float16_settings['tokenizer_settings'])\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, **bnb_float16_settings['model_settings'])\n\n    def generate_response(self, user_message):\n        system_message = \"You are a helpful assistant who answers questions in a succinct fashion based on the contexts given to you.\"\n        prompt = f\"<<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n[INST] {user_message} [/INST]\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(\"cuda\")\n        generated_text = self.model.generate(**inputs, **common_generate_settings)\n        full_response = self.tokenizer.decode(generated_text[0], skip_special_tokens=True)\n        # Extracting only the model's response\n        response_start_idx = full_response.find(\"[/INST]\") + len(\"[/INST]\")\n        return full_response[response_start_idx:].strip()\n\nThe test scripts for Ctranslate2 and llama_cpp are all in one script, but testing bitsandbytes testing took 2 scripts.  Here is the script that calls the script above.  Obviously, you'd comment/uncomment the models you want to test.  NOTE: This test is geared towards a RAG application - hence the long \"user message\" - because that is what my personal repository is all about:\nimport time\nimport pynvml\nimport warnings\nfrom chat_templates import (\n    #Dolphin_2_6_Mistral_7B,\n    #Falcon_7b_instruct,\n    #Gemma_2b_it,\n    #Gemma_7b_it,\n    #Hermes_2_Pro_Mistral_7b,\n    Llama_2_13b_chat_hf,\n    Llama_2_7b_chat_hf,\n    Marx_3B_V3,\n    #Microsoft_Phi_2,\n    Mistral_7B_Instruct_v0_2,\n    Neural_Chat_7b_v3_3,\n    #Rocket_3B,\n    SOLAR_10_7B_Instruct_v1_0,\n    StableLM_Zephyr_3B,\n)\n\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\n\nwarnings.filterwarnings(\"ignore\", message=\"Torch was not compiled with flash attention.\")\nwarnings.filterwarnings(\"ignore\", message=\"No module named 'triton'\")\nwarnings.filterwarnings(\"ignore\", module=\"xformers.*\")\nwarnings.filterwarnings(\"ignore\", module=\".*bitsandbytes.*\")\nwarnings.filterwarnings(\"ignore\", module=\".*transformers.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*Torch was not compiled with flash attention.*\", module=\".*transformers.models.llama.modeling_llama.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*Torch was not compiled with flash attention.*\", module=\".*transformers.models.mistral.modeling_mistral.*\")\n\nmodel_names = [\n    #\"Dolphin_2_6_Mistral_7B\",\n    #\"Dolphin_2_6_Phi_2\",\n    #\"Falcon_7b_instruct\",\n    #\"Gemma_2b_it\",\n    #\"Gemma_7b_it\",\n    #\"Hermes_2_Pro_Mistral_7b\",\n    \"Llama_2_13b_chat_hf\",\n    \"Llama_2_7b_chat_hf\",\n    #\"Marx_3B_V3\",\n    #\"Microsoft_Phi_2\",\n    \"Mistral_7B_Instruct_v0_2\",\n    \"Neural_Chat_7b_v3_3\",\n    #\"Phi_2_DPO\",\n    #\"Rocket_3B\",\n    #\"SOLAR_10_7B_Instruct_v1_0\",\n    #\"StableLM_Zephyr_3B\",\n    #\"phi_2_orange_v2\",\n]\n\ndef select_model_class(model_name):\n    model_classes = {\n        #\"Dolphin_2_6_Mistral_7B\": Dolphin_2_6_Mistral_7B,\n        #\"Dolphin_2_6_Phi_2\": Dolphin_2_6_Phi_2,\n        #\"Falcon_7b_instruct\": Falcon_7b_instruct,\n        #\"Gemma_2b_it\": Gemma_2b_it,\n        #\"Gemma_7b_it\": Gemma_7b_it,\n        #\"Hermes_2_Pro_Mistral_7b\": Hermes_2_Pro_Mistral_7b,\n        \"Llama_2_13b_chat_hf\": Llama_2_13b_chat_hf,\n        \"Llama_2_7b_chat_hf\": Llama_2_7b_chat_hf,\n        #\"Marx_3B_V3\": Marx_3B_V3,\n        #\"Microsoft_Phi_2\": Microsoft_Phi_2,\n        \"Mistral_7B_Instruct_v0_2\": Mistral_7B_Instruct_v0_2,\n        \"Neural_Chat_7b_v3_3\": Neural_Chat_7b_v3_3,\n        #\"Phi_2_DPO\": Phi_2_DPO,\n        #\"Rocket_3B\": Rocket_3B,\n        #\"SOLAR_10_7B_Instruct_v1_0\": SOLAR_10_7B_Instruct_v1_0,\n        #\"StableLM_Zephyr_3B\": StableLM_Zephyr_3B,\n        #\"phi_2_orange_v2\": phi_2_orange_v2,\n    }\n\n    return model_classes[model_name]()\n\ncumulative_stats = {model_name: {\"total_time\": 0, \"total_vram\": 0, \"count\": 0} for model_name in model_names}\n\ndef generate_and_save_responses(user_message):\n    responses = []\n    summary_info = []\n    for model_name in model_names:\n        print(f\"\\033[92mProcessing with {model_name}...\\033[0m\")\n        start_time = time.time()\n        model = select_model_class(model_name)\n        response = model.generate_response(user_message)\n\n        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        vram_usage_before_cleanup = memory_info.used / 1024**2\n\n        model.cleanup()\n\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        print(f\"\\033[92m{model_name} processing completed in {execution_time:.2f} seconds.\\033[0m\")\n\n        response_info = {\n            \"model_name\": model_name,\n            \"execution_time\": execution_time,\n            \"vram_usage_before_cleanup\": vram_usage_before_cleanup,\n            \"response_text\": f\"Response from {model_name} (Execution Time: {execution_time:.2f} seconds, VRAM Before Cleanup: {vram_usage_before_cleanup:.2f} MB):\\n\\n{response}\\n\\n---\\n\\n\"\n        }\n        responses.append(response_info)\n\n        summary_info.append((model_name, execution_time, vram_usage_before_cleanup))\n\n        cumulative_stats[model_name][\"total_time\"] += execution_time\n        cumulative_stats[model_name][\"total_vram\"] += vram_usage_before_cleanup\n        cumulative_stats[model_name][\"count\"] += 1\n\n        time.sleep(5)\n\n    sorted_responses = sorted(responses, key=lambda x: x[\"execution_time\"])\n    sorted_summary_info = sorted(summary_info, key=lambda x: x[1])\n\n    sorted_response_texts = [info[\"response_text\"] for info in sorted_responses]\n\n    for response_info in sorted_responses:\n        filename = f\"results_{response_info['model_name']}.txt\"\n        with open(filename, \"a\", encoding=\"utf-8\") as file:\n            file.write(response_info[\"response_text\"])\n\n    header = \"| Model Name                     | Execution Time (s) | VRAM Before Cleanup (MB) |\"\n    divider = \"+\" + \"-\" * (len(header) - 2) + \"+\"\n    summary_text = [divider, header, divider]\n    row_format = \"| {:30} | {:18.2f} | {:24.2f} |\"\n    for info in sorted_summary_info:\n        summary_text.append(row_format.format(*info))\n    summary_text.append(divider)\n    summary_text = \"\\n\".join(summary_text)\n\n    with open(\"results_tables.txt\", \"a\", encoding=\"utf-8\") as file:\n        file.write(summary_text + \"\\n\\n---\\n\\n\")\n\ndef print_average_stats():\n    for model_name, stats in cumulative_stats.items():\n        if stats[\"count\"] > 0:\n            avg_time = stats[\"total_time\"] / stats[\"count\"]\n            avg_vram = stats[\"total_vram\"] / stats[\"count\"]\n            print(f\"\\033[91m{model_name} - Average Execution Time: {avg_time:.2f} seconds, Average VRAM Usage Before Cleanup: {avg_vram:.2f} MB\\033[0m\")\n\nuser_message = \"\"\"\nOnly base your answer to the following question on the provided context/contexts accompanying this question. If you cannot answer based on the included context/contexts alone, please state so.\n\nMy question is: What is the deadline to hold a preliminary protective hearing in a dependency case?\n\nAnd here are the relevant contexts to base your answer off of:\n\n----------\nContext 1 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf ----------\n\n\u00a7 6:21. Time limits\u2014Preliminary protective hearing, Ga. Juv. Prac. & Proc. \u00a7 6:21\n\u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n1\nGa. Juv. Prac. & Proc. \u00a7 6:21\nGeorgia Juvenile Practice and Procedure with Forms | August 2022 Update\nMark H. Murphy\nChapter 6. Dependency Proceedings\n\u00a7 6:21. Time limits\u2014Preliminary protective hearing\nIf a child alleged to be dependent is removed from her home and is not returned home, the preliminary protective hearing must be held promptly and not later than 72 hours after the child is placed in foster care.1 If the 72-hour time period expires on a weekend or legal holiday, then the court is required to hold the hearing on the next day which is not a weekend or legal holiday.2\n\n----------\nContext 2 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf ----------\n\n\u00a7 6:35. Preliminary protective hearing\u2014Dependency petition..., Ga. Juv. Prac. & Proc....\n\u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n1\nGa. Juv. Prac. & Proc. \u00a7 6:35\nGeorgia Juvenile Practice and Procedure with Forms | August 2022 Update\nMark H. Murphy\nChapter 6. Dependency Proceedings\n\u00a7 6:35. Preliminary protective hearing\u2014Dependency petition if child not returned home\nUnder O.C.G.A. \u00a7 15-11-145(g), if the child is not released at the preliminary protective hearing, a petition for dependency shall be made and presented to the court within five days of such hearing. Westlaw. \u00a9 2022 Thomson Reuters. No Claim to Orig. U.S. Govt. Works. End of Document \u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n\n----------\nContext 3 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf ----------\n\n\u00a7 6:29. Preliminary protective hearing, Ga. Juv. Prac. & Proc. \u00a7 6:29\n\u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n1\nGa. Juv. Prac. & Proc. \u00a7 6:29\nGeorgia Juvenile Practice and Procedure with Forms | August 2022 Update\nMark H. Murphy\nChapter 6. Dependency Proceedings\n\u00a7 6:29. Preliminary protective hearing\nThe preliminary protective hearing is essentially a probable cause hearing designed to provide prompt judicial oversight of state intervention into the constitutionally protected parent-child relationship. It must be held promptly after a child is removed from the home and is intended to provide due process to the parties involved. The hearing must occur no later than 72 hours after\n\"\"\"\n\nfor _ in range(15):\n    generate_and_save_responses(user_message)\n\nprint_average_stats()\n\n\n\nCtranslate2\nimport os\nimport sys\nimport ctranslate2\nimport sentencepiece as spm\nimport time\nimport pynvml\nimport warnings\nimport gc\n\nuser_prompt = \"\"\"\nOnly base your answer to the following question on the provided context/contexts accompanying this question. If you cannot answer based on the included context/contexts alone, please state so.\n\nMy question is: What is the deadline to hold a preliminary protective hearing in a dependency case?\n\nAnd here are the relevant contexts to base your answer off of:\n\n----------\nContext 1 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf ----------\n\n\u00a7 6:21. Time limits\u2014Preliminary protective hearing, Ga. Juv. Prac. & Proc. \u00a7 6:21\n\u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n1\nGa. Juv. Prac. & Proc. \u00a7 6:21\nGeorgia Juvenile Practice and Procedure with Forms | August 2022 Update\nMark H. Murphy\nChapter 6. Dependency Proceedings\n\u00a7 6:21. Time limits\u2014Preliminary protective hearing\nIf a child alleged to be dependent is removed from her home and is not returned home, the preliminary protective hearing must be held promptly and not later than 72 hours after the child is placed in foster care.1 If the 72-hour time period expires on a weekend or legal holiday, then the court is required to hold the hearing on the next day which is not a weekend or legal holiday.2\n\n----------\nContext 2 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf ----------\n\n\u00a7 6:35. Preliminary protective hearing\u2014Dependency petition..., Ga. Juv. Prac. & Proc....\n\u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n1\nGa. Juv. Prac. & Proc. \u00a7 6:35\nGeorgia Juvenile Practice and Procedure with Forms | August 2022 Update\nMark H. Murphy\nChapter 6. Dependency Proceedings\n\u00a7 6:35. Preliminary protective hearing\u2014Dependency petition if child not returned home\nUnder O.C.G.A. \u00a7 15-11-145(g), if the child is not released at the preliminary protective hearing, a petition for dependency shall be made and presented to the court within five days of such hearing. Westlaw. \u00a9 2022 Thomson Reuters. No Claim to Orig. U.S. Govt. Works. End of Document \u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n\n----------\nContext 3 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf ----------\n\n\u00a7 6:29. Preliminary protective hearing, Ga. Juv. Prac. & Proc. \u00a7 6:29\n\u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works.\n1\nGa. Juv. Prac. & Proc. \u00a7 6:29\nGeorgia Juvenile Practice and Procedure with Forms | August 2022 Update\nMark H. Murphy\nChapter 6. Dependency Proceedings\n\u00a7 6:29. Preliminary protective hearing\nThe preliminary protective hearing is essentially a probable cause hearing designed to provide prompt judicial oversight of state intervention into the constitutionally protected parent-child relationship. It must be held promptly after a child is removed from the home and is intended to provide due process to the parties involved. The hearing must occur no later than 72 hours after\n\"\"\"\n\ndef main():\n    # List of model configurations\n    models = [\n        {\n            \"model_dir\": r\"[PRIVATE PATH REDACTED]\",\n            \"build_prompt\": build_prompt_solar_10_7b_instruct_v1_0\n        },\n        {\n            \"model_dir\": r\"[PRIVATE PATH REDACTED]\",\n            \"build_prompt\": build_prompt_neural_chat_7b_v3_3\n        },\n        {\n            \"model_dir\": r\"[PRIVATE PATH REDACTED]\",\n            \"build_prompt\": build_prompt_llama_2_7b_chat\n        },\n        {\n            \"model_dir\": r\"[PRIVATE PATH REDACTED]\",\n            \"build_prompt\": build_prompt_mistral_7b_instruct_v0_2\n        },\n        {\n            \"model_dir\": r\"D:\\Scripts\\chat-ctranslate2\\Llama-2-13b-chat-hf-ct2-int8\",\n            \"build_prompt\": build_prompt_llama_2_13b_chat\n        },\n    ]\n\n    context_length = 4095\n    max_generation_length = 512\n    max_prompt_length = context_length - max_generation_length\n\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    warnings.filterwarnings(\"ignore\", module=\"pynvml\")\n\n    results = {}\n\n    for model_config in models:\n        model_dir = model_config[\"model_dir\"]\n        build_prompt_func = model_config[\"build_prompt\"]\n\n        model_name = os.path.basename(model_dir)\n        print(f\"\\033[32mLoading the model: {model_name}...\\033[0m\")\n        intra_threads = max(os.cpu_count() - 4, os.cpu_count())\n        generator = ctranslate2.Generator(model_dir, device=\"cuda\", compute_type=\"int8\", intra_threads=intra_threads)\n        sp = spm.SentencePieceProcessor(os.path.join(model_dir, \"tokenizer.model\"))\n\n        model_results = []\n\n        for _ in range(15):\n            start_time = time.time()\n            dialog = [{\"role\": \"user\", \"content\": user_prompt}]\n            prompt_tokens = build_prompt_func(sp, dialog)\n            step_results = generator.generate_tokens(\n                prompt_tokens,\n                max_batch_size=1000,\n                batch_type=\"tokens\",\n                max_length=max_generation_length,\n                sampling_temperature=0.1,\n                sampling_topk=20,\n                sampling_topp=1,\n            )\n\n            memory_info_peak = pynvml.nvmlDeviceGetMemoryInfo(handle)\n            vram_usage_peak = memory_info_peak.used / 1024**2\n\n            print(\"\", flush=True)\n            text_output = \"\"\n            num_generated_tokens = 0\n            for word in generate_words(sp, step_results):\n                if text_output:\n                    word = \" \" + word\n                print(word, end=\"\", flush=True)\n                text_output += word\n                num_generated_tokens += 1\n            print(\"\")\n\n            end_time = time.time()\n            response_time = end_time - start_time\n\n            model_results.append({\n                \"response_time\": response_time,\n                \"peak_vram_usage\": vram_usage_peak\n            })\n\n        results[model_name] = model_results\n\n        del generator\n        del sp\n        gc.collect()\n\n        time.sleep(2)\n\n    pynvml.nvmlShutdown()\n\n    print(\"\\nAverage Results:\")\n    for model_name, model_results in results.items():\n        avg_response_time = sum(result['response_time'] for result in model_results) / len(model_results)\n        avg_peak_vram_usage = sum(result['peak_vram_usage'] for result in model_results) / len(model_results)\n        print(f\"Model: {model_name}\")\n        print(f\"Average Response Time: {avg_response_time:.2f} seconds\")\n        print(f\"Average Peak VRAM Usage: {avg_peak_vram_usage:.2f} MB\")\n        print()\n\ndef generate_words(sp, step_results):\n    tokens_buffer = []\n    for step_result in step_results:\n        is_new_word = step_result.token.startswith(\"\u2581\")\n        if is_new_word and tokens_buffer:\n            word = sp.decode(tokens_buffer)\n            if word:\n                yield word\n            tokens_buffer = []\n        tokens_buffer.append(step_result.token_id)\n    if tokens_buffer:\n        word = sp.decode(tokens_buffer)\n        if word:\n            yield word\n\ndef build_prompt_solar_10_7b_instruct_v1_0(sp, dialog):\n    user_prompt = dialog[0][\"content\"]\n    system_message = \"You are a helpful assistant who answers questions in a succinct fashion based on the contexts given to you.\"\n    prompt = f\"\"\"<s>### System:\\n{system_message}\\n### User:\\n{user_prompt}\\n### Assistant:</s>\"\"\"\n    dialog_tokens = sp.encode_as_pieces(prompt)\n    return dialog_tokens\n\ndef build_prompt_neural_chat_7b_v3_3(sp, dialog):\n    system_prompt = \"You are a helpful assistant who answers questions in a succinct fashion based on the contexts given to you.\"\n    user_prompt = dialog[0][\"content\"]\n    prompt = f\"### System:\\n{system_prompt}\\n### User:\\n{user_prompt}\\n### Assistant: \"\n    dialog_tokens = sp.encode_as_pieces(prompt)\n    return dialog_tokens\n\ndef build_prompt_llama_2_7b_chat(sp, dialog):\n    user_prompt = dialog[0][\"content\"]\n    system_message = \"You are a helpful assistant who answers questions in a succinct fashion based on the contexts given to you.\"\n    prompt = f\"<<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n[INST] {user_prompt} [/INST]\"\n    dialog_tokens = sp.encode_as_pieces(prompt)\n    return dialog_tokens\n\ndef build_prompt_llama_2_13b_chat(sp, dialog):\n    user_prompt = dialog[0][\"content\"]\n    system_message = \"You are a helpful assistant who answers questions in a succinct fashion based on the contexts given to you.\"\n    prompt = f\"<<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n[INST] {user_prompt} [/INST]\"\n    dialog_tokens = sp.encode_as_pieces(prompt)\n    return dialog_tokens\n    \ndef build_prompt_mistral_7b_instruct_v0_2(sp, dialog):\n    user_prompt = dialog[0][\"content\"]\n    prompt = f\"<s>[INST] {user_prompt} [/INST]</s>\\n\"\n    dialog_tokens = sp.encode_as_pieces(prompt)\n    return dialog_tokens\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nLlama_cpp\nThis script only processed one model at a time and was a royal pain in the ass to run manually multiples times...I ran into problems making it batch process and give reliable outputs for some reason so was forced to do it this way.  NOTE: I had to change the structure of the \"prompt\" to remove all newlines to get the model to respond properly, just FYI:\nimport llama_cpp\nimport time\nimport pynvml\n\nmodel_path = r\"D:\\Scripts\\chat-llama-cpp\\SOLAR-10.7B-Instruct-v1.0-GGUF\\solar-10.7b-instruct-v1.0.Q8_0.gguf\"\n\nmodel = llama_cpp.Llama(\n    model_path=model_path,\n    chat_format=\"solar\",\n    main_gpu=0,\n    n_gpu_layers=-1,\n    seed=-1,\n    n_ctx=4095,\n    use_mmap=True,\n    use_mlock=False,\n    n_batch=512,\n    offload_kqv=True,\n)\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\n\ndef generate_text(prompt, max_tokens=512, temperature=0.1, top_p=0.95, top_k=1):\n    start_time = time.time()\n    response = model.__call__(\n        prompt=prompt,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        # stop=[\"Q:\", \"\\n\"]\n    )\n    end_time = time.time()\n    processing_time = end_time - start_time\n    generated_text = response['choices'][0]['text']\n    print(f\"\\033[92mProcessing time: {processing_time:.2f} seconds\\033[0m\")\n    return generated_text\n\nif __name__ == \"__main__\":\n    prompt = \"\"\"\nOnly base your answer to the following question on the provided context/contexts accompanying this question. If you cannot answer based on the included context/contexts alone, please state so. || My question is: What is the deadline to hold a preliminary protective hearing in a dependency case? || And here are the relevant contexts to base your answer off of: || Context 1 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf | \u00a7 6:21. Time limits\u2014Preliminary protective hearing, Ga. Juv. Prac. & Proc. \u00a7 6:21 \u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works. 1 Ga. Juv. Prac. & Proc. \u00a7 6:21 Georgia Juvenile Practice and Procedure with Forms | August 2022 Update Mark H. Murphy Chapter 6. Dependency Proceedings \u00a7 6:21. Time limits\u2014Preliminary protective hearing If a child alleged to be dependent is removed from her home and is not returned home, the preliminary protective hearing must be held promptly and not later than 72 hours after the child is placed in foster care.1 If the 72-hour time period expires on a weekend or legal holiday, then the court is required to hold the hearing on the next day which is not a weekend or legal holiday.2 || Context 2 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf | \u00a7 6:35. Preliminary protective hearing\u2014Dependency petition..., Ga. Juv. Prac. & Proc.... \u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works. 1 Ga. Juv. Prac. & Proc. \u00a7 6:35 Georgia Juvenile Practice and Procedure with Forms | August 2022 Update Mark H. Murphy Chapter 6. Dependency Proceedings \u00a7 6:35. Preliminary protective hearing\u2014Dependency petition if child not returned home Under O.C.G.A. \u00a7 15-11-145(g), if the child is not released at the preliminary protective hearing, a petition for dependency shall be made and presented to the court within five days of such hearing. Westlaw. \u00a9 2022 Thomson Reuters. No Claim to Orig. U.S. Govt. Works. End of Document \u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works. || Context 3 | From File: Georgia Juvenile Law Practice and Procedure - August 2022.pdf | \u00a7 6:29. Preliminary protective hearing, Ga. Juv. Prac. & Proc. \u00a7 6:29 \u00a9 2022 Thomson Reuters. No claim to original U.S. Government Works. 1 Ga. Juv. Prac. & Proc. \u00a7 6:29 Georgia Juvenile Practice and Procedure with Forms | August 2022 Update Mark H. Murphy Chapter 6. Dependency Proceedings \u00a7 6:29. Preliminary protective hearing The preliminary protective hearing is essentially a probable cause hearing designed to provide prompt judicial oversight of state intervention into the constitutionally protected parent-child relationship. It must be held promptly after a child is removed from the home and is intended to provide due process to the parties involved. The hearing must occur no later than 72 hours after.\n\"\"\"\n    response = generate_text(prompt)\n    print(response)\n\nmemory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\nmax_vram_used = memory_info.used / 1024**2\nprint(f\"\\033[92mMax VRAM used: {max_vram_used:.2f} MB\\033[0m\")\n\n\nIn conclusion, this is a hobby of mine and I'm not a programmer by trade.  However, I've tried to control for as many constants as possible despite varying APIs between libraries - e.g. Ctranslate2 uses \"do_sample=false\" while I couldn't find anything identical in llama_cpp...\nFeedback is always welcome.  Constructive criticism is welcome as my goal is to have accurate testing not feed my ego about who's the best.  Thanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6373",
        "createdAt": "2024-03-28T18:02:36Z",
        "author": {
            "login": "BBC-Esq"
        }
    },
    {
        "title": "What kinf of prompt should I use when using llama-2-7b?",
        "bodyText": "When using llama-2-7b-chat, I can use the following instruction to test my model:\n./main -m models/llama-2-7b-chat/llama-2-7b-chat_q4.gguf --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.3 -ngl 100\n\nbut when I change my model to llama-2-7b, I can't use the instruction above. If I use the above instruction, I will get a weird response consists of lots of blank response. I guess the problem may be prompts/alpaca.txt doesn't match llama-2-7b. So I'm wondering which prompt should I use to test llama-2-7b?\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6275",
        "createdAt": "2024-03-24T09:59:44Z",
        "author": {
            "login": "vickychen928"
        }
    },
    {
        "title": "Finetuning blogs, experience, etc.?",
        "bodyText": "Hi,\nI am playing with finetune on MacBook M1 (although finetune does not seem to use GPU, is it planned?).\nI generated prompt formatted dataset with simple script.\nI have a few questions:\n\nWhat are the best models to finetune? I've been playing with Mistral based models (Synthia-7b and Zephyr) and now training with NousHermes-LLaMA2-13B.\nLlama.cpp says finetuning quantized models is not recommended, but several research papers say it should be OK. My results are not very satisfactory though\nAre there any good blogs/tutorials on finetuning llama-based models with llama.cpp? There's a lot with pytorch, peft, transformers, axolotl, I wonder if anyone had success with llama.cpp.\nI have a question on the output:\n\ntrain_opt_callback: iter=    11 sample=45/2308 sched=0.110000 loss=13.552320 dt=00:01:37 eta=00:31:00 |--------------->\n(What does the \"--->\" mean? Where does it end? is it a progressbar to infinity?  :)\nIt says iter= 11, sample 45/2308 (2308 is my sample size, which is OK). It says it will end in 31 minutes, which it does, but it only processes a few of the samples. Should I rerun the command to continue finetuning? What's the stopping criteria? I would to process all samples during finetuning, but it seems it has some preset iterations.\nShould I increase --adam-iter? Should I train more than one time?\nShould I lower the learning rate? (--adam-alpha ?) Seems quite large compared to other howtos.\nAny discussion forum/telegram/discord/... where we could chat about this and collectively improve? Or is this a good place? I can help with dataset generation, I have done some experiments.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4163",
        "createdAt": "2023-11-22T05:56:29Z",
        "author": {
            "login": "jooray"
        }
    },
    {
        "title": "Is there a way to validate or test the results generated by the model using a specific backend?",
        "bodyText": "I am trying to enable ssm_scan on unsupported GPU Vulkan backends. While ssm_scan can run on the CPU, it is not supported on Vulkan. However, I am unsure how to validate the results of my implementation for ssm_scan. Is there a way of cross validating the operators' results (or even intermediate results for a whole graph), e.g., comparing GPU's results with the results generated by CPU backend?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6345",
        "createdAt": "2024-03-27T12:48:49Z",
        "author": {
            "login": "wniu9"
        }
    },
    {
        "title": "How to run llama.cpp with grammar mode with the model and dataset that I want to evaluate?",
        "bodyText": "Hi, I am trying to run Llama-7B model with a specific test dataset to evaluate the model. However, I am not sure how this can be done. I have currently done cloning the repo, pip installed the requirements. I have the model installed in my separate directory.\nFor instance, I have llama-7b model installed at PATH/TO/llama-7b, and my dataset to PATH/TO/Dataset and I want to run the grammar mode.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6340",
        "createdAt": "2024-03-27T08:13:33Z",
        "author": {
            "login": "hgKang02"
        }
    },
    {
        "title": "Shouldn't prompt formats be an enumerator?",
        "bodyText": "Hi i was looking into the code for gpt_params and I'm wondering why there's a boolean property for each prompt format (currently just instruct for Alpaca format and chatml for ChatML)?\nShouldn't this just be a single enumerator property named something like prompt_format or something?\n(params.instruct == true && params.chatml == true) should never be possible so it doesn't make sense to me why they each have a property.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6335",
        "createdAt": "2024-03-27T03:03:05Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "Server : Autocomplete the user's input based on a large prompt with a single inference",
        "bodyText": "Here is my current setup.\n\nI am hosting my http LLM server using server.exe\nI am providing a large input prompt(1000 - 10000 tokens) and request the LLM to predict the user's input for an autocomplete feature.\nLLM gives a few suggestions, in the grammar format(gbnf) I had provided.\nNow the users types some text. Now I make a fresh http request, with the same input prompt, but this time I modify the grammar(gbnf) so that the suggestions starts with the text that user had typed. It effectively mimics a autocomplete feature for the user.\nThis approach is causing the huge number input tokens to be processed thru the LLM, each and every time we want to generate a new set of autocomplete suggestions.\n\nHere is my goal.\n\nI want to avoid multiple inference processing for the same input prompt. I want the input tokens to be processed only once and all subsequent requests to be processed for the same set of input tokens just with a different grammar file.\n\nHow can I achieve this efficiently? I am a newbie in LLM, so some of questions may not make sense.\n\nI saw there was some code related to KV cache. Can we save the state of the LLM with this cache for a given set of input tokens and generate different set of output tokens for different grammar files by utilizing this cache.\nOr can we somehow need to pause the output token generation after the first set of output tokens, wait for a new grammar file and then reset the sampling context with a new grammar format for generating the next set of output tokens.\nOr is there any other better way of achieving this goal. Please provide your valuable suggestions.\n\nThank you very much in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6319",
        "createdAt": "2024-03-26T09:36:39Z",
        "author": {
            "login": "jkarthic"
        }
    },
    {
        "title": "How does llama.cpp manage the memory?",
        "bodyText": "Hello, I'm wondering how does llama.cpp manage the memory:\n\nDoes llama.cpp allocate space for tensors including static parameter tensors and temporary tensors at once? I only accumulated the allocations of parameter tensors(e.g. blk.0.attn_q.weight) but no more temporary tensors (e.g. inp_embd). Could you please explain where the allocating process is?\nWill llama.cpp free all tensors at the end of the model inference process including both parameter tensors and temporary tensors? I'm also wondering where llama.cpp frees these tensors.\n\nI hope I described my confusions properly and thanks for your attention.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6324",
        "createdAt": "2024-03-26T12:52:19Z",
        "author": {
            "login": "VincentXWD"
        }
    },
    {
        "title": "Add Support for cognitivecomputations/dolphin-phi-2-kensho and other models that are configured like it",
        "bodyText": "By Fernando, Eric Hartford and David\nThis is a hack around pytorch + huggingface Transformers library to make the original Dolphin Phi-2 to behave in a way inspired by the Meta's paper \"MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases\"  (https://arxiv.org/abs/2402.14905)\nOne of the key ideas is that it works as if it was like \"an online passthrough\", by applying a loop on a module SuperClass, that groups layers, in a such way they get their forward method repeated in a loop.\nSo, in theory, you can observe more intelligence in the same way MegaDolphin 120b, Professor 155b, Venus120b and other huge models, but use way less vRAM, because instead of cloning the weights, we share them in the vRAM.\nAnd actually, this concept could be also used to enable the training of way more efficient models.\nWe hope the community enjoy it and make good use of it.\nIt won't work out of the box in the other models. Their \"modeling files\" should be changed accordingly to achieve the same effect.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6331",
        "createdAt": "2024-03-26T22:06:20Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "Quantization benchmark charts for 70B or higher",
        "bodyText": "I have seen this post: https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\n@Artefact2 posted a chart there which benchmarks each quantization on Mistral-7B, however I would be interested in the same chart for a bigger model, 70B in specific. Somebody else asked this in a comment on the gist, however Artefact2 is incapable of performing the same test on a bigger model due to hardware constraints.\nEven though Artefact2 expects these charts to look similar I'm still interested in them, because in my experience running a Q2 of a 70B/120B is a much smoother experience than running Mistral at Q2. While Q2 on a 30B (and partially also 70B) model breaks large parts of the model, the bigger models still seem to retain most of their quality. I assume this is because more information is preserved, like how parameters carry an information and having more parameters means carrying more information.\nI haven't seen anyone else post a chart, so if there is one or if somebody could make one I'd greatly appreciate it as my 2060 6GB probably already suffers from PTSD after running 70B+ on it (though the CPU did most of the heavy lifting). Either way I think about buying something better and it would be nice to know if the bigger models also require Q5 or higher to reach performance comparable to f16 or if Q3 is enough for these bigger models to function.\nThanks in advance ^^",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6330",
        "createdAt": "2024-03-26T20:55:59Z",
        "author": {
            "login": "Sumandora"
        }
    },
    {
        "title": "Docs for operators and quantization",
        "bodyText": "I am implementing the Ascend backend for llama.cpp #6035, and need to understand the mathematical principles for each ggml operator and quantization algorithms. Where can I find these documents? Thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6317",
        "createdAt": "2024-03-26T07:34:30Z",
        "author": {
            "login": "hipudding"
        }
    },
    {
        "title": "Inference at the edge",
        "bodyText": "Inference at the edge\nBased on the positive responses to whisper.cpp, and more recently, llama.cpp, it looks like there is a strong and growing interest for doing efficient transformer model inference on-device (i.e. at the edge).\n\nThe past few days, I received a large number of requests and e-mails with various ideas for startups, projects, collaboration. This makes me confident that there is something of value in these little projects. It would be foolish to let this existing momentum go to waste.\nRecently, I've also been seeing some very good ideas and code contributions by many developers:\n\n@ameenba suggested a way to improve Whisper Encoder evaluation at the cost of accuracy #137. Ultimately, this allowed to demonstrate semi efficent short voice command recognition on a Raspberry Pi 4 Twitter\n@wangchou et. al. demonstrated how to evaluate the Whisper Encoder efficiently using Apple Neural Engine #548\nA person on Twitter showed me how to improve llama.cpp efficiency by 10% with simple SIMD change Twitter\n@Const-me kindly provided efficient AVX2 quantization routines #27\nand many other examples\n\nThe AI field currently presents a wide range of cool things to do. Not all of them (probably most) really useful, but still - fun and cool. And I think a lot of people like to work on fun and cool projects (for now, we can leave the \"useful\" projects to the big corps :)). From chat bots that can listen and talk in your browser, to editing code with your voice or even running 7B models on a mobile device. The ideas are endless and I personally have many of them. Bringing those ideas from the cloud to the device, in the hands of the users is exciting!\nNaturally, I am thinking about ways to build on top of all this. So here are a few thoughts that I have so far:\n\nThis project will remain open-source\nI would like to explore the application of this approach to other models and build more examples to demonstrate it\nI would be really happy to see developers join in and help advance further the idea of \"inference at the edge\"\nThe strongest points of the current codebase are it's simplicity and efficiency. Performance is essential\nIt's early to build a full-fledged edge inference framework. The code has to remain simple and compact in order to allow for quick and easy modifications. This helps to explore ideas at a much higher rate. Bloating the software with the ideas of today will make it useless tomorrow\nThe AI models are improving at a very high rate and it is important to stay on top of it. The transformer architecture in it's core is very simple. There is no need to \"slap\" complex things on top of it\nHacking small tools and examples is a great way to drive innovation. We should not get lost into software engineering problems. Especially at the beginning, the goal is to prototype and not waste time in polishing products\nAnd most of all, it's important to have fun in the process!\n\nI hope that you share the hacking spirit that I have and would love to hear your ideas and comments about how you see the future of \"inference at the edge\".\nEdit: \"on the edge\" -> \"at the edge\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/205",
        "createdAt": "2023-03-16T11:43:50Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Add GPU support to ggml",
        "bodyText": "Intro\nThis issue is more suitable for the https://github.com/ggerganov/ggml repo, but adding it here for more visibility.\nFirst, I don't see adding a GPU framework that is tightly integrated with ggml anytime soon because it usually comes with a lot of maintenance drawbacks, architecture changes and issues. However, there is an alternative approach that might be relatively easy to implement and I think would be a very cool way for new developers to join in and help.\nDescription\nggml produces computation graphs which are basically directed acyclic graphs (DAGs) that can be easily exported, iterated, etc. A graph contains the information about all necessary tensor operations and buffers needed to evaluate the model. The idea is to first add basic ggml functionality for exporting the graphs in some trivial text format that can be parsed as a second step by a separate ggml tool. Having the exported graphs, one can process them and construct hardware-specific code for evaluating them. This way, we keep implementing existing and new transformer models as we currently do - with a focus for CPU execution, but we gain the benefit of being able to export the computation graphs and translate them for GPU execution.\nFor example, a ggml-cuda tool can parse the exported graph and construct the necessary CUDA kernels and GPU buffers to evaluate it on a NVIDIA GPU. Another tool, for example ggml-mps, can do similar stuff but for Metal Performance Shaders. Or maybe even a ggml-webgpu tool.\nThis approach preserves the cross-platform nature of ggml and allows custom hardware support, via compiler-like translation of the exported computation graphs.\nStill, the most difficult part of implementing the respective kernels for the targeted backend remains the biggest obstacle.\nHowever, I think this decoupled approach of the implementation would make the development process much easier and can potentially allow for some interesting optimizations. My biggest fear of adding a tightly integrated GPU backend to ggml is that I don't know the important details for supporting the respective backend, which could lead to bad software design decisions that in turn can have negative side-effects even on the core CPU implementation.\nWith the proposed approach in this issue, we eliminate this risk and allow multiple independent implementations to be provided without any negative side effects on the core ggml implementation.\nAnother cool thing about this idea is that there could be separate leading developers for each backend.\nSo if you have a good knowledge and understanding about a certain hardware architecture, you are one step away from initiating the kernel \"translation\" process and making a very significant contribution to the project.\nGuiding principles\nI don't know all the specifics of a good GPU implementation, but I believe one could try to adopt the fundamental principles of ggml.\nFor example, there could be a single memory buffer allocated and all the tensors can be distributed within that memory buffer at certain offsets. Each graph operation will correspond to a kernel with source tensors as input and a destination tensor for output which will be all part of that single memory buffer allocated at the start of the execution.\nAdditionally, I think we don't need to explicitly add 3rd party dependencies (e.g. CUDA SDK, OpenCL, etc.) to ggml to achieve that. The new ggml translation tools will simply read a computation graph and generate code for a certain GPU backend, which will be up to the user to compile and run.\nThe existing CPU code for each tensor operation is your reference implementation. Ideally, you would always want to implement the same computation in the corresponding new kernel and after that, you can try to optimize it for the specifics of the hardware. This is especially true for the 4-bit kernels.\nAll computations and buffers remain on the GPU. Avoid back-and-forth copies of data to the CPU RAM at all cost.\nTaking shortcuts and making custom hacks in favor of better performance is very welcome. \"General-purpose\" is \"bad\". For example, we can have a tool like ggml-cuda-llama which is a very custom ggml translator to CUDA backend which works only with LLaMA graphs and nothing else, but does some very LLaMA-specific optimizations. This is fine.\nKeep things minimalistic and don't over-engineer. For example, a CUDA translation tool will output a single C++ (or some other language) file with all the kernels and backend initialization code embedded in it. A simple C-style function for evaluation can be exported so that we can call this from other code bases. The actual translation tool should also be implemented as a single source file in a preferred language. (this guiding principle has to be defined a bit better, but we will figure it out as we go)\nThe GPU \"translators\" will likely remain second-class citizens from ggml point of view and they will need to adapt to the core CPU implementation - not the other way around.\nWhy?\nCurrently, ggml is one of the few ML frameworks that provides efficient 4-bit quantization and demonstrates effective application for quantized transformer evaluation. The code is compact, easily comprehensible with very little bloat. I think ggml has a slight leading edge in this regard compared to other general purpose frameworks and if we utilize it now, it has the potential of becoming a very respectable machine learning framework in the future with a focus for on-device inference.\nNote that there is a very large dose of \"reinventing the wheel\" in the outlined strategy. Therefore, if you want to get involved, it's very important to have the right mindset. Definitely do not approach this with: \"this has already been done in another project\" , \"we should do all those things that project X does\" or \"this is not going to scale well for all those reasons\", etc.\nI think the right mindset to approach this is: \"let's try to hack something fast, small and cool and see where it goes\"\nLinks\n\n\nThoughts about Inference at the edge\n\n\nStarting point for exporting ggml graphs: #589 (comment)\n\n\nSample computation graph for single-layer LLaMA 7B:\n\n\n\nUpdate 28 May 2023:\n\nMNIST prototype of the idea above: ggerganov/ggml#108\nThis is the pattern that we should follow and try to apply to LLM inference\nFirst attempt at full Metal-based LLaMA inference: #1642\nFirst attempt at full Vulkan-based LLaMA inference: #2039",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/915",
        "createdAt": "2023-04-12T11:11:42Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "How is tokenizer's add_prefix_space handled in llama.cpp?",
        "bodyText": "I'm debugging certain performance issue and I found that HF tokenizer adds prefix space by default. I'm wondering if llama.cpp does the same since it's not obvious in the code.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6295",
        "createdAt": "2024-03-25T10:12:47Z",
        "author": {
            "login": "anhnami"
        }
    },
    {
        "title": "GGML types IQ1_M and IQ2_M?",
        "bodyText": "@ikawrakow,\nI don't know the amount of work you'd need to do what follows if you didn't do it already on your private repo, but I think that it would be great to have a 1.8125bpw (and maybe a 2.8125bpw) GGML type, in order to improve the granularity of the FFN & attn.q.weight tensors quantization, and establish refined strategies for sub-IQ2_XXS and sub-IQ3_XXS quantized models. Beyond 3bpw, higher bpw intervals between the GGML types are less \"problematic\".\nI don't know where the mathematical breaking of catastrophic quality loss is reached (you already obviously lowered it below 1.5bpw with your IQ1_S_\"EvenBetter\" GGML type), but the attn.q.weight and ffn tensors (notably .up and .gate) might even be able to endure a sub 1.5bpw quant while allowing still a quant strategy using it to remain on the same \"curve\" as the one presented on Artefact's graph.\n\nAlso, and considering the massive improvements your IQ quants have brought in term of quality/size ratio, higher IQ GGML types in the 4.5 -  6bpw range are highly awaited, especially for the attn.v.weight, attn.output.weight, ffn.down, and output tensors.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6235",
        "createdAt": "2024-03-22T12:45:48Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "Performance difference between Ryzen 3700X and Ryzen 5950X",
        "bodyText": "I just upgraded my desktop CPU from a Ryzen 3700X to a Ryzen 5950X. I power limited the 5950X to 95W because otherwise my PC would sometimes crash under heavy load (possibly an issue with my motherboard since it's only a B450). The performance from the CPU change change alone in combination with 3200 MHz RAM and an RTX 3090 is as follows:\n\n\n\nModel\nLLAMA_CUBLAS\n-ngl\ntest\nt/s 3700X\nt/s 5950X\nSpeedup\n\n\n\n\nLLaMA 2 q4_0\nNo\n0\npp512\n24.65\n41.15\n1.67\n\n\nLLaMA 2 q4_0\nNo\n0\ntg128\n9.93\n11.00\n1.11\n\n\nLLaMA 2 q4_0\nYes\n0\npp512\n351.06\n379.81\n1.08\n\n\nLLaMA 2 q4_0\nYes\n0\ntg128\n9.93\n10.98\n1.11\n\n\nLLaMA 2 q4_0\nYes\n33\npp512\n4060.50\n4123.53\n1.02\n\n\nLLaMA 2 q4_0\nYes\n33\ntg128\n131.95\n133.00\n1.01\n\n\n\nThe biggest difference is for prompt processing without a GPU since this is compute bound. If a GPU is available the benefit is much smaller and if the entire model is on the GPU the difference is negligible.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5421",
        "createdAt": "2024-02-08T19:22:52Z",
        "author": {
            "login": "JohannesGaessler"
        }
    },
    {
        "title": "what data type does GGML quantization cast to on inference?",
        "bodyText": "Hello, I am interested to know the casting process during inference where an input signal is going through a quantized layer.\nIf I understand correctly we have the input which is x, I guess the type is float right?\nand the layer itself is qunatized, let's say INT8.\ndoes the layer weights get casted back to floats for computation or does the signal gets casted to int instead?\nwould love to know where I can find the answer in the code as well.\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6274",
        "createdAt": "2024-03-24T08:50:46Z",
        "author": {
            "login": "aviadingo"
        }
    },
    {
        "title": "Streaming output using OpenAI Python library with llama.cpp HTTP Server in Jupyter Notebook",
        "bodyText": "Hi,\nI'm using the OpenAI Python library with the llama.cpp HTTP Server and Jupyter Notebook. I'm trying to stream the output from the API response instead of receiving the full output at once. However, I'm having trouble getting the streaming to work properly.\nHere's the code I'm using:\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"http://127.0.0.1:8080/v1\",\n    api_key=\"sk-no-key-required\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Your task is to summarize\"},\n        {\"role\": \"user\", \"content\": \"In Northern Chile, a pick-up truck bumps along dusty old mining roads toward the Atacama Desert. A team of scientists is driving from the coastal town of Antofagasta, and they occasionally pass other vehicles on the road -mostly prospectors searching for metals and minerals. After an hour, they arrive at a lonely meteorological station situated in the driest part of a very dry desert\"}\n    ],\n    temperature=0.7,\n    stream=True\n)\n\nfor chunk in response:\n    content = chunk.choices[0].delta.get(\"content\", \"\")\n    print(content, end=\"\", flush=True)\n\nprint()\nI've set stream=True in the client.chat.completions.create() method, but the output is not being streamed as expected. Instead, it seems to wait for the entire response before printing it.\nI'm looking for any references, examples, or guidance on how to properly implement streaming with the OpenAI Python library when using the llama.cpp HTTP Server. I want to be able to display the generated text in real-time as it is being produced by the API.\nAny help or insights would be greatly appreciated. Thank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6264",
        "createdAt": "2024-03-23T22:11:32Z",
        "author": {
            "login": "qymab"
        }
    },
    {
        "title": "Our android port (Sherpa) is on Google play \ud83d\udc4d",
        "bodyText": "Hello everyone !\nOur android port of llama.cpp is on Google Play !\nThe repo is here : https://github.com/Bip-Rep/sherpa, we hope that more people can work on it because we are really amazed that it can run on a mobile (with at least 8GB RAM).\nFlutter devs, feel free to make PRs.\nWorking demo\n\nClick on the image to view the video on YouTube.\nIt shows a OnePlus 7 with 8Gb running Sherpa without speed up.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/750",
        "createdAt": "2023-04-03T21:12:50Z",
        "author": {
            "login": "ThibautLEAUX"
        }
    },
    {
        "title": "CPU Performance",
        "bodyText": "I am trying to setup the Llama-2 13B model for a client on their server. It has an AMD EPYC 7502P 32-Core CPU with 128 GB of RAM. I am getting the following results when using 32 threads\nllama_print_timings:        load time =   394.38 ms\nllama_print_timings:      sample time =   163.32 ms /   218 runs   (    0.75 ms per token,  1334.84 tokens per second)\nllama_print_timings: prompt eval time =   670.77 ms /    13 tokens (   51.60 ms per token,    19.38 tokens per second)\nllama_print_timings:        eval time = 27634.81 ms /   217 runs   (  127.35 ms per token,     7.85 tokens per second)\nllama_print_timings:       total time = 28653.21 ms\n\nTLDR: Is this performance good or bad? Can I do anything to improve is short of running the model on a GPU?\nI have never worked with anything related to machine learning, so I don't know what the expected performance for this kind of model on this machine is. I've searched other topics but most either don't mention any explicit numbers or things I am not familiar with. My client has tested the model on HuggingFace and they tell me it's much faster than what we are getting here. Is there some build option I am missing (I enabled OpenBLAS)? Or is this expected performance? Can I do something to improve it? I already converted the model using Q4.0 quantisation. If anyone ran the model on a GPU what sort of performance improvement should we expect?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3167",
        "createdAt": "2023-09-14T14:02:50Z",
        "author": {
            "login": "gurbindersingh"
        }
    },
    {
        "title": "Possibly possible size optimization, with a better precision",
        "bodyText": "Hi,\nI was thinking about reducing the memory size for the models. i'm terribly sorry if these obvious things are already implemented.. :)\n\nlow quantized models, like 4-5 bits are most common now and they have quite a small amount of possible values, especially considered the concentration of the values in certain regions. QLORA addresses that somewhat. it presents us with a nice opportunity.\n\nwhat i suggest is:  IF the weights currently are stored per parameter (which the file sizes hint at, at least), we can instead use a format, with the buckets of parameters per value. it has a lot of advantages, i will explain below. (keep patient, please, if it's starts with obvious things :)).\nThe bucket format is Value:[param_id,..].\nfor example:\n1:[123,3211,12315,1231,12313,546346,456464,1232,..]\n7:[45,676575,234234,...]\n(etc, where 2,3,4,5,6 are absent weight values in the model)\nthis way we save a lot of space and memory.\n\nsecond idea comes from the first one and i believe is not implemented yet.\n\nreal neurons are never noise proof and always change the conductivity, making weights non-constant yet stable. we can use it to our advantage by adding a random value during the phase of reading values before multiplication, when we do the inference.\nthis way we can virtually increase the precision, during the calculation, upgrading our 4bit data, to, say, 6bit or 8bit. we just need to take care and not to overlap the original 4 bit borders with our new improved value.\nsounds not so fascinating yet, isn't it. now the final part :).\n\nwe can make this work not just randomly but.. we can actually make it help us partially RESTORE the original precision.\n\nto do that, before saving the trained quantized model, we can simply sort the parameters' ids within the storage buckets in an order reflecting their original unquantized values that they had originally.\nso, if before quantization we had (param id:weight value):\n1:8, 2:4, 3:2, 4:8, 5:10, 6:5, 7:9, .......\nafter our QLORA quantization they all got shrank to value=1, fitting into the bucket 1 (for values of 1-10 in terms of original model).\nour stored model format in that case would be:\n(weight value: [param ids])\n1:{3,2,6,4,1,7,5}\nthen, at the inference step, we can \"dequantize\" and to \"restore\" the spread of these parameters across the values range of the bucket (1-10) in accordance with position of params within of the bucket.\nof course it is a loss in compare to original thing, but it is MUCH more precise than just quantized thing, even with QLORA.\nand the beauty is that it is done only during the calculation phase of the inference, so the whole model weights in memory will still be saved with the small footprint :).\nbonus: not that necessary step 4.\n\nthe final topping would be to predefine several common distribution functions. then, we can just save the id of that function at the moment of saving quantized data. it would require nearly no extra space as it's just some bytes per whole file. but later we could use that function to restore values more precisely across the bucket. although, it would quite slow down the calculations as it would not be mere multiplication anymore. yet, for higher precision at low memory it can work even much better than 3.\n\n:)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6099",
        "createdAt": "2024-03-16T11:55:22Z",
        "author": {
            "login": "drazdra"
        }
    },
    {
        "title": "Stopping a stream with Langchain LlamCpp in python",
        "bodyText": "Hi, I am looking to stop a stream that is ongoing for any given reason. I set up a stream with the handler as follows, I have a queue and a thread that manages downstream. I have been trying a few things but so far unsucessful. I figured I could pass a stop signal as a token but unsure how.\nThe callback class is:\nfrom langchain.callbacks.base import BaseCallbackHandler\n\nclass QueueCallback(BaseCallbackHandler):\n    def __init__(self, q):\n        self.q = q\n\n    def cancel(self, q):\n        return self.q.empty()\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        self.q.put(token)\n\n    def on_llm_end(self, *args, **kwargs: Any) -> None:\n        return self.q.empty()\n\nAnd model loaded:\nfrom langchain_community.llms import LlamaCpp\n\nllm = LlamaCpp(\n        model_path=model_source,\n        **params,\n        stop=['</s>', '###'], \n        callbacks=[QueueCallback(q)]\n)\n\nAnyone has an idea on how to achieve such hard stream stop seamlessly?\nPS: Llama.cpp is fantastic!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6212",
        "createdAt": "2024-03-21T20:27:33Z",
        "author": {
            "login": "calypset"
        }
    },
    {
        "title": "Why is the forward pass compute for transpose a no-op?",
        "bodyText": "llama.cpp/ggml.c\n    \n    \n        Lines 11646 to 11654\n      in\n      fa046ea\n    \n  \n  \n    \n\n        \n          \n           // ggml_compute_forward_transpose \n        \n\n        \n          \n            \n        \n\n        \n          \n           static void ggml_compute_forward_transpose( \n        \n\n        \n          \n                   const struct ggml_compute_params * params, \n        \n\n        \n          \n                   const struct ggml_tensor * dst) { \n        \n\n        \n          \n               // NOP \n        \n\n        \n          \n               UNUSED(params); \n        \n\n        \n          \n               UNUSED(dst); \n        \n\n        \n          \n           }",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6221",
        "createdAt": "2024-03-22T06:44:06Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "Reference \"Software Emulation\" implementation of GGML_OP_MUL_MAT",
        "bodyText": "Today I was wanting a simple reference implementation of ggml_compute_forward_mul_mat().\nNot a hyper optimized BLAS/AVX version.\nPurpose == sanity checking, making sure I understand the compute exactly.\nI hoped I would find it in the tests, but I haven't stumbled upon it yet.\nDoes one exist?\nI tried to write my own, but pretty sure its incorrect, as I was seeing strange behavior at runtime, and I expect memory corruption.\nLooking at the shapes I'm seeing, it appears that the weights buffers are already transposed.\nExample shapes:\nfloat *x; // Input with shape (4096, 2, 1, 1)\nfloat *y; // Weights with shape (4096, 11008, 1, 1)\nfloat *z; // Output with shape (11008, 2, 1, 1)\nNote how the matching inner dimension appears to be ne10.\nvoid ggml_ref_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst, void * wdata, size_t wsize) {\n    GGML_ASSERT(ggml_ref_can_mul_mat(src0, src1, dst));\n    LOG(\"ggml_compute_forward_mul_mat called\\n\");\n    LOG(\"backend type: %d\\n\", dst->backend);\n    LOG(\"src0: %s dtype: %d (%d,%d,%d,%d)\\n\", src0->name, src0->type, src0->ne[0], src0->ne[1], src0->ne[2], src0->ne[3]);\n    LOG(\"src1: %s dtype: %d (%d,%d,%d,%d)\\n\", src1->name, src1->type, src1->ne[0], src1->ne[1], src1->ne[2], src1->ne[3]);\n    LOG(\"dst: %s dtype: %d (%d,%d,%d,%d)\\n\", dst->name, dst->type, dst->ne[0], dst->ne[1], dst->ne[2], dst->ne[3]);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    ggml_to_float_t  const to_float = (ggml_to_float_t) dequantize_row_q4_K;\n\n    void* w = src0->data;\n\n    std::vector<float> dequantized_weights(ne00*ne01*ne02*ne03);\n    float* y = dequantized_weights.data();\n\n    // Attempt to convert Q4_K to floats\n    for (int64_t i03 = 0; i03 < ne03; ++i03) {\n        for (int64_t i02 = 0; i02 < ne02; ++i02) {\n            for (int64_t i01 = 0; i01 < ne01; ++i01) {\n                to_float((const char *) w + i01*nb01 + i02*nb02 + i03*nb03, y + i01*ne00 + i02*ne01 + i03*ne02, ne00);\n            }\n        }\n    }\n\n    float* x = (float*)(src1->data);\n\n    float* z = (float*)(dst->data);\n\n    ggml_vec_set_f32(ne0*ne1*ne2*ne3, z, 0);\n\n    for (int i = 0; i < ne3; ++i) {\n        for (int j = 0; j < ne2; ++j) {\n            for (int k = 0; k < ne1; ++k) {\n                for (int l = 0; l < ne0; ++l) {\n                    for (int m = 0; m < ne00; ++m) { // Shared inner dimension\n                        int indexZ = (((i * ne2 + j) * ne1 + k) * ne0) + l;\n                        int indexX = (((i * ne2 + j) * ne1 + k) * ne00) + m;\n                        int indexY = (((i * ne2 + j) * ne0 + l) * ne00) + m;\n                        z[indexZ] += x[indexX] * y[indexY];\n                        // z[i][j][k][l] += x[i][j][k][m] * y[i][j][l][m];\n                    }\n                }\n            }\n        }\n    }\n\n}",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6171",
        "createdAt": "2024-03-20T03:40:06Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "How can I load an in-context learning session and append a file prompt to generate output?",
        "bodyText": "Hello,\nI'm trying to understand how to load a session from disk with in-context learning. I would like to be able to preprocess the in-context learning prompt, persist it to disk, and load the saved session for each future use and append a new prompt (different each time) to generate a response.\nI initially thought the purpose of--prompt-cache was for this. I expect to be able to run:\n./main -ngl 84 -m models/llama-2-7b.Q4_0.gguf -c 4096 -n 400 -s 42 --temp 0.7 --repeat_penalty 1.1 --prompt-cache prompt.cache.bin -f ./prompts/chat-with-bob.txt\nto create a cache of the prompt template for in-context learning. I then expect to be able to run:\n./main -ngl 84 -m models/llama-2-7b.Q4_0.gguf -c 4096 -n 400 -s 42 --temp 0.7 --repeat_penalty 1.1 --prompt-cache prompt.cache.bin --prompt-cache-ro -f ./chat/default/new-prompt.txt\nwhere new-prompt.txt contains What is your first name?, which I'd expect the answer to be Bob given the cached prompt and the cached prompt to not be updated because --prompt-cache-ro. However, it seems that I cannot use the file parameter like this with prompt cache.\nThis is my expectation given:\nhttps://github.com/ggerganov/llama.cpp/discussions/2110\nhttps://github.com/ggerganov/llama.cpp/issues/1398\nI then came across:\nhttps://github.com/ggerganov/llama.cpp/pull/1169\nHowever, I do not see a --session parameter available for ./main. I do see llama_save_session_file in main.cpp but I do not see any examples of how to use it for my purposes.\nDoes anyone have any insights into how I can save an in-context learning template session, and load it for future use while append a text file?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5642",
        "createdAt": "2024-02-21T17:56:49Z",
        "author": {
            "login": "mnuppnau"
        }
    },
    {
        "title": "How bcast works",
        "bodyText": "I saw a testcase in test-backend-ops to test bcast with ADD, MUL and DIV. As far as I know, bcast only works for different dims, and all dims should be same or 1. But this case cast is adding tensor a(16,10,10,10) with b(32,10,10,10). I read the source code, but can't undersand clearly, could some one tell me how it works?\nDoes it bcast tensor a(16,10,10,10) to (32,10,10,10), and copy first \"0-16\" to \"17-31\"?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6190",
        "createdAt": "2024-03-21T02:41:53Z",
        "author": {
            "login": "hipudding"
        }
    },
    {
        "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
        "bodyText": "https://arxiv.org/abs/2403.09347\nI heard you re implementing flash attention on ggml\nI cam across this paper.\nBurstAttention looks like advanced variant of flashattention\nfor mgpu or large cluster\nDoes this have something useful for you\uff1f\n@ggerganov",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6200",
        "createdAt": "2024-03-21T14:32:41Z",
        "author": {
            "login": "sorasoras"
        }
    },
    {
        "title": "Server Doesn't Listen with --host 0.0.0.0 on Sonoma",
        "bodyText": "When I run the server with the command ./server --host 0.0.0.0 on macOS Sonoma 14.3, I can't access the server from another computer on the same network. It works on another Mac running macOS 12.7.\nOn the same computer running Sonoma, I can access it by opening either localhost:8080 or 192.168.1.101:8080.\nI created a test app in Python using Flask, and I was able to access it from another computer. When the firewall is enabled, my Flask app triggers a macOS dialog that asks whether I want to allow Python to listen for incoming connections. Strangely, running llama.cpp/server does not trigger this dialog.\nI tried turning off the firewall, but it didn't work either.\nI would greatly appreciate any insights or suggestions!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5275",
        "createdAt": "2024-02-02T07:29:35Z",
        "author": {
            "login": "chigkim"
        }
    },
    {
        "title": "Metal performance when using llama-cpp-python wrapper tanked from 0.2.28 to 0.2.29 (I know this is a llama.cpp core discussion.. but..)",
        "bodyText": "Hey all,\nI know this is a llama.cpp core project and this issue is about the python wrapper not by the same developers but I've been looking into this issue for a few days and it's kind of stumped me, I don't suppose anyone here with their knowledge of the llama.cpp codebase could think of why performance would tank on metal from version 6efb8eb to 4483396 of llama.cpp?\nThere's an issue here too, I've also done a little bit of digging here I'm just a bit stumped and would love some smart people to help :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6113",
        "createdAt": "2024-03-17T15:11:39Z",
        "author": {
            "login": "agnosticlines"
        }
    },
    {
        "title": "Simple API Wrapper",
        "bodyText": "A wrapper of llamacpp could trivialize instantiating and using a llm directly in C++. I'm referring to something like this:\n#include \"api_wrapper.h\"\n\nvoid main() {\n\n\tlanguage_model the_language_model;\n        the_language_model.params.model = \"C:/Users/Quill/Documents/ai/models/mistral-7b-v0.1.Q8_0.gguf\";\n        the_language_model.initialize();\n        std::string input_text = \"Hi, how are you?\";\n        std::string output_text;\n        the_language_model.process_text(input_text, output_text);\n        printf(\"model response: %s\\n\", output_text.c_str());\n\n}\nI have a small example project here which I don't really plan to update at all, just a proof of concept. Anyways, an API wrapper would be nice to have. I don't think such a thing exists, at least that I was able to find.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6065",
        "createdAt": "2024-03-14T20:28:03Z",
        "author": {
            "login": "montecarlo26"
        }
    },
    {
        "title": "Do the OAI completion endpoints automatically fix the prompt to have the correct format?",
        "bodyText": "For example, if I start the server with this:\n./server -m ../llama/llama-2-13b/ggml-model-f16.gguf\nThen I hit it from python using this:\ncompletion = self.client.chat.completions.create(\n            model=\"gpt-3.5-turbo-0125\",\n            response_format={ \"type\": \"json_object\" },\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": text}\n            ]\n            )\n\nI can't tell if I need to change my user content's text to match the llama-2-13b model I'm running's prompt format from their paper, which is the following format:\n<s>[INST] <<sys>> systemprompthere <</sys>> prompt [/INST]\nAny help is appreciated!\nP.S.- is there a way to require the server output json? I couldn't find a flag for using a json grammar with the server.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6121",
        "createdAt": "2024-03-17T22:52:03Z",
        "author": {
            "login": "codinalot"
        }
    },
    {
        "title": "The V100 SXM2 system has become surprisingly affordable",
        "bodyText": "There's a large number of brand-new Gigabyte T181-G20 barebone servers available in Hong Kong, each selling for around \u00a3120.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5229",
        "createdAt": "2024-01-30T23:53:20Z",
        "author": {
            "login": "bobqianic"
        }
    },
    {
        "title": "What techniques exist for running a large language model (LLM, 20GB+) on a resource-constrained GPU (8GB)?",
        "bodyText": "How can I use a large language model (LLM, 20GB+) for inference on a machine with a smaller GPU (8GB)?\nAre there ways to break computations down for efficient processing?\nThank you",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6124",
        "createdAt": "2024-03-18T05:00:11Z",
        "author": {
            "login": "BecauseTheWorldIsRound"
        }
    },
    {
        "title": "Contrastive search : A feature to strive for alongside speculative decoding?",
        "bodyText": "#3278\nlogikstate originally wrote this issue, and @KerfuffleV2 suggested moving it over to a discussion.\nWill paste in some of the previous discussion back here, but the gist is:\n-Could Llama.cpp support contrastive search alongside speculative decoding? It seems like this technique has the potential to radically improve model performance\n\nPrevious Discussion\nlogikstate\nThis paper has a method similar to speculative sampling that improves models by sampling the lower quality model for tokens to avoid thus increasing the quality of the output of the higher quality model. Allegedly leading to LLaMA-65B outperforming LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark.\nhttps://arxiv.org/abs/2309.09117\n\"We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.\"\n\nKerfuffleV2 commented Sep 22, 2023\nI was looking at this a few days ago, but it seems pretty complicated. Unlike the other samplers that you can just give the last tokens + current logits to, it seems like contrastive decoding requires a different approach. (Correct me if I'm wrong.)\nI tried to find a simple example of implementing it but wasn't successful.\n__\nIridiumMaster\nHere's what they list in their appendix:\nA.1 CODE IMPLEMENTATION\nWe include PyTorch implementations of contrastive decoding in Algorithm 1 and Algorithm 2\nAlgorithm1: Originalformulation\nexpert logits - unnormalized scores from the expert model\namateur logits - unnormalized scores from the amateur model # amateur temp - temperature to normalize amateur distribution # alpha - masking threshold\nexpert probs = softmax(expert logits, dim=-1)\namateur probs = softmax(amateur logits / amateur temp, dim=-1) cutoff = alpha*expert probs.max(dim=-1, keepdim=True).values\ndiffs = log(expert probs) - log(amateur probs)\ncd logits = diffs.masked fill(expert probs < cutoff, -float(\u2019inf\u2019))\nAlgorithm2: Ourformulation\nexpert logits - unnormalized scores from the expert model\namateur logits - unnormalized scores from the amateur model # alpha - masking threshold\nbeta - expert-amateur tradeoff parameter\ncutoff = log(alpha) + expert logits.max(dim=-1, keepdim=True).values diffs = (1 + beta)expert logits - betaamateur logits\ncd logits = diffs.masked fill(expert logits < cutoff, -float(\u2019inf\u2019))\nAnd here is GPT 3.5 16k Turbo's take on the approach required:\n    Prepare your expert and amateur language models. These models should be pre-trained and capable of generating text.\n\n    Calculate the unnormalized scores (logits) for each token from both the expert and amateur models.\n\n    Choose a hyperparameter alpha (\u03b1) to determine the masking threshold. This will be used to mask out tokens that have lower probability assigned by the expert model.\n\n    Calculate the weighted differences in likelihood (diffs) between the expert and amateur models. This can be done by subtracting the amateur logits from the expert logits and applying weights.\n\n    Apply the alpha-mask to filter out tokens with lower probability assigned by the expert model. This can be done by comparing the expert logits to a threshold obtained from alpha.\n\n    Apply the final CD logits by replacing the expert logits with the masked logits. Tokens that are below the masking threshold will be replaced with -inf to avoid selecting them during decoding.\n\n    Use the CD logits to generate text by selecting tokens with higher probabilities in the CD distribution. Greedy decoding or sampling techniques can be used based on your preference.\n\nBy following these steps, you can implement contrastive decoding to improve text generation from your language models.\n\nAnd here's what it has to say about your statement:\nIn the context of the paper, the statement holds true. Contrastive decoding does require a different approach compared to other sampling methods. Contrastive decoding involves searching for tokens that maximize a weighted difference in likelihood between a stronger expert model and a weaker amateur model. This requires calculating the differences in probabilities between the expert and amateur models, and then applying a masking threshold to filter out low-probability tokens. The resulting contrastive logits are used for text generation.\n\nIn contrast, other sampling methods like top-k sampling or nucleus sampling only require the last tokens and current logits to select the next token for text generation. These methods do not involve comparing probabilities between different models or applying specific masking techniques.\n\nTherefore, contrastive decoding does require a distinct approach that considers the differences between the expert and amateur models, making it distinct from other sampling techniques.\n\n\nIt seems like something that could be enabled as speculative decoding with smaller models is implemented, @KerfuffleV2 ?\n\nKerfuffleV2 commented Oct 1, 2023\nYes, it does kind of sound like something that could at least reuse parts of the existing speculative stuff. You might not even need a completely separate model: https://arxiv.org/abs/2309.08168\nBy the way, you might get more responses if you created this as a discussion rather than an issue.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3450",
        "createdAt": "2023-10-03T01:23:46Z",
        "author": {
            "login": "IridiumMaster"
        }
    },
    {
        "title": "Force 4-bit weight quantization only?",
        "bodyText": "I notice the Q4_K_M quantized llama2 models have some weight tensors that are 4-bit, and some that are 6-bit.\nIs there a way to force 4-bit only?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6150",
        "createdAt": "2024-03-19T01:35:02Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "Evalute textual sentence embedding as part of a prompt",
        "bodyText": "I'm looking for an example of how to use both tokens and embedding as input for llama_decode.\nI want to get an embedding of some sentence, and then prompt the model with the embedding and a question to make it answer based on the embedding.\nExample pseudo-code:\nconst auto embedding = get_embedding_for(\"The best thing is Math\");\nconst auto response = prompt(\"Fact:\", embedding, \"\\nExplain what the fact is\");\nI tried looking at the way the llava example embeds an image but I couldn't replicate it with textual (tokens) embedding.\nIs it possible to do?\nOr is using embedding like that unique for multimodal models?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6143",
        "createdAt": "2024-03-18T17:52:54Z",
        "author": {
            "login": "giladgd"
        }
    },
    {
        "title": "Is it possible to specify GPU order?",
        "bodyText": "Is it possible to specify GPU order?   if not, would orders find it useful?\nSay I have a system with 4 GPUs 2 3090s and 2 1080's and the 3090's are in slot 1 & 4 so showing up as GPU 0 & 3 and the 1080's are in slot 2 & 3 showing up as 1 & 2.  The reason is because slot 2 & 3 are PCIe 1x whereas slot 1 & 4 are PCIe 8x.     I'll like to prioritize loading models on the 3090's.   Sure with option ts I can do 0, 0 to skip the 1080's but if I have a model that will be bigger than the memory of both 3090's.  I'll like to max them out first before putting anything in the 1080's.   I'll also like to leave them in the slot for when I'm doing training which does need faster PCI bandwidth.\nIf this is not possible, would other's find it useful to be added?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6107",
        "createdAt": "2024-03-17T01:07:59Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "iPad App",
        "bodyText": "I've been playing with using llama to help me tell stories to my daughter at night. I wrote a simple native iPad app that uses llama.cpp, and provides some nice model / thread management capabilities on top of it. It runs quite well on my M2 iPad after a few tweaks to the memory allocations in llama.\nI'm curious if there's enough interest in something like this for me to continue polishing it and share it somewhere.\n\n  \n    \n    \n\n    trimmed.mp4",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/844",
        "createdAt": "2023-04-08T04:25:37Z",
        "author": {
            "login": "ocrickard"
        }
    },
    {
        "title": "Make llama.cpp `brew install`-able on Mac!",
        "bodyText": "Hey hey! - I'm a fellow llama.cpp enthusiast and am interested in making llama.cpp more accessible to people by making it brew install friendly!\nHere's my first attempt at it: https://github.com/Vaibhavs10/homebrew-tap/blob/main/Formula/llama.cpp.rb\nYou can install it via brew install vaibhavs10/tap/llama.cpp\nAfter a successful installation, you can run the llama --version from anywhere on your Mac.\nBut, of course, as you can see, this is far from being ready for widespread usage yet.\nCouple of issues that I can already see:\n\nSHA mismatches (since it clones the repo, it results in SHA mismatches whenever a change is made)\nI get\n\n-[MTLComputePipelineDescriptorInternal setComputeFunction:withType:]:722: failed assertion `computeFunction must not be nil.'\n\nwhen passing a model file via llama -m <path to GGUF> - I am at my wit's end to figure out why that is happening.\n3. It still doesn't wrap the llama.cpp server, which would be an excellent value addition, in my opinion.\nI'd love to work with you all/ community here to make this possible and also make it work with the server example.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6116",
        "createdAt": "2024-03-17T18:52:00Z",
        "author": {
            "login": "Vaibhavs10"
        }
    },
    {
        "title": "Very slow IQ quant performance on Apple Silicon || Expected performance on IQ llama.cpp implementation on Apple Silicon?",
        "bodyText": "Hey there,\nI've been playing about with the IQ quantisation methods, I have an M1 Max Pro with 64GB of RAM, I usually run mixtral finetunes (8x7b with 2 experts) at Q5_K_M and get reasonable performance (8-15t/s) and prompt evaluation normally takes 10-20seconds even on very large prompts.\nI downloaded an IQ2 XS quant of a 120B model and it's taking close to 10 minutes to evaluate the prompt and I'm getting like 1 token every 4-5 seconds, is this expected?\nPrompt evaluation:  50%|     | 2/4 [03:45<03:45, 112.99s/it]\nThis does eventually finish but it takes 9 minutes and then I get about 0.1 tokens per second.\nThe performance with an IQ2 XS of a 7b model is also pretty bad but at least it finishes before the heat death of the universe:\nOutput generated in 26.30 seconds (0.95 tokens/s, 25 tokens, context 1523, seed 1885244309)\nWhich is why I'm not sure if I'm running into a bug, if it's not been designed/optimised for metal or if this is expected performance\nI understand the original QuiP# paper and implementation is CUDA focused, is this just an area where it's not optimised? If that is the case is there any plans to optimise the metal implementation for these newer quants? Also not sure if this is relevant but during this process my CPU usage doesn't max out like it normally does when generating text with these models, normally it pins all my cores, this takes a few cores to 70% and python uses 400% of CPU compared to like 2800% or whatever it normally does.\nAlso wasn't sure if this should be an issue or a discussion so just decided to go on the safe side and make a discussion\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5617",
        "createdAt": "2024-02-20T19:34:57Z",
        "author": {
            "login": "agnosticlines"
        }
    },
    {
        "title": "Better offload?",
        "bodyText": "https://github.com/SJTU-IPADS/PowerInfer\nClaim 11x for 40B on 4090 vs llamacpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4534",
        "createdAt": "2023-12-19T14:42:32Z",
        "author": {
            "login": "Kimiko-AI"
        }
    },
    {
        "title": "GGUF vs bin?",
        "bodyText": "Hi everybody. I'm sorry if I asked an incorrect question, I'm not good at this.\nI am interested in maximum performance. Which is better than GGUF or bin?\nAlso, I saw a topic in which ggerganov advised to disable map and compile llama.cpp after such changes.\nDoes it really work? And how can I set parameters for high performance?\nAll I managed to do was launch \"./main -m ../openchat-3.5-0106.Q4_K_M.of \"Building a website can be done in 10 simple steps:\\step 1:\" -n 1024 -e\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6068",
        "createdAt": "2024-03-14T22:42:12Z",
        "author": {
            "login": "VadimBoev"
        }
    },
    {
        "title": "Need help running server on Android with OpenCL support",
        "bodyText": "As written on the readme, for running llama.cpp with CLBlast support on Android we need to add this before launching: export LD_LIBRARY_PATH=/vendor/lib64:$LD_LIBRARY_PATH\nThis works fine for main, but running server does not work, because it cannot find some required libs for networking.\nThat's the error:\nCANNOT LINK EXECUTABLE \"./server\": cannot locate symbol \"__emutls_get_address\" referenced by \"/data/data/com.termux/files/home/llama.cpp/server\"...\nSomeone has some idea on how to make that work?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5507",
        "createdAt": "2024-02-15T13:11:51Z",
        "author": {
            "login": "DGdev91"
        }
    },
    {
        "title": "Close inactive issue workflow",
        "bodyText": "we have more than 1k opened issues. It can be confusing for end users to understand what is obsolete or real issue.\n@ggerganov if you agree, we can create a simple github workflow:\nhttps://docs.github.com/en/actions/managing-issues-and-pull-requests/closing-inactive-issues",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6048",
        "createdAt": "2024-03-14T08:03:30Z",
        "author": {
            "login": "phymbert"
        }
    },
    {
        "title": "Question about metal parallel command buffers processing",
        "bodyText": "In the ggml-metal.m, there is a code to submit several metal work chunks in parallel:\nconst int n_cb = ctx->n_cb;\n..\nfor (int cb_idx = 0; cb_idx < n_cb; ++cb_idx) { // create multiple command buffers\n..\ndispatch_apply(n_cb, ctx->d_queue, ^(size_t iter) { // run multiple command buffers\nWhen I was trying to experiment with setting different tctx->n_cb for different LLMs on different Mac machines, I found very little performance difference between settings, e.g.  n_cb=1 was always as efficient as default n_cb= 64.\nThere is noticeable difference when inference is running while competing with other workloads that are heavy GPU users, but I suspect that was not the primary intent of implementing multiple buffers support.\nWhat type of workload am I missing that demonstrates performance advantage for having multiple metal command buffers?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6090",
        "createdAt": "2024-03-15T19:50:54Z",
        "author": {
            "login": "izard"
        }
    },
    {
        "title": "How to get or generate a file of log of probabilities of base model to measure KL divergence for perplexity?",
        "bodyText": "I read some threads here mentioning KL divergence is more appropriate than perplexity, so I decided to do it by my hand. But when I tried to run perplexity with --kl-divergence option, I faced below error message.\nkl_divergence: you must provide a name of a file containing the log probabilities of the base model\n\nI have no knowledge of KL divergence. Please teach me how to get or generate the file with probabilities. What I tried to do is to measure KL divergence for various quant types of llama.cpp on meta-llama/Llama-2-7b-chat-hf model.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5975",
        "createdAt": "2024-03-10T14:34:47Z",
        "author": {
            "login": "beebopkim"
        }
    },
    {
        "title": "Where to insert code to deploy to custom accelerator?",
        "bodyText": "I have a custom accelerator that can do matrix multiplications and an associated C++/C API.\nIt accepts bfloat16 inputs, and int4 weights.\nI am curious if someone can help me figure out the best place to insert this API?\nI am thinking ggml-quant.c\nAdditionally, the accelerator requires that weights be preloaded on off chip memory.\nSo it would be nice if there was some pass where I can find all the matrix multiplication ops, preload the weight tensors, and cache an association between the op instance, and the device buffer.\nAny guidance or ideas on this would be greatly appreciated!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6070",
        "createdAt": "2024-03-15T00:40:48Z",
        "author": {
            "login": "wilderfield"
        }
    },
    {
        "title": "Inference and embeddings directly on web browser with wasm",
        "bodyText": "Hi everyone,\nI've just made a WebAssembly binding for llama.cpp that can run completely on browser. It already have support for high-level API like completions, embeddings, as well as some low-level functions like sampling, (de)tokenization,...\nWrapper code is written in typescript, can be embedded inside a ReactJS project (there's an example on my repo).\nWould be nice if someone can play around and make a complete RAG-in-browser demo :-)\nLink to the project: https://github.com/ngxson/wllama",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6055",
        "createdAt": "2024-03-14T14:54:04Z",
        "author": {
            "login": "ngxson"
        }
    },
    {
        "title": "Presentation on llama.cpp on 22.02.24 at LIPS",
        "bodyText": "I will hold a presentation on llama.cpp at the First Large Language Models in Physics Symposium on the 22. of February. The title of the presentation is \"Efficient Matrix Multiplication Algorithms for Quantized Language Models\" with the following abstract:\n\nLarge language models have - as the name implies - large numbers of parameters. As such not only\nthe training costs but also the inference costs of these models are quite substantial. One strategy\nfor reducing inference costs is to quantize the model weights from 16 bit floating point values to\na format with 2-8 bits per weight. However, these custom data formats in turn require custom\ninference code. This talk describes the interplay of llama.cpp quantization formats and inference\ncode and how int8 tensor cores or integer intrinsics can be used to reach performance exceeding\nthat of standard floating point GEMM routines provided by e.g. cuBLAS\n\nAs the abstract implies, the talk will be about my work on llama.cpp regarding performance optimizations for matrix multiplication algorithms. Registration for in-person attendance is already closed but it is still possible to register for attendance via Zoom if someone wants to.\nThis is the first draft of the slides that I will be using:\nEfficient_Mat_Mul.pdf\nEdit: second draft:\nEfficient_Mat_Mul.pdf",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5551",
        "createdAt": "2024-02-17T18:47:42Z",
        "author": {
            "login": "JohannesGaessler"
        }
    },
    {
        "title": "Multiple GPUs (uneven split)",
        "bodyText": "Hi. I have a 3090 (24GB) and a 4080 (16GB) on my home, and thought I should try combining them to run bigger models.\nI went to aphrodite & vllm first since there are supposedly the go-tos for multi-GPU distribution, but both of them assume all GPUs have the same amount of VRAM available, so models won't load if I try to utilize them.\nDoes llama.cpp support uneven split of GBs/layers between multiple GPUs?\n(I have slow-ish internet connection so it took ages to DL a big AWQ model. Thought I'd ask here before downloading a GGUF version.)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6046",
        "createdAt": "2024-03-13T21:20:25Z",
        "author": {
            "login": "Lyrcaxis"
        }
    },
    {
        "title": "I need help to quantize video-llava model to quant size 4 or 8",
        "bodyText": "I am trying to convert video-llava according to instructions at #2948 but I am having issues because of convert.py file. Since, this is llava + vicuna should I add anything extra in the convert.py file or let it remain the same?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/6029",
        "createdAt": "2024-03-12T23:24:35Z",
        "author": {
            "login": "dnabanita7"
        }
    },
    {
        "title": "AQLM: Extreme Compression of Large Language Models via Additive Quantization",
        "bodyText": "Official Pytorch repository for Extreme Compression of Large Language Models via Additive Quantization\nhttps://arxiv.org/pdf/2401.06118.pdf\nhttps://github.com/Vahe1994/AQLM\nThoughts?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5984",
        "createdAt": "2024-03-10T21:27:22Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "server unknown argument: --mmproj",
        "bodyText": "Hi,\nI'm brand new to llama.cpp; just did a build on Windows (from the top of the master branch) and trying to run server.exe I'm seeing this output:\nserver.exe -m ggml-model-q4_k.gguf --mmproj mmproj-model-f16.gguf \nerror: unknown argument: --mmproj\nusage: server.exe [options]\n\nThe model and projection loads fine if I use llava-cli from the same local build.\nAre there additional steps (configuration options, or different branches) needed to have multimodal support enabled in server?\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5967",
        "createdAt": "2024-03-10T00:32:47Z",
        "author": {
            "login": "TemporalAgent7"
        }
    },
    {
        "title": "How to connect Mixtral 8x7b instructor in windows application using C++?",
        "bodyText": "I want to use Mixtral8x7b instrutor in windows application.\nIs it possible to connect using C++?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5883",
        "createdAt": "2024-03-05T10:09:19Z",
        "author": {
            "login": "aitechguy0105"
        }
    },
    {
        "title": "Building the server with llava/mmproj/multimodal",
        "bodyText": "I'm trying to get the server binary working with multimodal but mine is not being built with the --mmproj option from the master branch. llava-cli is being built. Can you tell me if I need to pull a separate branch or need to add any options to get the server working? Thanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5945",
        "createdAt": "2024-03-08T21:23:43Z",
        "author": {
            "login": "KerseyFabrications"
        }
    },
    {
        "title": "Writing Godot extension for llama.cpp",
        "bodyText": "Hi everyone,\nI've been writing a little extension for the Godot game engine to run LLM inference using llama.cpp.\nSo far, I have everything set up following the simple code example. However, when I try to run my project using my extension, every time I try to run some completions in the same context instance, it would always seem to use the very first prompt that I input.\nHere's what my code looks like\nI've been scratching my head trying to solve this problem for days now. Would really appreciate any help at all in trying to solve this. Thanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5907",
        "createdAt": "2024-03-06T16:59:18Z",
        "author": {
            "login": "hazelnutcloud"
        }
    },
    {
        "title": "Can (or How) you refine tune a GGUF model?",
        "bodyText": "I saw from theBloke discussion post that it's possible https://huggingface.co/TheBloke/Yi-6B-GGUF/discussions/2 but idk where to start, can you guys point me to a guide about this or any discussion?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5900",
        "createdAt": "2024-03-06T08:57:58Z",
        "author": {
            "login": "arvnoodle"
        }
    },
    {
        "title": "Using llama.cpp with AWS instances",
        "bodyText": "Description\nThe llama.cpp project offers unique ways of utilizing cloud computing resources. Here we will demonstrate how to deploy a llama.cpp server on a AWS instance for serving quantum and full-precision F16 models to multiple clients efficiently.\nSelect an instance\n\n\nGo to AWS instance listings: https://aws.amazon.com/ec2/pricing/on-demand/\n\n\nSort by price and find the cheapest one with NVIDIA GPU:\n\n\n\nCheck the specs:\n\n\n\nThe g4dn.xlarge instance has 1x T4 Tensor Core GPU with 16GB VRAM. Here are the NVIDIA specs for convenience:\n\n  Click me\n\n\n\n\nStart the instance and login over SSH\n\n\n\nAlso, make sure to enable inbound connections to port 8888 - we will need it later for the HTTP server\n\n\n\nSelect a model and prepare llama.cpp\nWe have just 16GB VRAM to work with, so we likely want to choose a 7B model. Lately, the OpenHermes-2.5-Mistral-7B model is getting some traction so let's go with it.\nWe will clone the latest llama.cpp repo, download the model and convert it to GGUF format:\nsudo apt update\nsudo apt install make g++ git-lfs\n\n# get the model data\ngit lfs install\ngit clone https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\n\n# clone llama.cpp and setup python conversion stuff\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\npip install -r requirements.txt\nln -sfn ../OpenHermes-2.5-Mistral-7B ./models/openhermes-7b-v2.5\n\n# convert to F16 GGUF\npython3 convert.py ./models/openhermes-7b-v2.5 --outfile ./models/openhermes-7b-v2.5/ggml-model-f16.gguf --outtype f16\n\n# quantize to Q8_0 and Q4_K\n./quantize ./models/openhermes-7b-v2.5/ggml-model-f16.gguf ./models/openhermes-7b-v2.5/ggml-model-q8_0.gguf q8_0\n./quantize ./models/openhermes-7b-v2.5/ggml-model-f16.gguf ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf q4_k\nDo some performance benchmarks\nThe T4 GPUs have just 320GB/s memory bandwidth, so we cannot expect huge tok/s numbers, but let's work with what we have.\nWe want to be serving requests in parallel, so we have to have an idea about the types of queries that we are going to be processing in order to setup some limits. Let's do the following assumptions:\n\n\n\nMax clients\nMax prompt\nMax Len\n\n\n\n\n4\n2048\n512\n\n\n\nWe assume that at any moment in time, there will be a maximum of 4 queries being processed in parallel. Each query can have a maximum individual prompt of 2048 tokens and each query can generate a maximum of 512 tokens. So in order to support this scenario, we need to have a KV cache of size 4*(2048 + 512) = 10240 tokens (1280 MiB, F16).\nLet's benchmark stock llama.cpp using the F16 model:\n# build the benchmark tool\nLLAMA_CUBLAS=1 make -j batched-bench\n\n# bench the F16 model\n./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4\n\nllama_new_context_with_model: total VRAM used: 14363.04 MiB (model: 13563.03 MiB, context: 800.00 MiB)\n\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\n|  2048 |    128 |    1 |   2176 |    1.914 |  1070.20 |    8.108 |    15.79 |   10.022 |   217.13 |\n|  2048 |    512 |    1 |   2560 |    1.828 |  1120.61 |   32.709 |    15.65 |   34.537 |    74.12 |\n\n  Legend\n\nPP: prompt size in tokens\nTG: text to generate in tokens\nB: number of batches (i.e. parallel requests)\nN_KV: required size of the KV cache in tokens\nT_PP: time to process the prompts\nS_PP: speed of processing the prompts in tok/s\nT_TG: time to generate the response\nS_TG: speed of the generation in tok/s\nT: total time to process the batches (i.e. requests)\nS: total speed (including prompt and text) in tok/s\n\n\nWe immediately notice that there is not enough VRAM to load both the F16 model and the 10240 tokens KV cache. This means the maximum clients we can serve in this case is just 1. The TG speed is also not great as we expected: ~16 t/s.\nLet's for a moment relax the requirements and say that the max prompt size would be 512 instead of 2048. This scenario now fits in the available VRAM and here are the results:\n# bench the F16 model using small prompt size of 512\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 512 128,512 1,2,3,4\n\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\n|   512 |    128 |    1 |    640 |    0.398 |  1288.00 |    7.589 |    16.87 |    7.987 |    80.13 |\n|   512 |    128 |    2 |   1280 |    0.798 |  1283.52 |    8.554 |    29.93 |    9.352 |   136.87 |\n|   512 |    128 |    3 |   1920 |    1.298 |  1183.32 |    9.053 |    42.42 |   10.351 |   185.49 |\n|   512 |    128 |    4 |   2560 |    1.838 |  1114.15 |    9.277 |    55.19 |   11.115 |   230.33 |\n|   512 |    512 |    1 |   1024 |    0.373 |  1372.17 |   30.693 |    16.68 |   31.066 |    32.96 |\n|   512 |    512 |    2 |   2048 |    0.784 |  1306.16 |   34.753 |    29.47 |   35.537 |    57.63 |\n|   512 |    512 |    3 |   3072 |    1.274 |  1205.64 |   37.243 |    41.24 |   38.517 |    79.76 |\n|   512 |    512 |    4 |   4096 |    1.846 |  1109.59 |   38.492 |    53.21 |   40.338 |   101.54 |\nllama.cpp supports efficient quantization formats. By using a quantum model, we can reduce the base VRAM required to store the model in memory and thus free some VRAM for a bigger KV cache. This will allow us to serve more clients with the original prompt size of 2048 tokens. Let's repeat the same benchmark using Q8_0 and Q4_K quantum models:\n# bench the Q8_0 model\n./batched-bench ./models/openhermes-7b-v2.5/ggml-model-q8_0.gguf 10240 0 99 0 2048 128,512 1,2,3,4\n\nllama_new_context_with_model: total VRAM used: 9169.84 MiB (model: 7205.84 MiB, context: 1964.00 MiB)\n\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\n|  2048 |    128 |    1 |   2176 |    2.596 |   788.77 |    4.974 |    25.73 |    7.570 |   287.44 |\n|  2048 |    128 |    2 |   4352 |    6.247 |   655.65 |    7.614 |    33.62 |   13.861 |   313.97 |\n|  2048 |    128 |    3 |   6528 |   11.200 |   548.56 |    9.499 |    40.43 |   20.699 |   315.38 |\n|  2048 |    128 |    4 |   8704 |   17.268 |   474.42 |   10.880 |    47.06 |   28.148 |   309.23 |\n|  2048 |    512 |    1 |   2560 |    2.514 |   814.65 |   20.158 |    25.40 |   22.672 |   112.92 |\n|  2048 |    512 |    2 |   5120 |    6.221 |   658.39 |   31.274 |    32.74 |   37.495 |   136.55 |\n|  2048 |    512 |    3 |   7680 |   11.136 |   551.71 |   39.606 |    38.78 |   50.743 |   151.35 |\n|  2048 |    512 |    4 |  10240 |   17.295 |   473.65 |   46.026 |    44.50 |   63.321 |   161.72 |\n\n# bench the Q4_K model\n./batched-bench ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf 10240 0 99 0 2048 128,512 1,2,3,4\n\nllama_new_context_with_model: total VRAM used: 6059.07 MiB (model: 4095.06 MiB, context: 1964.00 MiB)\n\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\n|  2048 |    128 |    1 |   2176 |    2.409 |   849.99 |    3.938 |    32.50 |    6.348 |   342.79 |\n|  2048 |    128 |    2 |   4352 |    5.864 |   698.52 |    6.554 |    39.06 |   12.417 |   350.48 |\n|  2048 |    128 |    3 |   6528 |   10.635 |   577.70 |    8.433 |    45.53 |   19.068 |   342.35 |\n|  2048 |    128 |    4 |   8704 |   16.522 |   495.82 |    9.795 |    52.27 |   26.317 |   330.74 |\n|  2048 |    512 |    1 |   2560 |    2.301 |   890.02 |   16.083 |    31.83 |   18.384 |   139.25 |\n|  2048 |    512 |    2 |   5120 |    5.804 |   705.69 |   27.014 |    37.91 |   32.818 |   156.01 |\n|  2048 |    512 |    3 |   7680 |   10.540 |   582.93 |   35.224 |    43.61 |   45.764 |   167.82 |\n|  2048 |    512 |    4 |  10240 |   16.542 |   495.22 |   41.611 |    49.22 |   58.153 |   176.09 |\nUsing the quantum models and a KV cache of size 4*(2048 + 512) == 10240 we can now successfully serve 4 clients in parallel and have plenty of VRAM left. The prompt processing speed is not as good as F16, but the text generation is better or similar.\nNote that llama.cpp supports continuous batching and sharing a common prompt. A sample implementation is demonstrated in the parallel.cpp example. Here is a sample run with the Q4_K quantum model, simulating 4 clients in parallel, asking short questions with a shared assistant prompt of 300 tokens, for a total of 64 requests:\nLLAMA_CUBLAS=1 make -j parallel && ./parallel -m ./models/openhermes-7b-v2.5/ggml-model-f16.gguf -n -1 -c 4096 --cont_batching --parallel 4 --sequences 64 --n-gpu-layers 99 -s 1\n\n  Results from `parallel`\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32002\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = mostly Q4_K - Medium\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \nllm_load_print_meta: general.name   = models\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 32000 '<|im_end|>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MiB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =   70.42 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 35/35 layers to GPU\nllm_load_tensors: VRAM used: 4095.06 MiB\n...............................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init: offloading v cache to GPU\nllama_kv_cache_init: offloading k cache to GPU\nllama_kv_cache_init: VRAM kv self = 512.00 MiB\nllama_new_context_with_model: kv self size  =  512.00 MiB\nllama_build_graph: non-view tensors processed: 740/740\nllama_new_context_with_model: compute buffer total size = 291.07 MiB\nllama_new_context_with_model: VRAM scratch buffer: 288.00 MiB\nllama_new_context_with_model: total VRAM used: 4895.07 MiB (model: 4095.06 MiB, context: 800.00 MiB)\n\nNo new questions so proceed with build-in defaults.\n\n\nmain: Simulating parallel requests from clients:\nmain: n_parallel = 4, n_sequences = 64, cont_batching = 1, system tokens = 299\n\nmain: Evaluating the system prompt ...\n\nProcessing requests ...\n\nmain: clearing the KV cache\nClient   0, seq    0, started decoding ...\nClient   1, seq    1, started decoding ...\nClient   2, seq    2, started decoding ...\nClient   3, seq    3, started decoding ...\nClient   1, seq   1/ 64, prompt   15 t, response   22 t, time  1.20 s, speed 30.83 t/s, cache miss 0  \nInput:    What is the best way to cook a steak?\nResponse: The best way to cook a steak depends on your personal preference, but here is a general guideline:\n\nClient   1, seq    4, started decoding ...\nClient   1, seq   4/ 64, prompt   13 t, response   12 t, time  0.69 s, speed 36.18 t/s, cache miss 0  \nInput:    Recommend some interesting books to read.\nResponse: Here are some books that I recommend for your reading pleasure:\n\nClient   1, seq    5, started decoding ...\nClient   3, seq   3/ 64, prompt   22 t, response   56 t, time  3.06 s, speed 25.48 t/s, cache miss 0  \nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\nResponse: I am familiar with the Special Theory of Relativity and I would be happy to explain it to you. The Special Theory of Relativity is a theory proposed by Albert Einstein in 1905 that explains the relationship between space and time. It has two postulates:\n\n...\n\nClient   2, seq  63/ 64, prompt   22 t, response   46 t, time  2.36 s, speed 28.80 t/s, cache miss 0  \nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\nResponse: Yes, I am familiar with the Special Theory of Relativity. It was developed by Albert Einstein in 1905 and it describes the physical laws that govern objects in motion. The theory is based on two postulates:\n\nClient   3, seq  61/ 64, prompt   22 t, response   96 t, time  4.52 s, speed 26.13 t/s, cache miss 0  \nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\nResponse: Yes, I am familiar with the Special Theory of Relativity. It was developed by Albert Einstein in 1905 and it is based on two postulates or principles. The first postulate is that the laws of physics are the same for all observers who are moving at a constant velocity relative to each other. The second postulate is that the speed of light in a vacuum is always the same, regardless of the motion of the source of light or the observer.\n\nmain: clearing the KV cache\n\nrun parameters as at 2023-11-26 14:56:55\n\nmain: n_parallel = 4, n_sequences = 64, cont_batching = 1, system tokens = 299\nExternal prompt file: used built-in defaults\nModel and path used:  ./models/openhermes-7b-v2.5/ggml-model-q4_k.gguf\n\nTotal prompt tokens:    956, speed: 17.48 t/s\nTotal gen tokens:      3794, speed: 69.39 t/s\nTotal speed (AVG):           speed: 86.87 t/s\nCache misses:             0\n\n\nllama_print_timings:        load time =   12698.38 ms\nllama_print_timings:      sample time =    1845.35 ms /  3858 runs   (    0.48 ms per token,  2090.66 tokens per second)\nllama_print_timings: prompt eval time =   51162.52 ms /  5021 tokens (   10.19 ms per token,    98.14 tokens per second)\nllama_print_timings:        eval time =     750.60 ms /    28 runs   (   26.81 ms per token,    37.30 tokens per second)\nllama_print_timings:       total time =   54680.08 ms\n\n\nRunning a demo HTTP server\nThe llama.cpp server example can be build and started like this:\n# start llama.cpp server, max 4 clients in parallel, prompt size 2048, max seq 512, listen on port 8888\nLLAMA_CUBLAS=1 make -j server && ./server -m models/openhermes-7b-v2.5/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512\n\n# send a completion request via curl\ncurl -s http://XXX.XXX.XXX.XXX:8888/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer no-key\" \\\n    -d '{\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Write a limerick about python exceptions\"\n            }\n        ]\n    }' | jq\n\n# ... result:\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"There once was a coder named Sue,\\nWho worked with Python, quite true.\\nShe'd write her code,\\nBut exceptions would load,\\nAnd leave her with feelings of askew.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1701012712,\n  \"id\": \"chatcmpl-sHBoOZIbYDI3M6vfzWREuNJxRJ0WuBqN\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 43,\n    \"prompt_tokens\": 48,\n    \"total_tokens\": 91\n  }\n}\nAn alternative way for really quick deployment of llama.cpp for demo purposes is to use the server-llm.sh helper script:\nbash -c \"$(curl -s https://ggml.ai/server-llm.sh)\"\nFor more info, see: #3868\nFinal notes\nThis was a short walkthrough of how to setup and bench llama.cpp in the cloud that I hope would be useful for people looking for a simple and efficient LLM solution. There are many details not covered here and one needs to understand some of the intricate details of the llama.cpp and ggml implementations in order to take full advantage of the available compute resources. Knowing when to use a quantum model vs F16 model for example requires understanding of the existing CUDA kernels and their limitations. The code base is still relatively simple and allows to easily customize the implementation according to the specific needs of a project. Such customizations can yield significant performance gains compared to the stock llama.cpp implementation that is available out-of-the-box from master.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4225",
        "createdAt": "2023-11-26T16:15:14Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "-ctk q8_0 -ctv q8_0  return CUDA error",
        "bodyText": "I run command like this:\n./main -m qwen_72b_q3_k.gguf -n -1 -c 2048 -ctk q8_0 -ctv q8_0 -ngl 99 -i --multiline-input  -cml\n\nIt returns error like this:\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 81/81 layers to GPU\nllm_load_tensors:        CPU buffer size =   510.47 MiB\nllm_load_tensors:      CUDA0 buffer size = 34467.06 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 1000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  2720.00 MiB\nllama_new_context_with_model: KV self size  = 2720.00 MiB, K (q8_0): 1360.00 MiB, V (q8_0): 1360.00 MiB\nllama_new_context_with_model:  CUDA_Host input buffer size   =    21.02 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   356.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    16.00 MiB\nllama_new_context_with_model: graph splits (measure): 2\nCUDA error: invalid argument\n  current device: 0, in function ggml_cuda_op_mul_mat at ggml-cuda.cu:10394\n  ggml_cuda_cpy_tensor_2d(src0_dd_i, src0, i03, i02/i02_divisor, dev[id].row_low, dev[id].row_high, stream)\nGGML_ASSERT: ggml-cuda.cu:255: !\"CUDA error\"\nAborted (core dumped)\n\nBut when I use f16 everything goes will.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5869",
        "createdAt": "2024-03-04T10:10:07Z",
        "author": {
            "login": "aisensiy"
        }
    },
    {
        "title": "How to connect Mixtral 8x7b instructor in windows application using C++?",
        "bodyText": "I want to connect Mixtral 8x7b instructor in windows application.\nIs it possible in C++?\nThe model should be running in memory.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5884",
        "createdAt": "2024-03-05T10:12:16Z",
        "author": {
            "login": "aitechguy0105"
        }
    },
    {
        "title": "How to recover from cuda Out of memory",
        "bodyText": "Hi,\nI would like to thank you all for llama.cpp !! It\u2019s great. I am new to llama.cpp and have just recently integrated into my cpp program and am running into an issue. The application work great but I cannot seem to be able to recover it from OOM error. I am using cuda compiled version with gpu offloading. When my application runs into OOM and even though I reload the model it still is causing segmentation fault next time I try to run inference. Is there any way to recover from OOM ? I did notice majority of examples exit  application in case of such failure but can I recover from OOM without restarting whole application ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5871",
        "createdAt": "2024-03-04T16:42:36Z",
        "author": {
            "login": "c0ffymachyne"
        }
    },
    {
        "title": "IQ3_S Improvements",
        "bodyText": "I have been exploring ways to improve perplexity for IQ3_S quantization while speeding it up on AVX/NEON, and I think I have found one. This uses the multiply-instead-of-codebook-lookup that I was asking about in #5676 for its speed boost, and the insight from the IQ4_NL quantization for its perplexity improvement. This is not backwards compatible, unfortunately, because it is changing the code book. It is also not ready for merging (I've broken IQ3_XXS, I think, for example), and represents me mucking around rather than making something presentable.\nI started by noticing that some values appeared much more often than others in the codebook. Specifically, the number of occurrences of the 8 values are: 436, 344, 327, 271, 223, 185, 112, 150. This is what you would expect to see from the distribution of weights being somewhat quadratic-ish, and is the fact that IQ4_NL uses to be better than similarly-sized methods. I decided to choose values in the codebook following the same polynomial as IQ4_NL. I fit a polynomial (0.08095843xxx + .0671659xx + 11.43774359x + 0.99047392) and ended up using the values [0, 3, 6, 9, 12, 16, 19, 23, 26, 31, 35, 40, 45, 50, 56, 62]. (I used a maximum of 62 because that's what the highest value was in the old codebook.)\nFor assembling these values into a codebook, I compute the values indices by computing (codebook_index * 0xd137151) & 0x0f0f0f0f, and then map those four indices in those four bytes to the values above. That magic number is the result of me trying out a few numbers until I found one that used each value an about-equal number of times, not some search computation, so it is very possible that it would be possible to find a better one that ends up with better-spaced points. It's also possible that there's a better list of values to use, but I didn't want to overfit anything to the one model I'm working with.\nI have only done this for AVX so far. It does all of those operations vectorized, working on 32 weights at a time.\nI've tested three versions\n\nIQ3_S as originally merged into master.\nIQ3_S as it currently exists in master.\nIQ3_S with my changes on top of the originally-merged on.\n\n\n\n\n\niq3_s original (e1b8efb)\niq3_s current (67be2ce)\nmine (67be2ce)\n\n\n\n\nPerplexity mistral7b_instruct, wiki.test, +/- 0.04639\n7.3965\n7.4942\n7.1268\n\n\nSpeed (Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz)\n4.92 t/s\n4.84 t/s\n7.66 t/s\n\n\nSpeed (Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz)\n0.98 t/s\n0.90 t/s\n1.90 t/s\n\n\n\nI have not figured out why the current version of iq3_s is performing worse than the baseline on these metrics. But in any case, the speed improvement for my version is pretty big! 158% - 210% of the original speed. Plus the perplexity is better.\nSo, to summarize, this breaks backwards compatibility with existing IQ3_S-quantified files, but seems like it may be worthwhile to pursue for performance and perplexity reasons. @ikawrakow  ?\nThe code is in https://github.com/PeterReid/llama.cpp/commits/iq3_s_quant_change_cleaned/",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5866",
        "createdAt": "2024-03-04T04:31:01Z",
        "author": {
            "login": "PeterReid"
        }
    },
    {
        "title": "When are 1bit quants coming?",
        "bodyText": "So, there is a paper The Era of 1-bit LLMs:\nAll Large Language Models are in 1.58 Bits by same people who wrote BitNet: Scaling 1-bit Transformers for\nLarge Language Models in October last year. So, is anybody going to try creating a 1bit quant algorithms?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5764",
        "createdAt": "2024-02-28T11:43:54Z",
        "author": {
            "login": "superchargez"
        }
    },
    {
        "title": "Semantic Emoji Finder thanks to BERT support",
        "bodyText": "I was able to use llama.cpp (via the python bindings) to generate BERT embeddings with relatively little memory (compared to pytorch and sentence_transformers) and allow for multi-word searches in my semantic emoji finder. I think a sentence-transformers version of this web app would use nearly 1GB of RAM; now implemented it's running in less than <200MB.\nI've deployed the changes on the deployed Dash app.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5767",
        "createdAt": "2024-02-28T13:24:43Z",
        "author": {
            "login": "astrowonk"
        }
    },
    {
        "title": "Is llama.cpp designed to be consumed via CLI or C programs?",
        "bodyText": "The reason I ask is because, I often see someone ask a question and they are sent to a sample C implementation instead of being given a command line command they can use to achieve their goals.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5395",
        "createdAt": "2024-02-07T18:05:27Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "Does llama.cpp support the newest Starcoder 2 models?",
        "bodyText": "...",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5777",
        "createdAt": "2024-02-28T20:06:01Z",
        "author": {
            "login": "JohnClaw"
        }
    },
    {
        "title": "Running on an A100 node",
        "bodyText": "[OUTDATED]\nI currently have access to a node with 8x A100 and doing some experiments, decided to share some of the results.\nSlow without CUDA_VISIBLE_DEVICES=0\nNot sure why, but if I run main without setting the environment CUDA_VISIBLE_DEVICES=0, the performance is ~8 times worse compared to when setting it:\n# with CUDA_VISIBLE_DEVICES=0\n\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0\n\nllama_print_timings:        load time =   617.80 ms\nllama_print_timings:      sample time =    34.84 ms /    62 runs   (    0.56 ms per token,  1779.72 tokens per second)\nllama_print_timings: prompt eval time =   108.45 ms /     8 tokens (   13.56 ms per token,    73.77 tokens per second)\nllama_print_timings:        eval time =   425.29 ms /    61 runs   (    6.97 ms per token,   143.43 tokens per second)\nllama_print_timings:       total time =   589.89 ms\n# without CUDA_VISIBLE_DEVICES=0\n\nggml_init_cublas: found 8 CUDA devices:\n  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0\n  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0\n\nllama_print_timings:        load time =   864.04 ms\nllama_print_timings:      sample time =    35.80 ms /    62 runs   (    0.58 ms per token,  1731.75 tokens per second)\nllama_print_timings: prompt eval time =   143.87 ms /     8 tokens (   17.98 ms per token,    55.60 tokens per second)\nllama_print_timings:        eval time =  3316.43 ms /    61 runs   (   54.37 ms per token,    18.39 tokens per second)\nllama_print_timings:       total time =  3517.09 ms\nAny ideas what is causing this?\nPerformance benchmarks\n\nUsing LLAMA_CUDA_MMV_Y=2 seems to slightly improve the performance\nUsing LLAMA_CUDA_DMMV_X=64 also slightly improves the performance\nAfter #3412, using -mmq 0 (-nommq) significantly improves prefill speed\nUsing CUDA 11.7\nBuilding with CMAKE_CUDA_ARCHITECTURES=native\n\n# bench for LLaMA 7B v2\nCUDA_VISIBLE_DEVICES=0 ../scripts/run-all-perf.sh llama-7b-v2 \"f16 q8_0 q4_0 q4_1 q5_0 q5_1 q6_k q5_k q5_k_s q4_k q4_k_s q3_k q3_k_s\" \"-ngl 999 -t 1 -n 128 -p 512 -mmq 0\"\n\n\n\nmodel\nsize\nparams\nbackend\nngl\nth\nmmq\ntest\nt/s\n\n\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n5454.71 \u00b1 7.94\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n72.07 \u00b1 0.06\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4129.29 \u00b1 3.80\n\n\nllama 7B mostly Q8_0\n6.67 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n101.68 \u00b1 0.06\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4122.66 \u00b1 0.98\n\n\nllama 7B mostly Q4_0\n3.56 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n142.93 \u00b1 0.11\n\n\nllama 7B mostly Q4_1\n3.95 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4130.02 \u00b1 1.82\n\n\nllama 7B mostly Q4_1\n3.95 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n141.04 \u00b1 0.10\n\n\nllama 7B mostly Q5_0\n4.33 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4029.09 \u00b1 1.92\n\n\nllama 7B mostly Q5_0\n4.33 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n122.89 \u00b1 0.07\n\n\nllama 7B mostly Q5_1\n4.72 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4024.18 \u00b1 1.64\n\n\nllama 7B mostly Q5_1\n4.72 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n124.55 \u00b1 0.08\n\n\nllama 7B mostly Q6_K\n5.15 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4469.75 \u00b1 2.08\n\n\nllama 7B mostly Q6_K\n5.15 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n103.06 \u00b1 0.06\n\n\nllama 7B mostly Q5_K - Medium\n4.45 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4532.36 \u00b1 2.69\n\n\nllama 7B mostly Q5_K - Medium\n4.45 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n118.11 \u00b1 0.09\n\n\nllama 7B mostly Q5_K - Small\n4.33 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4529.98 \u00b1 2.48\n\n\nllama 7B mostly Q5_K - Small\n4.33 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n121.53 \u00b1 0.09\n\n\nllama 7B mostly Q4_K - Medium\n3.80 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4532.25 \u00b1 1.97\n\n\nllama 7B mostly Q4_K - Medium\n3.80 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n127.15 \u00b1 0.09\n\n\nllama 7B mostly Q4_K - Small\n3.59 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4530.28 \u00b1 1.71\n\n\nllama 7B mostly Q4_K - Small\n3.59 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n132.32 \u00b1 0.10\n\n\nllama 7B mostly Q3_K - Medium\n3.07 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4488.63 \u00b1 3.58\n\n\nllama 7B mostly Q3_K - Medium\n3.07 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n111.70 \u00b1 0.08\n\n\nllama 7B mostly Q3_K - Small\n2.75 GiB\n6.74 B\nCUDA\n999\n1\n0\npp 512\n4492.20 \u00b1 2.91\n\n\nllama 7B mostly Q3_K - Small\n2.75 GiB\n6.74 B\nCUDA\n999\n1\n0\ntg 128\n99.86 \u00b1 0.06\n\n\n\nbuild: 39ddda2 (1301)\n# bench for LLaMA 13B v2\nCUDA_VISIBLE_DEVICES=0 time ../scripts/run-all-perf.sh llama-13b-v2 \"f16 q8_0 q4_0 q4_1 q5_0 q5_1 q6_k q5_k q5_k_s q4_k q4_k_s q3_k q3_k_s\" \"-ngl 999 -t 1 -n 128 -p 512 -mmq 0\"\n\n\n\nmodel\nsize\nparams\nbackend\nngl\nth\nmmq\ntest\nt/s\n\n\n\n\nllama 13B mostly F16\n24.24 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n3473.38 \u00b1 3.15\n\n\nllama 13B mostly F16\n24.24 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n39.76 \u00b1 0.01\n\n\nllama 13B mostly Q8_0\n12.88 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2463.15 \u00b1 1.60\n\n\nllama 13B mostly Q8_0\n12.88 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n61.61 \u00b1 0.01\n\n\nllama 13B mostly Q4_0\n6.86 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2466.40 \u00b1 0.18\n\n\nllama 13B mostly Q4_0\n6.86 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n91.55 \u00b1 0.03\n\n\nllama 13B mostly Q4_1\n7.61 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2468.36 \u00b1 0.30\n\n\nllama 13B mostly Q4_1\n7.61 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n90.44 \u00b1 0.03\n\n\nllama 13B mostly Q5_0\n8.36 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2395.81 \u00b1 0.31\n\n\nllama 13B mostly Q5_0\n8.36 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n79.22 \u00b1 0.02\n\n\nllama 13B mostly Q5_1\n9.10 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2393.12 \u00b1 1.20\n\n\nllama 13B mostly Q5_1\n9.10 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n79.45 \u00b1 0.01\n\n\nllama 13B mostly Q6_K\n9.95 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2728.76 \u00b1 0.79\n\n\nllama 13B mostly Q6_K\n9.95 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n65.74 \u00b1 0.01\n\n\nllama 13B mostly Q5_K - Medium\n8.60 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2754.25 \u00b1 1.30\n\n\nllama 13B mostly Q5_K - Medium\n8.60 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n76.23 \u00b1 0.02\n\n\nllama 13B mostly Q5_K - Small\n8.36 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2753.87 \u00b1 1.32\n\n\nllama 13B mostly Q5_K - Small\n8.36 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n78.50 \u00b1 0.03\n\n\nllama 13B mostly Q4_K - Medium\n7.33 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2755.60 \u00b1 1.31\n\n\nllama 13B mostly Q4_K - Medium\n7.33 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n81.86 \u00b1 0.03\n\n\nllama 13B mostly Q4_K - Small\n6.90 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2755.07 \u00b1 1.84\n\n\nllama 13B mostly Q4_K - Small\n6.90 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n85.59 \u00b1 0.02\n\n\nllama 13B mostly Q3_K - Medium\n5.90 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2748.00 \u00b1 1.39\n\n\nllama 13B mostly Q3_K - Medium\n5.90 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n68.66 \u00b1 0.02\n\n\nllama 13B mostly Q3_K - Small\n5.27 GiB\n13.02 B\nCUDA\n999\n1\n0\npp 512\n2748.35 \u00b1 1.63\n\n\nllama 13B mostly Q3_K - Small\n5.27 GiB\n13.02 B\nCUDA\n999\n1\n0\ntg 128\n59.50 \u00b1 0.01\n\n\n\nbuild: 39ddda2 (1301)\n# bench for LLaMA 7B at different batch sizes\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./llama-bench -m models/openllama-7b/ggml-model-f16.gguf -p 1,2,4,8,10,16,32,60,64,128,256,512,1024,2048 -ngl 100 -mmq 0 -t 1 -n 0\n\n\n\nmodel\nsize\nparams\nbackend\nngl\nthreads\nmmq\ntest\nt/s\n\n\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 1\n41.90 \u00b1 0.97\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 2\n50.21 \u00b1 0.03\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 4\n106.82 \u00b1 0.05\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 8\n212.72 \u00b1 1.26\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 10\n236.05 \u00b1 1.16\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 16\n414.91 \u00b1 2.69\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 32\n750.36 \u00b1 6.43\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 60\n1303.27 \u00b1 10.70\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 64\n1386.89 \u00b1 11.62\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 128\n2597.60 \u00b1 26.66\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 256\n4100.16 \u00b1 55.35\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 512\n5235.21 \u00b1 10.50\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 1024\n4857.50 \u00b1 3.01\n\n\nllama 7B mostly F16\n12.55 GiB\n6.74 B\nCUDA\n100\n1\n0\npp 2048\n3505.47 \u00b1 1.27\n\n\n\nbuild: 48edda3 (1330)\nFor reference, here is the same test on M2 Ultra\n\n\n\n\nmodel\nsize\nparams\nbackend\nngl\nthreads\ntest\nt/s\n\n\n\n\nllama2 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1490.51 \u00b1 1.33\n\n\nllama2 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1326.80 \u00b1 0.68\n\n\nllama2 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1355.31 \u00b1 0.77\n\n\nllama2 7B mostly Q4_1\n3.95 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1352.15 \u00b1 0.85\n\n\nllama2 7B mostly Q6_K\n5.15 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1106.62 \u00b1 0.19\n\n\nllama2 7B mostly Q5_K - Medium\n4.45 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1103.32 \u00b1 0.76\n\n\nllama2 7B mostly Q5_K - Small\n4.33 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1102.13 \u00b1 0.44\n\n\nllama2 7B mostly Q4_K - Medium\n3.80 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1169.10 \u00b1 0.37\n\n\nllama2 7B mostly Q4_K - Small\n3.59 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1178.26 \u00b1 0.44\n\n\nllama2 7B mostly Q3_K - Medium\n3.07 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1142.14 \u00b1 0.56\n\n\nllama2 7B mostly Q3_K - Small\n2.75 GiB\n6.74 B\nMetal\n999\n4\npp 512\n1119.44 \u00b1 0.24\n\n\nllama2 7B mostly F16\n12.55 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n40.84 \u00b1 0.05\n\n\nllama2 7B mostly Q8_0\n6.67 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n64.54 \u00b1 0.06\n\n\nllama2 7B mostly Q4_0\n3.56 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n90.69 \u00b1 0.17\n\n\nllama2 7B mostly Q4_1\n3.95 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n85.98 \u00b1 0.10\n\n\nllama2 7B mostly Q6_K\n5.15 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n71.38 \u00b1 0.08\n\n\nllama2 7B mostly Q5_K - Medium\n4.45 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n72.57 \u00b1 0.07\n\n\nllama2 7B mostly Q5_K - Small\n4.33 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n74.01 \u00b1 0.06\n\n\nllama2 7B mostly Q4_K - Medium\n3.80 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n83.57 \u00b1 0.15\n\n\nllama2 7B mostly Q4_K - Small\n3.59 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n86.78 \u00b1 0.18\n\n\nllama2 7B mostly Q3_K - Medium\n3.07 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n83.43 \u00b1 0.10\n\n\nllama2 7B mostly Q3_K - Small\n2.75 GiB\n6.74 B\nMetal\n999\n4\ntg 128\n85.22 \u00b1 0.10\n\n\n\nbuild: 99115f3 (1273)\n\n\n\nmodel\nsize\nparams\nbackend\nngl\nthreads\ntest\nt/s\n\n\n\n\nllama2 13B mostly F16\n24.24 GiB\n13.02 B\nMetal\n999\n4\npp 512\n790.32 \u00b1 0.28\n\n\nllama2 13B mostly Q8_0\n12.88 GiB\n13.02 B\nMetal\n999\n4\npp 512\n708.47 \u00b1 0.29\n\n\nllama2 13B mostly Q4_0\n6.86 GiB\n13.02 B\nMetal\n999\n4\npp 512\n722.70 \u00b1 0.12\n\n\nllama2 13B mostly Q4_1\n7.61 GiB\n13.02 B\nMetal\n999\n4\npp 512\n721.37 \u00b1 0.14\n\n\nllama2 13B mostly Q6_K\n9.95 GiB\n13.02 B\nMetal\n999\n4\npp 512\n589.94 \u00b1 0.17\n\n\nllama2 13B mostly Q5_K - Medium\n8.60 GiB\n13.02 B\nMetal\n999\n4\npp 512\n582.08 \u00b1 0.22\n\n\nllama2 13B mostly Q4_K - Medium\n7.33 GiB\n13.02 B\nMetal\n999\n4\npp 512\n622.52 \u00b1 0.17\n\n\nllama2 13B mostly Q3_K - Medium\n5.90 GiB\n13.02 B\nMetal\n999\n4\npp 512\n607.34 \u00b1 0.19\n\n\nllama2 13B mostly F16\n24.24 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n22.60 \u00b1 0.03\n\n\nllama2 13B mostly Q8_0\n12.88 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n37.69 \u00b1 0.01\n\n\nllama2 13B mostly Q4_0\n6.86 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n56.66 \u00b1 0.04\n\n\nllama2 13B mostly Q4_1\n7.61 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n52.65 \u00b1 0.02\n\n\nllama2 13B mostly Q6_K\n9.95 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n42.73 \u00b1 0.01\n\n\nllama2 13B mostly Q5_K - Medium\n8.60 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n44.42 \u00b1 0.02\n\n\nllama2 13B mostly Q4_K - Medium\n7.33 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n51.30 \u00b1 0.05\n\n\nllama2 13B mostly Q3_K - Medium\n5.90 GiB\n13.02 B\nMetal\n999\n4\ntg 128\n51.21 \u00b1 0.02\n\n\n\nbuild: 99115f3 (1273)\nreal\t3m2.119s\nuser\t0m8.147s\nsys\t0m8.614s",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3359",
        "createdAt": "2023-09-27T11:17:09Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Camelidae : Sparse Mixture of Experts (8x34b in less than 80gb)",
        "bodyText": "Anybody seen this?\nhttps://github.com/wuhy68/Parameter-Efficient-MoE",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4955",
        "createdAt": "2024-01-15T08:23:04Z",
        "author": {
            "login": "logikstate"
        }
    },
    {
        "title": "prompt is too long",
        "bodyText": "What is the best way to handle this when summarizing very long inputs?\nIs there a way to run llama.cpp just to get the token count?\nIf so I can chunk into batches, but pointless if the tooling does this already and there's just a flag I haven't found yet.\nCommand in python\nwith subprocess.Popen(\n            [\n                LLAMA_CPP_EXECUTABLE_PATH,\n                \"-m\",\n                LLAMA_CPP_MODEL_PATH,\n                \"-t\",\n                \"8\",\n                \"--ctx-size\",\n                \"4096\",\n                \"-p\",\n                prompt,\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            bufsize=1, \n        )\nFeel like pushing --ctx-size over 4096 will take too long and be not beneficial anyways.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5786",
        "createdAt": "2024-02-28T23:41:08Z",
        "author": {
            "login": "atopheim"
        }
    },
    {
        "title": "llama.cpp port to pure Golang",
        "bodyText": "I've released the framework I have been building for the last month.\nIt's the like 1:1 port of llama.cpp to pure Go, where the language really shines due to easier multi-threading model and channels.\nNot so performant and memory savvy as original one but I hope it would be useful for those who'd like to grok with ChatGPT-like projects with Go:)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/919",
        "createdAt": "2023-04-12T16:28:49Z",
        "author": {
            "login": "gotzmann"
        }
    },
    {
        "title": "gemma-7b-it poor results?",
        "bodyText": "It seems I get poor results from Gemma-7B-it 4 bit quantized model. It seems behaving very differently from other popular LLMs such as llama2-7b or mistral-7b. Almost all other models work well with default settings: I only need to do \"main -m MODEL -i --color\" to get a good interactive chat going. But if I do the same with gemma-7b, it is both too long winding, and ignore the latest prompt and keeps on rambling based on the first prompt. Do you have similar experience?\nWhat are your command line parameters while using llama.cpp for gemma-7b?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5751",
        "createdAt": "2024-02-27T16:32:42Z",
        "author": {
            "login": "zbruceli"
        }
    },
    {
        "title": "Why build cgraph for every single loop?",
        "bodyText": "I read source code recently, I got no idea why it will build cgraph for each loop, and split cgraph for every loop too. Can it be done that reuse cgraph or graph split by replace leaf tensors?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5749",
        "createdAt": "2024-02-27T11:16:54Z",
        "author": {
            "login": "hipudding"
        }
    },
    {
        "title": "LLava Server",
        "bodyText": "Hello,\nI have deployed a llava v1.5 server using the following command:\n./server -c 4096 -ngl 100 -m ../ggml-model-q4_k.gguf --host ux-nic-8 --port 8013 --mmproj ../mmproj-model-f16.gguf\nI am now trying to make an API call, like i normally do with other LLMs but I am having trouble formatting the prompt since it is a combination of image and text.\nI have searched about this, but no luck. Is there anybody that can help me understand how to format both the image and the text in the prompt to feed it to the llm as following?\napi_url=\"API URL\"\nheaders = {\"Content-Type\": \"application/json\"}\n\npload = {\n  'prompt': ***PROMPT***,\n  'temperature': 0,\n  'top_k': 40,\n  'top_p': 0.95,\n  'n_predict': 300,\n  'stream': False,\n  'repeat_penalty': 1.0,\n  'ignore_eos': False}\nrpload=json.dumps(pload)\n\nresponse = requests.post(api_url, headers=headers, data=rpload)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5589",
        "createdAt": "2024-02-19T14:12:37Z",
        "author": {
            "login": "jcgeo9"
        }
    },
    {
        "title": "Large batch sizes memory leak in llama.cpp?",
        "bodyText": "I notice that the larger the batch size, the more memory it requires to do consecutive batches.\nThis confuses me. Shouldn't the earlier batches not impact the future batches? As in, the memory from the earlier batches should be freed, and it shouldn't escalate beyond the first?\nI OOM on the third batch when I set the bs to 2048 tokens:\n\nIs this related to / caused by the fact that Flash Attention is not yet supported?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5696",
        "createdAt": "2024-02-24T03:25:53Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "What does the prompt context mean?",
        "bodyText": "Hello, I would like to understand what does the prompt context mean:\n\n-c N, --ctx-size N: Set the size of the prompt context. The default is 512, but LLaMA models were built with a context of 2048, which will provide better results for longer input/inference.\n\nQuestions:\n\nDoes it mean when I give the program a prompt, it will truncate it to 512 tokens?\nDoes the prompt context define the batch size in transformer processing? the prompt itself can be as long as I want but it will be chunked into batches of size 512 tokens?\nBeing an auto regressive model, is the prompt getting continuously appended to as the chat progresses with Q&A? i.e., when user types in a new question - the prompt includes not only that question but the entire chat history preceding that question? Is there any limit to how big it can get before it started getting truncated?\n\nsorry if the questions are basic for experts. I am trying to learn the field.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1838",
        "createdAt": "2023-06-13T16:00:26Z",
        "author": {
            "login": "siddhsql"
        }
    },
    {
        "title": "Cannot install on Windows. Please help.",
        "bodyText": "Dear Gurus\nPlease help.\ni did everything listed on the project page:\n\nused GIT to clone rep\nmade a folder for w64devkit-fortran-1.21.0.zip and unzipped archive content there\nadded pathtothesaidfolder\\bin to PATH= environmental variables. rebooted\nfrom w64devkit commandline window changed directory to llama.cpp one (starting from cmd with Admin rights does not help)\nissuing MAKE or make LLAMA_CUBLAS=1 (or using SET LLAMA_CUBLAS=1) - everything results in one and the same error\n\nMakefile:604: *** I ERROR: For CUDA versions < 11.7 a target CUDA architecture must be explicitly provided via CUDA_DOCKER_ARCH.  Stop.\nWindows 10\ni was trying to install in Ooba conda environment (Ooba works fine for me)\nCUDA Toolkit is installed (nvcc --version)\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:09:35_Pacific_Daylight_Time_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\nEDIT 01: i have added SET CUDA_DOCKER_ARCH=all\nnow during build the following error happens:\nnvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -Wno-deprecated-gpu-targets -arch=all -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -DNDEBUG -D_WIN32_WINNT=0x602 -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -IF:\\TEXTGEN\\text-generation-webui\\installer_files\\env/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Xassembler -muse-unaligned-vector-move  -Wno-array-bounds -Wno-pedantic\" -c ggml-cuda.cu -o ggml-cuda.o\nnvcc fatal   : Cannot find compiler 'cl.exe' in PATH\nmake: *** [Makefile:451: ggml-cuda.o] Error 1\nEDIT 02: after adding Desktop development with C++ to  Visual Studio Community 2022 17.9.1 (i already had Python there installed)\nand adding C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\bin\\Hostx64\\x64 to PATH to run C compiler (cl.exe) this results in the following error:\nnvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -Wno-deprecated-gpu-targets -arch=all -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -DNDEBUG -D_WIN32_WINNT=0x602 -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -IF:\\TEXTGEN\\text-generation-webui\\installer_files\\env/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Xassembler -muse-unaligned-vector-move  -Wno-array-bounds -Wno-pedantic\" -c ggml-cuda.cu -o ggml-cuda.o\nnvcc warning : The -std=c++11 flag is not supported with the configured host compiler. Flag will be ignored.\nggml-cuda.cu\ncl : Command line error D8021 : invalid numeric argument '/Wextra'\nmake: *** [Makefile:451: ggml-cuda.o] Error 2\nEDIT 03:\ninstalling gcc C++ had not helped: incompatible compiler OS\ninstalling CLang has not helper either\nafter a lot of googling and experimenting cmake .. -DLLAMA_CUBLAS=ON finally worked:\n\nhave Visual Studio installed (i used 2022 version). install Desktop C++, in Build Tools install CMake\nadd 3 paths to PATH= environmental variables\na) path to MS Visual C compiler (CL.EXE) C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\bin\\Hostx64\\x64\nb) path to CMAKE C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\nc) path to CUDA Toolkit  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin\nmanually copy the content of C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\extras\\visual_studio_integration\\MSBuildExtensions\nto: C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n\nWhy this is not in build instructions? Is it too obvious?\nthere were a lot of warnings during compilation\nnow binaries are in X:\\TEXTGEN\\text-generation-webui\\LLAMA.CPP\\build\\bin\\Release\nshould i move them to another folder inside LLAMA.CPP ?\n(quantization example suggests binaries are under the root of LLAMA.CPP, doesn't it?)\nwhat other settings should i change?\nhow do i verify compiled software working as intended?\nThe biggest trouble:\nbuilt locally: quantize.exe 11,532,800 bytes\npre-built  quantize.exe 59,904 bytes\nfrom llama-b2254-bin-win-cublas-cu12.2.0-x64 release\nTHE DIFFERENCE IN SIZE IS TREMENDOUS\n(though i quantized TinyLlama-1.1B-Chat-v1.0 downloaded from HF with both instances of quantize.exe into Q4_K_M\njust for testing, and resulting GGUF files appeared to be identical)\nstill i have no explanation of this 200x difference in size. help!\ni have no idea what i did wrong. please help!\nAny hints are helpful. i am not a programmer, and these models are definitely not plug-and-play for me.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5704",
        "createdAt": "2024-02-24T20:45:34Z",
        "author": {
            "login": "Dino-Zavrus"
        }
    },
    {
        "title": "Error in compiling on Mac M2",
        "bodyText": "I'm compiling on Mac with M2 chip. I have the following error while running make in the root folder of this project.\nerror: Could not find compiler \"gcc-5\" in PATH\nexpr: syntax error\nexpr: syntax error\nI llama.cpp build info: \nI UNAME_S:   Darwin\nI UNAME_P:   arm\nI UNAME_M:   arm64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -Wdouble-promotion \nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread   -Wno-array-bounds\nI NVCCFLAGS: -O3 \nI LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \nccache: error: Could not find compiler \"gcc-5\" in PATH\nI CC:        \nccache: error: Could not find compiler \"g++-5\" in PATH\nI CXX:       \n\n/opt/homebrew/bin/ccache gcc-5  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -Wdouble-promotion    -c ggml.c -o ggml.o\nccache: error: Could not find compiler \"gcc-5\" in PATH\nmake: *** [ggml.o] Error 1\n\nI have the gcc version:\n$ gcc --version                                                                                                               \nApple clang version 15.0.0 (clang-1500.1.0.2.5)\nTarget: arm64-apple-darwin23.2.0\n\nAny idea?\nThanks for help.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5433",
        "createdAt": "2024-02-09T21:08:43Z",
        "author": {
            "login": "jieyu11"
        }
    },
    {
        "title": "Documentation contributions",
        "bodyText": "Hey there,\nI am trying to understand various concepts implemented in this repository, but find myself crawling through c++ code for most of the answers.\nI get that llama.cpp has been rapidly growing, and that documentation is not the highest priority.\nWhile trying to understand the codebase, I will gladly add documentation where it is missing.\nI see several attempted strategies for documentation:\n\ncode documentation in the header and cpp files.\nreadme.md in most of the examples\na docs folder in the repository root (which seems mostly unused)\n\nWhen trying to add more documentation, which place should we focus on? A little guidance would be helpful.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5683",
        "createdAt": "2024-02-23T14:30:57Z",
        "author": {
            "login": "vriesdemichael"
        }
    },
    {
        "title": "Chat history/memory",
        "bodyText": "Does Llama.cpp have some built-in way to handle chat history in a way that the model can refer back to information from previous messages? Without simply sending the chat history as part of the prompt, I mean. In a similar way ChatGPT seems to be able to. I've tried using the --prompt-cache and --prompt-cache-all options but it seems to overrides the the cache file with each new prompt. I am not sure these even do what I need.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5504",
        "createdAt": "2024-02-15T11:30:59Z",
        "author": {
            "login": "gurbindersingh"
        }
    },
    {
        "title": "Which quantized gguf size infers at INT8?",
        "bodyText": "I see references to GGUF_TYPE_INT8 in ggml.c. Is there any particular gguf size that is guaranteed to be in that or does it have to be generated a certain way?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5662",
        "createdAt": "2024-02-22T14:47:49Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "Short context of llava?",
        "bodyText": "I have a question about the context using llava 1.6. Does the screenshot means that the total context of it is 2048 token? It seems that the output even not finished before using out the context.\nHere is the command:\n./llava-cli -m ~/ggml-yi-34b-f16-q_5_k.gguf \\\n  --mmproj ~/mmproj-llava-34b-f16-q6_k.gguf \\\n  --image ~/00263.jpg \\\n  -e \\\n  -p '<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\nProvide a full description.<|im_end|><|im_start|>assistant\\n'",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5656",
        "createdAt": "2024-02-22T04:02:24Z",
        "author": {
            "login": "arkohut"
        }
    },
    {
        "title": "Why do LLMs require tokenizers?",
        "bodyText": "Why do LLMs require tokenizers? Can't we simply utilize the entire UTF-8 range as the vocabulary? Why do we need BPE, SentencePiece?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5654",
        "createdAt": "2024-02-21T23:48:40Z",
        "author": {
            "login": "bobqianic"
        }
    },
    {
        "title": "Is PowerInfer the fastest or can it still be faster?",
        "bodyText": "Is PowerInfer the fastest or can it still be faster?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5339",
        "createdAt": "2024-02-05T10:15:37Z",
        "author": {
            "login": "ouvaa"
        }
    },
    {
        "title": "Performance 3x better when use performance core only on Intel gen 12th cpu",
        "bodyText": "I found by restrict threads and cores to performance cores only on Intel gen 12th processor, performance is much better than default.\nMy process is Intel core i7 12700H, this processor has 6 performance cores and 8 efficient cores.   When use numactl to bind threads to performance core only, the performance is better than use all the cores.   I tried 7B and 65B dataset with q4_0 quantization, both configuration show performance improvement.\n7B\nUse all cores:\n./main -m ./models/7B/ggml-model-q4_0.bin -n 32 -p \"Hiking is\"\nllama_print_timings:        eval time = 14941.65 ms /    31 runs   (  481.99 ms per run)\nUse performance core only:\nnumactl -C 0-5 ./main -m ./models/7B/ggml-model-q4_0.bin -n 32 -p \"Hiking is\" -t 6\nllama_print_timings:        eval time =  6256.60 ms /    31 runs   (  201.83 ms per run)\n2.4x performance improvement for 7B 4-bit model.\n65B model\n./main -m ./models/65B/ggml-model-q4_0.bin -n 32 -p \"Hiking is\"\nllama_print_timings:        eval time = 126128.34 ms /    31 runs   ( 4068.66 ms per run)\nnumactl -C 0-5 ./main -m ./models/65B/ggml-model-q4_0.bin -n 32 -p \"Hiking is\" -t 6\nllama_print_timings:        eval time = 42069.76 ms /    31 runs   ( 1357.09 ms per run)\n3x performance improvement to 65B 4-bit model.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/572",
        "createdAt": "2023-03-28T12:59:02Z",
        "author": {
            "login": "delock"
        }
    },
    {
        "title": "Incoming backends: Vulkan, Kompute, SYCL",
        "bodyText": "ref:\n\nVulkan: #2059 (@0cc4m)\nKompute: #4456 (@cebtenzzre)\nSYCL: #2690 (@abhilash1910)\n\nThere are 3 new backends that are about to be merged into llama.cpp. The tentative plan is do this over the weekend. Due to the large amount of code that is about to be merged, I'm creating this discussion for a quick communication channel between the maintainers in case problems arise.\nThe main goal after merging the backends is to make the CI green which would give some level of confidence that the existing stuff has not been broken. Even if the new backends don't function completely as expected, this would be acceptable as the idea is to improve over these with time. However, we want the CPU, CUDA and Metal backends to remain stable.\nI'm thinking to do the merges all at once (in a batch) and sync everything back to the ggml and whisper.cpp repos.\nIf you have any general comments / questions we can discuss here. I will keep high attention to the discussion until we finalize the merges. Will also put it in the readme for awareness. We can discuss code specifics in the respective PRs as usual and keep this discussion focused on high-priority stuff (if needed)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5138",
        "createdAt": "2024-01-26T08:50:49Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "How to use 7-bit integer quantization?",
        "bodyText": "\u0674",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5526",
        "createdAt": "2024-02-16T11:21:14Z",
        "author": {
            "login": "GermanAizek"
        }
    },
    {
        "title": "how to convert qlora trained model into a GGUF model?",
        "bodyText": "I am having issues with converting qlora trained model(4bit) into a GGUF model using this script (https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py) mainly because qlora trained model does not have a config.json. it has a adapter_config.json.\non running this command\n!python3 llama.cpp/examples/make-ggml.py resultsalpaca/checkpoint200 --model_type llama --outname alpaca-GGUF --quants F32\nthis is the error\nTraceback (most recent call last): File \"llama.cpp/examples/make-ggml.py\", line 98, in <module> main(args.model, args.model_type, args.outname, args.outdir, args.quants, args.keep_fp16) File \"llama.cpp/examples/make-ggml.py\", line 60, in main raise Exception(f\"Could not find config.json in {model}\") Exception: Could not find config.json in resultsalpaca/checkpoint200\ni manually loaded the config.json of the llama 13b model which i'm using as a base model into the folder then the below error came up.\nBuilding llama.cpp make: *** No rule to make target 'quantize'.  Stop. Traceback (most recent call last): File \"llama.cpp/examples/make-ggml.py\", line 98, in <module> main(args.model, args.model_type, args.outname, args.outdir, args.quants, args.keep_fp16) File \"llama.cpp/examples/make-ggml.py\", line 65, in main subprocess.run(f\"cd .. && make quantize\", shell=True, check=True) File \"/usr/lib/python3.8/subprocess.py\", line 516, in run raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command 'cd .. && make quantize' returned non-zero exit status 2.\nis there a way to do it without config.json file? if not then how to correctly generate config.json file of a qlora trained model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3489",
        "createdAt": "2023-10-05T17:10:01Z",
        "author": {
            "login": "Kainat-R"
        }
    },
    {
        "title": "Fine tuning batch parameter increases training time?",
        "bodyText": "I am fine tuning a mistral model using finetune and I am perplexed why increasing batch size for training greatly increases the time to train. This seems at odds with what I have understood for a long time.  Can someone help explain?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5313",
        "createdAt": "2024-02-03T19:27:31Z",
        "author": {
            "login": "Tachyon5"
        }
    },
    {
        "title": "is it possible to add logprob entry to server?",
        "bodyText": "Server doesn't have a logprob function such as https://textsynth.com/documentation.html#logprob.  This function can be useful in evaluation harnesses such as MMLU  to find what the model would pick for a multiple choice questions or other types of yes/no or limited choice questions.  I don't know enough about the codebase now if it would be easy or hard to add this but it could be useful if possible.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3795",
        "createdAt": "2023-10-26T13:01:12Z",
        "author": null
    },
    {
        "title": "Difference between GGMLFType",
        "bodyText": "For ggml, there are many types. Just like Q{i} means to {i} precision.\nI would like to know the meaning of the last _0, _1, _K_M and so on.\nIt would be great to share some insights and experience as well.\nclass GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3\n    MOSTLY_Q4_1_SOME_F16 = 4\n    MOSTLY_Q8_0          = 7\n    MOSTLY_Q5_0          = 8\n    MOSTLY_Q5_1          = 9\n    MOSTLY_Q2_K          = 10\n    MOSTLY_Q3_K_S        = 11\n    MOSTLY_Q3_K_M        = 12\n    MOSTLY_Q3_K_L        = 13\n    MOSTLY_Q4_K_S        = 14\n    MOSTLY_Q4_K_M        = 15\n    MOSTLY_Q5_K_S        = 16\n    MOSTLY_Q5_K_M        = 17\n    MOSTLY_Q6_K          = 18",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5471",
        "createdAt": "2024-02-13T03:41:50Z",
        "author": {
            "login": "Starlento"
        }
    },
    {
        "title": "error PermissionError: [Errno 13] Permission denied: 'models/7B//ggml-model-f16.bin' with the command \"python3 convert-pth-to-ggml.py models/7B/1\"",
        "bodyText": "Hi everyone,\nI am near the end of the procedure, at the time of making this command,\npython3 convert-pth-to-ggml.py models/7B/1\nI get this error:\nTraceback (most recent call last):\n  File \"/opt/openai/llama.cpp/convert-pth-to-ggml.py\", line 181, in <module>\n    hand()\n  File \"/opt/openai/llama.cpp/convert-pth-to-ggml.py\", line 171, in main\n    with open(fname_out, \"wb\") as fout:\nPermissionError: [Errno 13] Permission denied: 'models/7B//ggml-model-f16.bin'\n\nThe content of my model folder, contains the 7B, Tokenizer.model that I got from the internet, there is only the tokenizer_checklist.chk where I copied and pasted the content on a site.\nI'm struggling a bit and I thank you for your answers if you have already encountered this problem.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/556",
        "createdAt": "2023-03-27T14:55:51Z",
        "author": {
            "login": "MarcGach"
        }
    },
    {
        "title": "Support for Embedding Models",
        "bodyText": "I want to mainly throw my support for wanting a solid embedding model in GGML.  Even without typical LLM capabilities, I think there would be a TON of use for embedding documents of various types and doing similarity lookup, even if the return was simply the passage matched.  So much better than string search.\nI have found this fork, which appears to be abandoned but works\nhttps://github.com/skeskinen/bert.cpp\nNote for Windows users:  I needed to make two modifications to get it to work:\nPython sample_client.py - change\nwith open(os.path.join(os.path.dirname(__file__), txt_file), 'r') as f:\nto this\nwith open(os.path.join(os.path.dirname(__file__), txt_file), 'r', encoding=\"utf-8\") as f:\nAnd in the windows server.cpp, change\nssize_t bytes_received = read(socket, buffer, sizeof(buffer));\nto\nssize_t bytes_received = recv(socket, buffer, sizeof(buffer), 0);\nAnd it seems to at least run the example.\nI have also found this discussion from a month ago\n#3667\nWhich said it seemed close.\nBert is better than nothing, but BGE is one of the top retreival embeddings on the huggingface embedding leaderboards:\nhttps://huggingface.co/spaces/mteb/leaderboard\nHopefully this isn't too far off, as I'd love to just drop this into an app I'm building.\nThanks for all the effort on GGML - it is an amazing offering btw.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4117",
        "createdAt": "2023-11-17T22:10:58Z",
        "author": {
            "login": "SpaceCowboy850"
        }
    },
    {
        "title": "Are deterministic tokens skipped when using grammar constraints?",
        "bodyText": "Let's say I have a simple JSON grammar where I want to generate a simple structure like this\ninterface Car {\n  \"brand\": \"Toyota\" | \"Mercedes\" | \"Mazda\" | \"Audi\" | \"Volkswagen\"\n  \"build-year\": number\n}\nInvoking the LLM to predict one token is expensive. Many tokens are deterministic given the current state of the output. For instance, at the beginning the grammar HAS to start with\n{ \"brand\": \"\n\n(Note, depending on how restrictively you define your JSON grammar, there is more than one valid output at the beginning depending on if you allow infinite whitespace, newlines etc)\nNext, depending on the first token it generates, it can easily end up in a situation where a lot of tokens are again deterministic\n{ \"brand\": \"M\n\nNow it can either go into the \"Mercedes\" branch or the \"Mazda\" branch, this is the first point the LLM needs to actually be invoked. After that token we can go all the way to\n{ \"brand\": \"Mazda\", \"build-year\": \n\nBefore it needs to be invoked again.\nDoes this kind of optimization exist in llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5455",
        "createdAt": "2024-02-11T21:53:05Z",
        "author": {
            "login": "Azeirah"
        }
    },
    {
        "title": "SGLang & LookaheadDecoding",
        "bodyText": "Hi,\nDo you plan to implement these two optimization to the decoding process?\nI saw that LookaheadDecoding is in an exemple but if there is way to generalize the process it could be greate for optimized decoding...\nLink to the codes:\nSGLang\nLookaheadDecoding\nBest regards",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5454",
        "createdAt": "2024-02-11T21:08:43Z",
        "author": {
            "login": "ostix360"
        }
    },
    {
        "title": "Saving state",
        "bodyText": "Hi!\nIn llama_copy_state_data i see this:\n// v is not contiguous, copy row by row\ntmp_buf.resize(elt_size*kv_head);\nfor (int ir = 0; ir < (int) n_embd_v_gqa; ++ir) {\n    ggml_backend_tensor_get(kv_self.v_l[il], tmp_buf.data(), ir*elt_size*n_ctx, tmp_buf.size());\n    data_ctx->write(tmp_buf.data(), tmp_buf.size());\n}\n\nCan it be converted to something like this to reduce time to get V cache?\ntmp_buf.resize(n_embd_v_gqa*elt_size*n_ctx);\nggml_backend_tensor_get(kv_self.v_l[il], tmp_buf.data(), 0, tmp_buf.size());\ndata_ctx->write(tmp_buf.data(), tmp_buf.size());",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5440",
        "createdAt": "2024-02-10T17:10:15Z",
        "author": {
            "login": "HarewVlad"
        }
    },
    {
        "title": "Where is the cache from finetune located ?",
        "bodyText": "Hi.  I've been using finetune but Ctrl-C'd a few times.  The prompt tells me that \"train data seems to have changed.\" which is probably due to my many aborted runs, so I'm suspecting that a cache is created somewhere.  Running this command inside the repo, I thought that git status would give me the locations of uncomitted files to the current repo but nothing is indicated.  Maybe some external cache in the filesystem?  I'm using Ubuntu.\nRegards.\nMarc.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5449",
        "createdAt": "2024-02-11T02:36:24Z",
        "author": {
            "login": "marcthenarc"
        }
    },
    {
        "title": "Why llama_build_graph() function is called for every token generation?",
        "bodyText": "I am trying to understand the flow for forward pass / graph compute / inference .\nPlease correct me if my understanding is wrong.\nThe flow for inference starts after model and tensors are parsed from file.\nllama_decode() -> llama_decode_internal() -> llama_build_graph() (creates llm structure) -> llm.build_llama() -> ggml_build_forward_expand() -> ggml_build_forward_impl() -> ggml_visit_parents() -> llm.free()\nI think llama_decode() is called for every new token generation in decoding stage. This will call llama_build_graph() and all the nodes and tensor mapping to each node will happen every time for a new token generation.\n\nIs my understanding correct?\nWill it not add to the latency for new token. generation as this chain is called again and again?\nCan we store the mapping of tensors and nodes in a \"state\" and just called \"execute()\" on nodes in-order?\n\n@ggerganov",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4617",
        "createdAt": "2023-12-24T08:14:04Z",
        "author": {
            "login": "Nick-infinity"
        }
    },
    {
        "title": "is llama.cpp now faster than exllamav2? i just ran a sample and it seems so. is this how it is now?",
        "bodyText": "llama.cpp now the fastest way to run llms?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5397",
        "createdAt": "2024-02-07T20:41:36Z",
        "author": {
            "login": "sprappcom"
        }
    },
    {
        "title": "Is it possible to calculate CPU buffer size before loading model?",
        "bodyText": "I have 3 GPUs, two 24gb and 1 12gb for a total of 60gb of VRAM.  I'm trying goliath with a size of 66gb, so I figure, say 58gb for VRAM, 10-12 at most for CPU.  Running with a context size of 512.  How come CPU buffer size is still 67gb after almost filling up the VRAMs?\n66G Feb  9 07:58 goliath-120b.Q4_K_M.gguf\nllm_load_tensors: offloading 110 repeating layers to GPU\nllm_load_tensors: offloaded 110/138 layers to GPU\nllm_load_tensors:        CPU buffer size = 67364.36 MiB\nllm_load_tensors:      CUDA0 buffer size = 10584.06 MiB\nllm_load_tensors:      CUDA1 buffer size = 20978.38 MiB\nllm_load_tensors:      CUDA2 buffer size = 21809.56 MiB",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5432",
        "createdAt": "2024-02-09T20:02:36Z",
        "author": {
            "login": "segmond"
        }
    },
    {
        "title": "Trying to convert intfloat/e5-mistral-7b-instruct to GGUF",
        "bodyText": "Could not find a GGUF version of https://huggingface.co/intfloat/e5-mistral-7b-instruct\nI am trying to convert it to GGUF using convert.py but getting a KeyError: 'embed_tokens.weight'\nAny suggestions?\nThanks,\nAsh\npython convert.py models/e5-mistral-7b-instruct/\nLoading model file models\\e5-mistral-7b-instruct\\model-00001-of-00002.safetensors\nLoading model file models\\e5-mistral-7b-instruct\\model-00001-of-00002.safetensors\nLoading model file models\\e5-mistral-7b-instruct\\model-00002-of-00002.safetensors\nTraceback (most recent call last):\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 1295, in <module>\n    main()\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 1223, in main\n    model_plus = load_some_model(args.model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 1144, in load_some_model\n    model_plus = merge_multifile_models(models_plus)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 637, in merge_multifile_models\n    model = merge_sharded([mp.model for mp in models_plus])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 616, in merge_sharded\n    return {name: convert(name) for name in names}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 616, in <dictcomp>\n    return {name: convert(name) for name in names}\n                  ^^^^^^^^^^^^^\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 591, in convert\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\AI\\llama.cpp\\convert.py\", line 591, in <listcomp>\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n                                      ~~~~~^^^^^^\nKeyError: 'embed_tokens.weight'",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4786",
        "createdAt": "2024-01-05T17:53:59Z",
        "author": {
            "login": "AshD"
        }
    },
    {
        "title": "Has anyone gotten llama.cpp working within an Expo app?",
        "bodyText": "Hi there, has anyone gotten llama.cpp working within an Expo app? Thank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5430",
        "createdAt": "2024-02-09T17:41:02Z",
        "author": {
            "login": "hammer-ai"
        }
    },
    {
        "title": "Help w/ understanding why an old (hacked together) build of koboldcpp has much faster Mixtral prompt processing than mainline?",
        "bodyText": "Dating back to this commit: kalomaze@92497e1\nI get 15ms per token (~70t/s prompt processing) instead of the current 25ms-30ms per token (~40t/s prompt processing) before these PR changes were finalized into the mainline branch of the kobold fork. To me, this difference is pretty substantial (1.75x faster prompt eval times)\nUnfortunately, since this is a fork, I don't have a clean way to precisely map where and how this regression ostensibly happened. All I know is that my custom build I made to hack in faster prompt processing (before those two PRs were merged), to this day, the fastest build when it comes to Mixtral prompt processing compared to the latest llama.cpp or koboldcpp, and I'd like some help on trying to understand why that is, because there have been too many upstream improvements in other areas for me to stick to something like this.\nThe generation speeds are somewhat worse on this build (likely because of upstream improvements since then which have improved tg/s but not prompt eval speeds), but the prompt processing/batching seems to be clearly superior by a large margin.\nI noticed this quirk 2-3 weeks ago, so it doesn't appear to be a recent regression or anything like that (nor was it caused by the multi-GPU changes); it was around the same time that Mixtral was still new and getting the kinks worked out. I had hoped it was some odd temporary regression but it has persisted.\nPerhaps the ggml files could be diffed and compared to see if anything stands out that might be contributing to this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5227",
        "createdAt": "2024-01-30T21:26:25Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Making the output of LLAMA Server exact the same like LLAMA in the instruct mode",
        "bodyText": "Please help, help, help! I spent months in an attempt to solve this issue. I asked this question many times and nobody answered. I can't believe that no one is interested in solving this issue.\nI can't tune up llama server. No, it's quite simple, but the response is always awful with the same prompt. LLAMA Sever doesn't work like LLAMA INSTRUCTION MODE. Meanwhile, when I run llama.cpp in the instruction mode I receive exactly what I want. Below, I'll give you examples:\n\nLlama.cpp INSTRUCTION mode:\n\nalex@M1 llama.cpp % ./main -m ~/ai/mistral-7b-instruct-v0.2.Q4_K_M.gguf -ins --color --multiline-input -ngl 99\n\n\nWrite me a short description of the Lancome brand\nLanc\u00f4me is a renowned French luxury beauty brand, established in Paris in 1935 by Armand Petitjean. Known for its iconic fragrances, makeup, and skincare products, Lanc\u00f4me has been synonymous with elegance, sophistication, and innovation for nearly a century. The brand's offerings cater to both men and women, focusing on enhancing natural beauty through high-performance formulas and luxurious textures. From its iconic fragrances like Tresor and La Vie Est Belle, to its bestselling makeup collections such as Teint Idole foundation and Artliner Precision Point eyeliner, Lanc\u00f4me continues to set industry standards and inspire confidence with each new product launch. The brand's commitment to sustainability and ethical business practices further solidifies its position as a leader in the beauty industry.\n\n\nAnother example:\n\n\nWrite me a short description of the Lancome brand\nLanc\u00f4me is a French luxury beauty brand known for its high-quality skincare, makeup, and fragrances. Established in Paris in 1935 by Armand Petitjean, it was one of the first perfume houses to create exclusively feminine fragrances. The brand's name is derived from the Latin word \"lanscum,\" meaning \"poplar tree bud,\" symbolizing rebirth and renewal. Lanc\u00f4me offers a wide range of products including foundations, concealers, powders, mascaras, eyeliners, lipsticks, blushes, skincare items, and perfumes. The brand is renowned for its sophisticated and elegant packaging, as well as its commitment to innovation and research in beauty. Lanc\u00f4me has been the preferred choice of numerous celebrities and fashion icons throughout history, and is currently part of the L'Or\u00e9al Luxe division.\n\n\nEverything is correct. Next, I'm trying to do the same with LLAMA Server:\n~/ai/llama.cpp$ ./server -m models/mistral-7b-instruct-v0.2.Q4_K_M.gguf -c 2048 --host XXX.XXX.XXX.XX --port 8000 \nOk, let's do it with the most simple code and make an API request:\nimport requests\nimport json\n\nurl = \"http://XXX.XXX.XXX.XX:8000/completion\"\nheaders = {\"Content-Type\": \"application/json\"}\ndata = {\n        \"prompt\": \"Write me a short description of the Lancome brand\\n\",\n        'temperature': 0.8,\n        'top_k': 40,\n        'top_p': 0.95,\n        'repeat_penalty': 1.1,\n        'tfs_z': 1,\n        'min_p': 0.05,\n        'typical_p': 1\n    }\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json()['content'])\n\nBelow are the examples of the response:\n\nLancome is a French luxury cosmetics brand, owned by the L'Or\u00e9al Group. It was founded in Paris in 1935 by Armand Petitjean and named after his girlfriend, Marie-Antoinette Lancret-de-Fortuny. Lancome quickly gained popularity for its elegant packaging and high-quality products, including fragrances, makeup, and skincare. The brand is known for its iconic products such as the Hypnose mascara, Absolue foundation, and La Vie Est Belle perfume. With a focus on innovation and research, Lancome continues to create advanced formulas and chic designs that cater to women's diverse beauty needs. The brand is also committed to sustainability, using eco-friendly packaging and ingredients whenever possible.\n\nWTF? named after his girlfriend, Marie-Antoinette Lancret-de-Fortuny Where did the model get these garbish data?\n\nLancome is a luxury French cosmetics, perfumes, and skincare brand founded in 1935 by Armand Petitjean. The name \"Lancome\" is derived from the Latin word \"lanka,\" meaning \"slim,\" and \"komos,\" meaning \"beauty party.\" Initially focusing on makeup, Lancome soon expanded its offerings to include fragrances and skincare products. With a commitment to innovation and high-quality ingredients, Lancome has become synonymous with elegance and sophistication. Their products are sold in over 130 countries worldwide and are beloved by celebrities, fashion influencers, and beauty enthusiasts alike. From iconic fragrances like La Vie Est Belle to their best-selling skincare line, Lancome continues to push boundaries and set trends in the world of beauty.\n\nWTF? from the Latin word \"lanka,\" meaning \"slim,\" and \"komos,\"\n\nLancome is a renowned French luxury beauty brand that was established in Paris in 1935. It was founded by Armand Petitjean, who named the company after his favorite flower, the orchid (l'Orchidee in French). Lancome quickly gained popularity for its innovative skincare products and fragrances, using only the finest ingredients to create high-quality formulations. The brand prides itself on its commitment to scientific research and development, continuously pushing the boundaries of beauty technology. Lancome's extensive product range includes makeup, skincare, perfumes, and accessories, catering to both men and women. Its iconic products include the advanced skincare line, the famous Teint Idole foundation, and best-selling fragrances such as La Vie Est Belle and Idole. With a rich heritage, Lancome continues to be a leader in the global beauty industry, inspiring and empowering people to express their unique beauty.\n\nWTF? who named the company after his favorite flower, the orchid (l'Orchidee in French)\nFinally, example with CURL (how it was done in the official instruction).\nalex@M1 remote-llama % curl --request POST \\\n    --url http://XXX.XXX.XXX.XX:8000/completion \\\n    --header \"Content-Type: application/json\" \\\n    --data '{\"prompt\": \"Write me a short description of the Lancome brand\\n\"}'\n{\"content\":\"\\nLancome is a French luxury beauty brand that was founded in Paris in 1935 by Armand Petitjean. The name Lancome is derived from the Latin word for lake, \\\"lacuna,\\\" and the French word for beauty, \\\"beau.\\\" The brand has a rich history of innovation in skincare, makeup, and fragrances. Its iconic products include the Advanced G\u00e9nifique serum, Hydra-Absolue cream, and Artliner liquid eyeliner. Lancome's commitment to excellence and beauty is reflected in its slogan \\\"where art meets science.\\\" The brand is known for collaborating with top international makeup artists and influencers to create trendsetting looks and collections. Lancome products are sold worldwide in department stores, specialty beauty shops, and online at lancome.com.\",\"generation_settings\":{\"frequency_penalty\":0.0,\"grammar\":\"\",\"ignore_eos\":false,\"logit_bias\":[],\"min_p\":0.05000000074505806,\"mirostat\":0,\"mirostat_eta\":0.10000000149011612,\"mirostat_tau\":5.0,\"model\":\"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\"n_ctx\":2048,\"n_keep\":0,\"n_predict\":-1,\"n_probs\":0,\"penalize_nl\":true,\"presence_penalty\":0.0,\"repeat_last_n\":64,\"repeat_penalty\":1.100000023841858,\"seed\":4294967295,\"stop\":[],\"stream\":false,\"temp\":0.800000011920929,\"tfs_z\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"typical_p\":1.0},\"model\":\"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\"prompt\":\"Write me a short description of the Lancome brand\\n\",\"slot_id\":0,\"stop\":true,\"stopped_eos\":true,\"stopped_limit\":false,\"stopped_word\":false,\"stopping_word\":\"\",\"timings\":{\"predicted_ms\":26494.377,\"predicted_n\":180,\"predicted_per_second\":6.793894417672097,\"predicted_per_token_ms\":147.19098333333335,\"prompt_ms\":812.901,\"prompt_n\":12,\"prompt_per_second\":14.76194518151657,\"prompt_per_token_ms\":67.74175},\"tokens_cached\":192,\"tokens_evaluated\":12,\"tokens_predicted\":180,\"truncated\":false}%\n\n... again, hallucinations: The name Lancome is derived from the Latin word for lake, \"lacuna,\" and the French word for beauty, \"beau.\"\nWhile the output on the same remote machine using the instruct mode:\n\n\nWrite me a short description of the Lancome brand\nLanc\u00f4me is a renowned French cosmetics brand that has been captivating beauty enthusiasts since its inception in 1935. Known for its sophisticated elegance and commitment to innovation, Lanc\u00f4me offers an extensive range of high-quality products designed to enhance the natural beauty of women and men around the world. From iconic fragrances like \"Tr\u00e9sor\" and \"La Vie Est Belle,\" to their renowned skincare lines such as \"Advanced G\u00e9nifique\" and \"Absolue\", Lanc\u00f4me's extensive collection caters to various needs and preferences. The brand prides itself on its commitment to research and development, continuously pushing boundaries with groundbreaking formulas and advanced technologies. With a rich heritage and an unwavering dedication to beauty, Lanc\u00f4me continues to inspire confidence and allure for generations.\n\n\nWhy every basic instruct response writes the brand name correctly Lanc\u00f4me while the server's response writes Lancome?\nIn other words, every time I make an API request to the server, I experience the hallucinations of the model, while in the instruction mode it behaves correctly.\nWhy does it happen? Why I can't get exactly the same (quality) response from the LLAMA server, while LLAMA.CPP gives me exactly what I want in the instruction mode. I've been trying this with numerous queries and completely the same parameters but had no quality response with LLAMA Server.\nP.s.: I also mentioned that the strange behavior (hallucinations) happen only in the Completion mode. Even if I use the web interface for LLAMA Server, everything works just fine in the Chat mode. I suppose, LLAMA.CPP drops some crucial parameters in the completion mode.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5348",
        "createdAt": "2024-02-05T16:55:15Z",
        "author": {
            "login": "alexcardo"
        }
    },
    {
        "title": "When are grammars evaluated?",
        "bodyText": "I'm teaching a short little course on how to use llama.cpp with the python bindings to run llama 2 on the CPU. I'm touching on some of the main API points, and would like to touch on grammars in llama.cpp as well as I think it's a pretty useful feature. I'm having troubles finding documentation about it, so pointers if I've just missed a write up are welcomed.\nSpecifically, I'm wondering if grammars are evaluated on a token by token basis (e.g. streaming), or only after the llm has finished generating the output sequence? Or, perhaps evaluated even before the token has been chosen, e.g. used to reduce the set of candidate tokens in a way similar to top_k/top_p parameters?\nAny advice would be great!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5202",
        "createdAt": "2024-01-29T22:21:16Z",
        "author": {
            "login": "cab938"
        }
    },
    {
        "title": "how to generate longer outputs with what models of llama that is within 64gb cpu ram and 8gb vram?",
        "bodyText": "i saw on github issues that can generate 400k context but using gguf, i cant generate anything longer than 2048 no matter what parameters i put even if i put in -n 32400 -c 32400.\nso how to generate longer outputs?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5224",
        "createdAt": "2024-01-30T19:51:29Z",
        "author": {
            "login": "sprappcom"
        }
    },
    {
        "title": "creating gguf model from lora adapter",
        "bodyText": "I have a ggml adapter model created by convert-lora-to-ggml.py (ggml-adapter-model.bin).  Now my doubt is how to create the complete gguf model out of these?\nI have seen using\n./main -m models/llama-13b.Q8_0.gguf --lora models/loras/adapter_model.bin --color -c 4096 --temp 0.7 --repeat_penalty >1.1 -n 256 -p \"The conversation between human and AI assistant.\\n[|Human|] Qual'\u00e8 il significato della vita?\\n[|AI|] \" \nThis is working fine for me. But this is just for querying right?\nI want the merged full gguf model. How to create it ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5360",
        "createdAt": "2024-02-06T11:39:48Z",
        "author": {
            "login": "ragesh2000"
        }
    },
    {
        "title": "is an optimized communication language better for training and inference?",
        "bodyText": "let's say we\n\nreduce the letters to 13 instead of 26, that makes the requirement to process at least halved.\nreduce base words for faster processing.\nremove plurals, one word per form.\n\nlet's say we have this new language, will we have faster inference / training speed?\nhttps://github.com/ouvaa/ouvaa.github.io",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5342",
        "createdAt": "2024-02-05T13:04:55Z",
        "author": {
            "login": "ouvaa"
        }
    },
    {
        "title": "MOEish ideas",
        "bodyText": "CustomModelFormat Draft 4.txt\nthis format was an old idea before I had a full grasp on everything AI(which is still the case but a little better now) and definitely could not be used as is but some concepts may be interesting. The idea was sort of a what model am I going to be today model utilizing compressed deltas from a base model, where you can use two tokens to allow the user or appropriately trained model to select different recipes to give different mixes of the included deltas. a modernized version might make use of references to mergekit yaml files for the recipes. there may also be simpler ways of reaching similar goals. this could also serve as a try in memory before you commit to using storage space option for messing with merge recipes. I could also see keeping the deltas separate as an option so you could just download the ones you want rather than a curated selection.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5374",
        "createdAt": "2024-02-06T20:04:06Z",
        "author": {
            "login": "Swight1423"
        }
    },
    {
        "title": "fp16_performance_good on NVIDIA GeForce GTX 1660 Ti",
        "bodyText": "Troubleshooting some performance issues that popped up a while ago and we noticed this was added:\nconst bool fp16_performance_good = min_compute_capability >= CC_VOLTA;\nNVIDIA GeForce GTX 1660 Ti proclaims a compute capability of 7.5, but it apparently does not have any tensor cores and absolutely abysmal fp16 performance. I wonder if there is any way to better differentiate when to use it.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5329",
        "createdAt": "2024-02-05T02:49:51Z",
        "author": {
            "login": "LostRuins"
        }
    },
    {
        "title": "New SOTA 2-Bit Quant released: QuIP-Sharp",
        "bodyText": "Oobabooga implemented this into the webui and certainly in terms of memory, it seems a lot better than current Q2K, by a landslide. A Q2_K 13B model needs around 5.4 GB, while a 2-BIT QuIP model only needs around 3.8 GB https://huggingface.co/relaxml/Llama-2-13b-E8P-2Bit/tree/main . This means a 13B model can be fully offloaded on a 6 GB GPU.\nLikewise according to Oobabooga, a 70B model now fits entirely within 24 GB VRAM and at a context of 3072.\nThis is likely because its true 2 Bit and not a mixture of different bits like its the case with K quants in Llama.cpp.\nBased on this table provided by Oobabooga, perplexity looks promising:\n\nI think answers to the following questions need to be persued to check if an implementation would make sense.\n\n\nHow does a Llama 2 7B model at Q4K_S (which is 3.8 GB in size) compare to a Llama 2 13B QuIP-Sharp model which is also 3.8 GB, perplexity wise? If its better than 7B, then it would absolutely make sense to implement it.\n\n\nHow does Q2_K compare to 2-Bit QuIP-Sharp complexity wise? Now, even if it would be worse than Q2_K which is pretty likely, the massive memory savings can't be ignored. A 13B model measuring just around 3.8 GB is truly unprecedented.\n\n\nAccording to Ooba's data, it's pretty interesting how pure 2-Bit QuIP outperforms Exllama's 2.5 BPW one.\nNote: You may have heard about QuIP in the past. QuIP-Sharp is a new one that is drastically improved.\nHere's the link to the repo: https://github.com/Cornell-RelaxML/quip-sharp\n@slaren @ggerganov @ikawrakow  Curious to hear your opinions.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4327",
        "createdAt": "2023-12-04T17:28:03Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "Streaming LlamaCPP in Langserve or FastAPI",
        "bodyText": "Hi,\nI have built a rag app and I am loading a LLM with Llamacpp. However I have problems with making Streaming work for FastAPI or Langserve requests. Streaming is working in my Terminal, but I don't know what I have to change to make it work in FastAPI/Langserve.\nHere is my Langserve code:\nfrom langchain_community.vectorstores.pgvector import PGVector\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom langchain_core.runnables import RunnableParallel\nimport os\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings\nimport box \nimport yaml\nfrom langchain_community.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom operator import itemgetter\nfrom typing import TypedDict\nfrom fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse\nfrom langserve import add_routes\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom starlette.staticfiles import StaticFiles\nfrom langchain_core.output_parsers import StrOutputParser\n\nwith open('./config/config.yml', 'r', encoding='utf8') as ymlfile:\n    cfg = box.Box(yaml.safe_load(ymlfile))\n    \ndef build_llm(model_path, temperature=cfg.RAG_TEMPERATURE, max_tokens=cfg.MAX_TOKENS, callback = StreamingStdOutCallbackHandler()):\n        \n        callback_manager = CallbackManager([callback])\n        \n        n_gpu_layers = 1 # Metal set to 1 is enough. # ausprobiert mit mehreren\n        n_batch = 512 #1024 Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n\n        llm = LlamaCpp(\n                max_tokens = max_tokens,\n                n_threads = 8,#8, #f\u00fcr performance,\n                model_path=model_path,\n                temperature=temperature,\n                f16_kv=True,\n                n_ctx=15000, # 8k aber mann muss Platz lassen f\u00fcr Instruction, History etc. \n                n_gpu_layers=n_gpu_layers,\n                n_batch=n_batch,\n                callback_manager=callback_manager, \n                verbose=True, # Verbose is required to pass to the callback manager\n                top_p=0.75,\n                top_k=40,\n                repeat_penalty = 1.1,\n                streaming=True,\n                model_kwargs={\n                        #'repetition_penalty': 1.1,\n                        #'mirostat': 2,\n                },\n        )\n        \n        return llm\n\nembeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL_NAME,\n                                            model_kwargs={'device': 'mps'})\n\nPG_COLLECTION_NAME =  \"PGVECTOR_BKB\"\nmodel_path = \"./modelle/sauerkrautlm-mixtral-8x7b-instruct.Q4_K_M.gguf\"\n\nCONNECTION_STRING = \"MY_CONNTECTION_STRING\"\nvector_store = PGVector(\n    collection_name=PG_COLLECTION_NAME,\n    connection_string=CONNECTION_STRING,\n    embedding_function=embeddings\n)\n\nprompt= \"\"\"\n<s> [INST] Du bist RagBot, ein hilfsbereiter Assistent. Antworte nur auf Deutsch. Verwende die folgenden Kontextinformationen, um die Frage am Ende knapp zu beantworten. Wenn du die Antwort nicht kennst, sag einfach, dass du es nicht weisst. Erfinde keine Antwort! Falls der Nutzer allgemeine Fragen stellt, f\u00fchre Smalltalk mit Ihm.\n\n### Hier der Kontext: ###\n{context}\n\n### Hier die Frage: ###\n{question}\n\nAntwort: [/INST]\n\"\"\"\n\ndef model_response_prompt():\n    return PromptTemplate(template=prompt, input_variables=['input', 'typescript_string'])\nprompt_temp = model_response_prompt()\n\nllm = build_llm(model_path, temperature= cfg.NO_RAG_TEMPERATURE, max_tokens = cfg.NO_RAG_MAX_TOKENS)\n\nclass RagInput(TypedDict):\n    question: str\n\n\nfinal_chain = (\n        RunnableParallel(\n            context=(itemgetter(\"question\") | vector_store.as_retriever()),\n            question=itemgetter(\"question\")\n        ) |\n        RunnableParallel(\n            answer=(prompt_temp| llm),\n            docs=itemgetter(\"context\")\n        )\n\n).with_types(input_type=RagInput)\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\n        \"http://localhost:3000\"\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n#app.mount(\"/rag/static\", StaticFiles(directory=\"./source_docs\"), name=\"static\")\n@app.get(\"/\")\nasync def redirect_root_to_docs():\n    return RedirectResponse(\"/docs\")\n\n\n# Edit this to add the chain you want to add\nadd_routes(app, final_chain, path=\"/rag\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nWhen trying out in the Langserve Playground (http://0.0.0.0:8000/rag/playground/) the response gets streamed in my Terminal but not in the Playground.\nSo how can I make this work?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5265",
        "createdAt": "2024-02-01T17:59:48Z",
        "author": {
            "login": "weissenbacherpwc"
        }
    },
    {
        "title": "Different Inference-speed when loading model through hf-hub",
        "bodyText": "Hi,\ni get much faster inference speed (almost twice as fast) when I load a model like this:\nmodel_path = hf_hub_download(repo_id=repo_id, filename=model_file_name, repo_type=\"model\")\n\n# initialize LlamaCpp LLM model\nself.llm = LlamaCpp(\n            model_path=model_path,\n            seed=2,\n            temperature=0.5,  # Default 0.8\n            max_tokens=256,  # The maximum number of tokens to generate. Default 256\n            top_k=20,  # Default 40\n            top_p=0.85, # Default 0.95\n            n_ctx=1024, # Text context window, 0 = from model\n            n_gpu_layers=64,\n            n_batch=1024,  # Prompt processing maximum batch size Default 512\n            stop = ['</s>'],\n            #n_threads=8,  # Number of threads to use for generation --8 Am schnellsten!\n            #callback_manager=callbacks,\n            verbose=True,  # Verbose is required to pass to the callback manager\n            streaming=False  # Whether to stream the results, token by token.\n        )\n\ninstead of this:\nmodel_path \"Path/to/Model\"\n\n# initialize LlamaCpp LLM model\nself.llm = LlamaCpp(\n            model_path=model_path,\n            seed=2,\n            temperature=0.5,  # Default 0.8\n            max_tokens=256,  # The maximum number of tokens to generate. Default 256\n            top_k=20,  # Default 40\n            top_p=0.85, # Default 0.95\n            n_ctx=1024, # Text context window, 0 = from model\n            n_gpu_layers=64,\n            n_batch=1024,  # Prompt processing maximum batch size Default 512\n            stop = ['</s>'],\n            #n_threads=8,  # Number of threads to use for generation --8 Am schnellsten!\n            #callback_manager=callbacks,\n            verbose=True,  # Verbose is required to pass to the callback manager\n            streaming=False  # Whether to stream the results, token by token.\n        )\n\nWhat is the reason for it and how can i load a local model without hf-hub and the same speed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5283",
        "createdAt": "2024-02-02T12:45:49Z",
        "author": {
            "login": "Gitclop"
        }
    },
    {
        "title": "perplexity / imatrix / kl-div is not being calculated properly on Windows without a special flag",
        "bodyText": "Encountered a pretty nasty bug that helps explain some things about my past efforts w.r.t calibration datasets.\nperplexity.exe -m \"C:\\Users\\Kalo\\Documents\\GitHub\\llamacpp_git\\examples\\dpo_7b_quant\\DPO_7b_q8_0.gguf\" -f 8k_redo.txt -ngl 33 -c 512\nOn Windows, supplying -f to the perplexity example (and presumably imatrix as well which is derivative of it) results in higher perplexity because it is not reading it properly as a binary file like how it does on WSL / Ubuntu (regardless if you use -f or -bf, it is read properly there).\nHere is about 9k tokens worth with -f on Windows:\nperplexity: calculating perplexity over 18 chunks, batch_size=512\nperplexity: 0.40 seconds per pass - ETA 0.12 minutes\n[1]8.9604,[2]13.1455,[3]12.9100,[4]12.6800,[5]13.6468,[6]14.1241,[7]14.4815,[8]13.3205,[9]12.5099,[10]12.2798,[11]11.8491,[12]11.4355,[13]11.3987,[14]11.4812,[15]11.5342,[16]11.6241,[17]11.2636,[18]11.1466,\nFinal estimate: PPL = 11.1466 +/- 0.46029\n\nAnd here is -bf for comparison:\nperplexity: calculating perplexity over 19 chunks, batch_size=512\nperplexity: 0.40 seconds per pass - ETA 0.12 minutes\n[1]8.1417,[2]11.9650,[3]12.0453,[4]11.8829,[5]12.6807,[6]11.1073,[7]11.8471,[8]11.5785,[9]10.6265,[10]11.1294,[11]10.6127,[12]10.1494,[13]10.0213,[14]10.1782,[15]10.2959,[16]10.5644,[17]10.4623,[18]10.3628,[19]10.3186,\nFinal estimate: PPL = 10.3186 +/- 0.42002\n\nIn addition to this, KL divergence was not working properly on Windows whatsoever, because the output file wasn't being read as a binary file. I have made a PR to address this:\n#5273\nThis also makes me concerned that imatrix calculations are not working as intended either on Windows; maybe even if you use -bf. I will be investigating that shortly",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5274",
        "createdAt": "2024-02-02T07:04:48Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Requesting support for OLMO models from AI2",
        "bodyText": "Hi there! Are there any plans to support OLMo models? For example, https://huggingface.co/allenai/OLMo-7B",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5268",
        "createdAt": "2024-02-02T01:04:50Z",
        "author": {
            "login": "kerekovskik"
        }
    },
    {
        "title": "SOTA Quants",
        "bodyText": "@ikawrakow Are we able to us quantize to quantize gguf models using the new SOTA Quant methods that you implemented for Q2_K? Or, are they that still in the works? \ud83e\udd14",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4852",
        "createdAt": "2024-01-10T03:00:26Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "How much memory does imatrix calculation use?",
        "bodyText": "Hello,\nI want to try some of the newest quants, like IQ2_XS(S) or IQ3_XS, but I'm not sure if my setup (32 RAM + 6 VRAM) is capable of performing all of the imatrix evaluation steps on something as big as an 70B model in a reasonable amount of time.\nIf it's not, I saw that the code for the quantize.cpp seems to allow to requantize \"bigger\" quants like 4_K_M into smaller ones, like 3_K_M ( I might be wrong though).\nIs it possible to calculate the importance matrix using already quantized models, or I have to use fp16 files?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5222",
        "createdAt": "2024-01-30T18:44:39Z",
        "author": {
            "login": "avada-z"
        }
    },
    {
        "title": "1bit+ quantizing idea",
        "bodyText": "1bitquant.zip\nI took a bit different approach to the problem of shrinking down models. instead of doing a direct calculation I instead store if each value is less than the next and guess based on that information for a series of values. the quant method assumes integer values like you would get if the model was already quantized may still need the group delta values from the original model. I am pretty sure there is still room for improvement over my current implementation linked to this post. my implementation in vb.net just generates randomized data in a given range, finds the comparison data, attempts to use that data to reconstruct for the selected group size, then compares to the original and averages multiple runs. the part of the code that actually does the work is less than 100 lines of code. There is a GUI to mess with the values. if my calculations are accurate could get 30%ish accuracy when run on 4bit data and 24%ish when run on 8bit or 16bit as currently coded on average. haven't tested on real models. suggest group sizes greater than 32. needs more testing to find sweet spot.\nEdit: 8bit sweet spot seems to be around 32768 for group size where I got around 31% accuracy.\nEdit2 looks like my accuracy calculation may have issues but they seem to translate to this being better than the implementations I removed. the calculation was more a within what percentage of the correct answer do I get if I didn't make another mistake.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5164",
        "createdAt": "2024-01-28T04:09:15Z",
        "author": {
            "login": "Swight1423"
        }
    },
    {
        "title": "Simple guide for AMDGPU using hipblas",
        "bodyText": "Thanks #1087.\n\n\n\nInstall rocm, search in docs.amd.com find the rocm installation guide. Better use 5.4.2 for PyTorch support anyway.\nAfter finish, check with the hipblas hipcc and anything mentioned in the pull request.\n\n\n\ndownload any current master. And master 12b5900, replace ggml.c ggml-cuda.cu ggml-cuda.h to the current.\n\n\n\nmodify docs the pull request did change. And set hipblas ON in cmake files\n\n\n\nin terminal,\nexport CXX=hipcc\nCd to the llama.cpp dir,\nmkdir build, cd build.\nCMAKE_PREFIX_PATH=/opt/rocm cmake ..\nMake\n5.done.\n\n\n\nany running problem related to gfx????\nExport HSA_OVERRIDE_GFX_VERSION=9.0.0 / 9.0.6 / 9.0.8 / 9.0.a / 10.3.0\nTest it just one by one",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1141",
        "createdAt": "2023-04-23T15:19:36Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Loading GGML converted qlora models",
        "bodyText": "I have lora weights of a finetuned model (adapter_model.bin) and i created a ggml version of the file using the python file convert-lora-to-ggml.py and now i have the ggml_model.bin file.\nSo now how can i merge this to base model? or there is any other method to use the converted ggml model ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4018",
        "createdAt": "2023-11-10T12:22:40Z",
        "author": {
            "login": "ragesh2000"
        }
    },
    {
        "title": "turn GGUFs back to transformers model?",
        "bodyText": "Now llama.cpp fan used to fine tune,but after that can I convert models back to transformers to use with huggingface's eco system?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5212",
        "createdAt": "2024-01-30T10:30:39Z",
        "author": {
            "login": "lin-calvin"
        }
    },
    {
        "title": "Accelerating IQ2_XS",
        "bodyText": "Hi,\nI have been working on accelerating IQ2_XS's dot product and gotten eval time to about 75% of what it was on my machine. The commit showing my work is here: PeterReid@52b2738 .\nMy idea is to vectorize the computation of s2_1 and s2_2 in ggml_vec_dot_iq2_xs_q8_K. Before this commit, building those required picking the XMM registers apart into regular registers in 8 separate pieces, doing a memory reference for each of those pieces into the table that maps sign encodings to their meanings, and then assembling those pieces back together, one a time, into YMM registers.\nAfter the commit, the computation stays in the AVX2 registers. Rather than doing lookups in the table, it uses the fact that the bytes 0..127 (inputs to the sign lookup table) can be mapped to bytes with an even number of bits set (outputs of the sign table) with the formula x ^ (x<<1).\nUnfortunately, the mapping doesn't have the same order as the original sign table has, so I have to modify the .gguf file to work after this patch. The ordering of the sign table is basically arbitrary, but it would obviously be bad to break existing gguf files. That may make this a non-starter, but I figured it is worth showing this anyway. (The commented out nonsense in llama.cpp you may see in the commit is doing the conversion.)\nThe machine I'm working on is an elderly i5-4300U, so your results may be different.\nWhat I've done so far is specific to AVX2, but it seems like the same idea could be used on most platforms, and for IQ2_XXS.\nMy timings:\n\nPre change:\nlama_print_timings:        load time =    1719.99 ms\nllama_print_timings:      sample time =     147.08 ms /   192 runs   (    0.77 ms per token,  1305.43 tokens per second)\nllama_print_timings: prompt eval time =   13841.57 ms /    18 tokens (  768.98 ms per token,     1.30 tokens per second)\nllama_print_timings:        eval time =  161728.03 ms /   192 runs   (  842.33 ms per token,     1.19 tokens per second)\nllama_print_timings:       total time =  182960.32 ms /   210 tokens\n\nPost change:\nllama_print_timings:        load time =    1270.98 ms\nllama_print_timings:      sample time =     193.19 ms /   240 runs   (    0.80 ms per token,  1242.31 tokens per second)\nllama_print_timings: prompt eval time =    9554.77 ms /    18 tokens (  530.82 ms per token,     1.88 tokens per second)\nllama_print_timings:        eval time =  148493.11 ms /   241 runs   (  616.15 ms per token,     1.62 tokens per second)\nllama_print_timings:       total time =  164450.33 ms /   259 tokens",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5152",
        "createdAt": "2024-01-26T20:36:13Z",
        "author": {
            "login": "PeterReid"
        }
    },
    {
        "title": "What's an embeddings model I can use right now?",
        "bodyText": "I'm having a hard time on the embedding part of a RAG app. It seems llama.cpp supports generating embeddings, but I get really poor results with any gguf model I try. I tried some Bert-based models but those don't load because Bert isn't supported, and I see some discussions about adding those but as of now nothing like that is available.\nSo what model do people use to generate embeddings right now?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4913",
        "createdAt": "2024-01-13T14:31:59Z",
        "author": {
            "login": "Johnhersh"
        }
    },
    {
        "title": "How to use CUDA or BLAS",
        "bodyText": "With the master-8944a13 -\nAdd NVIDIA cuBLAS support (#1044) i looked forward if i can see any differences.\nSadly, i don't.\nI cannot even see that my rtx 3060 is beeing used in any way at all by llama.cpp's main.exe on Windows, using  the win-avx2 version.\nIs there anything that needs to be switched on to use cuda?\nThe system-Info line of main.exe shows like this:\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\n\nSo why is BLAS=0 ?\nIs there anything needed to use BLAS?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1070",
        "createdAt": "2023-04-19T19:17:39Z",
        "author": {
            "login": "maddes8cht"
        }
    },
    {
        "title": "llama.cpp and 7G Llama-2-Chat model: resource requirements, if possible",
        "bodyText": "I am a new to this project and would like to try inference with llama.cpp and a 7G Llama-2-Chat model: is this combination currently supported and what are the resource requirements? I have e.g. a VM with 8 vCPUs and 16 GB RAM or (if need be) a bare-medal server with 56 CPUs and 500 GB RAM available. The (underlying) hardware is fairly recent in both cases. I guess the bare-metal second could serve in principle, but could the VM as well?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5172",
        "createdAt": "2024-01-28T11:01:16Z",
        "author": {
            "login": "christian-2"
        }
    },
    {
        "title": "airoboros-33b-gpt4-2.0.ggmlv3.q5_K_M converted to GGUF \"loses\" some words",
        "bodyText": "I'm using a WIP branch of oobabooga/text-generation-webui, so there could of course be something more that needs to be updated.\nConverted airoboros-33b-gpt4-2.0.ggmlv3.q5_K_M.bin from TheBloke to GGUF using the python script and metadata from original model on huggingface.\nExample generated text by GGUF model:\n\nBatman: On the, Joker, I see no comedy in human or misfortune. The world is far from a laughing matter.\nJoker: Oh but it is! It's a grand farce played by fools who think they hold the script in their hands when all along we are puppets dancing to someone else's tune.\nBatman: Your is as twisted as your mind, Joker. There is no joy in causing pain and.\n\nIt kind of looks like it has forgotten the words \"contrary\" and \"suffering\", which would fit well where there appears to be missing words in the first and third sentences. It happens very regularly with the converted model, while the original GGML works fine.\nLog from converter script:\n* Using config: Namespace(input=PosixPath('airoboros-33b-gpt4-2.0.ggmlv3.q5_K_M.bin'), output=PosixPath('airoboros-33b-gpt4-2.0.q5_K_M.gguf'), name=None, desc=None, gqa=1, eps='5.0e-06', context_length=2048, model_metadata_dir=PosixPath('/tmp/tmp.X3SU5hc517'), vocab_dir=None, vocabtype='spm')\n\n=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===\n\n* Scanning GGML input file\n* GGML model hyperparameters: <Hyperparameters: n_vocab=32000, n_embd=6656, n_mult=4480, n_head=52, n_layer=60, n_rot=128, n_ff=17920, ftype=17>\nLoading vocab file '/tmp/tmp.X3SU5hc517/tokenizer.model', type 'spm'\n!! Note: When overriding params the --gqa, --eps and --context-length options are ignored.\n* Overriding params: Params(n_vocab=32000, n_embd=6656, n_mult=4480, n_layer=60, n_ctx=2048, n_ff=17920, n_head=52, n_head_kv=52, f_norm_eps=1e-06, f_rope_freq_base=None, f_rope_scale=None, ftype=None, path_model=None)\n* Overriding vocab: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n* Preparing to save GGUF file\n* Adding model parameters and KV items\n* Adding vocab item(s)\n* Adding 543 tensor(s)\n    gguf: write header\n    gguf: write metadata\n    gguf: write tensors\n* Successful completion. Output saved to: airoboros-33b-gpt4-2.0.q5_K_M.gguf\n\nAnything I'm likely doing wrong during conversion? Only passing the --input --output and --model-metdata-dir parameters. The metadata dir has tokenizer_config.json tokenizer_config.json tokenizer.model config.json from https://huggingface.co/jondurbin/airoboros-33b-gpt4-2.0/tree/main",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2822",
        "createdAt": "2023-08-26T21:25:35Z",
        "author": {
            "login": "JohanAR"
        }
    },
    {
        "title": "What is the problem with my WPF project",
        "bodyText": "I have this code for MainWindow\n`\n\n            <RadioButton Content=\"\u0413\u043b\u0430\u0432\u043d\u0430\u044f\"\n                         Height=\"50\"\n                         Foreground=\"White\"\n                         FontSize=\"14\"\n                         Style=\"{StaticResource MenuButtonTheme}\"\n                         IsChecked=\"True\"\n                         Command=\"{Binding ChangeToHome}\"/>\n\n            <RadioButton Content=\"\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0440\u0430\u0441\u043a\u0440\u0430\u0441\u043a\u0443\"\n                         Height=\"50\"\n                         Foreground=\"White\"\n                         FontSize=\"14\"\n                         Style=\"{StaticResource MenuButtonTheme}\"\n                         Command=\"{Binding ChangeToDiscovery}\"/>\n\n        </StackPanel>\n\n`\nand this code for MainViewModel.cs\nprivate void ChangeToHome (object value)\n{\n    CurrentView = HomeVM;\n    MessageBox.Show(\"Now in home\", \"alert\", MessageBoxButton.OK, MessageBoxImage.Information);\n}\n\nprivate void ChangeToDiscovery (object value)\n{\n    CurrentView = DiscoveryVM;\n    MessageBox.Show(\"Now in Discovery\", \"alert\", MessageBoxButton.OK, MessageBoxImage.Information);\n}\n\nprivate bool CanChangeView(object value)\n{\n    return true;\n}\n\n\npublic MainViewModel() \n{\n    HomeVM = new HomeViewModel();\n    DiscoveryVM = new DiscoveryViewModel();\n    CurrentView = HomeVM;\n    HomeViewCommand = new RelayCommand(ChangeToHome, CanChangeView);\n    DiscoveryViewCommand = new RelayCommand(ChangeToDiscovery, CanChangeView);\n}\n\nSo when i click on the radio button i should change views and be able to see some messages, but nothing happen.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5169",
        "createdAt": "2024-01-28T10:02:44Z",
        "author": {
            "login": "SCHIZ0FRENIA"
        }
    },
    {
        "title": "If fine tuning seemed to work without error and the loss went down does that mean I fine tuned?",
        "bodyText": "The finetune section of the examples dir says the fine tuning only works on llama models. I ran finetune.exe for a few hours not on a llama model, but a quantized Mistral 7B model and the loss went down. No errors. By the end of it I had two guff files and a bin file. I've tried every combination of opening these files in koboldcpp and it usually will not load a model and crashes.\nIf you could throw me a bone and tell me what the gguf files are for and what the one bin file is for when opening the model I'd appreciate it. Something is the lora adapters? The gguf files are just 125MB but the quantized model is about 6GB. So, does koboldcpp not allow me to run them because finetune.cpp really does not fine tune Mistral models and I haven't fine tuned anything, or do I just not know how to load the files?\nIf finetune.cpp cannot fine tune a Mistral 7B model where would I begin to modify the code so that it can?\nSomewhere here on line 100?\nstruct my_llama_lora_layer {\n// normalization\nstruct ggml_tensor * attention_norm_a;\nstruct ggml_tensor * attention_norm_b;\n// attention\nstruct ggml_tensor * wq_a;\nstruct ggml_tensor * wq_b;\nstruct ggml_tensor * wk_a;\nstruct ggml_tensor * wk_b;\nstruct ggml_tensor * wv_a;\nstruct ggml_tensor * wv_b;\nstruct ggml_tensor * wo_a;\nstruct ggml_tensor * wo_b;\n\n// normalization\nstruct ggml_tensor * ffn_norm_a;\nstruct ggml_tensor * ffn_norm_b;\n\n// ff\nstruct ggml_tensor * w1_a;\nstruct ggml_tensor * w1_b;\nstruct ggml_tensor * w2_a;\nstruct ggml_tensor * w2_b;\nstruct ggml_tensor * w3_a;\nstruct ggml_tensor * w3_b;\n\n};\nstruct my_llama_lora {\nstruct ggml_context * ctx = NULL;\nstd::vector<uint8_t> data;\nmy_llama_lora_hparams hparams;\n\nstruct ggml_tensor * tok_embeddings_a;\nstruct ggml_tensor * tok_embeddings_b;\n\nstruct ggml_tensor * norm_a;\nstruct ggml_tensor * norm_b;\nstruct ggml_tensor * output_a;\nstruct ggml_tensor * output_b;\n\nstd::vector<my_llama_lora_layer> layers;\n\n};",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5155",
        "createdAt": "2024-01-27T02:59:37Z",
        "author": {
            "login": "MotorCityCobra"
        }
    },
    {
        "title": "custom quantization",
        "bodyText": "Can I use this project to customize the quantification of each layer of llama?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5133",
        "createdAt": "2024-01-26T03:36:27Z",
        "author": {
            "login": "tusiqi1"
        }
    },
    {
        "title": "Base models supported by llama.cpp",
        "bodyText": "This will be a live list containing all major base models supported by llama.cpp. Having this list will help maintainers to test if changes break some functionality in certain architectures. Please feel free to add more items - just don't add duplicates or finetunes\n\nLLaMa\n\n7B\n\nF16: https://huggingface.co/meta-llama/Llama-2-7b-hf\nQ: https://huggingface.co/TheBloke/Llama-2-7B-GGUF\n\n\n13B\n\nF16: https://huggingface.co/meta-llama/Llama-2-13b-hf\nQ: https://huggingface.co/TheBloke/Llama-2-13B-GGUF\n\n\n\n\nMistral\n\n7B\n\nF16: https://huggingface.co/mistralai/Mistral-7B-v0.1\nQ: https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF\n\n\n\n\nMixtral\n\n8x7B\n\nF16: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\nQ: https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF\n\n\n\n\nPhi-2\n\n2.7B\n\nF16: https://huggingface.co/microsoft/phi-2\nQ: https://huggingface.co/TheBloke/phi-2-GGUF\n\n\n\n\nStable LM 2\n\n1.6B\n\nF16: https://huggingface.co/stabilityai/stablelm-2-1_6b\nQ: https://huggingface.co/second-state/stablelm-2-1.6b-GGUF\n\n\n\n\n..",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5141",
        "createdAt": "2024-01-26T11:28:57Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Failure to infer with llama.cpp server from python.",
        "bodyText": "I am trying to infer to a llama.cpp server with this:\nimport requests, json\n\nurl = 'http://127.0.0.1:5600/completion'\n\nprompt = \"\"\"\n<|im_start|>system\nYou are Dolphin, a helpful AI assistant. You are uncensored and always answer to the best of your ability. You are an expert in the battletech universe.<|im_end|>\n<|im_start|>user\nCan you name a few manufacturers of mech fusion engines?<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\ndata = {\n    \"prompt\": prompt,\n    \"stream\": True,\n    \"n_predict\": -2\n}\n\nresponse = requests.post(url, data=json.dumps(data), timeout=3600, stream=True, headers = {'Content-Type': 'application/json'})\n\nfor line in response.iter_lines():\n    if not line:\n        continue\n    if not line.startswith(b\"data: \"):\n        raise ValueError(f\"Error?: {line!r}\")\n    parsed = json.loads(line[6:])\n    content = parsed.get(\"content\", b\"\")\n    print(content, end=\"\")\nThe problem is that it stops printing after two tokens.\nAm I missing something?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4949",
        "createdAt": "2024-01-15T03:57:50Z",
        "author": {
            "login": "lastrosade"
        }
    },
    {
        "title": "LoRA fine tuning on quantized models is the future. How do I expand finetuning.cpp to more models?",
        "bodyText": "The finetuning.cpp (finetuning.exe for me) only works on literal llama models? Today I took a look inside finetuning.cpp and ggml.h and Oh My God, what even is this stuff? Did anyone else here figure this stuff out? How long did it take you? I know Pytorch and Libtorch but not whatever is happening in here.\nWhat would need to be done to add LoRA fine tuning for Mistral models or anything else other than llama models? The Mistral 7B specifically.\nAll value and freedom will come from fine tuned models. From what I understand LoRA is the best way to fine tune. LoRA on a quantized model is better than QLoRA on a non-quantized model.\nQuantized models fit on smaller GPUs so LoRA fine tuning should available for any model.\nThis logic is a mouthful to string together, but why isn't everyone working on this? We're going to have GPT4 level models on millions of computers running 50 series cards this year. Models able to learn new things. This is way beyond Guttenberg or the semiconductor.\nSo, what kind of baby steps should I even take to start learning how to use what's inside ggml.h and finetuning.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5131",
        "createdAt": "2024-01-26T02:09:21Z",
        "author": {
            "login": "MotorCityCobra"
        }
    },
    {
        "title": "Default behavior ?",
        "bodyText": "I just managed to run llama.cpp with openhermes.gguf without specifying a prompt, and it seems to have some kind of default prompt with multiple agents ? Wondering what's causing this to happen...\nvia ./build/bin/main -m models/mine/openhermes-2.5-mistral-7b.Q2_K.gguf -ngl 128 -i :\nmain: interactive mode on.\nsampling:\n        repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampling order:\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to LLaMa.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n\n User 0: Can someone explain to me the difference between a PAC and SAC?\nUser 1: PACs are Political Action Committees. They're usually formed by corporations or unions to raise money for political campaigns. Some people also form super PACs which are similar but can accept unlimited amounts of money from individuals. SACs are Supervisory Appellate Committees, and there's only one of those that I know about: the National Rifle Association's SAC.\n\nFrom what I understand, the NRA's SAC is composed of attorneys who work with the NRA on legal issues related to gun rights.\nUser 3: To add onto that, PACs and SACs are different in function, but they sound similar because \"PAC\" stands for political action committee, and \"SAC\" was a term coined by the NRA (National Rifle Association) for their Supervisory Appellate Committee.\n\nA PAC is\n an organization that pools campaign contributions from members and donates to other candidates, usually in the same party. They can also engage in lobbying activities.\n\nA SAC is a committee established by the NRA for What is llama cpp ?\n\na super PAC. The NRA's SAC consists of attorneys who work on legal issues related to gun rights, such as the constitutionality of gun control laws.<|im_end|>\n\n<|im_start|>.\nUser 1: > What is llama cpp ?\n\nI have no idea what you're talking about with \"llama cpp\" and it seems to be irrelevant to the context of this discussion.\nUser 3: Oh, sorry, it was an error in my response. I meant to say \"What is a cpp llama?\" but for some reason I wrote it as \"What is llama cpp?\" My apologies.<|im_end|>\n\n<|im_start|>.\nUser 1: > I meant to say \"What is a cpp llama?\"\n\nI have no idea what that might refer to.<|im_end|>\n\n<|im_start|> assistant\n\n\"C++ Llama\" doesn't seem to have any relevance or meaning in the context of this discussion. Could you please explain what it refers to?<|im_end|>\n\n<|im_start|> assistant\n\n\"C++ Llama\" is not a term I'm familiar with, and it doesn't seem to have any relevance or meaning in the context of this discussion. Could you please explain what it refers to?<|im_end|>\n\n<|im_start|>.\nUser 3: I apologize for any confusion. \"C++ Llama\" is not a recognized term and does not seem to have any relevance or meaning in the context of this discussion. If you could provide more context, I may be able to assist you better.<|im_end|>\n\nAs you can see from this non-sense conversation",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5127",
        "createdAt": "2024-01-25T22:52:16Z",
        "author": {
            "login": "CMorrison82z"
        }
    },
    {
        "title": "Does finetune support phi-2",
        "bodyText": "I recently ran a finetune on a mistral model and all seems great.\nHowever, when I run the same text on the phi-2, I obtain the following log when running a test prompt\n<main.log added as comment>\nmainly these two lines\n[1705465495] llama_apply_lora_from_file_internal: incompatible tensor dimensions (2560 and 4096); are you sure that this adapter is for this model?\n[1705465496] main: error: unable to load model\nIs there something I have to set to make the tensor dimensions the same.  When I run the same test on the mistral LORA\nllama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\nAlso note when I try to merge them I get a general assert\nllama.cpp\\export-lora.exe --model-base models\\phi-2.Q5_K_M.gguf --model-out models\\phi-2-ecsql.Q5_K_M.gguf --lora-scaled lora\\phi2\\ggml-lora-150-f32.gguf 1.0\nDevice 0: NVIDIA GeForce RTX 2070, compute capability 7.5, VMM: yes\nGGML_ASSERT: F:\\development\\GPT\\llama.cpp\\ggml.c:3190: ggml_can_repeat(b, a)\nphi2.main.log",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4995",
        "createdAt": "2024-01-17T05:28:08Z",
        "author": {
            "login": "candcconsulting"
        }
    },
    {
        "title": "Sampling settings can dramatically change generation throughput",
        "bodyText": "It looks like there's still some low hanging fruit when it comes to unoptimized sampler logic in llama.cpp.\n30% of the time is spent sampling for q8_0 Mistral 7b when generating 1024 tokens! (that is, if you use topk=0 or topk=32000 to avoid restricting the initial candidate set)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5073",
        "createdAt": "2024-01-22T08:21:38Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Fine tune data format for llama2 + prompting after exporting LORA",
        "bodyText": "Hi all,\nI'm using fine-tune on data where each entry looks like this:\n<s>\nInstructions:Please generate a conversation about daily activities\n and hobbies, with a focus on the speaker's interests. \nThe conversation should be 2-5 exchanges long and varied \n\n[{\"conversation\":[{\"speaker\":\"person1\",\"message\":\"Quelques choses que je fais tous les jours ?\"},\n{\"speaker\":\"person2\",\"message\":\"Je pr\u00e9f\u00e8re faire du sport, de la lecture et de la cuisine.\"},\n{\"speaker\":\"person1\",\"message\":\"Ah, c'est int\u00e9ressant. J'aime aussi faire du sport et de la \nmusique.\"}],\"conversation_summary\":\"A conversation about daily activities and hobbies, with a focus on sports and \nmusic.\"},{\"conversation\":[{\"speaker\":\"person1\",\"message\":\"Qu'est-ce que tu fais dans tes loisirs ?\"},\n{\"speaker\":\"person2\",\"message\":\"Je pr\u00e9f\u00e8re faire du yoga, de la peinture et de l'\u00e9criture.\"},\n{\"speaker\":\"person1\",\"message\":\"Ah, c'est tr\u00e8s calme. J'aime aussi faire du yoga et de la \nphotographie.\"}],\"conversation_summary\":\"A conversation about daily activities and hobbies, with a focus on relaxation \nand creative pursuits.\"},{\"conversation\":[{\"speaker\":\"person1\",\"message\":\"Qu'est-ce que tu fais dans tes loisirs ?\"},\n{\"speaker\":\"person2\",\"message\":\"Je pr\u00e9f\u00e8re faire du v\u00e9lo, de la musique et de l'histoire.\"},\n{\"speaker\":\"person1\",\"message\":\"Ah, c'est tr\u00e8s int\u00e9ressant. J'aime aussi faire du v\u00e9lo et de la \ncuisine.\"}],\"conversation_summary\":\"A conversation about daily activities and hobbies, with \na focus on outdoor pursuits and culinary interests.\"}]\n\nI'm using the <s> between each entry and I'm running the fine tune via:\n# finetune LORA adapter\n/Users/ahughes/git/llama.cpp/finetune \\\n        --model-base /Users/ahughes/git/LLMs/llama-2-7b.Q5_0.gguf \\\n        --checkpoint-in  /Users/ahughes/git/LLMs/llama-7-ft/chk-lora-llama2-7b.gguf-LATEST.gguf \\\n        --checkpoint-out /Users/ahughes/git/LLMs/llama-7-ft/chk-lora-llama2-7b.gguf-ITERATION.gguf \\\n        --lora-out /Users/ahughes/git/LLMs/llama-7-ft/chk-lora-llama2-7b.gguf-ITERATION.bin \\\n        --train-data \"llm.cpp.txt\" \\\n        --save-every 20 \\\n        --use-checkpointing \\\n        --ctx 600 \\\n        --epochs 10 \\\n        --threads 8\t\\\n        --sample-start \"<s>\"\n\nI run until the loss rate is < 0.02 or so, 20 ish epochs. But when I then run the model afterwards I seem to only get nonsense.\nFor example, I'll prompt:\nProvide 2 conversations about 'Basic Greetings and Introductions', each conversation should be 2-5\nexchanges long and varied\n\nand get a response:\nThe conversations should be about 'Basic Greetings and Introductions'\nA conversation about 'Basic Greetings and Introductions' would include:\nA conversation about 'Basic Greetings and Introductions' would NOT include:\nHere are 2 conversations about 'Basic Greetings and Introductions', each conversation should be 2-5 exchanges long and varied.\nThe conversations should be about 'Basic Greetings and Introductions' and would include:\nA conversation about 'Basic Greetings and Introductions' would NOT include:\n\nAny ideas on what Im doing wrong with the tuning?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4757",
        "createdAt": "2024-01-03T13:00:55Z",
        "author": {
            "login": "AoifeHughes"
        }
    },
    {
        "title": "Changing the number of experts with a Mixtral GGUF?",
        "bodyText": "I'm using ooba webui, and I notice that when I look at the Exllamav2 model loader, it has an option, 'Number of experts per token' for Mixtral that lets you set it to a different value to the usual value of 2.\nBut when I use the llama.cpp loader (because I'm using an 8bpp GGUF of Mixtral), that option isn't available.\nI want to see how good a response I can get from Mixtral, so I don't want to switch to a lower bpp so the model fits on my GPU, because that would make the response worse in a different way.\nIs there any way to get a higher number of experts while still using a GGUF?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5114",
        "createdAt": "2024-01-24T15:25:15Z",
        "author": {
            "login": "araleza"
        }
    },
    {
        "title": "Running on specific GPU",
        "bodyText": "How can I run llama cpp on specific GPU (I have few GPUs on my PC), both for main and server executables?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5117",
        "createdAt": "2024-01-24T16:43:15Z",
        "author": {
            "login": "rami2102"
        }
    },
    {
        "title": "Building llama.cpp via flutter on windows",
        "bodyText": "I'm having an issue when trying to build llama.cpp as part of the windows build of my flutter project Maid: https://github.com/MaidFoundation/Maid\nIf i build the project separately from llama.cpp it builds fine and if i build llama.cpp (and butler) separately from Maid they both build fine, its just when i try to combine the build process to have dll's build with maid that i get the error.\nI know from reading the error it looks like a permissions issue which would usually be rectified by running in an administrator CMD or power-shell on windows, but that doesn't work any way Ive tried it.\nThe project builds fine on Linux and Android and the CMake file for Linux is logically very similar to windows so i don't really see whats causing the issue.\nBelow is the error:\n[   +5 ms]   CMake Error at llama/cmake_install.cmake:41 (file):\n[        ]     file cannot create directory:\n[        ]     C:/maid/build/windows/x64/$<TARGET_FILE_DIR:maid>/lib.  Maybe need\n[        ]     administrative privileges.\n[        ]   Call Stack (most recent call first):\n[        ]     cmake_install.cmake:310 (include)\n[   +5 ms] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: The command \"setlocal [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.exe\" -DBUILD_TYPE=Release -P cmake_install.cmake [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: if %errorlevel% neq 0 goto :cmEnd [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :cmEnd [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :cmErrorLevel [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: exit /b %1 [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :cmDone [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: if %errorlevel% neq 0 goto :VCEnd [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :VCEnd\" exited with code 1. [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] Done Building Project \"C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj\" (default targets) -- FAILED.\n[        ] Build FAILED.\n[        ] \"C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj\" (default target) (1) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\ALL_BUILD.vcxproj\" (default target) (3) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\llama\\common\\common.vcxproj\" (default target) (5) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj\" (default target) (6) ->\n[        ] (ClCompile target) ->\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(1055,31): warning C4305: 'initializing': truncation from 'double' to 'float' [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(1205,40): warning C4566: character represented by universal-character-name '\\u0120' cannot be represented in the current code page (1252) [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(1206,40): warning C4566: character represented by universal-character-name '\\u010A' cannot be represented in the current code page (1252) [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(1207,40): warning C4566: character represented by universal-character-name '\\u0120' cannot be represented in the current code page (1252) [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(1208,40): warning C4566: character represented by universal-character-name '\\u010A' cannot be represented in the current code page (1252) [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(2244,60): warning C4566: character represented by universal-character-name '\\u010A' cannot be represented in the current code page (1252) [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(9753,28): warning C4146: unary minus operator applied to unsigned type, result still unsigned [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\llama.cpp(9783,28): warning C4146: unary minus operator applied to unsigned type, result still unsigned [C:\\maid\\build\\windows\\x64\\llama\\llama.vcxproj]\n[        ] \"C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj\" (default target) (1) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\ALL_BUILD.vcxproj\" (default target) (3) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\llama\\common\\common.vcxproj\" (default target) (5) ->\n[        ]   C:\\maid\\src\\llama.cpp\\common\\console.cpp(253,30): warning C4267: 'initializing': conversion from 'size_t' to 'DWORD', possible loss of data [C:\\maid\\build\\windows\\x64\\llama\\common\\common.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\common\\console.cpp(407,28): warning C4267: 'initializing': conversion from 'size_t' to 'int', possible loss of data [C:\\maid\\build\\windows\\x64\\llama\\common\\common.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\common\\common.cpp(785): warning C4715: 'gpt_random_prompt': not all control paths return a value [C:\\maid\\build\\windows\\x64\\llama\\common\\common.vcxproj]\n[        ] \"C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj\" (default target) (1) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\ALL_BUILD.vcxproj\" (default target) (3) ->\n[        ] \"C:\\maid\\build\\windows\\x64\\butler\\butler.vcxproj\" (default target) (8) ->\n[        ]   C:\\maid\\src\\llama.cpp\\common\\log.h(379,19): warning C4996: 'fopen': This function or variable may be unsafe. Consider using fopen_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details. [C:\\maid\\build\\windows\\x64\\butler\\butler.vcxproj]\n[        ]   C:\\maid\\src\\llama.cpp\\common\\log.h(387,97): warning C4996: 'strerror': This function or variable may be unsafe. Consider using strerror_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details. [C:\\maid\\build\\windows\\x64\\butler\\butler.vcxproj]\n[        ] \"C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj\" (default target) (1) ->\n[        ] (PostBuildEvent target) ->\n[        ]   C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: The command \"setlocal [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.exe\" -DBUILD_TYPE=Release -P cmake_install.cmake [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: if %errorlevel% neq 0 goto :cmEnd [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :cmEnd [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :cmErrorLevel [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: exit /b %1 [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :cmDone [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: if %errorlevel% neq 0 goto :VCEnd [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ] C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppCommon.targets(161,5): error MSB3073: :VCEnd\" exited with code 1. [C:\\maid\\build\\windows\\x64\\INSTALL.vcxproj]\n[        ]     13 Warning(s)\n[        ]     1 Error(s)\n[        ] Time Elapsed 00:00:16.92\n[  +16 ms] Building Windows application... (completed in 19.3s)\n[        ] \"flutter windows\" took 19,857ms.\n[   +2 ms] Build process failed.\n[        ]\n           #0      throwToolExit (package:flutter_tools/src/base/common.dart:10:3)\n           #1      _runBuild (package:flutter_tools/src/windows/build_windows.dart:253:5)\n           <asynchronous suspension>\n           #2      buildWindows (package:flutter_tools/src/windows/build_windows.dart:106:5)\n           <asynchronous suspension>\n           #3      BuildWindowsCommand.runCommand (package:flutter_tools/src/commands/build_windows.dart:56:5)\n           <asynchronous suspension>\n           #4      FlutterCommand.run.<anonymous closure> (package:flutter_tools/src/runner/flutter_command.dart:1350:27)\n           <asynchronous suspension>\n           #5      AppContext.run.<anonymous closure> (package:flutter_tools/src/base/context.dart:150:19)\n           <asynchronous suspension>\n           #6      CommandRunner.runCommand (package:args/command_runner.dart:212:13)\n           <asynchronous suspension>\n           #7      FlutterCommandRunner.runCommand.<anonymous closure> (package:flutter_tools/src/runner/flutter_command_runner.dart:348:9)\n           <asynchronous suspension>\n           #8      AppContext.run.<anonymous closure> (package:flutter_tools/src/base/context.dart:150:19)\n           <asynchronous suspension>\n           #9      FlutterCommandRunner.runCommand (package:flutter_tools/src/runner/flutter_command_runner.dart:294:5)\n           <asynchronous suspension>\n           #10     run.<anonymous closure>.<anonymous closure> (package:flutter_tools/runner.dart:112:9)\n           <asynchronous suspension>\n           #11     AppContext.run.<anonymous closure> (package:flutter_tools/src/base/context.dart:150:19)\n           <asynchronous suspension>\n           #12     main (package:flutter_tools/executable.dart:90:3)\n           <asynchronous suspension>\n\n\n[ +267 ms] ensureAnalyticsSent: 263ms\n[        ] Running 0 shutdown hooks\n[        ] Shutdown hooks complete\n[        ] exiting with code 1\nPS C:\\maid> flutter update\nCould not find a command named \"update\".\n\n\nRun 'flutter -h' (or 'flutter <command> -h') for available flutter commands and options.\nPS C:\\maid> flutter upgrade\nFlutter is already up to date on channel beta\nFlutter 3.16.0-0.2.pre \u2022 channel beta \u2022 https://github.com/flutter/flutter.git\nFramework \u2022 revision fe6553b689 (6 days ago) \u2022 2023-10-11 19:07:11 -0700\nEngine \u2022 revision 249cc9b86c\nTools \u2022 Dart 3.2.0 (build 3.2.0-210.2.beta) \u2022 DevTools 2.28.1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3790",
        "createdAt": "2023-10-26T01:57:45Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "A special token '\\u0000' will cause an assert error in 'llm_load_vocab'",
        "bodyText": "I'm trying to fit an InternLM2 model for llama.cpp, but I get an assertion error when using llama.cpp for inference, below is the error stack. The commit ID of llama.cpp code is 77bc1bb\n$  ./main -m  ./internlm2-base-7b/ggml-model-f16.gguf -n 400  -e -p \"Building a website can be done in 10 simple steps:\\nStep 1:\"\nLog start\nmain: build = 1930 (f8ca46e0)\nmain: built with gcc (GCC) 10.2.0 for x86_64-pc-linux-gnu\nmain: seed  = 1706096206\nllama_model_loader: loaded meta data with 17 key-value pairs and 227 tensors from ./internlm2-base-7b/ggml-model-f16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = internlm2\nllama_model_loader: - kv   1:                               general.name str              = InternLM\nllama_model_loader: - kv   2:                   internlm2.context_length u32              = 32768\nllama_model_loader: - kv   3:                      internlm2.block_count u32              = 32\nllama_model_loader: - kv   4:                 internlm2.embedding_length u32              = 4096\nllama_model_loader: - kv   5:              internlm2.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                   internlm2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   7:             internlm2.attention.head_count u32              = 32\nllama_model_loader: - kv   8: internlm2.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv   9:          internlm2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  10:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,92544]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  12:                      tokenizer.ggml.scores arr[f32,92544]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,92544]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  162 tensors\nGGML_ASSERT: llama.cpp:3074: codepoints_from_utf8(word).size() > 0\nNo symbol table is loaded.  Use the \"file\" command.\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\n0x00007fadd375c12c in waitpid () from /lib64/libpthread.so.0\nNo symbol \"frame\" in current context.\n\nI further checked and found that the token that caused the error was token \\u0000 in the InternLM2 vocabulary, which would be converted into string \"\\u0000\" by codepoints_from_utf8, which corresponds to the string terminator in C language, resulting in word  size is 0, causing the assertion here to report an error (because I added some debug code, the actual error line number is llama.cpp:3053)\n\n\nI tried to comment out the assertion at llama.cpp:3053, and the model could be generated normally without any other errors. So I would like to ask about the significance of this assertion. Can we relax the assertion conditions here? If I can't remove the assertion, I'd love some advice on how to get around it, thanks.\nI searched and found a dscussion similar to my problem, but I didn't get much information.\nHere is my sys & env info.\nCollecting environment information...\nPyTorch version: 1.13.1+cu117\nIs debug build: False\nCUDA used to build PyTorch: 11.7\nROCM used to build PyTorch: N/A\n\nOS: CentOS Linux 7 (Core) (x86_64)\nGCC version: (conda-forge gcc 13.1.0-0) 13.1.0\nClang version: Could not collect\nCMake version: version 3.26.4\nLibc version: glibc-2.17\n\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\nPython platform: Linux-3.10.0-957.el7.x86_64-x86_64-with-glibc2.17\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                256\nOn-line CPU(s) list:   0-255\nThread(s) per core:    2\nCore(s) per socket:    64\nSocket(s):             2\nNUMA node(s):          8\nVendor ID:             AuthenticAMD\nCPU family:            23\nModel:                 49\nModel name:            AMD EPYC 7H12 64-Core Processor\nStepping:              0\nCPU MHz:               2600.000\nCPU max MHz:           2600.0000\nCPU min MHz:           1500.0000\nBogoMIPS:              5199.78\nVirtualization:        AMD-V\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              512K\nL3 cache:              16384K\nNUMA node0 CPU(s):     0-15,128-143\nNUMA node1 CPU(s):     16-31,144-159\nNUMA node2 CPU(s):     32-47,160-175\nNUMA node3 CPU(s):     48-63,176-191\nNUMA node4 CPU(s):     64-79,192-207\nNUMA node5 CPU(s):     80-95,208-223\nNUMA node6 CPU(s):     96-111,224-239\nNUMA node7 CPU(s):     112-127,240-255\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc art rep_good nopl xtopology nonstop_tsc extd_apicid aperfmperf eagerfpu pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_l2 cpb cat_l3 cdp_l3 hw_pstate sme retpoline_amd ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip overflow_recov succor smca",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5111",
        "createdAt": "2024-01-24T14:23:34Z",
        "author": {
            "login": "SolenoidWGT"
        }
    },
    {
        "title": "When are matrices transposed for llama.cpp (since ggml_mul_mat expects the second matrix to be transposed) ?",
        "bodyText": "Given that comment in the ggml_mul_mat definition:\n    // A: k columns, n rows => [ne03, ne02, n, k]\n    // B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]\n    // result is n columns, m rows => [ne03 * x, ne02 * y, m, n]\n    GGML_API struct ggml_tensor * ggml_mul_mat(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n\nand given this discussion (ggerganov/ggml#563) which explicily transposes the second matrix before the mul_mat, it is clear that ggml consideres the second matrix argument to mul_mat as transposed.\nI'd like to know where/when the matrices are transposed for llama.cpp ? is this done offline during the pytorch -> gguf conversion ? at run-time when loading the weight data ?\nThanks !",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5098",
        "createdAt": "2024-01-23T16:06:25Z",
        "author": {
            "login": "hmarechal"
        }
    },
    {
        "title": "New hierarchical+parallel speculative decoding method, badly named \"Lookahead\"",
        "bodyText": "Paper: https://arxiv.org/abs/2312.12728v2\nSo instead of only drafting a single sequence, draft multiple sequences.\nThen the model can validate more than just one sequence.\nThey also use a trie to efficiently utilizes the KV cache.\nThis works because draft sequences will most likely share prefixes, just like in beam search.\nThe actual draft method is not explicitly mentioned and it can be generic.\nAlthough it seems it's mostly just n-gram lookup because the output is grounded (aka copied from the prompt/data).\nI think it is doable with the current API.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5091",
        "createdAt": "2024-01-23T04:16:13Z",
        "author": {
            "login": "bullno1"
        }
    },
    {
        "title": "Making quants by using the GPU",
        "bodyText": "Is it already, or would it be reasonably possible to use the compute capabilities of our GPUs to quantize models with Llama.CPP?\nThis considering notably the additional calculation brought by the new iMatrix feature when making guided quants, and notably the IQ ones (which are out of my i7-6700k league for quantizing 70b models in a reasonable time).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5087",
        "createdAt": "2024-01-22T22:56:41Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "Creating a minimal model loadable by llama.cpp",
        "bodyText": "As a side-project, I'm attempting to create a minimal GGUF model that can successfully be loaded by llama.cpp (through llama-cpp-python) - very much related to this question: #5038\nThe code that I'd like to successfully run is:\nimport sys\nfrom llama_cpp import Llama\nLlama(model_path=sys.argv[1:][0])\nor, somewhat similar from llama.cpp directly with ./llama.cpp/main -m minimal.gguf -I\nI do not expect actual inference to work, I'd just like for it to be able to load the model.\nLooking at gguf-py/gguf-py/examples/writer.py, I modified it as follows:\n#!/usr/bin/env python3\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\n# Necessary to load the local gguf package\n#sys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom gguf import GGUFWriter  # noqa: E402\n\n\n# Example usage:\ndef writer_example() -> None:\n    # Example usage with a file\n    gguf_writer = GGUFWriter(\"example.gguf\", \"llama\")\n\n    gguf_writer.add_name(\"py007_tinyllama-1.1b-chat-v0.3\")\n    gguf_writer.add_context_length(2048)\n    gguf_writer.add_embedding_length(2048)\n    gguf_writer.add_block_count(22)\n    gguf_writer.add_feed_forward_length(5632)\n    gguf_writer.add_rope_dimension_count(64)\n    gguf_writer.add_head_count(32)\n    gguf_writer.add_head_count_kv(4)\n    gguf_writer.add_layer_norm_rms_eps(0.000010)\n    gguf_writer.add_rope_freq_base(10000.000000)\n    gguf_writer.add_file_type(10)\n    gguf_writer.add_tokenizer_model(\"llama\")\n    gguf_writer.add_token_list([\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\"])\n    gguf_writer.add_token_scores([0.000000, 0.000000, 0.000000, 0.000000])\n    gguf_writer.add_token_types([2, 3, 3, 6])\n    gguf_writer.add_bos_token_id(1)\n    gguf_writer.add_eos_token_id(2)\n    gguf_writer.add_unk_token_id(0)\n    gguf_writer.add_quantization_version(2)\n\n    tensor1 = np.ones((32,), dtype=np.float32) * 100.0\n    tensor2 = np.ones((64,), dtype=np.float32) * 101.0\n    tensor3 = np.ones((96,), dtype=np.float32) * 102.0\n\n    gguf_writer.add_tensor(\"tensor1\", tensor1)\n    gguf_writer.add_tensor(\"tensor2\", tensor2)\n    gguf_writer.add_tensor(\"tensor3\", tensor3)\n\n    gguf_writer.write_header_to_file()\n    gguf_writer.write_kv_data_to_file()\n    gguf_writer.write_tensors_to_file()\n\n    gguf_writer.close()\n\n\nif __name__ == '__main__':\n    writer_example()\nThat code produces a GGUF file, that if dumped with llama.cpp/gguf-py/scripts/gguf-dump.py looks like this:\n* Loading: example.gguf\n* File is LITTLE endian, script is running on a LITTLE endian host.\n\n* Dumping 23 key/value pair(s)\n      1: UINT32     |        1 | GGUF.version = 3\n      2: UINT64     |        1 | GGUF.tensor_count = 3\n      3: UINT64     |        1 | GGUF.kv_count = 20\n      4: STRING     |        1 | general.architecture = 'llama'\n      5: STRING     |        1 | general.name = 'py007_tinyllama-1.1b-chat-v0.3'\n      6: UINT32     |        1 | llama.context_length = 2048\n      7: UINT32     |        1 | llama.embedding_length = 2048\n      8: UINT32     |        1 | llama.block_count = 22\n      9: UINT32     |        1 | llama.feed_forward_length = 5632\n     10: UINT32     |        1 | llama.rope.dimension_count = 64\n     11: UINT32     |        1 | llama.attention.head_count = 32\n     12: UINT32     |        1 | llama.attention.head_count_kv = 4\n     13: FLOAT32    |        1 | llama.attention.layer_norm_rms_epsilon = 9.999999747378752e-06\n     14: FLOAT32    |        1 | llama.rope.freq_base = 10000.0\n     15: UINT32     |        1 | general.file_type = 10\n     16: STRING     |        1 | tokenizer.ggml.model = 'llama'\n     17: [STRING]   |        4 | tokenizer.ggml.tokens\n     18: [FLOAT32]  |        4 | tokenizer.ggml.scores\n     19: [INT32]    |        4 | tokenizer.ggml.token_type\n     20: UINT32     |        1 | tokenizer.ggml.bos_token_id = 1\n     21: UINT32     |        1 | tokenizer.ggml.eos_token_id = 2\n     22: UINT32     |        1 | tokenizer.ggml.unknown_token_id = 0\n     23: UINT32     |        1 | general.quantization_version = 2\n\n* Dumping 3 tensor(s)\n      1:         32 |    32,     1,     1,     1 | F32     | tensor1\n      2:         64 |    64,     1,     1,     1 | F32     | tensor2\n      3:         96 |    96,     1,     1,     1 | F32     | tensor3\n\n(I straight-up took all KV values from TheBlokes tinyllm model)\nMy issue is that when I try to load that model up with llama.cpp, I get this:\n./llama.cpp/main -m example.gguf -i\nLog start\nmain: build = 1925 (381ee19)\nmain: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\nmain: seed  = 1705703041\nllama_model_loader: loaded meta data with 20 key-value pairs and 3 tensors from example.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\nllama_model_loader: - kv   2:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv   4:                          llama.block_count u32              = 22\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 10\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,4]       = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\"]\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,4]       = [0.000000, 0.000000, 0.000000, 0.000000]\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,4]       = [2, 3, 3, 6]\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:    3 tensors\nllama_model_load: error loading model: unordered_map::at: key not found\nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params: error: failed to load model 'example.gguf'\nmain: error: unable to load model\nI can't seem to locate the source of the error loading model: unordered_map::at: key not found message, perhaps because I haven't touched C++ in a really, really long time.\nDoes anyone have any pointers as to how to progress on this goal? Is there a better way to build a barebones minimal model?\nFull disclosure, details on the internals of actual models elude me - this here is me embarking on the journey to learn more about these internals ;)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5044",
        "createdAt": "2024-01-19T22:27:44Z",
        "author": {
            "login": "MadsRC"
        }
    },
    {
        "title": "Building llama.cpp for Android as a .so library",
        "bodyText": "Hello there, for the past week I've been trying to make llama.cpp use clblast in my android app (I'm using modified version of java bindings; https://github.com/kherud/java-llama.cpp). I don't really know much about cmake and make, and I'm wondering how would I trigger the build in android studio to get similar results to this: https://github.com/ggerganov/llama.cpp#building-the-project-using-termux-f-droid\nSo far I've tried:\n...\n# checkout llama.cpp\ninclude(FetchContent)\nFetchContent_Declare(\n        llama.cpp\n        GIT_REPOSITORY https://github.com/ggerganov/llama.cpp.git\n        GIT_TAG b1792\n)\nFetchContent_MakeAvailable(llama.cpp)\n\n# checkout CLBlast\nFetchContent_Declare(\n        CLBlast\n        GIT_REPOSITORY https://github.com/CNugteren/CLBlast.git\n        GIT_TAG 1.6.1\n)\nFetchContent_Populate(CLBlast)\n\n# Copy include/clblast.h from CLBlast to llama.cpp root directory todo\nconfigure_file(${clblast_SOURCE_DIR}/include/clblast.h ${llama.cpp_SOURCE_DIR}/clblast.h COPYONLY)\n\n\n# Copy headers from OpenBLAS to llama.cpp root directory (header files are currently copied directly from my device)\nconfigure_file(../cblas.h ${llama.cpp_SOURCE_DIR}/cblas.h COPYONLY)\nconfigure_file(../openblas_config.h ${llama.cpp_SOURCE_DIR}/openblas_config.h COPYONLY)\n\n# Set LLAMA_CLBLAST=1 for llama.cpp\nset(LLAMA_CLBLAST 1)\nadd_definitions(-DLLAMA_CLBLAST=1)\n#target_compile_definitions(llama PRIVATE LLAMA_CLBLAST=1)\n\n...\n\nI manually built the clblast library on device but it seems it's not using it ...\nDoes anybody know what am I missing here? Thanks :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4960",
        "createdAt": "2024-01-15T18:07:38Z",
        "author": {
            "login": "samolego"
        }
    },
    {
        "title": "Put back the old Q2_K quant as Q3_K_XS",
        "bodyText": "On a Llama 70b model, the old Q2_K allowed to gain 600MB compared to the Q3_K_S, for a minor bump in perplexity (< 1%), and that is precious for the 36GB VRAM users like me. I imagine that for lesser size of models, it can matter for users with less RAM as well.\nConsidering that it was working well, and that we now have XS & XXS quants, could we have it back in the form of a Q3_K_XS, @ikawrakow , and even have an intermediate Q3_K_XXS to fill the gap with the Q2_K & lower incrementation ?\nSome 70b models with 32k context capabilities start to appear, and it exists in smaller size also with various context lengths, and such granularity would be a great way to exploit them.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5055",
        "createdAt": "2024-01-21T01:44:21Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "Is there anyone run llamacpp on Jetson Orin SoC or other devices, how about the performance?",
        "bodyText": "I want to know the performance of 7b or 13b models on device chips, especially the first token latency",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5059",
        "createdAt": "2024-01-21T04:47:17Z",
        "author": {
            "login": "adamydwang"
        }
    },
    {
        "title": "How to convert fine-tuned model to GGUF ?",
        "bodyText": "Hey guys,\nI am trying to convert a fine-tuned model from huggingface jzdesign/falcon-finetune-midjourney-falcon .GGUF format. The fine-tuned one only contains a .json file that points to the base model and a .bin file\nwhich contains all the weights.\nmy folder structure:\n\\fine_tune\n-adapter_model.bin\n-adapter_model.json\nWhen I try to run\n!python llama.cpp/convert.py fine_tune/ \\\n  --outfile fine_tune_convert \\\n  --outtype q8_0\n\nI get\n/content/llama.cpp/gguf-py\nTraceback (most recent call last):\n  File \"/content/llama.cpp/convert.py\", line 1658, in <module>\n    main(sys.argv[1:])  # Exclude the first element (script name) from sys.argv\n  File \"/content/llama.cpp/convert.py\", line 1577, in main\n    model_plus = load_some_model(args.model)\n  File \"/content/llama.cpp/convert.py\", line 1343, in load_some_model\n    raise Exception(f\"Can't find model in directory {path}\")\nException: Can't find model in directory fine_tune\n\nSo far I followed the tutorial from [https://github.com//discussions/2948]\nIn the end, what I would like to have, is a file like falcon-finetune-midjourney-falcon.gguf\nDoes someone know how to solve this problem ?\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4997",
        "createdAt": "2024-01-17T12:06:40Z",
        "author": {
            "login": "heyili"
        }
    },
    {
        "title": "LLaMA* : non-uniform hidden state",
        "bodyText": "Description\nThis is an idea that we discussed recently with @ikawrakow and we are sharing it here in case it is of any interest to the community.\nThe original LLaMA model has constant hidden dimension throughout all the layers of the model. Based on some observations and intuition about the variable importance of the layers in the LLaMA architecture during quantization, we propose a slight change in the architecture that makes the size of the hidden state variable - i.e. it changes across the layers of the model.\nWe've brainstormed on a few variations of this idea and it can be expressed in different ways. Below is a schematic that illustrates one possible way to do it:\n\nLeft: original LLaMA 7B, Right: LLaMA* with increasing hidden dimension. L is the layer index, starting from 1.\nNote that the LLaMA* model can have ~x2 times less parameters for the same number of layers, depending on the specific implementation\nTensor shape changes, (L is the layer index, starting from 1):\n\ntoken_embd.weight, from [4096, 32000] to: [128, 32000]\nattn_q.weight, attn_k.weight, from [4096, 4096] to:\n\noption 1: [L*128, L*128]\noption 2: [L*128, 4096]\n\n\nattn_v.weight, from [4096, 4096] to:\n\noption 1: [L*128, L*128]\noption 2: [4096, L*128]\n\n\nffn_up, ffn_gate, from [4096, 11008] to [L*128, 11008]\nffn_down, from [11008, 4096] to [11008, L*128]\n\nOther shapes are obvious or remain the same.\nupscale operator\nLikely, instead of linear increase of the hidden dimension (L -> L+1) as shown in the figure, an increase by a factor of 2 every U number of layers would be better for example. This is not illustrated in the figure for simplicity, but the alternatives should be obvious. The exact implementation of the upscale operator is not specified, but ideas from other NN architectures could be borrowed. The most straight-forward upscaling could be down via matrix multiplication with an extra rectangular 2D tensor [in_embd, out_embd] learned during training for each layer.\nVariations\n\nThe hidden dimension does not need to increase linearly. It can increase as a power function and remain constants for a few layers at a time. It can also probably decrease and then increase (like an U shape). The main point is that it is no longer constant\nThe number of attention heads could remain constant (32) or change with the size of the hidden dimension\nThe hidden state above has dimension of 128 in the first layer, but other numbers might be tried. The main requirement is to select the hidden size and the increase function such that we end up with roughly half the number of parameters compared to vanilla LLaMA\n\nWhy would this work? Why is this better?\nProbably it is not. It's just something we intuitively think could lead to an improvement (for example, same or similar performance with less parameters) and we haven't seen proposed yet. If it has already been proposed, then just ignore this post. But other than this, it's just a hypothesis that might or might not be worth looking into.\nWe think it would be interesting if a small LLaMA* model is trained and compared to a vanilla LLaMA model. Probably the llama2.c project could be utilized for training, or maybe even the train tools in llama.cpp could be enough.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4147",
        "createdAt": "2023-11-20T14:13:08Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Minimal GGUF file",
        "bodyText": "While developing an application that uses llama.cpp, I ran into the issue of having to test model loading. Naturally, this requires an actual model to load, and for the time being I'm using TheBlokes TinyLlama Q2 GGUF model. That model was the smallest I could find, at around 482MB.\nI seem to remember seeing a minimal GGUF model used during the testing of llama.cpp, but I can't for the life of me figure out if I'm just imagining it.\nDoes anyone know of barebones GGUF model that will read as a proper GGUF, but be small enough to use during testing?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5038",
        "createdAt": "2024-01-19T12:40:44Z",
        "author": {
            "login": "mrh-chain"
        }
    },
    {
        "title": "I made a PHP integration with llama.cpp server",
        "bodyText": "Hello! I made an async PHP framework, and included llama.cpp integration to serve output from LLMs through WebSockets or just plain HTTP: https://resonance.distantmagic.com/tutorials/connect-to-llama-cpp/\nResonance is async PHP framework based on Swoole, aimed for SaaS systems, or being an infrastructure communication hub.\nThanks to your great work with continuous batching in llama.cpp, I think llama.cpp will be a go-to solution for enterprise SaaS, so I decided to include it as a default solution in my framework.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5028",
        "createdAt": "2024-01-18T22:17:31Z",
        "author": {
            "login": "mcharytoniuk"
        }
    },
    {
        "title": "Maximum Model Size for M2 Ultra with 196 GB RAM",
        "bodyText": "I'm currently exploring the capabilities of the M2 Ultra and its 192 GB RAM configuration. I've read that it's possible to fit the Llama 2 70B model. However, I'm curious if this is the upper limit or if it's feasible to fit even larger models within this memory capacity.\nAny insights or experiences regarding the maximum model size (in terms of parameters) that can comfortably fit within the 192 GB RAM would be greatly appreciated.\nThank you in advance!\nedit: 192 GB \ud83d\ude05",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3026",
        "createdAt": "2023-09-05T11:32:02Z",
        "author": {
            "login": "gileneusz"
        }
    },
    {
        "title": "Navigate prompt during the multiline input",
        "bodyText": "Is there an option to navigate the prompt during the multi line input? For example, i made a mistake in the prompt, I can delete something only within one line. I can't use the navigate buttons to return to the previous line to make changes here. As I see the keyboard is locked. Correct me if I'm wrong or provide the solution.\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/5001",
        "createdAt": "2024-01-17T15:11:28Z",
        "author": {
            "login": "alexcardo"
        }
    },
    {
        "title": "Force llama server to act exactly as llama in instruction mode.",
        "bodyText": "",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4999",
        "createdAt": "2024-01-17T14:01:36Z",
        "author": {
            "login": "alexcardo"
        }
    },
    {
        "title": "Should we add a ngram cache API to llama.cpp?",
        "bodyText": "Description\nThis idea was prompted from a recently proposed approach for speculative decoding: Prompt Lookup Decoding\nIn short, we draft tokens from the prompt using the last N ~ 3 generated tokens. With large prompt and repetitive text (code, summarization, etc.) this can trivially yield a significant inference speed-up.\nAn obvious extension is that we can search for draft tokens not just in the prompt but in a larger corpus of data if we had one 1. Additionally, the corpus could be dynamically updated with time based on the specific generations that occur locally. Maintaining such a corpus can obviously be done individually by each 3rd party project, but I'm wondering if it would be a good idea to create a basic implementation that ships with llama.cpp and can be used directly.\nAPI proposal\nThis is a work in progress - suggestions are welcome:\nstruct llama_ngram_cache;\nstruct llama_ngram_cache_params {\n    int ngram_size_max; // max size of stored n-grams\n    int ngram_capacity; // max number of n-grams\n    ...\n};\n\nllama_ngram_cache_init(struct llama_ngram_cache_params params);\nllama_ngram_cache_free(struct llama_ngram_cache * cache);\n\nllama_ngram_cache_load(const char * fname);\nllama_ngram_cache_save(const char * fname, struct llama_ngram_cache * cache);\n\n// add a corpus of tokens to be added to the cache\nllama_ngram_cache_add(struct llama_ngram_cache * cache, llama_token * tokens, int n_tokens);\n\n// query for n-gram\nllama_ngram_cache_get(struct llama_ngram_cache * cache, llama_token prefix, int n);\nSample usage\nWithout speculative cache:\n// standard decode of a new token\nllama_batch_clear(batch);\nllama_batch_add(batch, new_token_id, n_cur, { 0 }, true);\nllama_decode(ctx, batch);\nWith speculative cache\nnc = llama_ngram_cache_load(\"my-ngram-cache.gguf\");\n...\n// decode a new token + speculate 2 more\nllama_batch_clear(batch);\nllama_batch_add(batch, llama_ngram_cache_get(nc, new_token_id, 3), { 0 }, true);\nllama_decode(ctx, batch);\nImplementation considerations\n\nWe would need some efficient data structure to store the n-grams\nWe would like to have a maximum capacity of the cache. When we reach it, we should likely start to evict n-grams that are \"old\" or \"less-likely\"\nNeed to keep some kind of \"metric\" for each n-gram so we can rank which are \"better\"\nThe cache is model-specific - restrict this programatically?\n\nFootnotes\n\n\nhttps://twitter.com/mzh1024/status/1728978863890518158 (@wsxiaoys) \u21a9",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4235",
        "createdAt": "2023-11-27T11:51:59Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "HellaSwag scores using the perplexity tool",
        "bodyText": "The HellaSwag test\nThe test is done by using examples of sentences that makes sense and others which do not make sense. The sentences are presented to the model in groups (tasks) of four with the same start but different endings. One of the endings in each group is labeled as the one that makes most sense. The probability of the model to predict the different endings are computed and if the labeled ending got the highest probability it it considered a correct prediction. The resulting accuracy score is the percent of groups with correct predictions.\nMore info can be found in the paper https://arxiv.org/abs/1905.07830\nIn order to compute the HellaSwag scores you need to download the datafile here: klosax/hellaswag_text_data.\nSimple usage that runs the test on 400 random tasks in the datafile:\n./perplexity --hellaswag -f hellaswag_val_full.txt -m modelfile.gguf\nFor a more accurate test you can specify the number of tasks to use in the computation:\n./perplexity --hellaswag --hellaswag-tasks N -f hellaswag_val_full.txt -m modelfile.gguf\nThe table below is showing the score (0-shot acc_norm) for various models using 400 randomized HellaSwag tasks. The numbers in the \"Leaderboard\" column are taken from HuggingFace Open LLM Leaderboard. Note that the Leaderboard scores are about 1.5 higher because they are 10-shot. The \"Params\" column is the total number of elements in the model file and \"Train tokens\" are the total number of tokens the model was trained on.\n\n\n\nModel\nF16\nQ8_0\nQ5_1\nQ4_0\nQ2_K\nHF Leaderboard\nParams 10^9\nTrain tokens 10^9\n\n\n\n\nLLaMA-7b\n75.25\n75.25\n75.75\n73.75\n69.25\n77.80\n6.74\n1000\n\n\nLLaMA-13b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n80.90\n13.02\n1000\n\n\nLLaMA-33b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n84.70\n32.53\n1400\n\n\nLLaMA-65b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n86.10\n65.29\n1400\n\n\nLLaMA2-7b\n77.50\n77.50\n77.00\n77.25\n72.50\n78.60\n6.74\n2000\n\n\nLLaMA2-13b\n78.50\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n82.10\n13.02\n2000\n\n\nLLaMA2-70b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n87.30\n68.98\n2000\n\n\nOpenLLaMA-3b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\nn/a\n62.60\n3.43\n1000\n\n\nOpenLLaMA-7b\n70.75\n70.75\n70.50\n69.25\n65.50\n67.80\n6.74\n1000\n\n\nOpenLLaMA-13b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0n/a\n71.10\n13.02\n1000\n\n\nOpenLLaMAv2-3b\n69.75\n69.75\n68.25\n68.00\n\n71.60\n3.43\n1000\n\n\nOpenLLaMAv2-7b\n75.00\n75.25\n75.00\n74.00\n71.75\n72.20\n6.74\n1000\n\n\nFalcon-7b\n76.75\n76.75\u00a0\n76.25\u00a0\n74.50\u00a0\nn/a\u00a0\n78.10\n7.22\n1500\n\n\nFalcon-40b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n79.25\n85.30\n41.84\n1000\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nMPT-7b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n77.57\n6.65\n1000\n\n\nMPT-7b-8k\n75.25\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n6.65\n1500\n\n\nMPT-30b\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n82.40\n29.96\n1000\n\n\nGPT-J-6b\n64.50\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n67.60\n6.05\n402\n\n\nPythia-70m-dd\n26.25\n27.25\n23.50\n21.00\n\u00a0\n27.20\n0.07\n300\n\n\nPythia-160m-dd\n30.75\n31.00\n28.50\n28.00\n\u00a0\n31.40\n0.16\n300\n\n\nPythia-410m-dd\n40.25\n40.50\n40.00\n38.00\n\u00a0\n41.30\n0.41\n300\n\n\nPythia-1b-dd\n49.00\n49.25\n49.50\n48.25\n\u00a0\n49.70\n1.01\n300\n\n\nPythia-1.4b-dd\n54.25\n53.50\n54.50\n53.25\n\u00a0\n55.00\n1.41\n300\n\n\nPythia-2.8b-dd\n58.75\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n60.70\n2.78\n300\n\n\nPyhtia-6.9b-dd\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n67.00\n6.86\n300\n\n\nPythia-12b-dd\n68.25\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n70.30\n11.85\n300\n\n\nGPT2\n28.50\n29.75\n26.75\n31.00\n\u00a0\n31.60\n0.16\n?\n\n\nGPT2-Medium\n39.25\n40.75\n37.75\n37.25\n\u00a0\n40.20\n0.41\n?\n\n\nGPT2-Large\n43.00\n43.25\n43.00\n44.25\n\u00a0\n45.60\n0.84\n?\n\n\nGPT2-XL\n53.75\n53.75\n53.00\n52.50\n\u00a0\n51.40\n1.64\n?\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nRandom accuracy\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n25.00\n\u00a0\n\u00a0\n\n\nHuman accuracy\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n> 95.00\n\u00a0\n\u00a0\n\n\n\nThe second part of the table contains models not yet supported in llama.cpp, but support may be added in the future.\nThe HellaSwag scores are correlated to the number of model parameters:\n\nThe 400 task 0-shot HellaSwag scores are highly correlated to the OpenLLM Leaderboard 10-shot HellaSwag scores:\n\n(This post will be updated)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2321",
        "createdAt": "2023-07-22T13:13:05Z",
        "author": {
            "login": "klosax"
        }
    },
    {
        "title": "Getting GGML_ASSERT false && not implemented (LLAMA_MPI enabled build)",
        "bodyText": "Hello All, I followed the instructions in README to enable MPI and it fails with this message. I looked at the code and found that there is an  assertion.\n5834 #ifdef GGML_USE_MPI\n5835     // TODO: needs fix after #3228\n5836     GGML_ASSERT(false && \"not implemented\");\n5837     //ggml_mpi_eval_init(lctx.ctx_mpi, &n_tokens, &n_past, &n_threads);\n5838 #endif\nLet me know if I am doing anything wrong. The command line i am using is,\nmpirun  --hostfile machinefile -np 2 ./main -m  -p  -n 1024 -c 512\nI would appreciate all the help. Thanks in Advance",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4407",
        "createdAt": "2023-12-11T12:53:02Z",
        "author": {
            "login": "NoelVictor89"
        }
    },
    {
        "title": "Reuse Layers without consuming extra memory",
        "bodyText": "https://www.reddit.com/r/LocalLLaMA/comments/194zwyc/instant_frankenmerges_with_exllamav2/\nCan something similar to this thread be implemented in llama.cpp where you don't need to merge a model and load the the whole thing in RAM if you are just reusing layers from the same model. You can just load the layers from the base model once and do multiple forward passes on specific layers.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4956",
        "createdAt": "2024-01-15T08:42:36Z",
        "author": {
            "login": "LiquidGunay"
        }
    },
    {
        "title": "Apple MLX framework released",
        "bodyText": "I just stumbled upon this : https://github.com/ml-explore/mlx\n\"MLX is an array framework for machine learning on Apple silicon, brought to you by Apple machine learning research.\"\n\nCan someone help me understand, how will this affect llama.cpp and whisper.cpp?\nLooks like in the examples they quote those.\nCan we leverage this in our repos and make them even faster?\nBest,\nAdi",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4345",
        "createdAt": "2023-12-06T07:37:19Z",
        "author": {
            "login": "AdithyanI"
        }
    },
    {
        "title": "Is it possible to run very big llama model?",
        "bodyText": "big means that a single GPU is not able to hold all parameters, even after we use quantimization.\nIn this case, parallelism and communication is needed between GPUs",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2950",
        "createdAt": "2023-09-01T06:25:16Z",
        "author": {
            "login": "2catycm"
        }
    },
    {
        "title": "MoE gate logging",
        "bodyText": "would it be possible log gate layer model decision to understand what percentage of each expert is responsible for each token?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4907",
        "createdAt": "2024-01-13T07:50:38Z",
        "author": {
            "login": "LorenzoBoccaccia"
        }
    },
    {
        "title": "mixtral-8x7b-instruct-v0.1.Q4_0.gguf perf on MacBook Pro M3 Max 36GB vs Xeon 3435X 256GB 2x 20GB RTX 4000 GPUs",
        "bodyText": "I hope this is the right place to ask, otherwise please advise where to put.\nIn LM Studio I tried mixtral-8x7b-instruct-v0.1.Q4_0.gguf on a MacBook Pro M3 Max 36GB and a Xeon 3435X 256GB 2x 20GB RTX 4000 GPUs and 20 (of the 32) layers offloaded to the 2 GPUs.\nHard to believe the M3 with 30 tokens/s is 2x faster than the Xeon. Is Apple Silicon simply better optimized or what parameters to tweak on the Xeon? (I see threads is at 4 by default and it's a 32 thread CPU.)\nThanks for any help\nG.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4743",
        "createdAt": "2024-01-02T22:05:08Z",
        "author": {
            "login": "ai-bits"
        }
    },
    {
        "title": "Chat with PDF, Doc",
        "bodyText": "One of the biggest use case of LLMs especially for businesses is chatting with PDF and Docs privately.\nWould it be difficult to add this as feature in llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4021",
        "createdAt": "2023-11-10T06:22:32Z",
        "author": {
            "login": "aiaicode"
        }
    },
    {
        "title": "Project status",
        "bodyText": "[NO LONGER UPDATED]\nBelow is a summary of the functionality provided by the llama.cpp project.\n\nThe goal is to have a birds-eye-view of what works and what does not\nCollaborators are encouraged to add things to the list and update the status of existing things as needed\nThe list should be simple without too much details about the specific problems - these belong to dedicated issues\n\nLegend (feel free to update):\n\u2705 - Working correctly\n\u2601\ufe0f - Partially working\n\u274c - Failing\n\u2753 - Status unknown (needs testing)\n\ud83d\udd2c - Under investigation\n\ud83d\udea7 - Currently in development\n\n\n\nFeature\nExecutable\nStatus\nIssues\n\n\n\n\nInference\n\n\n\n\n\nSingle-batch decoding\nmain, simple\n\u2705\n\n\n\nParallel / batched decoding\nbatched\n\u2705\n\n\n\nContinuous batching\nparallel\n\u2705\n\n\n\nSpeculative sampling\nspeculative\n\u2705\n\n\n\nTree-based speculative sampling\nspeculative\n\u2705\n\n\n\nSelf-speculative sampling\nspeculative\n\ud83d\udea7\n#3565\n\n\nLookahead sampling\nlookahead\n\u2705\n\n\n\nInfill\ninfill\n\u2705\n\n\n\nREST API\nserver\n\u2705\n\n\n\nEmbeddings\nembedding\n\u2705\n\n\n\nGrouped Query Attention CPU\nmain\n\u2705\n\n\n\nGrouped Query Attention CUDA\nmain\n\u2705\n\n\n\nGrouped Query Attention OpenCL\nmain\n\u2705\n\n\n\nGrouped Query Attention Metal\nmain\n\u2705\n\n\n\nSession load / save\nmain\n\u2705\n\n\n\nK-quants (256) CUDA\nmain\n\u2705\n\n\n\nK-quants (64) CUDA\nmain\n\u2705\n\n\n\nK-quants (256) Metal\nmain\n\u2705\n\n\n\nK-quants (64) Metal\nmain\n\u2601\ufe0f\n#3276\n\n\nSpecial tokens\nmain\n\u2705\n\n\n\nGrammar sampling\nmain, server\n\u2705\n\n\n\nBeam search\nbeam-search\n\u2753\n#3471 (comment)\n\n\nLoRA\nmain\n\u2601\ufe0f\n#3333 #3519\n\n\nSPM tokenizer\ntest-tokenizer-0-llama\n\u2705\n\n\n\nBPE tokenizer\ntest-tokenizer-0-falcon\n\u2705\n\n\n\nModels\n\n\n\n\n\nLLaMA v1\nmain\n\u2705\n\n\n\nLLaMA v2\nmain\n\u2705\n\n\n\nFalcon\nmain\n\u2705\n\n\n\nStarCoder\nmain\n\u2705\n\n\n\nBaichuan\nmain\n\u2705\n\n\n\nMPT\nmain\n\u2705\n\n\n\nPersimmon\nmain\n\u2705\n\n\n\nLLaVA\nllava\n\u2705\n\n\n\nRefact\nmain\n\u2705\n\n\n\nBloom\nmain\n\u2705\n\n\n\nStableLM-3b-4e1t\nmain\n\u2705\n\n\n\nTraining\n\n\n\n\n\nFinetuning CPU\nfinetune\n\u2705\n\n\n\nFinetuning Metal\nfinetune\n\ud83d\udd2c\n\n\n\nBackends\n\n\n\n\n\nCPU x64\nggml\n\u2705\n\n\n\nCPU Arm\nggml\n\u2705\n\n\n\nGPU CUDA\nggml-cuda\n\u2705\n\n\n\nGPU ROCm\nggml-cuda\n\u2705\n\n\n\nGPU Metal\nggml-metal\n\u2705\n\n\n\nGPU OpenCL\nggml-opencl\n\u2705\n\n\n\nGPU Vulkan\nggml-vulkan\n\ud83d\udea7\n#2059",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3471",
        "createdAt": "2023-10-04T13:49:51Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Support for intfloat/e5-mistral-7b-instruct",
        "bodyText": "In trying to wrap my head around this, I think I've found that llama.cpp would need to support two new features to get this embedding model to work optimally:\n\n\nwe need away to probe the values at the last layer, before the LM head (or ideally skip the LM head all together). Does the current embedding endpoint do exactly that? I couldn't fully follow where it grabs its values from\n\n\nWe need a way to pass in an attention mask along with the batch of inputs, or calculate one.\n\n\nBefore I explore this more, am I on the right path here that a) llama doesn't currently have these features and b) they are needed and c) they are in theory sufficient (or close to it) to get e5-mistral to work as intended.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4863",
        "createdAt": "2024-01-10T18:04:28Z",
        "author": {
            "login": "nathanpbell"
        }
    },
    {
        "title": "How to run multiple prompts?",
        "bodyText": "I am running llamav2, I have a file with multiple prompts in seperate lines, but the model seems to intepret them as one single prompt, how should I prepare my prompts?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4855",
        "createdAt": "2024-01-10T10:39:06Z",
        "author": {
            "login": "Mr-Jeffery"
        }
    },
    {
        "title": "ggml_repeat no cuda impl",
        "bodyText": "I'm seeing a number of embedding models based off of\nhttps://github.com/skeskinen/bert.cpp\nIncluding this\nhttps://github.com/xyzhang626/embeddings.cpp\nWhich implements BGE.  But none of them are GPU accelerated.  It seems that they all rely on ggml_repeat which doesn't have a cuda implementation, so we are stuck constantly going back and forth to the GPU if we were to put the other layers on the GPU.  Do I have that right?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4841",
        "createdAt": "2024-01-09T15:18:57Z",
        "author": {
            "login": "SpaceCowboy850"
        }
    },
    {
        "title": "Better quantized models for Mixtral-8x7b",
        "bodyText": "I have published improved quantizations for Mixtral-8x7b  on Huggingface.\nFor more details see #4364.\nNote, these are for the base, not instruct tuned, Mixtral-8x7b (https://huggingface.co/mistralai/Mixtral-8x7B-v0.1). I'm planning to spend some time learning how to best quantize chat/instruct tuned models next.\nThe table below shows a comparison between these models and the current llama.cpp quantization approach using Wikitext perplexities for a context length of 512 tokens.\nThe \"Quantization Error\" columns in the table are defined as (PPL(quantized model) - PPL(int8))/PPL(int8).\nRunning the full fp16 Mixtral8x7b model on the systems I have available takes too long, so I'm comparing against the 8-bit quantized model, where I get PPL = 4.1049 (but from past experience the 8-bit quantization should be basically equivalent to fp16).\n\n\n\nQuantization\nModel file\nPPL(llama.cpp)\nQuantization Error\nPPL(new quants)\nQuantization Error\n\n\n\n\nQ2_K\nmixtral-8x7b-q2k.gguf\n7.4660\n81.9%\n5.0576\n23.2%\n\n\nQ3_K_S\nmixtral-8x7b-q3k-small.gguf\n4.4601\n8.65%\n4.3848\n6.82%\n\n\nQ3_K_M\nmixtral-8x7b-q3k-medium.gguf\n4.4194\n7.66%\n4.2884\n4.47%\n\n\nQ4_K_S\nmixtral-8x7b-q4k-small.gguf\n4.2523\n3.59%\n4.1764\n1.74%\n\n\nQ4_K_M\nmistral-8x7b-q4k-medium.gguf\n4.2523\n3.59%\n4.1652\n1.47%\n\n\nQ5_K_S\nmixtral-7b-q5k-small.gguf\n4.1395\n0.84%\n4.1278\n0.56%\n\n\nQ4_0\nmixtral-8x7b-q40.gguf\n4.2232\n2.88%\n4.2001\n2.32%\n\n\nQ4_1\nmistral-8x7b-q41.gguf\n4.2547\n3.65%\n4.1713\n1.62%\n\n\nQ5_0\nmistral-8x7b-q50.gguf\n4.1426\n0.92%\n4.1335\n0.70%",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4800",
        "createdAt": "2024-01-06T17:25:37Z",
        "author": {
            "login": "ikawrakow"
        }
    },
    {
        "title": "how to use convert.py",
        "bodyText": "I wanna convert \"flozi00/Mistral-7B-german-assistant-v5\" with 3 safetensors files to GGUF ?\nWhat is the command line for that?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4426",
        "createdAt": "2023-12-12T13:09:57Z",
        "author": {
            "login": "Gee1111"
        }
    },
    {
        "title": "Borrow ideas from LLM in a flash",
        "bodyText": "I'm wondering if any of the techniques proposed in the following paper could be implemented here: https://huggingface.co/papers/2312.11514\nhttps://arxiv.org/abs/2312.11514\nThis goes above my level of understanding, but I'm wondering if they give enough technical details to run this in a new example. The results were that they were able to load a model that was twice the size of available DRAM with major increases in inference speed. So figured I'd make a discussion post as an initial seed and see where this idea goes.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4582",
        "createdAt": "2023-12-22T03:29:12Z",
        "author": {
            "login": "bachittle"
        }
    },
    {
        "title": "Interesting paper - LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
        "bodyText": "https://arxiv.org/abs/2401.01325\n\nWe propose Self-Extend to elicit LLMs\u2019 inherent long context capabilities. To overcome the positional O.O.D issue, Self-Extend uses the simple FLOOR (//) operation as the mapping function to map unseen large relative positions to those encountered during pretraining. This idea stems from two intuitions: 1) For texts with a long distance between words, the exact position does not need to be precise. It is sufficient to understand the overall meaning of the text as long as the relative ordering of the different parts is maintained. When answering a question about information from a lengthy text, we never remember the recise position of each word, just the general position and order of the relevant information. Since natural language texts tend to have similar semantics within a short range (e.g. a paragraph), close or even equal position encodings should be adequate for maintaining the relative ordering of useful information. This aligns with the floor operation. 2) In natural language texts, most of the time, while a small bag of words (n-grams) appears together in one area, all the tokens in that bag have only one possible order due to the conventions of the language grammar. Although theoretically, a bag of tokens could appear in any order, in practice it is rare for a small set of words to have more than one sensible ordering. For example, \u201dunnecessary encodings\u201d can be tokenized as \u201dunn\u201d, \u201decessary\u201d, \u201d enc\u201d and \u201dodings\u201d2, but these tokens can only meaningfully appear in that order. This suggests that maintaining precise position information is unnecessary in a small region, which also aligns with the floor operation.\n\nThe proposed algorithm\n\nPerplexity results\n\nIt appears pretty simple to implement and promises good results, so perhaps it's worth giving it a try. Authors limited themselves to 16k context lengths due to hardware constraints, so it's not clear how well this scales beyond 16k.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4785",
        "createdAt": "2024-01-05T15:05:55Z",
        "author": {
            "login": "Galunid"
        }
    },
    {
        "title": "Llama.cpp support in ModelFusion library (JS/TS)",
        "bodyText": "ModelFusion provides a JS/TS client for Llama.cpp.\nIt supports:\n\ntext generation\ntext streaming\nstructure generation with automatic gbnf creation from json schema\nsame for structure streaming\ntext embeddings\ntokenization\nprompt templates for models\n\nHere's an example for structure generation:\nconst structure = await generateStructure(\n  llamacpp\n    .CompletionTextGenerator({\n      // run openhermes-2.5-mistral-7b.Q4_K_M.gguf in llama.cpp\n      promptTemplate: llamacpp.prompt.ChatML,\n      maxGenerationTokens: 1024,\n      temperature: 0,\n    })\n    // automatically restrict the output to your schema using GBNF:\n    .asStructureGenerationModel(jsonStructurePrompt.text()),\n\n  zodSchema(\n    z.object({\n      characters: z.array(\n        z.object({\n          name: z.string(),\n          class: z\n            .string()\n            .describe(\"Character class, e.g. warrior, mage, or thief.\"),\n          description: z.string(),\n        })\n      ),\n    })\n  ),\n\n  \"Generate 3 character descriptions for a fantasy role playing game. \"\n);",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2517",
        "createdAt": "2023-08-04T18:18:00Z",
        "author": {
            "login": "lgrammel"
        }
    },
    {
        "title": "GGML_ASSERT: llama.cpp:5403: n_embd_gqa == n_embd",
        "bodyText": "I am trying to start a code completion server locally with the llama-cpp-python wrapper, see here.\nI am currently running this to start the server: python3 -m llama_cpp.server --model models/replit-code-v1_5-3b.Q4_0.gguf --n_ctx 16192 --n_gpu_layers -1 --use_mlock False and the program crashes with the error message\n...\nllm_load_tensors: ggml ctx size       =    0.07 MiB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: system memory used  =   54.07 MiB\nllm_load_tensors: VRAM used           = 1807.51 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\n.........................................................................................\nllama_new_context_with_model: n_ctx      = 16192\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model: KV self size  = 2024.00 MiB, K (f16): 1012.00 MiB, V (f16): 1012.00 MiB\nGGML_ASSERT: (long path)/llama.cpp:5403: n_embd_gqa == n_embd`\n\nNow I realize I am using a downstream library to get to llama.cpp, but I figured that somebody here might be able to tell me what the assertion above even means. It might then be obvious what I'm doing wrong.\nThanks in advance",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4795",
        "createdAt": "2024-01-06T13:33:24Z",
        "author": {
            "login": "Tillerino"
        }
    },
    {
        "title": "Roadmap (short-term)",
        "bodyText": "These will be the priorities for the next few days:\n\n\n Reduce inference memory usage via ggml scratch buffers, no hardcoded memory buffer sizes and support infinite interactive mode\nI know how to fix this and this is important since the GH issues are being flooded with complaints about seg faults and crashes\n\n\n Finalize SIMD accelerated quantization and merge ggml back in the parent repo:\n\n AVX\n\n quantize_row_q4_0()\n quantize_row_q4_1()\n dequantize_row_q4_0()\n dequantize_row_q4_1()\n\n\n ARM_NEON\n\n quantize_row_q4_0()\n quantize_row_q4_1()\n dequantize_row_q4_0()\n dequantize_row_q4_1()\n\n\n\nI suspect this could improve performance for prompt batch processing\n\n\n Deprecate ggml_vec_mad_xxx() routines and simplify ggml_forward_mul_mat_xxx()\nThis should lead to some significant code reduction in ggml.c\n\n\n  Separate the perplexity computation from main.cpp into standalone example program called perplexity\n\n\n Move main.cpp into a standalone example program and move utils.h/utils.cpp into ./examples to be shared by all examples\n\n\n Add llama_state to allow parallel text generation sessions with a single model\nI will do this in a similar way it is done in whisper.cpp\n\n\n Extend llama_state to support loading individual model tensors. Needed for LoRA personalities support\n\n\n Add 2-bit integer quantization\n\n\n\nWhen the above things are ready we will have a good foundation to start porting more models and create more example applications to demonstrate the usage of ggml.\nNew roadmap: #784",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/457",
        "createdAt": "2023-03-24T07:11:56Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Prompt Cache",
        "bodyText": "When you start the program, it has to load the model, then tokenize the prompt and run it through the model. The time to load the model has been mostly eliminated. Tokenizing is also very fast. It would be nice if it were possible to cache the result of running the prompt through the model and load it on startup as well.\nI write long prompts, because it gives better results, but it can take a few minutes for the whole thing to process before the window is responsive. With this change, it should be nearly instant.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/745",
        "createdAt": "2023-04-03T18:53:33Z",
        "author": {
            "login": "apaz-cli"
        }
    },
    {
        "title": "When will the mixtral branch be merged to master?",
        "bodyText": "Currently mixtral8x7B is a fantastic model, which is supported on mixtral branch. But it's not supported on the related bindings, such as llama-cpp-python. Once it's merged on master,  it shall be supported quickly by the related bindings. great thanks to everyone contributing this project.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4779",
        "createdAt": "2024-01-05T01:35:58Z",
        "author": {
            "login": "AaronYFQ"
        }
    },
    {
        "title": "HQQ (Half-Quadratic Quantization of Large Machine Learning Models)",
        "bodyText": "@ggerganov https://mobiusml.github.io/hqq_blog/ \ud83e\udd14",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4806",
        "createdAt": "2024-01-07T09:48:23Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "Anybody tried LLMLingua ?",
        "bodyText": "https://github.com/microsoft/LLMLingua/tree/main\nIt tried to compress prompt and document so can be much smaller, I just wondered if we can use it with llama cpp (gguf)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4711",
        "createdAt": "2023-12-31T04:54:53Z",
        "author": {
            "login": "x4080"
        }
    },
    {
        "title": "Layer-wise Inference that reduce greatly reduce memory usage",
        "bodyText": "AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card. No quantization, distillation, pruning or other model compression techniques that would result in degraded model performance are needed.\nhttps://github.com/lyogavin/Anima/tree/main/air_llm\nKind of interesting method for inference reducing memory usage\nhttps://huggingface.co/blog/lyogavin/airllm\nI was wondering if this can be implement in llama.cpp in some way so that make small vram GPU usable.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4310",
        "createdAt": "2023-12-03T17:41:58Z",
        "author": {
            "login": "sorasoras"
        }
    },
    {
        "title": "DDR3 vs SSD PCIE 5.0",
        "bodyText": "With almost the same speed, which hints...",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3260",
        "createdAt": "2023-09-19T11:55:03Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Separate thread counts for prompt eval and generation",
        "bodyText": "It seems that depending on the system, the optimal thread count might not be the same for prompt eval, and token prediction. For example, here are the llama-bench result for my old 4-cores laptop:\n\nAs we can see, the fastest prompt eval seels to be achived with only 2 threads, whereas the fastest token generation is with 4 threads.\nSo I was wondering if it was feasible to have two variants of the --threads argument to optimize for speed on each system?\nI haven't looked into the details of the implementation, so maybe it is required to have the same threads for eval and generation, but if it's not the case, I think it would be a nice improvement.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4760",
        "createdAt": "2024-01-03T15:49:36Z",
        "author": {
            "login": "stduhpf"
        }
    },
    {
        "title": "I made a janky-ish bash script that gives a LLM live access to a shell, calculator, notepad, time etc",
        "bodyText": "edit I made a github:\ngithub:\nhttps://github.com/skidd-level-100/jankllama\nDisclaimer:\nthis was written in like 5 mins before going to sleep, it is going to be terribly written.\nI am still working on it but its very impressive so far, for example I have asked it to scan my network with nmap count the number of devices then multiply it by 4 and get the square root of that number (in similar wording to that) and it did it, chained commands together used my calculator function and got the right answer.\nthe way it has \"live\" time access is everytime the user inputs something the time is attached to your message, it also can run the date command (or any non-interactive bash/linux utility available in a container) on a whim, and of coarse its shell access is in a locked down resource limited read-only container(podman).\nit taking notes well is in progress (works but needs more prompt engineering and maybe background automation)\nit is pretty quick to run (slower than vanilla) has resume-able sessions (although slower the longer the ctx, not by a ton)\nI'll most likely make a python version, but for now the easy system integration to linux is REALLY nice to have with bash.\nhow it works (rough 1 minute write up)\nit runs in non interactive mode with reverse prompts to end it when it is done marking a function to run\nexample:\n\"\nbot:\nbla bla bla bla, I will run a shell command\nCMD\n\"\ndue to llama cmd options '-r ' and it being non-interactive it ends the program passes the output to my bash script the script picks up the command runs it on a container (killing it after 3 seconds in case of loop)  and passes the output back into the stored prompt with some fancy labeling, then re runs llama on the modified context and waits.\nsince its \"non-interactive\" the bot will pass a 'userinput into the screen after it needs feedback or more instructions, then the script adds some fancy labeling before and after your message, opens the stored output in your default terminal editor (this is nice for being able to edit anything it has/will say and fixing errors) once you save the file and close your editor the modified context gets passed back to llama. This is also nice because the bot can chain commands and functions together without user input interrupting.\nanyways guys its pretty jank for now and built in 5~ hours (mostly prompt engineering the script itself is like 80 lines) or so but in theory when I release it the code quality wont totally suck.\nlet me know any functions you would like to see the LLM be able to pass arguments into it should be \"easy\" to implement them in a semi modular fashion.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4731",
        "createdAt": "2024-01-02T06:50:14Z",
        "author": {
            "login": "skidd-level-100"
        }
    },
    {
        "title": "PowerInfer: Built on llama.cpp, Now 11x Faster",
        "bodyText": "I think the main breakthrough is that it can arrange the position of weight parameters more scientifically based on the frequency of neuron activation, placing the frequently activated weights in faster-reading caches to improve inference speed. They developed a Neuron-aware Operator that can bypass neurons that are not activated, and also developed an offline profiling technique that allows the Neuron Placement Policy to be predetermined.\nGithub: https://github.com/SJTU-IPADS/PowerInfer\nPaper: https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf\nRelevant discussions: #4543 #4542 #4534",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4548",
        "createdAt": "2023-12-20T17:17:00Z",
        "author": {
            "login": "bobqianic"
        }
    },
    {
        "title": "Opinions about template based GEMM kernels",
        "bodyText": "Hi, this is Mingfei from intel pytorch team and we want to help optimize the performance of llama.cpp on intel hardware. I need some guidelines about how to make contributions in this project:\n\nFirstly about the intel Xe GPU: the programming language is SYCL and also we have a template based GEMM solution called XeTLA (you can consider they are counterparts to cuda and cutlass). So I was wondering is it proper to use XeTLA in this project, or is there any plan to integrate nvidia's cutlass?\nSecond question is about the x86 CPU optimization: is it proper to use jitted kernels in this project? e.g. xbyak which is a C++ JIT assembler for x86. Some of my colleagues already did some good work in jblas which shows a pretty good result.\n\nAny opinion is welcome :) feel free to comment so that we can find the most proper manner to contribute.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3965",
        "createdAt": "2023-11-06T06:07:02Z",
        "author": {
            "login": "mingfeima"
        }
    },
    {
        "title": "Is there a way to let the model have access to the current time in case I ask what time is it?",
        "bodyText": "The thing is I am running a model from Hugging face dolphin mistral but it doesn't have access to current time, how could I go on solving my issue so it has access to current time when running on chat or -cml, thanks in advance",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4730",
        "createdAt": "2024-01-02T03:13:10Z",
        "author": {
            "login": "joseiriarte1982"
        }
    },
    {
        "title": "Does anyone have a plan to support BlueLM-7B-Chat?",
        "bodyText": "BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab.\nIn this article, there is an example of supporting BlueLM by modifying some code. When will the official support for BlueLM be available?\nAlthough the title of the article is Yi-6B-200K, in the fourth section, an example using BlueLM-7B-Chat is introduced to demonstrate how to convert a model not yet supported by llama.cpp into a gguf file and perform inference using llama.cpp.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4734",
        "createdAt": "2024-01-02T10:24:53Z",
        "author": {
            "login": "limour-blog"
        }
    },
    {
        "title": "llama-cpp-python agent framework for chat, structured output and function calling",
        "bodyText": "llama-cpp-agent Framework\nIntroduction\nThe llama-cpp-agent framework is a tool designed for easy interaction with Large Language Models (LLMs). It provides a simple yet robust interface using llama-cpp-python, allowing users to chat with LLM models, execute structured function calls and get structured output.\nKey Features\n\nSimple Chat Interface: Engage in seamless conversations with LLMs.\nStructured Output: Get structured output from LLMs.\nFunction Calling: Execute structured outputs from LLMs, enhancing the interaction capabilities.\nFlexibility: Suited for various applications from casual chatting to specific function executions.\n\nInstallation\nTo get started with the llama-cpp-agent LLM framework, follow these steps:\n\nEnsure you have Python installed on your system.\nClone the repository from GitHub link.\nInstall the necessary dependencies as listed in the requirements.txt file.\n\nUsage Examples\nSimple Chat Example\nThis example demonstrates how to initiate a chat with an LLM model.\nimport json\nfrom llama_cpp import Llama\nfrom llama_cpp_agent.llm_agent import LlamaCppAgent\nfrom llama_cpp_agent.messages_formatter import MessagesFormatterType\nmain_model = Llama(\n    \"../gguf-models/dpopenhermes-7b-v2.Q8_0.gguf\",\n    n_gpu_layers=35,\n    f16_kv=True,\n    use_mlock=False,\n    embedding=False,\n    n_threads=8,\n    n_batch=1024,\n    n_ctx=8192,\n    last_n_tokens_size=1024,\n    verbose=False,\n    seed=42,\n)\nwrapped_model = LlamaCppAgent(main_model, debug_output=True,\n                              system_prompt=\"You are an advanced AI assistant.\", predefined_messages_formatter_type=MessagesFormatterType.CHATML)\n\nwrapped_model.get_chat_response('Write a long poem about the USA.', temperature=0.7)\nStructured Output\nThis example shows how to get structured JSON output.\nfrom enum import Enum\n\nfrom llama_cpp import Llama, LlamaGrammar\nfrom pydantic import BaseModel, Field\n\nfrom llama_cpp_agent.llm_agent import LlamaCppAgent\nfrom llama_cpp_agent.gbnf_grammar_generator.gbnf_grammar_from_pydantic_models import generate_gbnf_grammar_and_documentation\n\nmain_model = Llama(\n    \"../gguf-models/dpopenhermes-7b-v2.Q8_0.gguf\",\n    n_gpu_layers=35,\n    f16_kv=True,\n    use_mlock=False,\n    embedding=False,\n    n_threads=8,\n    n_batch=1024,\n    n_ctx=8192,\n    last_n_tokens_size=1024,\n    verbose=False,\n    seed=-1,\n)\n\n\ntext = \"\"\"The Feynman Lectures on Physics is a physics textbook based on some lectures by Richard Feynman, a Nobel laureate who has sometimes been called \"The Great Explainer\". The lectures were presented before undergraduate students at the California Institute of Technology (Caltech), during 1961\u20131963. The book's co-authors are Feynman, Robert B. Leighton, and Matthew Sands.\"\"\"\n\n\nclass Category(Enum):\n    Fiction = \"Fiction\"\n    NonFiction = \"Non-Fiction\"\n\n\nclass Book(BaseModel):\n    \"\"\"\n    Represents an entry about a book.\n    \"\"\"\n    title: str = Field(..., description=\"Title of the book.\")\n    author: str = Field(..., description=\"Author of the book.\")\n    published_year: int = Field(..., description=\"Publishing year of the book.\")\n    keywords: list[str] = Field(..., description=\"A list of keywords.\")\n    category: Category = Field(..., description=\"Category of the book.\")\n    summary: str = Field(..., description=\"Summary of the book.\")\n\n\ngbnf_grammar, documentation = generate_gbnf_grammar_and_documentation([Book])\ngrammar = LlamaGrammar.from_string(gbnf_grammar, verbose=False)\n\n\nwrapped_model = LlamaCppAgent(main_model, debug_output=True,\n                              system_prompt=\"You are an advanced AI, tasked to create JSON database entries for books.\\n\\n\\n\" + documentation)\n\n\nwrapped_model.get_chat_response(text, temperature=0.15, grammar=grammar)\nFunction Calling Example\nThis example shows how to do function calling.\nimport json\n\nfrom llama_cpp import Llama, LlamaGrammar\n\nfrom llama_cpp_agent.llm_agent import LlamaCppAgent\nfrom llama_cpp_agent.gbnf_grammar_generator.gbnf_grammar_from_pydantic_models import generate_gbnf_grammar_and_documentation\n\nfrom example_function_call_models import SendMessageToUser, GetFileList, ReadTextFile, WriteTextFileSection\nfrom llama_cpp_agent.messages_formatter import MessagesFormatterType\n\ngbnf_grammar, documentation = generate_gbnf_grammar_and_documentation(\n    [SendMessageToUser, GetFileList, ReadTextFile, WriteTextFileSection], \"function\", \"function_params\", \"Function\",\n    \"Function Parameter\")\ngrammar = LlamaGrammar.from_string(gbnf_grammar, verbose=False)\n\nmain_model = Llama(\n    \"../gguf-models/dpopenhermes-7b-v2.Q8_0.gguf\",\n    n_gpu_layers=35,\n    f16_kv=True,\n    use_mlock=False,\n    embedding=False,\n    n_threads=8,\n    n_batch=1024,\n    n_ctx=8192,\n    last_n_tokens_size=1024,\n    verbose=False,\n    seed=42,\n)\nwrapped_model = LlamaCppAgent(main_model, debug_output=True,\n                              system_prompt=\"You are an advanced AI, tasked to assist the user by calling functions in JSON format.\\n\\n\\n\" + documentation,\n                              predefined_messages_formatter_type=MessagesFormatterType.CHATML)\n\nresponse = wrapped_model.get_chat_response('Write a long poem about the USA in the \"HelloUSA.txt\" file.',\n                                           temperature=0.15, grammar=grammar)\n\nfunction_call = json.loads(response)\n\nif function_call[\"function\"] == \"write-text-file-section\":\n    call_parameters = function_call[\"function_params\"]\n    call = WriteTextFileSection(**call_parameters)\n    call.run()\nAdditional Information\n\nDependencies: pydantic for grammars based generation and of course llama-cpp-python.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4690",
        "createdAt": "2023-12-29T19:47:15Z",
        "author": {
            "login": "Maximilian-Winter"
        }
    },
    {
        "title": "llama.cpp server add prompt template",
        "bodyText": "Is there anyway to add a prefix and a suffix or  a prompt template to llama.cpp server  that will work with opensource LLMs in API mode?\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4722",
        "createdAt": "2024-01-01T04:03:43Z",
        "author": {
            "login": "mth93"
        }
    },
    {
        "title": "30B model now needs only 5.8GB of RAM? How?",
        "bodyText": "(Edit: apologies, I should have clarified initially I'm running on Linux OS. I didn't realize it might not be obvious from the screenshot alone for a non-Linux users.All tests are done on Ubuntu based Linux Mint 21.1)\nI've been only playing with 30B model so far, since neither 7B nor 13B were very engaging.\nAs recently as yesterday 30B model fill just close to 30GB, but today's release now runs fine with less than 6GB (and that's with system memory usage).\nInitially I thought it must be a bug, but I couldn't notice any quality loss in responses, and then I saw there was some major change introduced only hours ago, but the fundamentals of those changes are a little over my head.\nMaybe someone smarter than me can at least roughly explain, in basic terms (if that's even possible at all), how memory usage dropped  5 times overnight?\nThanks a lot in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/638",
        "createdAt": "2023-03-30T22:17:47Z",
        "author": {
            "login": "pugzly"
        }
    },
    {
        "title": "PowerInfer 11x speedup",
        "bodyText": "I keep seeing posts about powerinfer https://github.com/SJTU-IPADS/PowerInfer giving an 11x speedup.\nFrom what I understand it keeps often used terms in GPU memory and less often used terms in CPU memory.\nIt looks like it needs to rework models in order to accomplish this.\nAny thoughts?\n11x speedup!!!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4692",
        "createdAt": "2023-12-29T21:15:07Z",
        "author": {
            "login": "iplayfast"
        }
    },
    {
        "title": "Is YaLM supportable?",
        "bodyText": "Yandex has been using YaLM neural networks in its voice assistant, Alice and its search engine Yandex Search. YaLM 100B has been released under the Apache 2.0 license, which permits research and commercial use. They have a repository with the download script here: https://github.com/yandex/YaLM-100B. Is this something that can be converted, quantized, and, given enough RAM, used with llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/936",
        "createdAt": "2023-04-13T06:10:24Z",
        "author": {
            "login": "patrakov"
        }
    },
    {
        "title": "llama.cpp inference",
        "bodyText": "Prerequisites\nPlease answer the following questions for yourself before submitting an issue.\n\n I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n I carefully followed the README.md.\n I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n I reviewed the Discussions, and have a new bug or useful enhancement to share.\n\nFeature Description\nPlease provide a detailed written description of what you were trying to do, and what you expected llama.cpp to do as an enhancement.\nI want to inference llama.cpp (model: 7B-chat) locally with a dataset from huggingface (e.g. wmt16 Link: https://huggingface.co/datasets/wmt16).\nHow can i accomplish this?\nI could not find any special answers on the internet.\nMotivation\nPlease provide a detailed written description of reasons why this feature is necessary and how it is useful to llama.cpp users.\nI needed for a university project, where i have to measure the power consumption of Llama2 on a local machine.\nPossible Implementation\nI am currently trying to learn langchain to create an inference file.\nIf you have an idea as to how it can be implemented, please write a detailed description. Feel free to give links to external sources or share visuals that might be helpful to understand the details better.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4643",
        "createdAt": "2023-12-26T10:51:06Z",
        "author": {
            "login": "s0569591"
        }
    },
    {
        "title": "value has been optimized out",
        "bodyText": "When gdb debugger in VSCode, value has been optimized out, I want to see the value of the variable, how should it be resolved\uff1f\nSystem\uff1a6.2.0-39-generic  4022.04.1-Ubuntu\ngcc (Ubuntu 11.4.0-1ubuntu122.04) 11.4.0\ng++ (Ubuntu 11.4.0-1ubuntu122.04) 11.4.0\nGNU gdb (Ubuntu 12.1-0ubuntu122.04) 12.1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4640",
        "createdAt": "2023-12-26T10:21:55Z",
        "author": {
            "login": "18222901154"
        }
    },
    {
        "title": "[Model Request] Cerebras' BTLM-3B-8K",
        "bodyText": "Bittensor Language Model (BTLM-3B-8k-base) is a 3 billion parameter language model with an 8k context length trained on 627B tokens of SlimPajama. BTLM-3B-8k-base sets a new standard for 3B parameter models, outperforming models trained on hundreds of billions more tokens and achieving comparable performance to open 7B parameter models. BTLM-3B-8k-base can also be quantized to 4-bit to fit in devices with as little as 3GB of memory. The model is made available with an Apache 2.0 license for commercial use.\nBTLM was trained by Cerebras in partnership with Opentensor on the newly unveiled Condor Galaxy 1 (CG-1) supercomputer, the first public deliverable of the G42-Cerebras strategic partnership.\nBTLM-3B-8k was trained with a similar architecture to CerebrasGPT with the addition of SwiGLU nonlinearity, ALiBi position embeddings, and maximal update parameterization (muP). The model was trained for 1 epoch of SlimPajama-627B. 75% of training was performed with 2k sequence length. The final 25% of training was performed at 8k sequence length to enable long sequence applications\nhttps://www.cerebras.net/machine-learning/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/\nhttps://huggingface.co/cerebras/btlm-3b-8k-base\nhttps://arxiv.org/abs/2309.11568",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4663",
        "createdAt": "2023-12-28T12:16:09Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "How to use StarCoder? tensor 'output.weight' not found",
        "bodyText": "Hi , I try to use starcoderbase-1b on llama.cpp\uff0cbut it fail.\nfirst, I clone starcoderbase-1b:\ngit clone https://huggingface.co/bigcode/starcoderbase-1b\nsecond, I convert it to gguf:\npython convert-hf-to-gguf.py /data/models/starcoderbase-1b/ --outfile startcoder1b.gguf\nand then, I run it on llama.cpp:\n./main -m startcoder1b.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e\nbut it fail:\nLog start\nmain: build = 1699 (b9f4795)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: seed  = 1703645466\nllama_model_loader: loaded meta data with 17 key-value pairs and 292 tensors from startcoder1b.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = starcoder\nllama_model_loader: - kv   1:                               general.name str              = StarCoder\nllama_model_loader: - kv   2:                   starcoder.context_length u32              = 8192\nllama_model_loader: - kv   3:                 starcoder.embedding_length u32              = 2048\nllama_model_loader: - kv   4:              starcoder.feed_forward_length u32              = 8192\nllama_model_loader: - kv   5:                      starcoder.block_count u32              = 24\nllama_model_loader: - kv   6:             starcoder.attention.head_count u32              = 16\nllama_model_loader: - kv   7:          starcoder.attention.head_count_kv u32              = 1\nllama_model_loader: - kv   8:     starcoder.attention.layer_norm_epsilon f32              = 0.000010\nllama_model_loader: - kv   9:                          general.file_type u32              = 1\nllama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<fim_prefix>\", \"<f...\nllama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,48891]   = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"\u0120\u0120\u0120\u0120 \u0120\u0120...\nllama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\nllama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - type  f32:  194 tensors\nllama_model_loader: - type  f16:   98 tensors\nllm_load_vocab: special tokens definition check successful ( 19/49152 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = starcoder\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 49152\nllm_load_print_meta: n_merges         = 48891\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 2048\nllm_load_print_meta: n_head           = 16\nllm_load_print_meta: n_head_kv        = 1\nllm_load_print_meta: n_layer          = 24\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 16\nllm_load_print_meta: f_norm_eps       = 1.0e-05\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 8192\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 1B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 1.14 B\nllm_load_print_meta: model size       = 2.12 GiB (16.01 BPW) \nllm_load_print_meta: general.name     = StarCoder\nllm_load_print_meta: BOS token        = 0 '<|endoftext|>'\nllm_load_print_meta: EOS token        = 0 '<|endoftext|>'\nllm_load_print_meta: UNK token        = 0 '<|endoftext|>'\nllm_load_print_meta: LF token         = 145 '\u00c4'\nllm_load_tensors: ggml ctx size       =    0.11 MiB\nerror loading model: create_tensor: tensor 'output.weight' not found\nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params: error: failed to load model 'startcoder1b.gguf'\nmain: error: unable to load model\nDoes anyone know how to solve this problem?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4651",
        "createdAt": "2023-12-27T08:35:55Z",
        "author": {
            "login": "ClarkWain"
        }
    },
    {
        "title": "Named cache in /completion",
        "bodyText": "I wanted to discuss here before requesting it as a new feature or try implementing it.\nServer now supports cache parameter. When cache is on, my understanding is that when the new prompt is processed instead of resetting the context, it tries to determine the matching prefix and starts generation from point where the last generation and new prompt defers.\nNamed Cache:\nIn addition to default last generation, this new parameter allows creation of cache entry by name. The context from the first generation is saved in L2 cache (eg. memory if free or disk). All subsequent requests with named_cache loads the the context from entries saved earlier, and continues from there.\nI might be wrong, my assumptions are, for a long prompt, load from memory or disk is cheaper than evaluating it from start.\nIf the idea is sound. Any pointers for implementation would be nice. Based on my study so far, saving and loading are straight forward (from save-load-state) and shared states around slots needs to be take care of.  My concerns are around continious batching, not sure how this approach affect the continious batching.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4649",
        "createdAt": "2023-12-27T07:32:38Z",
        "author": {
            "login": "aniljava"
        }
    },
    {
        "title": "An OpenAI Compatible Web Server for llama.cpp",
        "bodyText": "Hey everyone,\nJust wanted to share that I integrated an OpenAI-compatible webserver into the llama-cpp-python package so you should be able to serve and use any llama.cpp compatible models with (almost) any OpenAI client.\nCheck out the README but the basic setup process is\npip install llama-cpp-python[server]\nexport MODEL=./models/7B\npython3 -m llama_cpp.server\n\nThen just navigate to http://localhost:8000/docs to start playing around with it using the Swagger UI.\n\nIn terms of compatibility I've tested it with the official OpenAI python library by just swapping out openai.api_base for the server URL and it seems to work. I've also had success using it with @mckaywrigley chatbot-ui which is a self hosted ChatGPT ui clone you can run with docker. Just launch with -e OPENAI_API_HOST=<api-url> to get started.\ndocker run -e OPENAI_API_HOST=<api-url> -e OPENAI_API_KEY=\"\" -p 3000:3000 ghcr.io/mckaywrigley/chatbot-ui:main\n\nCaveats\n\nChat completion is quite slow until I can implement a solution to cache parts of the llama state (if anyone can help with this, very much appreciated)\nCertain features aren't implemented yet like logprobs and anything that's OpenAI specific but llama.cpp doesn't support like best_of parameter is just ignored silently.\nTools which rely on tiktoken or some other OpenAI model-specific tokenizer may not work or be buggy, just a heads up",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/795",
        "createdAt": "2023-04-05T22:06:54Z",
        "author": {
            "login": "abetlen"
        }
    },
    {
        "title": "Mac M1/M2 Speed Optimization \ud83d\udd25",
        "bodyText": "Mac M1/M2 users: If you are not yet doing this, use \"-n 128 -mlock\" arguments; also, make sure only to use 4/n threads. Thank me later :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/913",
        "createdAt": "2023-04-12T10:53:51Z",
        "author": {
            "login": "xISSAx"
        }
    },
    {
        "title": "StyleTTS2 - Text to speech comparable to Eleven Labs quality.",
        "bodyText": "Has anybody seen this? Just had a play around with the collab notebooks and it really is very good quality and it also has voice cloning.  I only ask because I can't find anybody talking about this anywhere at all.  It may have slipped under everybodys radar?\nhttps://github.com/yl4579/StyleTTS2",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4138",
        "createdAt": "2023-11-19T17:33:23Z",
        "author": {
            "login": "logikstate"
        }
    },
    {
        "title": "How to convert to gguf format with tokenizer.json file?",
        "bodyText": "I have tried to convert llama-2-7b model to GGUF format to deploy with llama.cpp. But they do not include tokenizer.model file which is needed to convert process. But they have tokenizer.json file. So Is there any method to use tokenizer.json file to create model in GGUF format? If not, is there any way to generate tokenizer.model file?\nMany thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3498",
        "createdAt": "2023-10-06T09:56:43Z",
        "author": {
            "login": "LiamHoang26"
        }
    },
    {
        "title": "How do I build this in windows (cublas)",
        "bodyText": "I want to try out the cublast (#1412) (master) build to offload some of the layers to the gpu. trying to build this in windows is proving to be a bit difficult for me. I have installed cmake and have installed the nvidia cuda toolkit and I even installed Build Tools for Visual Studio 2022. every time I try to run:\nmkdir build\ncd build\ncmake .. -DLLAMA_CUBLAS=ON\ncmake --build . --config Release\n\nI get:\ncmake .. -DLLAMA_CUBLAS=ON\nCMake Error at CMakeLists.txt:2 (project):\n  Running\n\n   'nmake' '-?'\n\n  failed with:\n\n   The system cannot find the file specified\n\n\nCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\nCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n-- Configuring incomplete, errors occurred!\n\nI have also tried with make:\ng++: warning: Files/NVIDIA: linker input file unused because linking not done\ng++: error: Files/NVIDIA: linker input file not found: No such file or directory\ng++: warning: GPU: linker input file unused because linking not done\ng++: error: GPU: linker input file not found: No such file or directory\ng++: warning: Computing: linker input file unused because linking not done\ng++: error: Computing: linker input file not found: No such file or directory\ng++: warning: Toolkit/CUDA/v12.1/targets/x86_64-linux/include: linker input file unused because linking not done\ng++: error: Toolkit/CUDA/v12.1/targets/x86_64-linux/include: linker input file not found: No such file or directory\nmake: *** [Makefile:188: llama.o] Error 1\n\nnvcc:\nC:\\Users\\kevin>nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:36:15_Pacific_Daylight_Time_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\n\nOK, I think I got it to build by using powershell -> and dropping the files from the cuda toolkit. since I have version 12.1 the path is a little different. I copied the files from here:\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\extras\\visual_studio_integration\\MSBuildExtensions to here\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\nas per:\nhttps://stackoverflow.com/questions/56636714/cuda-compile-problems-on-windows-cmake-error-no-cuda-toolset-found\nthen ran the regular cmake build procedure in powershell\nIf anyone can list how they got this built in windows that would really help.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1431",
        "createdAt": "2023-05-13T15:54:28Z",
        "author": {
            "login": "kevkid"
        }
    },
    {
        "title": "Local MoE Inference over the network - viable or dumb?",
        "bodyText": "Would it be possible to have a setup with two separate machines, each with a single GPU running on the same local internet connection, in which:\n\nThey both host 4 experts of Mixtral (or any arbritary amount, really) on GPUs. Let's say the \"Host\" machine hosts Experts 0, 1, 2, 3 while the \"Secondary\" machine hosts 4, 5, 6, 7.\nAt inference time, they would both compute different experts. Each machine only computes the experts that it has access to.\nBoth machines (the 'host' and the 'secondary' machine) get transferred the hidden states of the computed layers for each expert over the network.\n\nMy thinking is, because you are not transferring a large amount of data (only 16kb for each hidden state), the latency of the network is perhaps not enough of an issue for this to be a bottleneck, especially over local Ethernet with full-duplex communication.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4608",
        "createdAt": "2023-12-23T03:56:12Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Questions about order of MoE matmul for CPU/GPU inference",
        "bodyText": "Right now, my understanding of how CPU layers handle matrix multiplication for each expert in a MoE (for prompt processing / evaluation):\n\nEach (current) layer of each token is evaluated by the router across the entire batch\nTopk=2 is chosen (for Mixtral) to be evaluated for each\nIt groups together the matmuls for a single expert across all tokens in the current batch that use that expert and goes one expert at a time\n\nCould you in theory sort of the order of expert operations so that it optimizes the order in terms of pairwise occurences, and would this be any better / more optimal than just grouping per expert across all layers in the current batch (as it does right now)?\nAs in, after the router, you sort the order of experts by pairwise occurences, but you still do one expert at a time.\nMy thinking is that this would lead to more optimal memory access patterns / caching, at least for the CPU inference, but it might be pointless / a micro-optimization.\nAny thoughts @JohannesGaessler ?\nEven during generation time, some experts are accessed more rarely per token (this code is not tracking pair-wise occurences, it's just counting how many times in total each expert is accessed per token):\nGenerating (1 / 16 tokens)\n0: 8  1: 7  2:10  3:10  4:12  5: 4  6: 5  7: 8\nGenerating (2 / 16 tokens)\n0:11  1: 6  2:13  3: 7  4: 5  5:10  6: 7  7: 5\nGenerating (3 / 16 tokens)\n0:10  1: 8  2: 7  3: 9  4: 8  5: 6  6:12  7: 4\nGenerating (4 / 16 tokens)\n0: 9  1: 7  2: 1  3: 8  4:11  5: 9  6: 9  7:10\nGenerating (5 / 16 tokens)\n0: 9  1: 9  2: 7  3: 9  4:11  5: 6  6: 8  7: 5\nGenerating (6 / 16 tokens)\n0: 7  1: 9  2:12  3: 6  4:10  5: 6  6: 6  7: 8\nGenerating (7 / 16 tokens)\n0: 9  1: 5  2: 4  3:11  4: 5  5:10  6:12  7: 8\nGenerating (8 / 16 tokens)\n0: 8  1:10  2: 7  3: 8  4: 4  5: 6  6:11  7:10\nGenerating (9 / 16 tokens)\n0:11  1: 9  2: 5  3: 7  4: 6  5:10  6: 6  7:10\nGenerating (10 / 16 tokens)\n0:10  1: 9  2: 7  3: 8  4:11  5: 9  6: 4  7: 6\nGenerating (11 / 16 tokens)\n0: 6  1:12  2: 8  3: 6  4:10  5: 6  6: 9  7: 7\nGenerating (12 / 16 tokens)\n0: 5  1:13  2: 9  3: 6  4:10  5:11  6: 6  7: 4\nGenerating (13 / 16 tokens)\n0:10  1:12  2: 9  3: 7  4:10  5: 8  6: 5  7: 3\nGenerating (14 / 16 tokens)\n0: 7  1: 6  2:10  3: 4  4:11  5:14  6: 6  7: 6\nGenerating (15 / 16 tokens)\n0:10  1: 9  2:12  3: 6  4: 7  5: 5  6: 7  7: 8\n\nI'll try to look into counting pairwise occurences but it might be beyond my ability",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4628",
        "createdAt": "2023-12-25T13:18:31Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "MoE of Multiple LoRa Adapter",
        "bodyText": "stupid question ahead\ni want to do experiment with MoE but using lora adapter kind thing. My understanding is currently lora adapter is applied to layers in llama_apply_lora_from_file_internal but i want to maintain multiple adapters without fusing  with base model (not apply on top of another). maybe route to best adapter + base combo bashed on prompt using some another router adapter or could be small bert that tags type of field prompt  is related etc.\nMight sound stupid but i want to play around these ideas any guidance on how to achieve something like that would very helpful thank you",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2363",
        "createdAt": "2023-07-24T08:30:17Z",
        "author": {
            "login": "forrackun"
        }
    },
    {
        "title": "hiding display of prompt file to start generation (-f option)",
        "bodyText": "Is there a way to avoid the display of a long prompt loaded with -f option without modifying the source code?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1498",
        "createdAt": "2023-05-17T08:50:07Z",
        "author": {
            "login": "l0d0v1c"
        }
    },
    {
        "title": "How do I  convert PTH model into gguf",
        "bodyText": "I found the pth to ggml but not gguf conversion script. What should i do?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4609",
        "createdAt": "2023-12-23T12:00:44Z",
        "author": {
            "login": "sorasoras"
        }
    },
    {
        "title": "Using Nvidia GPU on Windows for running models",
        "bodyText": "Hello,\nthe title basically describes my problem. I'm on windows, I have installed CUDA and when trying to make with cuBLAS it says your not on linux and then stops making. Is there just no GPU support for Windows or am I missing something?\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4488",
        "createdAt": "2023-12-15T22:59:56Z",
        "author": {
            "login": "sussyboiiii"
        }
    },
    {
        "title": "Why does \"server\" always unexpectedly generate continuous dialogues?",
        "bodyText": "I started a llamacpp server for chatting. But its response for one prompt consisted of a whole conversation.\nI expected:\n\nSend a question to server through HTTP POST\nServer respond an answer\nrepeat 1&2\nThe procedure should be like these:\n[send request]User: Hello\n[response]AI: Hi, can I help you?\n[send request]User: Yeah, I met a problem that...\n[repsonse]AI: I understood that...\n\nHowever, it worked like:\n\nSend a question to server through HTTP POST\nreturn a series of answers and questions that it generated automatically, till the end of the conversation.\nThat is to say:\n[send request]User: Hello\n[response]AI: Hi, can I help you?   User: Yeah, I met a problem that... AI: I understood that... User: Thank you. Bye. AI: Welcome, Bye.\n\nWhen I used \"main\" and -i parameter, it worked exactly like my expectation. Meanwhile, I also noticed if I pressed Enter, it would generate the conversation continuously including both questions and answers.\nWhat's the essential issue? And can it be addressed? Thanks a lot!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4592",
        "createdAt": "2023-12-22T09:09:09Z",
        "author": {
            "login": "R0ck1n"
        }
    },
    {
        "title": "Reset Server",
        "bodyText": "Hello everyone!\nI am using Llama.cpp Server with API Endpoint \"POST /completion\". I would like to reset the server to the initial state after having some conversation in order to avoid a restart and a complete reload of a model. Is there a way to do such reset? Or am I using it in the wrong way? (I would like to start \"a new topic\" that has nothing to do with the previous conversation.)\nThanks for your help!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4590",
        "createdAt": "2023-12-22T07:51:20Z",
        "author": {
            "login": "gocursor"
        }
    },
    {
        "title": "How do you enable OpenBLAS when building with zig?",
        "bodyText": "You have instructions for Make and CMake but for zig they're missing in the readme. Thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3781",
        "createdAt": "2023-10-25T18:42:39Z",
        "author": {
            "login": "xoich"
        }
    },
    {
        "title": "Run Llama.cpp in silent mode",
        "bodyText": "Is there an option to run the executable using a saved prompt without printing anything but the prediction by the selected model ? Some kind of silent mode in opposition to the --verbose.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/777",
        "createdAt": "2023-04-05T14:52:17Z",
        "author": {
            "login": "OsaCode"
        }
    },
    {
        "title": "Finetuning Deepseek Coder",
        "bodyText": "Hello, I would like to request your help. I\u2019m trying to finetune the following model: TheBloke/deepseek-coder-1.3b-base-GGUF deepseek-coder-1.3b-base.Q8_0.gguf.\nI ran the the finetuning using the following command:\n./finetune \\\n        --model-base train/deepseek-coder-1.3b-base.Q8_0.gguf \\\n        --checkpoint-in  train/chk-deepseek-coder-1.3b-base.Q8_0-LATEST.gguf \\\n        --checkpoint-out train/chk-deepseek-coder-1.3b-base.Q8_0-ITERATION.gguf \\\n        --lora-out train/lora-deepseek-coder-1.3b-base.Q8_0-ITERATION.bin \\\n        --train-data \u201ctrain/data.txt\" \\\n        --save-every 0 \\\n        --threads 6 --adam-iter 30 --batch 4 --ctx 128 \\\n        --sample-start \"<\uff5cbegin\u2581of\u2581sentence\uff5c>\u201d\n\nAfter the finetuning, I ran the model with the lora adapter using the following command:\n./main -m train/deepseek-coder-1.3b-base.Q8_0.gguf --lora train/lora-deepseek-coder-1.3b-base.Q8_0-LATEST.bin --color --temp 0.1 -e -p \u201cSample prompt here\u201d\n```\n\nAnd I get the following error:\n```\nllama_apply_lora_from_file_internal: unsupported tensor dimension 1\nllama_init_from_gpt_params: error: failed to apply lora adapter\n```\n\nHere\u2019s the full log:\n```\nLog start\nmain: build = 1644 (8a5be3b)\nmain: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0\nmain: seed  = 1703175858\nllama_model_loader: loaded meta data with 22 key-value pairs and 219 tensors from train/deepseek-coder-1.3b-base.Q8_0.gguf (version GGUF V3 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight q8_0     [  2048, 32256,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   25:            blk.2.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   43:            blk.4.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   61:            blk.6.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   79:            blk.8.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor   97:           blk.10.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  115:           blk.12.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  133:           blk.14.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  151:           blk.16.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  169:           blk.18.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  187:           blk.20.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  205:           blk.22.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight q8_0     [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight q8_0     [  2048,  5504,     1,     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight q8_0     [  5504,  2048,     1,     1 ]\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  217:               output_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor  218:                    output.weight q8_0     [  2048, 32256,     1,     1 ]\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = deepseek-ai_deepseek-coder-1.3b-base\nllama_model_loader: - kv   2:                       llama.context_length u32              = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv   4:                          llama.block_count u32              = 24\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5504\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 16\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 16\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000\nllama_model_loader: - kv  11:                    llama.rope.scale_linear f32              = 4.000000\nllama_model_loader: - kv  12:                          general.file_type u32              = 7\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,31757]   = [\"\u0120 \u0120\", \"\u0120 t\", \"\u0120 a\", \"i n\", \"h e...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 32013\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32014\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32014\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   49 tensors\nllama_model_loader: - type q8_0:  170 tensors\nllm_load_vocab: mismatch in special tokens definition ( 243/32256 vs 236/32256 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 32256\nllm_load_print_meta: n_merges         = 31757\nllm_load_print_meta: n_ctx_train      = 16384\nllm_load_print_meta: n_embd           = 2048\nllm_load_print_meta: n_head           = 16\nllm_load_print_meta: n_head_kv        = 16\nllm_load_print_meta: n_layer          = 24\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 5504\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 100000.0\nllm_load_print_meta: freq_scale_train = 0.25\nllm_load_print_meta: n_yarn_orig_ctx  = 16384\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = mostly Q8_0\nllm_load_print_meta: model params     = 1.35 B\nllm_load_print_meta: model size       = 1.33 GiB (8.50 BPW) \nllm_load_print_meta: general.name     = deepseek-ai_deepseek-coder-1.3b-base\nllm_load_print_meta: BOS token        = 32013 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\nllm_load_print_meta: EOS token        = 32014 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nllm_load_print_meta: PAD token        = 32014 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nllm_load_print_meta: LF token         = 126 '\u00c4'\nllm_load_tensors: ggml ctx size = 1364.72 MiB\nllm_load_tensors: mem required  = 1364.72 MiB\n.............................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 100000.0\nllama_new_context_with_model: freq_scale = 0.25\nllama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\nllama_build_graph: non-view tensors processed: 508/508\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Max\nggml_metal_init: picking default device: Apple M2 Max\nggml_metal_init: default.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/michaelbonon/coding/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nllama_new_context_with_model: compute buffer total size = 70.19 MiB\nllama_new_context_with_model: max tensor size =    66.94 MiB\nggml_metal_add_buffer: allocated 'data            ' buffer, size =  1364.72 MiB, ( 1366.34 / 21845.34)\nggml_metal_add_buffer: allocated 'kv              ' buffer, size =    96.03 MiB, ( 1462.38 / 21845.34)\nggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    67.02 MiB, ( 1529.39 / 21845.34)\nllama_apply_lora_from_file_internal: applying lora adapter from 'train/lora-deepseek-coder-1.3b-base.Q8_0-LATEST.bin' - please wait ...\nllama_apply_lora_from_file_internal: r = 4, alpha = 4, scaling = 1.00\nllama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\nllama_apply_lora_from_file_internal: unsupported tensor dimension 1\nllama_init_from_gpt_params: error: failed to apply lora adapter\nggml_metal_free: deallocating\nmain: error: unable to load model\n```",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4565",
        "createdAt": "2023-12-21T16:35:05Z",
        "author": {
            "login": "m1chae1bx"
        }
    },
    {
        "title": "Mixtral 8x7b",
        "bodyText": "This is not a small model, but it is shown to preform at the level as GPT 4 and is open source. I'm super curious if anyone has gotten it working on their machine.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4539",
        "createdAt": "2023-12-19T22:38:43Z",
        "author": {
            "login": "Runtrons"
        }
    },
    {
        "title": "PowerInfer",
        "bodyText": "Anybody seen this? Claims to put important tensors on GPU and less important tensors on CPU for massive performance boost.\nhttps://github.com/SJTU-IPADS/PowerInfer",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4536",
        "createdAt": "2023-12-19T20:21:32Z",
        "author": {
            "login": "logikstate"
        }
    },
    {
        "title": "How does llama.cpp works ?",
        "bodyText": "Could someone provide a very simple explanation about it ? Just a flow diagram, some pseudo code, a little working demo.\nIt would be great to have a working example using a high level use just for learning purposes even if it were terribly slow.\nthank you",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4531",
        "createdAt": "2023-12-19T07:09:52Z",
        "author": {
            "login": "FiveTechSoft"
        }
    },
    {
        "title": "\ud83d\ude2dmake: *** [Makefile:499: build-info.h] Error 2",
        "bodyText": "I use\n make LLAMA_CUBLAS=1\nand it output:\nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -pthread -march=native -mtune=native \nI CXXFLAGS: -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -pthread -march=native -mtune=native \nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \nI CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nI CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n\n: not foundld-info.sh: 2: \n: not foundld-info.sh: 5: \nscripts/build-info.sh: 24: Syntax error: end of file unexpected (expecting \"then\")\nmake: *** [Makefile:499: build-info.h] Error 2\nhere is my setup and environment  info:\nGPU A100\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Sun_Mar_21_19:15:46_PDT_2021\nCuda compilation tools, release 11.3, V11.3.58\nBuild cuda_11.3.r11.3/compiler.29745058_0\n\ngit log\ncommit 178b1850ebd21b349cebbee887950e435c5aa2d3 (HEAD -> master, tag: b1187, origin/master, origin/HEAD)\nAuthor: Georgi Gerganov <ggerganov@gmail.com>\nDate:   Wed Sep 6 12:40:57 2023 +0300\n\n    k-quants : fix zero-weight guard in Q6_K (ref #3040)\n\nPython 3.10.12\npip list | egrep \"torch|numpy|sentencepiece\"\n\n   numpy         1.24.0\n  sentencepiece 0.1.98\n\nGNU Make 4.2.1\n\nlscpu\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 48 bits virtual\nCPU(s):                          16\nOn-line CPU(s) list:             0-15\nThread(s) per core:              2\nCore(s) per socket:              8\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           106\nModel name:                      Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\nStepping:                        6\nCPU MHz:                         2899.998\nBogoMIPS:                        5799.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       384 KiB\nL1i cache:                       256 KiB\nL2 cache:                        10 MiB\nL3 cache:                        48 MiB\nNUMA node0 CPU(s):               0-15\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave\n                                  avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves\n                                  wbnoinvd arat avx512vbmi avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid arch_capabilities",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3052",
        "createdAt": "2023-09-07T03:34:12Z",
        "author": {
            "login": "satelliter"
        }
    },
    {
        "title": "Llama multi GPU",
        "bodyText": "I have Llama2 running under LlamaSharp (latest drop, 10/26) and CUDA-12.  I took a screen capture of the Task Manager running while the model was answering questions and thought I'd provide you the feedback.  There are 4 A6000 GPUs on the system with 128GB of system ram.  It works, and also loads and runs the 70b models (albeit a bit more slowly).  Though it does use all the GPUs, it mostly puts the burden on GPU0.\nI wanted to upload a larger video file, but the limit is 10mb.\n\n  \n    \n    \n\n    GPUPerf_Llama2_13bModel.mp4",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3804",
        "createdAt": "2023-10-26T21:59:29Z",
        "author": {
            "login": "PaulaScholz"
        }
    },
    {
        "title": "M2 Pro 32GB Cannot Use Metal",
        "bodyText": "I am trying to run llama.cpp with the following model https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/tree/main, however, I cannot seem to make the GPU support work.\nI build llama.cpp with make as instructed, however, when I try to run any model with --ngl 1 flag, I get the following error.\n./main -m ./codellama-13b-instruct.Q6_K.gguf --n_gpu_layers 1\nLog start\nmain: build = 1364 (9f6ede1)\nmain: built with clang version 14.0.6 for arm64-apple-darwin20.0.0\nmain: seed  = 1697017297\nllama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from ./codellama-13b-instruct.Q6_K.gguf (version GGUF V2 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight q6_K     [  5120, 32016,     1,     1 ]\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor    4:              blk.0.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor    7:         blk.0.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor    8:              blk.0.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   13:              blk.1.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   16:         blk.1.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   17:              blk.1.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   22:             blk.10.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   24:             blk.10.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   25:        blk.10.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   26:             blk.10.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   31:             blk.11.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   33:             blk.11.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   34:        blk.11.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   35:             blk.11.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   38:           blk.12.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   40:             blk.12.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   42:             blk.12.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   43:        blk.12.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   44:             blk.12.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   45:             blk.12.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   47:           blk.13.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   49:             blk.13.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   51:             blk.13.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   52:        blk.13.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   53:             blk.13.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   54:             blk.13.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   58:             blk.14.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   60:             blk.14.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   61:        blk.14.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   62:             blk.14.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   64:             blk.15.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   65:             blk.15.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   66:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   67:            blk.2.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   68:            blk.2.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   69:              blk.2.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   70:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   71:              blk.2.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   72:         blk.2.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   73:              blk.2.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   74:              blk.2.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   75:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   76:            blk.3.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   77:            blk.3.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   78:              blk.3.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   79:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   80:              blk.3.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   81:         blk.3.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   82:              blk.3.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   83:              blk.3.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   84:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   85:            blk.4.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   86:            blk.4.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   87:              blk.4.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   88:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   89:              blk.4.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   90:         blk.4.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   91:              blk.4.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   92:              blk.4.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   93:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   94:            blk.5.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor   95:            blk.5.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   96:              blk.5.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor   97:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor   98:              blk.5.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor   99:         blk.5.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  100:              blk.5.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  101:              blk.5.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  102:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  103:            blk.6.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  104:            blk.6.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  105:              blk.6.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  106:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  107:              blk.6.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  108:         blk.6.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  109:              blk.6.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  110:              blk.6.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  111:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  112:            blk.7.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  113:            blk.7.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  114:              blk.7.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  115:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  116:              blk.7.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  117:         blk.7.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  118:              blk.7.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  119:              blk.7.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  120:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  121:            blk.8.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  122:            blk.8.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  123:              blk.8.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  124:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  125:              blk.8.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  126:         blk.8.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  127:              blk.8.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  128:              blk.8.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  129:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  130:            blk.9.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  131:            blk.9.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  132:              blk.9.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  133:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  134:              blk.9.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  135:         blk.9.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  136:              blk.9.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  137:              blk.9.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  138:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  139:           blk.15.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  143:        blk.15.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  144:             blk.15.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  146:           blk.16.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  148:             blk.16.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  151:        blk.16.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  152:             blk.16.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  153:             blk.16.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  155:           blk.17.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  157:             blk.17.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  160:        blk.17.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  161:             blk.17.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  162:             blk.17.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  164:           blk.18.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  166:             blk.18.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  169:        blk.18.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  170:             blk.18.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  171:             blk.18.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  173:           blk.19.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  175:             blk.19.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  178:        blk.19.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  179:             blk.19.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  180:             blk.19.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  182:           blk.20.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  184:             blk.20.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  187:        blk.20.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  188:             blk.20.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  189:             blk.20.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  191:           blk.21.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  193:             blk.21.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  196:        blk.21.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  197:             blk.21.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  198:             blk.21.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  200:           blk.22.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  202:             blk.22.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  205:        blk.22.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  206:             blk.22.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  207:             blk.22.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  209:           blk.23.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  211:             blk.23.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  214:        blk.23.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  215:             blk.23.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  216:             blk.23.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  218:           blk.24.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  220:             blk.24.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  222:             blk.24.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  223:        blk.24.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  224:             blk.24.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  225:             blk.24.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  227:           blk.25.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  229:             blk.25.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  231:             blk.25.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  232:        blk.25.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  233:             blk.25.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  234:             blk.25.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  236:           blk.26.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  238:             blk.26.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  240:             blk.26.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  241:        blk.26.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  242:             blk.26.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  243:             blk.26.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  245:           blk.27.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  247:             blk.27.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  249:             blk.27.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  250:        blk.27.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  251:             blk.27.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  252:             blk.27.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  254:           blk.28.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  256:             blk.28.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  258:             blk.28.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  259:        blk.28.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  260:             blk.28.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  261:             blk.28.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  263:           blk.29.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  265:             blk.29.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  267:             blk.29.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  268:        blk.29.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  269:             blk.29.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  270:             blk.29.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  272:             blk.30.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  273:             blk.30.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  275:             blk.30.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  276:             blk.30.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  277:                    output.weight q6_K     [  5120, 32016,     1,     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  279:           blk.30.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  284:             blk.31.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  286:             blk.31.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  287:        blk.31.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  288:             blk.31.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  290:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  291:           blk.32.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  292:           blk.32.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  293:             blk.32.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  294:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  295:             blk.32.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  296:        blk.32.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  297:             blk.32.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  298:             blk.32.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  299:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  300:           blk.33.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  301:           blk.33.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  302:             blk.33.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  303:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  304:             blk.33.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  305:        blk.33.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  306:             blk.33.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  307:             blk.33.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  308:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  309:           blk.34.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  310:           blk.34.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  311:             blk.34.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  312:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  313:             blk.34.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  314:        blk.34.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  315:             blk.34.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  316:             blk.34.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  317:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  318:           blk.35.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  319:           blk.35.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  320:             blk.35.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  321:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  322:             blk.35.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  323:        blk.35.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  324:             blk.35.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  325:             blk.35.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  326:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  327:           blk.36.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  328:           blk.36.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  329:             blk.36.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  330:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  331:             blk.36.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  332:        blk.36.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  333:             blk.36.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  334:             blk.36.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  335:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  336:           blk.37.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  337:           blk.37.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  338:             blk.37.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  339:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  340:             blk.37.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  341:        blk.37.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  342:             blk.37.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  343:             blk.37.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  344:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  345:           blk.38.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  346:           blk.38.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  347:             blk.38.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  348:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  349:             blk.38.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  350:        blk.38.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  351:             blk.38.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  352:             blk.38.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  353:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  354:           blk.39.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\nllama_model_loader: - tensor  355:           blk.39.ffn_gate.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  356:             blk.39.ffn_up.weight q6_K     [  5120, 13824,     1,     1 ]\nllama_model_loader: - tensor  357:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - tensor  358:             blk.39.attn_k.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  359:        blk.39.attn_output.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  360:             blk.39.attn_q.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  361:             blk.39.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\nllama_model_loader: - tensor  362:               output_norm.weight f32      [  5120,     1,     1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture str     \nllama_model_loader: - kv   1:                               general.name str     \nllama_model_loader: - kv   2:                       llama.context_length u32     \nllama_model_loader: - kv   3:                     llama.embedding_length u32     \nllama_model_loader: - kv   4:                          llama.block_count u32     \nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \nllama_model_loader: - kv  10:                       llama.rope.freq_base f32     \nllama_model_loader: - kv  11:                          general.file_type u32     \nllama_model_loader: - kv  12:                       tokenizer.ggml.model str     \nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \nllama_model_loader: - kv  19:               general.quantization_version u32     \nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q6_K:  282 tensors\nllm_load_print_meta: format           = GGUF V2 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32016\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 16384\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_head           = 40\nllm_load_print_meta: n_head_kv        = 40\nllm_load_print_meta: n_layer          = 40\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 13824\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: model type       = 13B\nllm_load_print_meta: model ftype      = mostly Q6_K\nllm_load_print_meta: model params     = 13.02 B\nllm_load_print_meta: model size       = 9.95 GiB (6.56 BPW) \nllm_load_print_meta: general.name   = codellama_codellama-13b-instruct-hf\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.12 MB\nllm_load_tensors: mem required  = 10183.96 MB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 1000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model: kv self size  =  400.00 MB\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: (null)\nggml_metal_init: default.metallib not found, loading from source\nggml_metal_init: loading './llama.cpp/ggml-metal.metal'\nggml_metal_init: loaded kernel_add                                    0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_add_row                                0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul                                    0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_row                                0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_scale                                  0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_silu                                   0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_relu                                   0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_gelu                                   0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_soft_max                               0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_soft_max_4                             0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_diag_mask_inf                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_diag_mask_inf_8                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_f32                           0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_f16                           0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q4_0                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q4_1                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q8_0                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q2_K                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q3_K                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q4_K                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q5_K                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_get_rows_q6_K                          0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_rms_norm                               0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_norm                                   0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_rope_f32                               0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_rope_f16                               0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_alibi_f32                              0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_cpy_f32_f16                            0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_cpy_f32_f32                            0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_cpy_f16_f16                            0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_concat                                 0x0 | th_max =    0 | th_width =    0\nggml_metal_init: loaded kernel_sqr                                    0x0 | th_max =    0 | th_width =    0\nggml_metal_init: GPU name:   (null)\nggml_metal_init: hasUnifiedMemory              = false\nggml_metal_init: recommendedMaxWorkingSetSize  =     0.00 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nllama_new_context_with_model: compute buffer total size = 81.13 MB\nllama_new_context_with_model: max tensor size =   128.24 MB\nggml_metal_add_buffer: error: failed to allocate 'data            ' buffer, size =     0.00 MB\nllama_new_context_with_model: failed to add buffer\nggml_metal_free: deallocating\nllama_init_from_gpt_params: error: failed to create context with model './codellama-13b-instruct.Q6_K.gguf'\nmain: error: unable to load model",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3580",
        "createdAt": "2023-10-11T09:46:46Z",
        "author": {
            "login": "uetuluk"
        }
    },
    {
        "title": "llama.cpp server support for alternate EOS/antiprompt settings to support non-llama prompt formats",
        "bodyText": "Hi there,\nsupport for the Obsidian 3B models was just added recently, however attempting to use them in multimodal form with llama.cpp server is an exercise in frustration as we have no way to set the EOS for the model, which then causes it to continue repeating itself until it caps out on tokens. I'm building out a captioning tool that is dependent on speed, so just filtering out the response past the first ### isn't a viable option.  llama.cpp main supports manually setting --reverse-prompt or even works in instruction mode (which catches the ### properly) but doesn't work as a server.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4474",
        "createdAt": "2023-12-14T17:31:17Z",
        "author": {
            "login": "SanDiegoDude"
        }
    },
    {
        "title": "Newer uncensored models",
        "bodyText": "I have been searching for a \"truly\" uncensored model like the first llama models or alpaca models if im not mistaken have been. If you ask them some weird stuff they wont say its unethical they'll just reply as normally they would.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4443",
        "createdAt": "2023-12-13T15:48:02Z",
        "author": {
            "login": "sussyboiiii"
        }
    },
    {
        "title": "Phi-1.5 support?",
        "bodyText": "It seems microsoft open-source phi-1.5 model (1.3B parameter model -> textbook is all you need) and put it in huggingface. (https://huggingface.co/microsoft/phi-1_5/tree/main), claiming comparable performance to 7B-13B models. Any chance to support this model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3139",
        "createdAt": "2023-09-12T09:10:55Z",
        "author": {
            "login": "spikespiegel"
        }
    },
    {
        "title": "What is the principle of quantize(eg. Q4_0)?",
        "bodyText": "I want to learn the principle of the quantize, but it's a little difficult for me to read the source code. So I need some formula or describe\uff0cthank you\uff01",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3546",
        "createdAt": "2023-10-08T09:03:19Z",
        "author": {
            "login": "Tr-buaa"
        }
    },
    {
        "title": "llama.cpp and lora - forcing 16 bit or a merge is kind of defeating the purpose of a separate lora file ?",
        "bodyText": "\" the simultaneous use of LoRAs and GPU acceleration is only supported for f16 models\"\nGiven LORA already forces mmap to be disabled, we have full access on the memory.\nWhy not during load: dequantize any LORA layer  to FP16 -> apply lora -> quantize again\nFor best quality it would be possible to point to a FP16 model to load the raw layer.\nLike a on-the-fly combination, so the GPU kernels will not even know it was a LORA.\nHaving to merge LORAs kind of defeats the purpose of it, that's just a preprocessed finetune ?:)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4317",
        "createdAt": "2023-12-04T03:02:20Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "How does llama.cpp use UNK token in generation?",
        "bodyText": "Hello, I wanted to know how does the UNK token is used in generation, or if it is even used. I assume the UNK token is used when there is no token with high probability during the decoding process. Does anyone have an idea?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4468",
        "createdAt": "2023-12-14T13:12:46Z",
        "author": {
            "login": "m1chae1bx"
        }
    },
    {
        "title": "How to generate multiple answers in LLAMA.ccp?",
        "bodyText": "I have only recently started to experiment with the LLAMA2 model. For example, in the API of GPT3.5, we can use the parameter n to adjust the number of outputs. How to do this in LLama.cpp?\nSpecifically.\nresponse=lcpp_llm(prompt=prompt2, max_tokens=256, temperature=0.9, top_p=1, repeat_penalty=1.2, top_k=150, logprobs=5,    echo=False,)\nHow to get :\nresponse[\"choices\"][1] or more, instead of only response[\"choices\"][0]",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4467",
        "createdAt": "2023-12-14T12:43:09Z",
        "author": {
            "login": "Hyperplane2021"
        }
    },
    {
        "title": "Phi-2 Support?",
        "bodyText": "Microsoft just released Phi-2 (2.7B params) with pretty impressive performance. Any plans to support the Phi series of models?\nBlog Post: https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\nModel: https://huggingface.co/microsoft/phi-2",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4450",
        "createdAt": "2023-12-13T20:38:15Z",
        "author": {
            "login": "z11h"
        }
    },
    {
        "title": "Better quantized models for Mistral-7B",
        "bodyText": "I have been working on improved quantization methods in a private clone of llama.cpp. While I'm not quite ready yet to publicly release the results, I'm curious to hear from others if the perplexity improvements I observe translate into actual benefits in practical use. Hence, I have decided to publish the improved quantized models for Mistral-7B on Huggingface in this repository.\nThe quantized models are fully compatible with the current llama.cpp, so can be used out-of-the-box.\nThe quantization approach for these models differs from what is available in llama.cpp by the usage of an \"importance matrix\", which is used as weights in a weighted MSE minimization when preparing the quants. The \"importance matrix\" is obtained via a \"calibration run\" that uses some training dataset (I have used the training part of Wikitext).\nThe following table shows a perplexity comparison between the improved quantized models and the current llama.cpp quants. The improvement in perplexity decreases with middle size, so I have not added Q5_K_M and Q6_K models to the comparison. The values in the Error columns are defined as (PPL(quantized model) - PPL(fp16))/PPL(fp16). All perplexity are for a context size of 512.\n\n\n\nQuantization\nPPL(llama.cpp)\nError\nPPL(new quants)\nError\n\n\n\n\nQ3_K_S\n6.0692\n6.62%\n6.0021\n5.44%\n\n\nQ3_K_M\n5.8894\n3.46%\n5.8489\n2.75%\n\n\nQ4_K_S\n5.7764\n1.48%\n5.7349\n0.75%\n\n\nQ4_K_M\n5.7539\n1.08%\n5.7259\n0.59%\n\n\nQ5_K_S\n5.7258\n0.59%\n5.7100\n0.31%\n\n\nQ4_0\n5.8189\n2.23%\n5.7924\n1.76%\n\n\nQ4_1\n5.8244\n2.32%\n5.7455\n0.94%\n\n\nQ5_0\n5.7180\n0.45%\n5.7070\n0.26%\n\n\nQ5_1\n5.7128\n0.36%\n5.7057\n0.24%\n\n\n\nGiven the interest in very small models, I have added an \"extra small\" 2-bit quantization with a model size of 2.47 GB (2.3 GiB). Except for output.weight (uses Q6_K) and attn_v.weight (uses Q4_K), all other tensors are quantized with Q2_K (so, 2.5625 bits per weight). The perplexity of this model for a context size of 512 is 6.7099, decreasing to 5.5744 for a context length of 4096.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4364",
        "createdAt": "2023-12-07T16:31:47Z",
        "author": {
            "login": "ikawrakow"
        }
    },
    {
        "title": "We need CogVLM - extremely good image and text analysis, feels like a multi generational step forward.",
        "bodyText": "I've just seen CovVLM which is a Vicuna 7B language model behind a 9B vision tower (laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) on a opensource license.\nI've compared it with llava-1.5 (not even compareable) and Qwen-VL and it beats Qwen-VL by a margin in OCR abilities, detection of details and no or almost no hallucinations.\nIt understands handwritten as well as typed letters, context, fine details, background graphics\nIt can also locate tiny visual targets with pixel coordinates\nI'm quite blown away that I didn't know it before..\nI believe that this is what we need, it has similarities to llava but adds an additional expert model, so that's not super quick to implement.\nIn addition the ViT needs K-type quantization support.\nDefinitely worth a close look\nURL: https://github.com/THUDM/CogVLM\nWebdemo: http://36.103.203.44:7861/\nPaper: https://github.com/THUDM/CogVLM/blob/main/assets/cogvlm-paper.pdf\nLook at this example, I asked for a JSON representation - not cherry picked, it can actually extract all of the content with minimal errors:\n\nThe image appears to be a driver's license from California. Here's a JSON representation:\n{\n  \"License_Number\": \"DL 11234568\",\n  \"Expiration_Date\": \"08/31/2014\",\n  \"Class\": \"C\",\n  \"License_Holder\": \"Iva Cardholder\",\n  \"Address\": \"2570 24th Street, Anytown, CA 95818\",\n  \"Veteran\": \"YES\",\n  \"Sex\": \"F\",\n  \"Hair_Color\": \"Brown\",\n  \"Eyes\": \"Brown\",\n  \"Height\": \"125 lb\",\n  \"Weight\": \"125 lb\",\n  \"Issue_Date\": \"08/31/2009\",\n  \"Birth_Date\": \"08/31/1977\",\n  \"VISUALS\": {\n      \"left\": \"Iva Cardholder\",\n      \"right\": \"Iva Cardholder\",\n      \"top\": \"California Bear\",\n      \"bottom\": \"Gold Rush Miner\"\n    }\n}\n\nHere is what QWEN-VL does:\n{\n  \"id\": \"123456\",\n  \"issue_date\": \"08/31/2014\",\n  \"expiration_date\": \"08/31/2024\",\n  \"cardholder\": {\n    \"name\": \"Ina Cordero\",\n    \"gender\": \"F\",\n    \"race\": \"A\",\n    \"eye_color\": \"B\",\n    \"hair_color\": \"R\",\n    \"eyebrow_color\": \"N\",\n    \"height\": \"5'6\"\",\n    \"weight\": \"126 lb\"\n  },\n  \"class\": \"C\",\n  \"type\": \"DRIVER LICENSE\",\n  \"state\": \"CA\",\n  \"country\": \"USA\"\n}\n\nHere is llava1.5-13B:\n{\n\"image\": \"https://i.imgur.com/39vZv.jpg\",\n\"description\": \"A California driver's license with a woman's picture on it. The license is blue and white and has a picture of a bear on it. The license number is 11324567890.\"\n}\n\nHere is GPT4-Vision:\n{\n  \"State\": \"California\",\n  \"Document_Type\": \"Driver License\",\n  \"License_Number\": \"D1234568\",\n  \"Expiration_Date\": \"08/31/2014\",\n  \"Last_Name\": \"Cardholder\",\n  \"First_Name\": \"Ima\",\n  \"Address\": \"2570 24th Street Anytown, CA 95818\",\n  \"Date_Of_Birth\": \"08/31/1977\",\n  \"Restriction\": \"None\",\n  \"Sex\": \"F\",\n  \"Height\": \"5'-05\\\"\",\n  \"Weight\": \"125 lb\",\n  \"Hair_Color\": \"BRN\",\n  \"Eye_Color\": \"BRN\",\n  \"Issue_Date\": \"08/31/2009\",\n  \"Veteran\": \"Yes\",\n  \"Organ_Donor\": \"Yes\",\n  \"Signature\": \"Ima Cardholder\"\n}\n\nI've not yet looked into architectural challenges. But this is literally game changer..\nThat's seriously good OCR and its image detection abilities are beyond anything I've remotely seen from llava 1.5/ShareGPT4V\n@monatis @FSSRepo\nUpdate:\nAdded OpenAI GPT4-Vision output for comparison. It is about the same, also with a mistake. I'd put CogVLM on the same level als GPT4-Vision based on the result. It's mostly a matter of prompt at this point (and GPT4 is leading on the language side)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4350",
        "createdAt": "2023-12-06T23:36:44Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "make LLAMA_CUBLAS=1 - nvcc fatal : Value 'native' is not defined for option 'gpu-architecture'",
        "bodyText": "I was wondering if anyone else has come across this issue before when trying to compile llama with cublas.\nIm getting an error \" Value 'native' is not defined for option 'gpu-architecture' \"\nIm trying to compile llama on Ubuntu 22.04, and I have installed 5x Nvidia P40 (24gb) and 2x Nvidia P100's (one with 12gb, and one 16gb)\nI tried to follow what was being said here, but it appears theyre trying to figure it out for windows and never came to a definite conclusion.\n#1070\nHere is my log.\nroot@devops:/ai/llm/llama.cpp.gpu# make clean\nI llama.cpp build info:\nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI LDFLAGS:\nI CC:       cc (Ubuntu 11.3.0-1ubuntu122.04.1) 11.3.0\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu122.04.1) 11.3.0\nrm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\nremoved 'common.o'\nremoved 'ggml.o'\nremoved 'k_quants.o'\nremoved 'llama.o'\nremoved 'build-info.h'\nroot@devops:/ai/llm/llama.cpp.gpu# make LLAMA_CUBLAS=1\nI llama.cpp build info:\nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\nI LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\nI CC:       cc (Ubuntu 11.3.0-1ubuntu122.04.1) 11.3.0\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu122.04.1) 11.3.0\ncc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\ncc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\nnvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\nnvcc fatal   : Value 'native' is not defined for option 'gpu-architecture'\nmake: *** [Makefile:191: ggml-cuda.o] Error 1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2142",
        "createdAt": "2023-07-08T00:12:43Z",
        "author": {
            "login": "FileDotZip"
        }
    },
    {
        "title": "It\u2019s crazy in these days, I think AI base system will soon be available",
        "bodyText": "Imagine in 2024, we will use gpt4-like local llm running in consumer laptop...",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4438",
        "createdAt": "2023-12-13T13:02:38Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Do the ready-made CUBLAS binaries not work with AXV1 / noavx2 CPU? Win11 error 0xc0000142",
        "bodyText": "When trying to run the main.exe binaries from\nllama-b1621-bin-win-cublas-cu11.7.1-x64.zip\nalways gets an error\n\nI checked both under windows 10 and 11 and gets the same error.\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020\nCuda compilation tools, release 11.2, V11.2.67\nBuild cuda_11.2.r11.2/compiler.29373293_0\n\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 546.01                 Driver Version: 546.01       CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3080      WDDM  | 00000000:03:00.0  On |                  N/A |\n| 30%   31C    P8              19W / 320W |   4886MiB / 10240MiB |     60%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\nMy CPU does not support AVX2   -  Xeon(R) CPU E5-1650 v2 @ 3.50GHz\nsystem_info: n_threads = 10 / 12 | AVX = 1 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |\nIs it because of the CPU it receives this error ?\nThe binaries for AVX1 work for me without any problem.\nIn that case, do I have to make my own compilation for CUBLAS and AVX1 ?\nI have not been able to find any information on this subject",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4417",
        "createdAt": "2023-12-11T22:28:20Z",
        "author": {
            "login": "ktostest"
        }
    },
    {
        "title": "How does the llama.cpp do batch processing?",
        "bodyText": "Hi guys,\nI'm new to the llama.cpp and ggml, I want to understand how the code does batch processing.\nI saw lines like ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens) in build_llama, where no batch dim is considered. Could you guys help me to understand how the model forward with batch input? That will help me a lot, thanks in advance! @ggerganov",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4371",
        "createdAt": "2023-12-08T03:54:54Z",
        "author": {
            "login": "xyzhang626"
        }
    },
    {
        "title": "Decreased speed when Running AI Program in Parallel",
        "bodyText": "Hello,\nI am currently using llama Cpp, and I have encountered an issue with running tasks in parallel. I am attempting to run it on 4 separate tasks parallelly, but I have noticed a significant decrease in performance (speed) compared to when running one task. The processing time becomes surprisingly slow, which costs about 2 minutes for 4 questions with codellama-13b-instruct.Q2_K.gguf.\n(In fact, I thought only 3 was in parallel processing)\nHere is a description of my working environment and configuration:\n\nPlatform: AWS EC2 instance with type of g3.8xlarge\nCPU: 32 vCPU; Memory 244GiB.\nGPU: NVIDIA Tesla M60 GPU 2*8GiB.\nCommand Executed:\n\ngit clone https://github.com/ggerganov/llama.cpp.git\nmake LLAMA_CUBLAS=1\n/home/ubuntu/server/server -m /home/ubuntu/models/codellama-13b-instruct.Q2_K.gguf --port xxxx --host xxxx -t 9 --tensor-split 5,5 -t 32 -ngl 40 -c 16384 -b 512 -np 4\nSingle Task Runtime: Average time from request to completion of generation was 29s. \nThe prompt eval speed was 52.884 tokens/second. \nEval speed was 6.498 tokens/second.\nParallel Processed Task Runtime: 10 requests were submitted. Initially, the first one was addressed.\nslot 0 : kv cache rm - [0, end)\nprompt eval time =  \u2026  35.03 tokens per second)\n eval time =  \u2026    7.87 tokens per second)\ntotal time =   28281.47 ms\nOnce the first one was completely answered, the next three questions were processed and responded to. For the final six questions, the connections were closed.\nNote that, judging from the following output, I thought that only questions 2 to 4 were processed in parallel, whereas the first question was not.\nslot 1 : kv cache rm - [0, end)\nslot 2 : kv cache rm - [0, end)\nslot 3 : kv cache rm - [0, end)\nslot 0 released (274 tokens in cache)\n\u2026\nprompt eval time =  \u2026  16.42 tokens per second)\n eval time =  \u2026    1.56 tokens per second)\nslot 2 released (131 tokens in cache)\n\nprompt eval time =  \u2026  16.42 tokens per second)\n eval time =  \u2026    1.55 tokens per second)\nslot 3 released (148 tokens in cache)\n\nprompt eval time =  \u2026  16.43 tokens per second)\n eval time =  \u2026    2.97 tokens per second)\nslot 1 released (276 tokens in cache)\n\nCould there be potential issues with the parallel processing feature of the program? Or are there any configuration settings that I might be missing for optimal parallel execution? I would appreciate your guidance to resolve this performance issue.\nThank you\nBest regards",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4369",
        "createdAt": "2023-12-08T00:35:25Z",
        "author": {
            "login": "n1330"
        }
    },
    {
        "title": "Trying to optimise the prompt eval time for a fixed input token size for llava.cpp",
        "bodyText": "I am trying to read and modify the llava-cli.cpp, llava.cpp, llama.cpp  in hope that i can improve prompt eval time\nMy total token input is limited to  644 tokens.  this incudes the image context and the text context.\n\nAs you see the prompt eval time is the the most for my case and i plan to keep input at fixed length.\nEven After setting the batch_size to token length like 644 or higher. the tokens are processed in batch less than the input value\n\nWhy is  the batch processed in steps 35, 576, 33  and 1. I would like this to happen one one go to maybe speed up the process.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4292",
        "createdAt": "2023-12-02T09:18:02Z",
        "author": {
            "login": "jjiteshh"
        }
    },
    {
        "title": "Running llama.cpp directly on iOS devices",
        "bodyText": "For my Master's thesis in the digital health field, I developed a Swift package that encapsulates llama.cpp, offering a streamlined and easy-to-use Swift API for developers. The SpeziLLM package, entirely open-source, is accessible within the Stanford Spezi ecosystem: StanfordSpezi/SpeziLLM (specifically, the SpeziLLMLocal target).\nInternally, SpeziLLM leverages a precompiled XCFramework version of llama.cpp. We chose this approach as using llama.cpp via the provided Package.swift file in the repo requires the use of unsafeFlags(_:), which prevents semantic versioning via SPM as discussed in the Swift community forum and on StackOverflow. By compiling llama.cpp into an XCFramework and exposing it as a binaryTarget(_:) in SPM, we enable proper semantic versioning of the package. You can explore the complete source code and the respective GitHub Actions here:  StanfordBDHG/llama.cpp.\nI welcome any feedback on the implementation, particularly concerning the llama.cpp inference (take a closer look at this source file)\nAn example workflow utilizing the Llama 2 7B model running on an iPhone 15 Pro with 6GB of main memory looks like this:\n(the SpeziLLM repo includes this example as a UI test application)\n\n  \n    \n    \n\n    SpeziLLM.mp4",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4423",
        "createdAt": "2023-12-12T06:13:24Z",
        "author": {
            "login": "philippzagar"
        }
    },
    {
        "title": "openai api compatibility",
        "bodyText": "I know that llama-cpp-python has openai compatibility https://abetlen.github.io/llama-cpp-python/ but it seems to me this functionality would be better in llama.cpp's server.\nThe server currently has an api, but I'm not aware of any documentation for it, or how compatible it is to openai.\nI'm using autogen with text-generation-webui and it works. I'd rather keep things simpler and just use the llama.cpp server.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3683",
        "createdAt": "2023-10-19T14:58:28Z",
        "author": {
            "login": "iplayfast"
        }
    },
    {
        "title": "gguf_init_from_buffer to be implemented?",
        "bodyText": "I'm not sure if this is the correct place to ask: I was looking through the code and hoping there was a way to stream from a file buffer so I can load from cloud store directly into ram without having to use the filesystem, when I found this:\nhttps://github.com/ggerganov/llama.cpp/blame/8a7b2fa528f130631a5f43648481596ab320ed5a/ggml.h#L2069C1-L2069C59\nIs there any current effort to implement this? I may take a stab at it, if not.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4411",
        "createdAt": "2023-12-11T19:48:33Z",
        "author": {
            "login": "Tachyon5"
        }
    },
    {
        "title": "Look that model Mixtral-8x7B-Instruct-v0.1-GGUF is crazy.",
        "bodyText": "Totally a new level of reasoning for opensource llm. Easily beat gpt 3.5 and is close to gpt4  and is not even finetuned.\nhttps://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/discussions/1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4416",
        "createdAt": "2023-12-11T22:20:40Z",
        "author": {
            "login": "mirek190"
        }
    },
    {
        "title": "Merge Kompute from GPT4ALL",
        "bodyText": "Could we merge Kompute support from GPT4ALL?\nI think they have modified llama.cpp that contain a functional Vulkan support via Kompute.\nhttps://github.com/nomic-ai/llama.cpp\nIt work great on my 7900XTX with good speed.\nIt's there any reason we cannot do that.\nI have heard a experiential implementation of kompute but not much has change\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4410",
        "createdAt": "2023-12-11T16:24:02Z",
        "author": {
            "login": "sorasoras"
        }
    },
    {
        "title": "Google just shipped libggml from llama-cpp into its Android AICore",
        "bodyText": "https://twitter.com/tarantulae/status/1733263857617895558\nIt's interesting to know.\nMaybe ggml would get first class support in android and get NPU/ASIC acceleration from google someday.\nsome discussion\nhttps://www.reddit.com/r/LocalLLaMA/comments/18eb8u0/google_just_shipped_libggml_from_llamacpp_into/",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4404",
        "createdAt": "2023-12-10T17:33:14Z",
        "author": {
            "login": "sorasoras"
        }
    },
    {
        "title": "Vertical Sharding of Models",
        "bodyText": "If we want to distribute shards of a larger model (say llama 70B or larger) across several machines, we can cut off the architecture and weights at the end of a specified transformer block, outputting intermediate activations, which would get fed back into the next shard.\nHow easy/hard would it be to generate .gguf files that don't lose performance? Is there work being done on that?\nIf not, I would love to help getting this to work, got it to work on tinygrad already.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4403",
        "createdAt": "2023-12-10T12:34:39Z",
        "author": {
            "login": "jaxs-ribs"
        }
    },
    {
        "title": "When posting a request to the server, what is the format for specifying logit biases?",
        "bodyText": "Does anyone know what the format for specifying logit biases? I can't find any info on it!\nI've been posting json like this to the lllama.cpp server, and everything works apart from the 'logit-bias'\n{\n'prompt': \"\",\n'n_predict': 128,\n'temperature': 0.9,\n'mirostat': 2,\n'mirostat-ent': 5,\n'image_data': [{'data': '', 'id': 12}],\n'stream': True,\n'logit-bias': [[, 100], [, 100], [, 100]]\n}\nFrom looking at the server code, it seems to expect an array of arrays, but when I breakpoint it it find the logit-bias array but can't handle the values inside, so does nothing.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4392",
        "createdAt": "2023-12-09T12:24:54Z",
        "author": {
            "login": "phasiclabs"
        }
    },
    {
        "title": "Mistral AI released the 8x7B MOE model",
        "bodyText": "https://twitter.com/MistralAI/status/1733150512395038967\nmagnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&dn=mixtral-8x7b-32kseqlen&tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4379",
        "createdAt": "2023-12-08T17:26:26Z",
        "author": {
            "login": "czkoko"
        }
    },
    {
        "title": "Server web UI features and direction",
        "bodyText": "removed by author",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4401",
        "createdAt": "2023-12-10T04:04:29Z",
        "author": {
            "login": "crasm"
        }
    },
    {
        "title": "Cuda not utilized for token generation but only for prompt processing",
        "bodyText": "Hi,\nI'm trying to use my RTX 4080 16GB with llamacpp, and I have a strange slow speed with token generation. I compiled llamacpp with cuda support, and when I try to use a model I get this output:\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\nllm_load_print_meta: model size     = 68,98 B\nllm_load_print_meta: general.name   = airoboros-l2-70b-2.1\nllm_load_print_meta: BOS token = 1 ''\nllm_load_print_meta: EOS token = 2 ''\nllm_load_print_meta: UNK token = 0 ''\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0,23 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  = 28558,72 MB (+ 1280,00 MB per state)\nllm_load_tensors: offloading 22 repeating layers to GPU\nllm_load_tensors: offloaded 22/83 layers to GPU\nllm_load_tensors: VRAM used: 10945 MB\nThe problem is that when I look at the usage under Ubuntu or Windows I see that the GPU is working only when llamacpp processes the prompt, but when it's generating tokens all the work is done by the CPU, while the GPU is idle. I run the test with:\n./main -t 7 -ngl 22 -m /LLM/models/airoboros-l2-70b-2.1.Q4_K_M.gguf --color -c 4096 -b 1024 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Write a short story about llamas\"\nI have the same problem even with 13B models, that fit 100% into VRAM. Even with 43/43 layers offloaded the token generation is done only by the CPU.\nCan anyone help me understand whether this is the expected behavior or a bug?\nThank you",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3027",
        "createdAt": "2023-09-05T14:10:20Z",
        "author": {
            "login": "emadeck"
        }
    },
    {
        "title": "Is it possible to use gpt-2 models with llama.cpp?",
        "bodyText": "Hi. I want to say thanks for the very cool projects llama.cpp and ggml. I use them in my iOS app. I built some of the core of my project from llama.cpp, some from examples like gpt-2.\nIn the llama.cpp code I found these lines:\nenum llm_arch {\n    LLM_ARCH_LLAMA,\n    LLM_ARCH_FALCON,\n    LLM_ARCH_BAICHUAN,\n    LLM_ARCH_GPT2,\n    LLM_ARCH_GPTJ,\n    LLM_ARCH_GPTNEOX,\n    LLM_ARCH_MPT,\n    LLM_ARCH_STARCODER,\n    LLM_ARCH_PERSIMMON,\n    LLM_ARCH_REFACT,\n    LLM_ARCH_BLOOM,\n    LLM_ARCH_STABLELM,\n    LLM_ARCH_QWEN,\n    LLM_ARCH_UNKNOWN,\n};\nAnd I can assume that somehow llama.cpp supports gpt-2 models.\nAm I right?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4380",
        "createdAt": "2023-12-08T17:39:41Z",
        "author": {
            "login": "guinmoon"
        }
    },
    {
        "title": "Does the server example support n_keep?",
        "bodyText": "I'm using the server example, and it isn't clear to me if I can specify n_keep (so that the prompt is always kept in context).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4375",
        "createdAt": "2023-12-08T11:56:23Z",
        "author": {
            "login": "d-z-m"
        }
    },
    {
        "title": "Ficta AI text interface",
        "bodyText": "https://github.com/Michael-F-Ellis/ficta\nI've just released v1.3.2 which can interface to llama.cpp server /v1/chats/completions.\nficta is a command line program that lets you use OpenAI's completion API from any text editor.\nIt exists because I found it frustrating to write short stories and essays via the ChatGPT web interface. With ficta, the developing story becomes the prompt. ficta also lets you change LLM model and parameters freely in mid-stream.\nficta attempts to adhere to the Unix/Linux philosophy that programs should do one thing well and cooperate with other programs. In the case of ficta, you specify some text files to watch and it monitors them for changes. When you edit and save a file, ficta handles sending the contents of the file to the OpenAI API endpoint and updating your file with the response.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4366",
        "createdAt": "2023-12-07T18:38:56Z",
        "author": {
            "login": "Michael-F-Ellis"
        }
    },
    {
        "title": "could'nt llama.cpp can decode with multi embeding data?",
        "bodyText": "(llava to get description of images),I have some pictures and would like to receive descriptive text about the content of the images.If there is only one image, then llava can complete this function very well\u3002I can load image as embedding data and use llama_decode for it.but now I have some images.so I want to embedding these images with batch,and decode it only once.I had modified some of the code for multi images embedding(image_filenames->clip_image_f32_batch),I want to know if llama_decode can dproduce decoding\u3002thanks for your replay\u3002",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4357",
        "createdAt": "2023-12-07T09:04:35Z",
        "author": {
            "login": "bleedingfight"
        }
    },
    {
        "title": "Caching",
        "bodyText": "I feel like I'm being a pest, but that's not my intention.  I just want to know whether the new OAI-like endpoint v1/chat/completions supports or will support caching prior text.  I can't get it to work. I've submitted two issues, #4329 and #4287, that have so far not garnered a single reply. I've also tried the unofficial support forum and stackoverflow.  I've read the docs and attempted to figure it out from the server.cpp code.\nI understand that llama.cpp is a (marvelous) volunteer effort and no one has any obligation to provide support,  but I'd greatly appreciate at least a quick \"No, it's not supported\" or \"Yes, but you're doing it wrong\" so I won't keep spinning my wheels.\nThanks,\nMike",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4341",
        "createdAt": "2023-12-05T23:08:01Z",
        "author": {
            "login": "Michael-F-Ellis"
        }
    },
    {
        "title": "VRAM on prompt processing",
        "bodyText": "Hello there, does the size of VRAM have any effect on prompt processing? I see that in CuBlas the larger the batch size, the faster the prompt processing seems to get, however also consuming more VRAM. In that case, is it worth to get an RTX 2080ti 22G (which is available at \uffe52800/$400USD in China) rather than a RTX 2080ti 11G (~\uffe51800/$250USD) just for the shake of loading larger models and faster prompt processing time?\nThank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1618",
        "createdAt": "2023-05-28T02:01:00Z",
        "author": {
            "login": "fgdfgfthgr-fox"
        }
    },
    {
        "title": "How to include the installation parameters and make BLAS arguments in the requirements.txt?",
        "bodyText": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f\nI am building a simple project in my local machine and then will deploy on AWS ubuntu server as an API.\n\ud83d\udc49\ud83c\udffb In the development environment I have installed this application using:\n$env:CMAKE_ARGS = \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"\npip install llama-cpp-python\n\ud83d\udc49\ud83c\udffb When I do the pip freeze > requirements.txt (obviously) it is not going to include these env arguments.\n\u2049 Question\nHow can In include these arguments in the requirements so that in the deployment environment I don't have to configure things manually? - And is that even possible?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4344",
        "createdAt": "2023-12-06T04:59:24Z",
        "author": {
            "login": "AayushSameerShah"
        }
    },
    {
        "title": "Where are the architectures implemented?",
        "bodyText": "Hello, I am very new to LLMs so please forgive my ignorance. I want to understand how these models are actually being implemented. So far, I understand that ggml helps with tensor ops, and the llama.cpp file has a lot of the workings of llama itself. Where are mistral and other llms implemented? Can we generate it from just the config file?\nAny help would be greatly appreciated! My goal is to understand how llama.cpp works! Cheers!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4328",
        "createdAt": "2023-12-04T21:33:31Z",
        "author": {
            "login": "noelfranthomas"
        }
    },
    {
        "title": "Add energy efficiency tracking index to llama-bench tests",
        "bodyText": "While reporting the Apple M metrics that measures Perplexity and Token per Seconds, I thought it would be nice to also track energy consumption to compare not only performance but costs to that gained performance with cores and memory scale.\nI have done an small POC and for a llama-bench with three models I got the following result:\n\nOf course, that is a very raw result as it is only a POC, however much more information can be extracted from the data source  (powermetrics), here is a sample of one datapoint:\n{'is_delta': True,\n 'elapsed_ns': 1003612708,\n 'hw_model': 'Mac13,1',\n 'kern_osversion': '23B81',\n 'kern_bootargs': '',\n 'kern_boottime': 1701259079,\n 'timestamp': datetime.datetime(2023, 12, 2, 16, 55, 39),\n 'gpu': {'freq_hz': 643.424,\n  'idle_ns': 586788125,\n  'idle_ratio': 0.584201,\n  'dvfm_states': [{'freq': 389, 'used_ns': 7373208, 'used_ratio': 0.0073407},\n   {'freq': 486, 'used_ns': 0, 'used_ratio': 0.0},\n   {'freq': 648, 'used_ns': 410266541, 'used_ratio': 0.408458},\n   {'freq': 778, 'used_ns': 0, 'used_ratio': 0.0},\n   {'freq': 972, 'used_ns': 0, 'used_ratio': 0.0},\n   {'freq': 1296, 'used_ns': 0, 'used_ratio': 0.0}],\n  'sw_requested_state': [{'sw_req_state': 'P1',\n    'used_ns': 0,\n    'used_ratio': 0.0},\n   {'sw_req_state': 'P2', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_req_state': 'P3', 'used_ns': 417857000, 'used_ratio': 1.0},\n   {'sw_req_state': 'P4', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_req_state': 'P5', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_req_state': 'P6', 'used_ns': 0, 'used_ratio': 0.0}],\n  'sw_state': [{'sw_state': 'SW_P1', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_state': 'SW_P2', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_state': 'SW_P3', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_state': 'SW_P4', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_state': 'SW_P5', 'used_ns': 0, 'used_ratio': 0.0},\n   {'sw_state': 'SW_P6', 'used_ns': 0, 'used_ratio': 0.0}],\n  'gpu_energy': 117}}\nHere is the dirty & quick python code used to create the graph:\nimport plistlib\nimport pandas\nimport matplotlib.pyplot as plt\n\n\nxmls = open(\"sample.out\")\nplist = xmls.read()\n\nrecords = []\n\nraw_samples = plist.split(\"</plist>\")\n\n\nfor raw_sample in raw_samples[:-1]:\n    records.append(\n      plistlib.loads(\n        (\n            raw_sample + \"</plist>\"\n        ).strip(\"\\n\").strip(\"\\x00\").encode(\"utf-8\")\n      )\n    )\n\ndf = pandas.DataFrame(columns=[\"frequency\", \"energy\"])\nfor record in records:\n    df.loc[len(df)] = [record['gpu']['freq_hz'], record['gpu']['gpu_energy']/1000]\n\n\nax1 = df[0:100].frequency.plot(color='r', marker='x')\nax2 = df[0:100].energy.plot(secondary_y=True, color='k', marker='o')\n\nax1.set_ylabel(\"Frequency (MHz)\")\nax2.set_ylabel(\"Energy (Watts)\")\nplt.show()\nand here the PowerMetrics command line to track the test:\nsudo nice -n 10 powermetrics --samplers gpu_power -f plist -i 1000 > sample.out\nllama-bench command used to generate this data:\n./llama-bench \\\n  -m ../llama/llama-2-7b/ggml-model-f16.gguf \\\n  -m ../models/llama2/llama-2-7b.Q8_0.gguf \\\n  -m ../models/llama2/llama-2-7b.Q4_0.gguf \\\n  -p 512 -n 128 -ngl 99 2> /dev/null\n\nPowermetrics offers much more metrics through samplers, I only use gpu_power as it would be the more juicy one, however there is a thermal energy tracking as well that could be useful for benchmarking.\nIn order to make this work we would need to:\n\nkick in its own process the powermetrics tool when the test starts and stop it when the test finishes\neither capture and process data in c++ and determine what should be the metric/index for each test (watts per hour?)\noptionally dump the raw data into a file to post process it for further study.\n\nIf there is enough interest on it, I could use some spare time to work on it.\nRelated projects:\n\nhttps://github.com/tlkh/asitop/blob/main/asitop/parsers.py",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4297",
        "createdAt": "2023-12-02T18:14:39Z",
        "author": {
            "login": "RafaAguilar"
        }
    },
    {
        "title": "local using network activity",
        "bodyText": "i am seeing network usage when i run the local llama.cpp on ubuntu, where can i find more info on this?\nim looking for llm generators that dont use any network whatsoever",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4290",
        "createdAt": "2023-12-02T08:51:43Z",
        "author": {
            "login": "d7zda"
        }
    },
    {
        "title": "even after offloading all layers to gpu, is it normal for CPU to use up all threads at 100% utilization?",
        "bodyText": "even after offloading all layers to gpu, is it normal for CPU to use up all threads at 100% utilization?\ni thought i already offloaded to gpu (which i did) but why is CPU still being used?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3210",
        "createdAt": "2023-09-16T04:46:14Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "how to enable cublas : GGML CUDA Force MMQ in compilation?",
        "bodyText": "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found 1 CUDA devices:\nDevice 0: Tesla P40, compute capability 6.1\nI am looking for a way to enable force MMQ but it does not seems to work.\nCMake Warning:\nManually-specified variables were not used by the project:\nLLAMA_CUDA_USE_TENSOR_CORES",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4320",
        "createdAt": "2023-12-04T08:07:01Z",
        "author": {
            "login": "sorasoras"
        }
    },
    {
        "title": "Embeddings and GPU usage",
        "bodyText": "When generating embeddings (both using the embedding utility compiled from this repo, and also using langchain and llama-cpp-python) on a MacBook M1, I've noticed that there seem to be no additional load on the GPU, but rather puts the load on the CPU.\nIt's most likely me that is missing something, but I was under the impression that generating the embeddings could be GPU accelerated as well?\nWhen prompting the model with the main utility (and also langchain and llama-cpp-python), the GPU sees usage. Looking at the output, it does seem like Metal is being initiated:\nllama.cpp: loading model from /Users/user/projects/models/llama2/llama-2-7b-chat/ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\nllama_new_context_with_model: kv self size  =  256.00 MB\nggml_metal_init: allocating\nggml_metal_init: using MPS\nggml_metal_init: loading '/Users/user/projects/llmExperiments/venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\nggml_metal_init: loaded kernel_add                            0x127c0a610\nggml_metal_init: loaded kernel_add_row                        0x127c0a870\nggml_metal_init: loaded kernel_mul                            0x127c0aad0\nggml_metal_init: loaded kernel_mul_row                        0x127c0ad30\nggml_metal_init: loaded kernel_scale                          0x127c0af90\nggml_metal_init: loaded kernel_silu                           0x127c0b1f0\nggml_metal_init: loaded kernel_relu                           0x127c0b450\nggml_metal_init: loaded kernel_gelu                           0x127c0b6b0\nggml_metal_init: loaded kernel_soft_max                       0x127c0b910\nggml_metal_init: loaded kernel_diag_mask_inf                  0x127c0bb70\nggml_metal_init: loaded kernel_get_rows_f16                   0x127c0bdd0\nggml_metal_init: loaded kernel_get_rows_q4_0                  0x127c0c030\nggml_metal_init: loaded kernel_get_rows_q4_1                  0x127c0c290\nggml_metal_init: loaded kernel_get_rows_q2_K                  0x127c0c4f0\nggml_metal_init: loaded kernel_get_rows_q3_K                  0x127c0c750\nggml_metal_init: loaded kernel_get_rows_q4_K                  0x127c0c9b0\nggml_metal_init: loaded kernel_get_rows_q5_K                  0x127c0cc10\nggml_metal_init: loaded kernel_get_rows_q6_K                  0x127c0ce70\nggml_metal_init: loaded kernel_rms_norm                       0x127c0d0d0\nggml_metal_init: loaded kernel_norm                           0x127c0d5c0\nggml_metal_init: loaded kernel_mul_mat_f16_f32                0x127c0db60\nggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x127c0ddc0\nggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x127c0e020\nggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x127c0e280\nggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x127c0e4e0\nggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x127c0e740\nggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x127c0e9a0\nggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x127c0f040\nggml_metal_init: loaded kernel_rope                           0x127c0f560\nggml_metal_init: loaded kernel_alibi_f32                      0x127c0fe20\nggml_metal_init: loaded kernel_cpy_f32_f16                    0x127c106b0\nggml_metal_init: loaded kernel_cpy_f32_f32                    0x127c10f40\nggml_metal_init: loaded kernel_cpy_f16_f16                    0x127c116b0\nggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\nggml_metal_init: hasUnifiedMemory             = true\nggml_metal_init: maxTransferRate              = built-in GPU\nllama_new_context_with_model: max tensor size =   102.54 MB\nggml_metal_add_buffer: allocated 'data            ' buffer, size =  3648.31 MB, ( 3648.77 / 21845.34)\nggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.77 / 21845.34)\nggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.77 / 21845.34)\nggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.77 / 21845.34)\nggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.77 / 21845.34)\nAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n\nllama_print_timings:        load time =  3358.52 ms\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings: prompt eval time =  3358.23 ms /   137 tokens (   24.51 ms per token,    40.80 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time =  3359.20 ms\n\nllama_print_timings:        load time =  3358.52 ms\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings: prompt eval time =  2740.87 ms /    97 tokens (   28.26 ms per token,    35.39 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time =  2742.21 ms\n\nllama_print_timings:        load time =  3358.52 ms\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings: prompt eval time =  2330.50 ms /    96 tokens (   24.28 ms per token,    41.19 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time =  2331.56 ms\n\n... Cut for brevity",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2658",
        "createdAt": "2023-08-18T11:14:17Z",
        "author": {
            "login": "MadsRC"
        }
    },
    {
        "title": "Recomputing similar layers? (Layer substitution)",
        "bodyText": "@KerfuffleV2 I was interested by your layer skipping work and wanted to pose a question: If we can identify the layers that are doing \"more or less\" the same thing as another layer, and then run them again while they're still loaded into VRAM, can we effectively skip layers by 'substituting' them?\nEssentially what I'm thinking is:\n\nWe initialize some random logit scores (I'm not sure how the input scores are typically initialized, is it at zero for all token embeddings? I think logits is not the word I want to use for what the start of this process looks like), then run each layer on those predetermined values, and see which layers have the highest similarity in terms of how the activation changed the scores (via some similarity metric)\nInstead of skipping those 'similar layers' entirely, we instead choose to compute the same layer twice, which in theory means that less total layers are loaded into memory, but we are 'sharing' layers instead.\nSo if layers 13 and 16 seem to have an extremely similar activation impact, we instead keep layer 13 and run it again where the 16th layer in the original model would have been ran\n\nI recognize I'm probably making a lot of assumptions here, and I could be extremely off on the practicality, but I think it's maybe worth exploring, especially for larger models that might have more 'redundancy'.\nThe assumption I'm making is that certain layers might share a lot of similarities, or may be near identical in how they change the activations in isolation, but the order in which they are applied inherently matters. For example, if there's a shared group of 'grammar layers' (this is more so theoretically speaking, we don't really know what the hidden layers are doing precisely), and every couple layers it 'checks for grammar', but they all same the serve ultimate 'purpose', this is what that would ultimately aim to optimize for.\nThe impact of each layer is context-dependent, yes, but how much of the layers are truly 'shared' in what they are attempting to achieve, especially in models with 40+ layers? The process of hidden layers being applied sequentially is what led me to this concept.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4303",
        "createdAt": "2023-12-03T05:29:19Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Can Convert to GGUF use multi-core?",
        "bodyText": "This is when using convert.py.\nIs it possible to speed up the conversion from Hugging Face Model to GGUF by utilizing multi-cores?\nI'm using MacStudio, but it has 24 cores, but only 1 core is used, so I feel like it's a waste.\nThere are mainly three things I would like to ask:\n\nI would like to know if the feature is already implemented.\nare there any plans to implement this feature?\nI would like to ask if there is a reason why multi-core processing (parallel processing) is theoretically impossible in the first place.\n\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4262",
        "createdAt": "2023-11-30T06:45:42Z",
        "author": {
            "login": "Taikono-Himazin"
        }
    },
    {
        "title": "Could performant GPU decompression algorithms increase memory?",
        "bodyText": "Inspired by a discussion from the localllama subreddit.\n\n\nmy question is not identical to the idea explored above, which uses high bandwidth storage\n\nIf we loaded a 180B f16 model to RAM, pre-compressed by a factor of 20 (18GB total), then sent pieces to the GPU for decompression (1GB->20gb) and ran inference, would we be able to run this massive model at a tolerable speed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4276",
        "createdAt": "2023-12-01T04:48:56Z",
        "author": {
            "login": "BarfingLemurs"
        }
    },
    {
        "title": "AMD Instinct MI100 ROCm Guide for llama.cpp",
        "bodyText": "I wanted to share what I've learned in the many many hours spent trying to get x2 MI100s working with llama.cpp. This guide may also apply to other cards like the MI25, MI60, MI200, MI300 etc. Hopefully this saves someone from the nightmare that is ROCm. If anyone has any questions please don't hesitate to ask me!\nThe Problem: x1 MI100 works fine on Arch Linux however x2+ GPUs results in segfaults and eventually crashes. I believe this may be caused due to the requirement of the amdgpu-dkms proprietary driver but I'm not 100% sure.\nErrors on Arch Linux with the latest rocm-hip-sdk\n\namdgpu: init_user_pages: Failed to get user pages: -1\namdgpu: init_user_pages: Failed to get user pages: -1\namdgpu: init_user_pages: Failed to get user pages: -1\n... (100s of these errors followed by a segfault)\n:1:rocmemory.cpp            :945 : 3790242352 us: 64007: [tid:0x6cfb0df5cc00] Failed to lock memory to pool, failed with hsa_status: 4096\n:1:rocdevice.cpp            :1897: 3790242359 us: 64007: [tid:0x6cfb0df5cc00] Failed creating memory\n:1:memory.cpp               :347 : 3790242365 us: 64007: [tid:0x6cfb0df5cc00] Video memory allocation failed!\n:1:memory.cpp               :308 : 3790242370 us: 64007: [tid:0x6cfb0df5cc00] Can't allocate memory size - 0x05BE1000 bytes!\n:1:rocblit.cpp              :2668: 3790242374 us: 64007: [tid:0x6cfb0df5cc00] Buffer create failed, Buffer: 0xb9c535f0\n:1:rocdevice.cpp            :3253: 4009260835 us: 64502: [tid:0x6d0daeb8cc00] hsa_amd_pointer_info() failed\n\nSteps to get Multi-GPU working\n\nFollow https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF and pass-through all Instinct cards into a VM with Ubuntu 22.04 or a Ubuntu 22.04 based distribution.\nSetup ROCm first before adding the GPUs into the VM else you will have to deal with the GPU Reset Bug when restarting.\nInstall the following version of libstdc++ with $ sudo apt install libstdc++-12-dev\nFinish setting up ROCm 5.6 using this guide and make sure the environment variables are set: https://rocm.docs.amd.com/en/latest/deploy/linux/os-native/install.html\nShutdown and add the Instinct Cards to your VM\nFinish your install of llama.cpp normally by compiling with LLAMA_HIPBLAS=1 and enjoy!\n\nAdditional Notes:\n\nDisable CSM in BIOS if you are having trouble detecting your GPU.\nIf you run into issues compiling with ROCm, try using cmake instead of make.\namdgpu-install may have problems when combined with another package manager. It's better to stick to 1 install method.\nDon't forget to edit LLAMA_CUDA_DMMV_X, LLAMA_CUDA_MMV_Y etc for slightly better t/s.\nUse AMD_LOG_LEVEL=1 when running llama.cpp to help with troubleshooting.\nDocker seems to have the same problem when running on Arch Linux.\n\nx2 MI100 Speed - 70B t/s with Q6_K\n\nllama_print_timings:      sample time =   412,48 ms /   715 runs   (    0,58 ms per token,  1733,43 tokens per second)\nllama_print_timings: prompt eval time =  5360,81 ms /   262 tokens (   20,46 ms per token,    48,87 tokens per second)\nllama_print_timings:        eval time = 85709,90 ms /   713 runs   (  120,21 ms per token,     8,32 tokens per second)\n\nI hope this saves someone from the nightmare of ROCm. Have fun!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2824",
        "createdAt": "2023-08-27T05:42:47Z",
        "author": {
            "login": "Trat8547"
        }
    },
    {
        "title": "Can we fine-tune LLaVa models in llama.cpp?",
        "bodyText": "I understand there is inference support for LLaVa models in llama.cpp now, but is it possible to finetune them, too? Are the image embeddings fundamentally incompatible with llama.cpp's finetune program, or could finetuning of LLaVa be done in a similar way as regular text-only LLMs?\nIf anything, would someone please explain what the mmproj/llava projector is, and why it doesn't fit in the GGUF file? Maybe that will help answer the question, too.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4266",
        "createdAt": "2023-11-30T10:16:03Z",
        "author": {
            "login": "mashdragon"
        }
    },
    {
        "title": "BOS always has the highest probability in logits?",
        "bodyText": "I'm trying to build a Rust integration with llama.cpp, but I'm having an issue where everything works fine except that the BOS token (the second value, with index of 1) always has the highest probability in logits.\n\nThis causes it to always be picked with greedy sampling, which causes the model to never be able to complete anything because BOS is always empty.\nI know I could probably manually exclude this token or use some higher-quality sampling or something, but I've been using simple.cpp as a reference, and I see absolutely no code in there that looks like it could be accounting for this sort of issue.\nIs it known why/how this happens and what is the best way to prevent / work around it?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4264",
        "createdAt": "2023-11-30T07:58:15Z",
        "author": {
            "login": "LoganDark"
        }
    },
    {
        "title": "GBNF Function Calling Grammar Generator for llama.cpp to make function calling with every model supporting grammar based sampling. (most models, I only had problems with Deepseek Code Instruct)",
        "bodyText": "Hello! \ud83d\udc4b\nI'd like to introduce a tool I've been developing: a GGML BNF Grammar Generator tailored for llama.cpp.\n\ud83d\udd0d Features:\n\nGGML BNF Grammar Creation: Simplifies the process of generating grammars for LLM function calls in GGML BNF format.\nAutomatic Documentation: Produces clear, comprehensive documentation for each function call, aimed at improving developer efficiency.\nUser-Friendly Design: Though straightforward to use, it doesn't compromise on functionality, making it suitable for a wide range of developers.\n\nAt the moment it supports following types as function parameter:\n\nstring\nboolean\nnumber\nfloat\n\n\ud83d\udc68\u200d\ud83d\udcbb Benefits for llama.cpp Users:\n\nIncreased Accuracy: Helps reduce syntax errors and misunderstandings in LLM function calls.\n\n\ud83d\udd17 https://github.com/Maximilian-Winter/llama_cpp_function_calling\n\ud83e\udd1d Looking for Input:\nWould be happy to hear from you \u2013 whether it's feedback, potential contributions, or just a discussion on future improvements. Let's collaborate to enhance the llama.cpp coding experience!\nExample output using OpenHermes and the example functions in gpt_functions.py\n>Can you write a long poem about the USA in the \"HelloUSA.txt\" file?\n<|im_start|>system\nAvailable Functions:\n\nsend_message:\n  Description:Sends a message to the User.\n  Parameters:\n    inner_thoughts (string, required): Your inner thoughts or inner monologue while writing the message.\n    message (string, required): Message you want to send to the user.\n\nread_file:\n  Description:Returns content of a file.\n  Parameters:\n    inner_thoughts (string, required): Your inner thoughts or inner monologue while writing the file path.\n    File (string, required): The path of the file you want to open.\n    require_heartbeat (boolean, required): Set this to true to get control back after execution, to chain functions together.\n\nwrite_file:\n  Description:Writes to a file.\n  Parameters:\n    inner_thoughts (string, required): Your inner thoughts or inner monologue while writing the file.\n    File (string, required): The path of the file you want to write.\n    Content (string, required): The content of the file you want to write.\n    require_heartbeat (boolean, required): Set this to true to get control back after execution, to chain functions together.\n\npython_interpreter_command:\n  Description:Execute Python command.\n  Parameters:\n    inner_thoughts (string, required): Your inner thoughts or inner monologue while writing the command.\n    command (string, required): The Python command to execute.\n    require_heartbeat (boolean, required): Set this to true to get control back after execution, to chain functions together.<|im_end|>\n<|im_start|>user\nMaximilian: \"Can you write a long poem about the USA in the \"HelloUSA.txt\" file?\" <|im_end|>\n<|im_start|>assistant\nLlama.generate: prefix-match hit\n{\"function\":\"write_file\",\"params\":{\"inner_thoughts\":\"I'm thinking about composing a poem on the United States and saving it as 'HelloUSA.txt'.\",\"File\":\"HelloUSA.txt\",\"Content\":\"In search of liberty, they came from far and wide,\\nA melting pot of cultures, a land so grand and vast.\\nFrom coast to coast, through mountains high and tide,\\nTheir dreams intertwined, their stories merged at last.\\n\\nBoldly they ventured, brave and free,\\nSeeking opportunity, hope, and solace too,\\nWith unity and strength, they forged a nation strong,\\nAnd freedom's light would forever shine anew.\\n\\nThrough strife and struggle, battles fierce and fight,\\nThey built a home where all could find their place,\\nWhere justice reigns, and rights are right,\\nAnd every soul can carve its own unique grace.\\n\\nUnited we stand, with colors bold,\\nRed, white, and blue our flag unfurls,\\nTogether now, our future told,\\nOf unity, not division, evermore.\\n\\nThis land of dreams, where destiny unfolds,\\nA beacon bright, a shining star that guides,\\nA story written by hearts of gold,\\nAcross these lands, across these tides.\",\"require_heartbeat\":true}}",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4273",
        "createdAt": "2023-11-30T20:37:13Z",
        "author": {
            "login": "Maximilian-Winter"
        }
    },
    {
        "title": "KeyError: ('torch._utils', '_rebuild_parameter') when convert  CodeShell-7B-Chat-int4",
        "bodyText": "when use convert.py convert  CodeShell-7B-Chat-int4\\pytorch_model.bin output below error, please help me find reason,thanks!\n(base) D:\\llama_tools>Python convert.py models/CodeShell-7B-Chat-int4/\nLoading model file models\\CodeShell-7B-Chat-int4\\pytorch_model.bin\nTraceback (most recent call last):\nFile \"D:\\llama_tools\\convert.py\", line 1229, in \nmain()\nFile \"D:\\llama_tools\\convert.py\", line 1162, in main\nmodel_plus = load_some_model(args.model)\nFile \"D:\\llama_tools\\convert.py\", line 1077, in load_some_model\nmodels_plus.append(lazy_load_file(path))\nFile \"D:\\llama_tools\\convert.py\", line 751, in lazy_load_file\nreturn lazy_load_torch_file(fp, path)\nFile \"D:\\llama_tools\\convert.py\", line 707, in lazy_load_torch_file\nmodel = unpickler.load()\nFile \"D:\\llama_tools\\convert.py\", line 696, in find_class\nreturn self.CLASSES[(module, name)]\nKeyError: ('torch._utils', '_rebuild_parameter')",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4255",
        "createdAt": "2023-11-29T15:11:55Z",
        "author": {
            "login": "workmanihcj"
        }
    },
    {
        "title": "Please help decode",
        "bodyText": "need help, how to get a list of floats from this ? num_elements = 4194304\n(gdb) p *tensor\n$4 = {type = GGML_TYPE_Q4_0, backend = GGML_BACKEND_CPU, buffer = 0x0, n_dims = 2, ne = {          4096, 1024, 1, 1}, nb = {18, 2304, 2359296, 2359296}, op = GGML_OP_NONE,?\nhttps://twitter.com/introsp3ctor/status/1729646601155600837?t=XEOrcmg9d3LvLq4TX6bAlg&s=19\nhttps://twitter.com/introsp3ctor/status/1729651540892307469?t=w6k5V4gqXVyCzh6XfzmVwQ&s=19\nBasically I'm trying to figure out how to decode these quantitized columns because they're showing as being absolutely huge\nOne of them had 300,000 elements in it",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4250",
        "createdAt": "2023-11-29T02:09:51Z",
        "author": {
            "login": "jmikedupont2"
        }
    },
    {
        "title": "Potential ideas for LoRA adapters",
        "bodyText": "Hi,\nThank you for the repo and initial lora adapter support.\nWe explored a few experiments in the fastLLaMa repo.\nWhat we did:\n\nCached the lora matrix multiplication results in the convert-lora-to-ggml.py script.\n\nInstead of performing the calculation during runtime,\nwe cached and saved these results so when we load the adapters, it is faster.\nWe also have arguments that allow users to use the same implementation as seen in this repo.\nWe also have an argument that lets the users cache the lora adapters in fp32 or fp16 mode.\nfp16 mode seemed to be decent and might benefit downstream applications.\n\n\nAdded support for detaching lora adapters.\n\nWe added support to detach adapters in llama.cpp here.\n\n\n\nDo these features seem like something that would be relevant to this repo?\nIf yes we would be happy to help implement these!\nHappy hacking :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1101",
        "createdAt": "2023-04-21T13:55:00Z",
        "author": {
            "login": "PotatoSpudowski"
        }
    },
    {
        "title": "Possible performance degradation. Also which BLAS implementation is right for me?",
        "bodyText": "I used to get around 6.5-7.5 tokens a second on a llama2-70b model. now I get 3.5 t/s. (cpu only)\nI've been out of the AI game for a little over a month so my system software has updated a lot.\nThe things that have changed on my end is I'm now using gguf instead of ggml so maybe I missed when the change is actually happening, but I feel like that isn't it. maybe it's the memory mapping? I've heard that's slower.\nI can try it with --no-mmap to see if that makes a difference, but I was wondering if anyone has an idea.\nAlso side note will Intel MKL use both my cpu and gpu? I was actually wondering if intel oneapi might be better for me and then saw today that it looks like that's what Intel MKL is. right now I build with openblas support and get blas=1 when running, but running with and without blas doesn't seem to make a difference.\nEPYC Milan-X 7473X 24-Core 2.8GHz 768MB L3\n\n512GB of HMAA8GR7AJR4N-XN HYNIX 64GB (1X64GB) 2RX4 PC4-3200AA DDR4-3200MHz ECC RDIMMs\n\nMZ32-AR0 Rev 3.0 motherboard\n\n6x 20tb WD Red Pros on ZFS with zstd compression\n\nSABRENT Gaming SSD Rocket 4 Plus-G with Heatsink 2TB PCIe Gen 4 NVMe M.2 2280\n\nAMD 7900xtx",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4239",
        "createdAt": "2023-11-27T17:41:02Z",
        "author": {
            "login": "sirus20x6"
        }
    },
    {
        "title": "What is the memory bandwidth of NVIDIA P40",
        "bodyText": "Regarding the memory bandwidth of the NVIDIA P40, I have seen two different statements. One is from the NVIDIA official spec, which says 347 GB/s, and the other is from the TechpowerUP database, which says 694.3 GB/s. So, what exactly is the bandwidth of the P40? Does anyone know?\n\n\nhttps://resources.nvidia.com/en-us-virtualization-and-gpus/p40-product-brief\nhttps://www.techpowerup.com/gpu-specs/tesla-p40.c2878",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4221",
        "createdAt": "2023-11-26T00:11:56Z",
        "author": {
            "login": "bobqianic"
        }
    },
    {
        "title": "Splitting the Heat: Input Temperature & Output Temperature",
        "bodyText": "At the moment, LLM software seems to have a schism in the implementation of the temperature option, as I documented here:\n#3914\nI realized that it might be best to modify it so that you can do two temperature 'passes':\n\nAn Input Temperature which is ran before any other sampler and changes the original distribution before any changes are made\nAn Output Temperature which will come last after all the truncation samplers (such as Top K, Top P, etc) have been ran.\n\nThe expected implementation of Temperature (as it is used in OpenAI's models and also inference backends) is to modify the original distribution so that truncation samplers such as Top P or Top K aren't strictly necessary, but the current implementation as it is in llama.cpp functions like my description of Output Temperature.\nThis can be confusing because truncation fundamentally changes the output in a way that is very similar to lower temperature, except it explicitly cuts out bad choices to do this rather than scaling the model's confidence. This is not a flawed approach, but we want to have interpretability in what the model is doing in response to sampler changes instead of people just setting options they don't understand and getting a very skewed and sometimes unnatural representation of what the model is actually predicting.\nThis would give users freedom because:\n\nYou can apply temperature after the model has selected a set of high quality candidates (post-truncation) to 'randomize' in a way that won't invite the 'low quality' token choices, but instead just works like a way to make the model avoid overly predictable outputs while staying in a safe range.\nYou can apply temperature before the model has selected its list of candidates in the case that you wanted the make the raw probabilities a little less pre-determined overall before you cut out the unlikely candidates.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4237",
        "createdAt": "2023-11-27T13:49:12Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Does the gguf file support additional training?",
        "bodyText": "Can the gguf file be retrained with a novel scenario text file?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4222",
        "createdAt": "2023-11-26T03:50:35Z",
        "author": {
            "login": "kbuwel"
        }
    },
    {
        "title": "Safetensors conversion error: KeyError: 'transformer.h.0.input_layernorm.bias'",
        "bodyText": "Hello everyone! \ud83d\udc4b\nI am trying to convert Poro-34B safetensors-formatted models into GGUF with:\npython3 convert.py ../Poro-34B/\n\nHowever, I get the following error:\nLoading model file ../Poro-34B/model-00001-of-00014.safetensors\nLoading model file ../Poro-34B/model-00001-of-00014.safetensors\nLoading model file ../Poro-34B/model-00002-of-00014.safetensors\nLoading model file ../Poro-34B/model-00003-of-00014.safetensors\nLoading model file ../Poro-34B/model-00004-of-00014.safetensors\nLoading model file ../Poro-34B/model-00005-of-00014.safetensors\nLoading model file ../Poro-34B/model-00006-of-00014.safetensors\nLoading model file ../Poro-34B/model-00007-of-00014.safetensors\nLoading model file ../Poro-34B/model-00008-of-00014.safetensors\nLoading model file ../Poro-34B/model-00009-of-00014.safetensors\nLoading model file ../Poro-34B/model-00010-of-00014.safetensors\nLoading model file ../Poro-34B/model-00011-of-00014.safetensors\nLoading model file ../Poro-34B/model-00012-of-00014.safetensors\nLoading model file ../Poro-34B/model-00013-of-00014.safetensors\nLoading model file ../Poro-34B/model-00014-of-00014.safetensors\nTraceback (most recent call last):\n  File \"convert.py\", line 1228, in <module>\n    main()\n  File \"convert.py\", line 1161, in main\n    model_plus = load_some_model(args.model)\n  File \"convert.py\", line 1078, in load_some_model\n    model_plus = merge_multifile_models(models_plus)\n  File \"convert.py\", line 593, in merge_multifile_models\n    model = merge_sharded([mp.model for mp in models_plus])\n  File \"convert.py\", line 572, in merge_sharded\n    return {name: convert(name) for name in names}\n  File \"convert.py\", line 572, in <dictcomp>\n    return {name: convert(name) for name in names}\n  File \"convert.py\", line 547, in convert\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n  File \"convert.py\", line 547, in <listcomp>\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\nKeyError: 'transformer.h.0.input_layernorm.bias'\n\nAm I doing something wrong, or is this particular variation of safetensors unsupported? \ud83e\udd14\n(Using the most recent lama.cpp HEAD 8e672efe632bb6a7333964a255c4b96f018b9a65)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4175",
        "createdAt": "2023-11-22T21:57:23Z",
        "author": {
            "login": "villesundell"
        }
    },
    {
        "title": "Measured Quantization probability differences for Mistral 7b and Llama 13b",
        "bodyText": "https://www.reddit.com/r/LocalLLaMA/comments/1816h1x/how_much_does_quantization_actually_impact_models/\nMistral 7b average KL divergence:\n\nLlama 13b average KL divergence:",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4183",
        "createdAt": "2023-11-23T11:54:18Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Endless Responses and Imaginary Conversations in llama.cpp",
        "bodyText": "When running llama.cpp with the provided command in the terminal, the models' responses extend beyond the expected answers, creating imaginary conversations. Instead of succinctly answering questions, the output includes system information and unrelated details. This behavior persists across various models. The issue interferes with the intended use of the models.\nExecute the following command in the terminal:\n./main -ngl 32 -m ./models/openhermes-2.5-mistral-7b-16k.Q4_K_M.gguf -p \"system \\n Your name is nevil.  \\n user \\n Who are you?  \\n assistant\"\nThe model's response includes extraneous system information and unrelated details, deviating from the expected behavior.\nsystem_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n <|im_start|>system \\n Your name is nevil. <|im_end|> \\n <|im_start|>user \\n Who are you? <|im_end|> \\n <|im_start|>assistant|e I am Nevil, the AI language model trained by OpenAI. How can I assist you today? <|im_end|>\\n \n  <|im_start|>user \\n Can you tell me something about OpenAI? <|im_end|>\\n \n<|im_start|>assistant \\n Of course! OpenAI is a research laboratory focused on artificial intelligence. It was founded in 2015 by several individuals including Elon Musk, Sam Altman and Ilya Sutskever, among others. The organization strives to build safe and beneficial AI systems that can bring significant value to the world. Their primary aim is to develop and promote AI technology that will benefit everyone.\\n\\n OpenAI has made several breakthroughs in artificial intelligence research, including the development of the GPT series (Generative Pre-trained Transformer), which powers many AI language models like myself, DALL-E for image generation, and the FiveThirtyThree model for predicting elections. Their work is diverse and impressive! Let me know if you'd like to learn more or need assistance with anything related to OpenAI. <|im_end|> \n<|im_start|>user \\n What can GPT series do? <|im_end|>\\n [end of text]\n\n\nAnd so on with other models I have tried, how can I fix it.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4187",
        "createdAt": "2023-11-23T20:13:59Z",
        "author": {
            "login": "jovanlopez32"
        }
    },
    {
        "title": "Batch Generation from a List of Prompts, Multiple Completions per Prompt",
        "bodyText": "For creating a small text corpus for some training experiments,\nI'm searching for an option to create a large amount outputs from a list of prompts,\nwith n outputs per prompt with a configurable temperature.\nThis would be very useful to have I think, for example:\nprompt1: Create a fantasy story featuring a fairy and an elf as characters\nprompt2: Create a fantasy story featuring an ogre and a knight as characters\n... and so on.\n\nFor each prompt n outputs which temperature t.\nI searched in the code, but couldn't  find a simple method to do that.\nI can code this too, if someone would guide me to the right place in the code to add this functionality.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4191",
        "createdAt": "2023-11-23T23:05:16Z",
        "author": {
            "login": "tzipproth"
        }
    },
    {
        "title": "interactive mode gpu utilization",
        "bodyText": "is it normal for the gpu% shown under rocm-smi to remain 100% during interactive mode while idling? (not sending prompts or outputs)\ni feel like the usage should be closer to 0% while the vram% should still indicate the loaded LLM mem usage.\ni recently setup stable-diffusion-webui with the deforum extension and it seemed to jack up rocm a bit, because even after i close the terminal sessions and log out then log back in, it still shows 100% gpu usage with the high avgpwr wattage.\nim concered about the high avgpwr, i dont want high power consumption or gpu usage with little activity\nnov 24 edit:\ni remember a couple of days ago of chatting with the model and the PC was very quiet, the fans were silent and the gpu AVGPWR did not budge from 110w more or less.\nNow, its everytime that i prompt the fans increase their speed like crazy and the gpu avgpwr shoots up to near cap of around 320w or so, then when it completes the response it settles back down.\nis there a setting to get it like before?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4144",
        "createdAt": "2023-11-20T07:04:16Z",
        "author": {
            "login": "d7zda"
        }
    },
    {
        "title": "Finetune: Not training on input part of prompt?",
        "bodyText": "Hi,\nthis is just a discussion topic. axolotl has an option train_on_inputs: false, where human input is masked from labels during finetuning. I do not know if it is useful and if it should be supported (for example by another tag that would separate human input and desired output in the dataset). What do you think?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4193",
        "createdAt": "2023-11-24T02:18:22Z",
        "author": {
            "login": "jooray"
        }
    },
    {
        "title": "[User] ChatML Support",
        "bodyText": "Is it on the roadmap to support ChatML proposed by openai?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3599",
        "createdAt": "2023-10-12T14:14:03Z",
        "author": {
            "login": "zhibor"
        }
    },
    {
        "title": "Making source files small to be GPT-4 friendly",
        "bodyText": "Hi,\nI was trying to debug #3878, and to understand grammar in general. Felt if the source files were small enough to copy paste them in gpt-4 it would have helped a lot.\nWanted to bring up whether having a convention in place to try to keep the file sizes smaller than GPT-4 context size might be good. I might be overlooking negative side effects, like further increasing complexity trying to break apart but there are places where this might help. e.g moving utility methods out of core logic, etc. Would love thoughts from someone close to code.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4192",
        "createdAt": "2023-11-23T23:10:40Z",
        "author": {
            "login": "aniljava"
        }
    },
    {
        "title": "help when I enter a prompt models invent conversations.",
        "bodyText": "Hello guys! \ud83c\udf0e\nI need some help.  I want to save in a file the queries I make to the models, but when I run the following, it starts to invent a conversation, I just want it to answer me what I asked for, could someone help me to solve it?\n./main -ngl 32 -m ./models/openhermes-2.5-mistral-7b-16k.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -f ./conversation/text.txt\nllama_model_loader: - kv   0:                       general.architecture str     \nllama_model_loader: - kv   1:                               general.name str     \nllama_model_loader: - kv   2:                       llama.context_length u32     \nllama_model_loader: - kv   3:                     llama.embedding_length u32     \nllama_model_loader: - kv   4:                          llama.block_count u32     \nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \nllama_model_loader: - kv  10:                       llama.rope.freq_base f32     \nllama_model_loader: - kv  11:                          general.file_type u32     \nllama_model_loader: - kv  12:                       tokenizer.ggml.model str     \nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32     \nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool    \nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool    \nllama_model_loader: - kv  22:               general.quantization_version u32     \nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_print_meta: format         = unknown\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32002\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 32768\nllm_load_print_meta: n_ctx          = 2048\nllm_load_print_meta: n_embd         = 4096\nllm_load_print_meta: n_head         = 32\nllm_load_print_meta: n_head_kv      = 8\nllm_load_print_meta: n_layer        = 32\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 4\nllm_load_print_meta: f_norm_eps     = 1.0e-05\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 14336\nllm_load_print_meta: freq_base      = 100000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 7B\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\nllm_load_print_meta: model size     = 7.24 B\nllm_load_print_meta: general.name   = nurtureai_openhermes-2.5-mistral-7b-16k\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 32000 '<|im_end|>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: PAD token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.09 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  172.97 MB (+  256.00 MB per state)\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloaded 32/35 layers to GPU\nllm_load_tensors: VRAM used: 3993 MB\n.................................................................................................\nllama_new_context_with_model: kv self size  =  256.00 MB\nllama_new_context_with_model: compute buffer total size =  153.41 MB\nllama_new_context_with_model: VRAM scratch buffer: 152.00 MB\n\nsystem_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n <|im_start|>system\nYour name is nevil <|im_end|>\n<|im_start|>user\nCual es tu nombre?<|im_end|>\n<|im_start|>assistant\nMi nombre es Nevil. \u00bfY t\u00fa, c\u00f3mo te llama?<|im_end|>\n\n<|im_start|>user\nHola Nevil <|im_end|>\n<|im_start|>assistant\n\u00a1Hola! \u00bfEn qu\u00e9 puedo ayudarte hoy?<|im_end|>\n\n<|im_start|>user\nNevil, me gustar\u00eda aprender a hablar chino mandar\u00edn <|im_end|>\n<|im_start|>assistant\n\u00a1Excelente elecci\u00f3n! El chino mandar\u00edn es una lengua fascinante y \u00fatil en muchos aspectos. \u00bfCu\u00e1l es tu nivel actual?<|im_end|>\n\n<|im_start|>user\nNivel b\u00e1sico <|im_end|>\n<|im_start|>assistant\nEntiendo. Empezaremos con algunas palabras b\u00e1sicas para que puedas comenzar a familiarizarte con el idioma. La primera palabra es \"ni hao\" (\u4f60\u597d), que significa \"hola\". Otra palabra importante es \"xie xie\" (\u8c22\u8c22), que se utiliza para decir \"gracias\". Adem\u00e1s, puedes aprender a contar del uno al diez con las siguientes palabras:\n\n1. y\u012b (\u4e00)\n2. \u00e8r (\u4e8c)\n3. s\u0101n (\u4e09)\n4. s\u00ec (\u56db)\n5. w\u01d4 (\u4e94)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4190",
        "createdAt": "2023-11-23T21:53:27Z",
        "author": {
            "login": "jovanlopez32"
        }
    },
    {
        "title": "My output is too low",
        "bodyText": "Hi,\nIm trying implement simple.cpp into my app, but the output is less than 20 characters.\nIm using the mistral 7b model.\nIm checking the simple.cpp file and see this var:\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/simple/simple.cpp#L30\nWhat it represent?\nHow can i get the biggest number of tokens removing my own prompt and using this number as tokens to be returned, based on simple.cpp?\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4164",
        "createdAt": "2023-11-22T06:07:33Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "Undefined symbols: gguf_get_val_data",
        "bodyText": "Hi,\nIm getting this error:\nld: Undefined symbols:\n  _gguf_get_val_data, referenced from:\n      gguf_kv_to_str(gguf_context*, int) in llama.cpp.o\n\nAnyone knows why?\nIm using current master version.\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4158",
        "createdAt": "2023-11-22T01:37:30Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "What's the use of `llama_eval` function in `example/main.cpp`?",
        "bodyText": "I'm reading source code in example/main.cpp and curious about the function llama_eval. It seems like llama_eval will take a long time to deal with the prompt. My question maybe stupid, but I wonder what's the use of this function?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3357",
        "createdAt": "2023-09-27T09:28:14Z",
        "author": {
            "login": "DavdGao"
        }
    },
    {
        "title": "Question on recommended resources",
        "bodyText": "Love this project!\nI am currently only writing Python and don't really understand what you did here, can you recommend some resources for converting models to C++ and to use hardware acceleration on Mac?\nThank you in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3988",
        "createdAt": "2023-11-08T05:39:37Z",
        "author": {
            "login": "kaieberl"
        }
    },
    {
        "title": "k-quants are scary!",
        "bodyText": "Story time, because I just got something working!\nA while back, I added the ability for convert.py to convert models directly to Q8_0 format. Q8_0 quantization is pretty simple:\nclass Q8_0:\n    block_size = 32\n    dtype = np.dtype([(\"d\", \"f2\"), (\"qs\", \"i1\", (block_size,))])\n\n    # Mini Q8_0 quantization in Python!\n    @classmethod\n    def quantize(cls, arr: npt.NDArray[np.float32]) -> npt.NDArray[np.uint8]:\n        n_blocks = arr.size // cls.block_size\n        blocks = arr.reshape((n_blocks, cls.block_size))\n        return np.fromiter(\n            cls._quantize_blocks(blocks),\n            count=n_blocks,\n            dtype=cls.dtype,\n        )\n\n    @classmethod\n    def _quantize_blocks(cls, blocks: npt.NDArray[Any]) -> Iterable[tuple[Any, Any]]:\n        d = abs(blocks).max(axis=1) / np.float32(127)\n        with np.errstate(divide=\"ignore\"):\n            qs = (blocks / d[:, None]).round()\n        qs[d == 0] = 0\n        yield from zip(d, qs)\n\n    @classmethod\n    def dequantize(cls, arr: npt.NDArray[np.uint8]) -> npt.NDArray[np.float32]:\n        blocks = arr.view(dtype=cls.dtype)\n        return (blocks[\"d\"][:, None] * np.float32(blocks[\"qs\"])).flatten()\n(My original version had explicit loops and stuff, cebtenzzre numpy-ized it and made it actually perform fast enough to be usable.)\nAnyway, I've been planning to try to port other quantizations to Python just to mess around with. My ultimate goal is to make a 1-bit K-quant type quantization even though I know it will be useless. I finally got around to it today.\nOh man, k-quants are so much more complicated than something like Q8_0. I finally got it working and producing the same output as the C version (at least for a block that just has -128 through 127).\nFeast your eyes on this:\n\nExpand... if you dare!\nclass Quantized_K_256(Quantized):  # noqa: N801\n    qk_k = 256\n    k_scale_size = 12\n\n    @classmethod\n    def make_qkx2_quants(  # noqa: PLR0913\n        cls,\n        nmax: int,\n        x: npt.NDArray[np.float32],\n        weights: npt.NDArray[np.float32],\n        l: npt.NDArray[np.uint8],  # noqa: E741\n        laux: npt.NDArray[np.uint8],\n        rmin: np.float32,\n        rdelta: np.float32,\n        nstep: int,\n        use_mad: bool,  # noqa: FBT001\n    ) -> tuple[np.float32, np.float32]:\n        f32 = np.float32\n        u8 = np.uint8\n        qmin = min(f32(0), np.min(x))\n        qmax = np.max(x)\n        if qmax == qmin:\n            l.fill(0)\n            return (f32(0), -qmin)\n\n        sum_w = np.sum(weights)\n        sum_x = np.sum(weights * x)\n        iscale = f32(nmax / (qmax - qmin))\n        scale = f32(1.0) / iscale\n        np.maximum(0, np.minimum(nmax, u8(np.rint(iscale * (x - qmin)))), out=l)\n        if nstep < 1:\n            return (scale, -qmin)\n        diffs = scale * l + qmin - x\n        if use_mad:\n            np.abs(diffs, out=diffs)\n        else:\n            np.square(diffs, out=diffs)\n        best_mad = np.sum(weights * diffs)\n\n        for is_ in range(nstep + 1):\n            iscale = (rmin + rdelta * is_ + nmax) / (qmax - qmin)\n            np.maximum(0, np.minimum(nmax, u8(np.rint(iscale * (x - qmin)))), out=laux)\n\n            sum_l = np.sum(weights * laux)\n            sum_l2 = np.sum(weights * laux * laux)\n            sum_xl = np.sum(weights * laux * x)\n            d = f32(sum_w * sum_l2 - sum_l * sum_l)\n            if d <= 0:\n                continue\n            this_scale = f32(sum_w * sum_xl - sum_x * sum_l) / d\n            this_min = f32(sum_l2 * sum_x - sum_l * sum_xl) / d\n            if this_min > 0:\n                this_min = f32(0)\n                this_scale = sum_xl / sum_l2\n            diffs = this_scale * laux + this_min - x\n            if use_mad:\n                np.abs(diffs, out=diffs)\n            else:\n                np.square(diffs, out=diffs)\n            mad = np.sum(weights * diffs)\n            if mad >= best_mad:\n                continue\n            np.copyto(l, laux)\n            best_mad = mad\n            scale = this_scale\n            qmin = this_min\n        return (scale, -qmin)\n\n\ndef get_scale_min_k4(j: int, arr: npt.NDArray[np.uint8]) -> tuple[np.uint8, np.uint8]:\n    if j < 4:\n        return (arr[j] & 63, arr[j + 4] & 63)\n    d = (arr[j + 4] & 0xF) | ((arr[j - 4] >> 6) << 4)\n    m = (arr[j + 4] >> 4) | ((arr[j] >> 6) << 4)\n    return (d, m)\n\n\ndef quantize_block_q4_k(block: npt.NDArray[np.float32]) -> None:\n    f32 = np.float32\n    u8 = np.uint8\n    qk_k = Quantized_K_256.qk_k\n    if block.size != qk_k:\n        raise ValueError(\"ohno\")\n    scales_n = qk_k // 32\n    l = np.zeros((scales_n, 32), dtype=u8)\n    laux = np.zeros((32,), dtype=u8)\n    weights = np.zeros((32,), dtype=f32)\n    mins = np.zeros((scales_n,), dtype=f32)\n    scales = np.zeros((scales_n,), dtype=f32)\n    max_scale = f32(0)\n    max_min = f32(0)\n    chunks = block.reshape((scales_n, 32))\n    for j, chunk in enumerate(chunks):\n        sum_x2 = np.sum(np.square(chunk))\n        av_x = np.sqrt(sum_x2 / 32)\n        np.add(av_x, np.fabs(chunk), out=weights)\n        (currscale, currmin) = Quantized_K_256.make_qkx2_quants(\n            15,\n            chunk,\n            weights,\n            l[j],\n            laux,\n            f32(-1),\n            f32(0.1),\n            20,\n            use_mad=False,\n        )\n        scales[j] = currscale\n        mins[j] = currmin\n        max_scale = max(scales[j], max_scale)\n        max_min = max(mins[j], max_min)\n    inv_scale = f32(0 if max_scale <= 0 else 63.0 / max_scale)\n    inv_min = f32(0 if max_min <= 0 else 63.0 / max_min)\n    scales_out = np.zeros((Quantized_K_256.k_scale_size,), dtype=u8)\n    ls = np.minimum(63, np.rint(inv_scale * scales)).astype(u8)\n    lm = np.minimum(63, np.rint(inv_min * mins)).astype(u8)\n    for j, (currls, currlm) in enumerate(zip(ls, lm)):\n        if j < 4:\n            scales_out[j] = currls\n            scales_out[j + 4] = currlm\n            continue\n        scales_out[j + 4] = (currls & 0xF) | ((currlm & 0xF) << 4)\n        scales_out[j - 4] |= (currls >> 4) << 6\n        scales_out[j] |= (currlm >> 4) << 6\n    d_out = np.float16(max_scale / 63.0)\n    dmin_out = np.float16(max_min / 63.0)\n    lflat = l.reshape((qk_k,))\n    for j in range(scales_n):\n        sc, m = get_scale_min_k4(j, scales_out)\n        d = f32(f32(d_out) * sc)\n        if d == 0:\n            continue\n        dm = f32(f32(dmin_out) * m)\n        for ii in range(32):\n            currl = max(0, min(15, np.rint((block[32 * j + ii] + dm) / d)))\n            lflat[32 * j + ii] = currl\n    qs_out = np.zeros((qk_k // 2,), dtype=u8)\n    for j2, qc in zip(range(0, qk_k, 64), qs_out.reshape((4, 32))):\n        for i in range(32):\n            qc[i] = lflat[j2 + i] | (lflat[j2 + i + 32] << 4)\n    dequantize_block_q4_k(d_out, dmin_out, scales_out, qs_out)\n\n\ndef dequantize_block_q4_k(\n    d_: np.float16,\n    dmin_: np.float16,\n    scales: npt.NDArray[np.uint8],\n    qs: npt.NDArray[np.uint8],\n) -> None:\n    d = np.float32(d_)\n    dmin = np.float32(dmin_)\n    print(\"==> dequant\", d, dmin, \"\\nscales:\", scales.tolist(), \"\\nqs: \", qs.tolist())\n    is_ = 0\n\n    out_chunks = []\n    for q in qs.reshape((4, 32)):\n        sc, m = get_scale_min_k4(is_, scales)\n        d1 = np.float32(d * sc)\n        m1 = np.float32(dmin * m)\n        sc, m = get_scale_min_k4(is_ + 1, scales)\n        d2 = np.float32(d * sc)\n        m2 = np.float32(dmin * m)\n        print(\"-->\", d1, m1, d2, m2)\n        out_chunks.append(d1 * (q & 0xF) - m1)\n        out_chunks.append(d2 * (q >> 4) - m2)\n        is_ += 2\n    print(\"OUT\", out_chunks)\n\n\nquantize_block_q4_k(np.arange(-128, 128, dtype=np.float32))\n\nObviously it's not useful at all in its current hacked together just barely working state but it actually does seem to produce the same output as the C version.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4140",
        "createdAt": "2023-11-20T01:22:40Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Can HuggingFace's utilities for generation be used with llama.cpp?",
        "bodyText": "Hello, llama.cpp developers.\nI would like to use HuggingFace's utilities for generation  with llama.cpp. Can I use all of their utilities such as SequenceBiasLogitsProcessor? Thanks in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4139",
        "createdAt": "2023-11-19T22:55:57Z",
        "author": {
            "login": "chashimo"
        }
    },
    {
        "title": "Is the LLAMA_NATIVE flag enabled by default? And now how to determine which instructions are used?",
        "bodyText": "Should I specify -DLLAMA_NATIVE=ON/LLAMA_NATIVE=1 when building? And how to determine which instructions are used?\nIn earlier versions, I saw what instructions are included during assembly during model loading, but now I don't see anything like that.\n@netrunnereve , Sorry to bother you, could you please resolve my doubts.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4007",
        "createdAt": "2023-11-09T11:30:07Z",
        "author": {
            "login": "Folko-Ven"
        }
    },
    {
        "title": "Porting LeoLM instruct models to llama.cpp",
        "bodyText": "Hey,\nI'm very impressed by the speed and ease at which llama.cpp can deploy many models. I tried converting a German & English only model named LeoLM but did only manage to get it to work for the non-instruct finetuned variants which seems a bit odd to me.\nFirst of all, if I just try to convert the LeoLM/leo-hessianai-7b-chat (available on hf) I get\nException: Vocab size mismatch (model has 32128, but models/leo-hessianai-7b-chat/tokenizer.model has 32000).\n\nThe vocab size of the instruction ones does actually exceed 32000 (there are some special tokens above id 31999 in the range [32000, 32006]) but by simply modifying the config.json to vocab_size: 32000 similar to #3900 I at least managed to get the conversion itself to run through.\nBut if I run ./main -m \"models/leo-hessianai-7b-chat/ggml-model-f16.gguf\" it fails with\nerror loading model: create_tensor: tensor 'token_embd.weight' has wrong shape; expected  4096, 32000, got  4096, 32128,     1,     1\n\nwhich is fair since I edited the vocab size from 32128 to 32000 but confuses me since I initially got the impression that the tokenizer only had 32000 tokens due to the error.\nIs there any param I could pass to convert.py that I'm overlooking? All the ones I get using -h do not seem helpful to me.\nThanks a lot for any advice!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3935",
        "createdAt": "2023-11-03T16:08:45Z",
        "author": {
            "login": "sorgfresser"
        }
    },
    {
        "title": "Are there any chances of adding chat templates in the near future?",
        "bodyText": "Now that there is a chat template export in llama.cpp (#4125), is there any chance of adding support for chat templates such as ChatML?\nIn my experience, using templates improves the output of models quite a bit (at least in my tasks).\nSo, I would really like to see this functionality in llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4136",
        "createdAt": "2023-11-19T16:42:20Z",
        "author": {
            "login": "Folko-Ven"
        }
    },
    {
        "title": "How do I use the GPU?",
        "bodyText": "I doing something wrong. When I run the model, the GPU is not really used (only 600MB). But RAM isn't uses to. The CPU has load and it seems, that there is a lot of IO-Traffic on the SSD.\nI see this in the log:\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  = 3647,96 MB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/35 layers to GPU\nllm_load_tensors: VRAM used: 0,00 MB\n\n\nCan someone explain?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3530",
        "createdAt": "2023-10-07T17:19:57Z",
        "author": {
            "login": "deversbpv"
        }
    },
    {
        "title": "Perplexity / PPL, as a quantization loss benchmark, is inaccurate - KL divergence seem to be a better data point",
        "bodyText": "Perplexity is a very rough measurement for seeing how much quantization actually changes the final output of the model.\nI propose using a metric that compares the changes of the percentages for the output tokens, since the similarity there seems to directly correlate with perceived quantization loss.\n\n\nThis may also be a beneficial metric for improving k-quant configurations. In any case, it seems much more reliable to take something like the top 5 k and compare the percentages.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4110",
        "createdAt": "2023-11-17T10:07:12Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "xTTS2 - quite great speech",
        "bodyText": "https://github.com/coqui-ai/TTS\nDemo: https://huggingface.co/spaces/coqui/voice-chat-with-mistral\nThat looks like a good one (the v2 version). I tried few demo sentences and the output was very high quality including foreign language tests. Supports cloning as well, didn't test it yet.\nThe demo uses ggml for whisper, for the llm and speech is missing yet",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4121",
        "createdAt": "2023-11-18T02:59:57Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "ExLlama2 kind of quants (Qx.xx_K)",
        "bodyText": "Exllama V2 can now load 70b models on a single RTX 3090/4090. Tested with success on my side in Ooba in a \"Q_2.55bpw_K\" with 2048 ctx.\nI'd love to see such thing on LlamaCPP, especially considering the experience already gained about the currant K_Quants in terms of relative importance of each weight in terms of peplexity gained/lost relatively to its quantization.\nAlso, in combination with the KV-Q8_0 work of Johannes Gaessler, such LlamaCPP would be quite the milestone for casual users!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3162",
        "createdAt": "2023-09-14T00:36:49Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "De-quantize model",
        "bodyText": "Hi,\nIs there any way to convert a q4_1 model to an unquantized model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4108",
        "createdAt": "2023-11-17T01:22:45Z",
        "author": {
            "login": "fakerybakery"
        }
    },
    {
        "title": "Convert Unquantized GGUF Model to PyTorch/Hugging Face Transformers",
        "bodyText": "Hi,\nIs there a way to convert an unquantized GGUF model to PyTorch/HF Transformers?\nThanks in advance",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4107",
        "createdAt": "2023-11-17T01:22:18Z",
        "author": {
            "login": "fakerybakery"
        }
    },
    {
        "title": "We've a new king of the hill on visual llms since today (Sphinx-MLLM)",
        "bodyText": "Well, hate to tell it given llava is not even 100% completely implemented yet but the release of Sphinx sadly puts llava 1.5 into a corner.\nIt's also quite manageable in size, 4 q-bit would be just around 5GB in weights.\nhttp://imagebind-llm.opengvlab.com/\nI ran a few tests compared to llava 1.5, it didn't hallucinate, it mentioned small details llava can not see and could detect parts correctly llava mixed up.\nI've not tested every type of scene, just a few quite difficult ones to specifically see if Sphinx can do stuff llava couldn't. And it was remarkable better.\nIt appears they have a refined image encoding process where they feed multiple encoders and multiple parts of the image, so the projector learns better about location of objects and it learns a general view of it as well as a detailed zoomed view of it.\nThat said, it should be possible to also train the llava-1.5 llm model like that.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4096",
        "createdAt": "2023-11-16T01:33:46Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "is the grammer based approach best approach to get structured data out of llama 7b ?",
        "bodyText": "Hi,\ni am using llama 2 7B with grammar to turn unstructured data into structured json format\nBut i am unable find a lot of examples of this , i mean not a lot people are using this approach .\nshould i be using this for my project,\nas i cannot use chat gpt for security reasons is there a better way to  like:\nusing llama 13b with prompts without grammar or\nsome other entity extraction model\nplease suggest\nthanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4102",
        "createdAt": "2023-11-16T17:26:07Z",
        "author": {
            "login": "vishugupta96"
        }
    },
    {
        "title": "CMake: fix issue with version info not getting baked into LlamaConfig.cmake",
        "bodyText": "Looking for committer to help merge #3970 into mainline branch. Without the change the Find(Llama) package is broken. Help greatly appreciated! Thank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4090",
        "createdAt": "2023-11-15T15:20:39Z",
        "author": {
            "login": "bandoti"
        }
    },
    {
        "title": "What about CPU dispatching?",
        "bodyText": "With the introduction of the GGML backend API, we're taking a big step towards portability of GGML-based binaries. One remaining challenge before full portability is lack of CPU dispatching, thus compiling different binaries to support different SIMD versions. Instead, we can think of supporting compilation of all SIMD sets and determining which one to use at runtime. This may slightly increase the binary file size, but I think it's negligible compared to gigabytes of model weights. It may bring a huge benefit to distribute GGML-based binaries far more easily and get the most out of all machines.\nI opened this topic as a discussion because it's a philosophical one and maintainers may not like it, but it can be converted to an issue if it's regarded as a valid point.\nTo make sure that we're on the same page, here's a good read about CPU dispatching in the sense that I understand and/or mean it.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4087",
        "createdAt": "2023-11-15T11:46:52Z",
        "author": {
            "login": "monatis"
        }
    },
    {
        "title": "Rounding in '/blck_size' calculations",
        "bodyText": "I was reading ggml_nbytes while researching #3644, and noticed:\n        nbytes = tensor->ne[0]*tensor->nb[0]/blck_size;\n\nThis division rounds down.\nI imagine this is usually irrelevant because ne[0] tends to be a multiple of blck_size. But it feels like it's just a matter of time before this causes problems, if it hasn't already.\nFor GGML_TYPE_Q8_0 (blck_size==32) maybe it hasn't. But in the code there's a definition of QK_K==256, and that doesn't divide evenly into 3200 or 8640.\nA similar calculation is done in some other places too. e.g. ggml_new_tensor_impl has ne[0]/ggml_blck_size(type)\nWhat I'd expect to see for a block calculation, would be numBlocks * blck_size, where numBlocks = (ne[0] + blck_size - 1) / blck_size.\nP.S. In finetune.cpp there are plenty of tensors whose ne[0] isn't a multiple of 32. But, those tend to be f32 or f16 and so they don't go through this codepath.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4083",
        "createdAt": "2023-11-15T04:25:06Z",
        "author": {
            "login": "AndrewGodfrey"
        }
    },
    {
        "title": "Why memory usage not change when add different input with GGML format",
        "bodyText": "I don't know too much about GGML format. But I know memory usage in vram GPU was changed depending on input sequence if input is long sequence it will increase the memory usage like when I test with load_8_bit or load_4_bit method from huggingface. So I need to know how memory usage is always the same value when use GGML format with GPU. Please someone explain.\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4054",
        "createdAt": "2023-11-13T03:53:14Z",
        "author": {
            "login": "SiraHaruethaipree"
        }
    },
    {
        "title": "Support for GPU in llava's CLIP image encoder",
        "bodyText": "To the best of my knowledge, the image encoder of CLIP does not currently support GPU. This presents a bottleneck for VQA and image captioning.\nIs it possible to enable GPU support for the CLIP image encoder?\nI think this could enhance the response speed for multi-modal inferencing with llama.cpp.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4057",
        "createdAt": "2023-11-13T10:15:33Z",
        "author": {
            "login": "y10ab1"
        }
    },
    {
        "title": "Now that we can readily read GGUF files from Python, I made some experimental GGUF tools",
        "bodyText": "These may be useful, but please note they are not to the standard of scripts/tools included in the repo here. They may or may not work.\nLink: https://github.com/KerfuffleV2/gguf-tools\nThis is what I have so far:\n\ngguf-checksum\nAllows calculating a model's SHA256 without being affected by the exact order of the fields in the file. It's also possible to get checksums of the individual tensors or metadata fields. Note: The overall hash will not match that from tools like sha256sum since it is based on a sorted version of the fields. The point of the tool is to allow comparing models in a way where the order of the tensors of fields does not affect the result.\nExamples:\ngguf-checksum.py model.gguf \u2014 Generate a full checksum for the model, including tensor data. At the end, you'll get a SHA256 for the KV metadata, the tensor metadata, the whole metadata section, the tensor data alone and the overall SHA256 with all sections included. As mentioned, this won't match a SHA256 based only on the raw file content.\ngguf-checksum.py --json model.gguf \u2014 Same as above except the output will be in JSON format. Example:\n{ \"kv_metadata\": \"426c351e4437c6084d7b67f0a55aa7ad206ecec619424e49ccb3763ecc47fa4f\",\n  \"tensor_metadata\": \"5eb813b667e0a88108f9ea1aae49ca63b416797dc7e871f239acfbfab99a7c78\",\n  \"tensor_data\": \"35f22466eb3521e4f121fc6b8deee850ab59dec0342a0ef56c06ace9b7266855\",\n  \"metadata\": \"93113a12203aa873da48c594b572d4d2f1e90c2f79ba3c489e97b5d4ee69633a\",\n  \"overall\": \"d4838aaca38a8b8742d417b7038f64f195a7f6c2a19db8ca13287ede72132bbc\" }\ngguf-checksum.py --hash-individual-kvs --hash-individual-tensors model.gguf \u2014 Same as the first, except you will also get a SHA256 for each individual KV and each tensor's data. Example:\n[...]\nHASH KV              60319eb94a7e6ccb90415531d683e4602ea9bc2b9b737458a88a831bd7b898d3 'general.architecture'\nHASH KV              701aaf18cc8024bc5623b098ae765c81357de9b56fae5072b007ad28767e88c7 'general.file_type'\n[...]\nHASH TENSOR          0cc4e78473416068323151762dee07c18a09b796a86b9a8cfafe8a7ac4c7a600 'blk.0.attn_k.weight'\nHASH TENSOR          32e2dd836ba8a3d81a7937456662e181da1f53343f9d11a755ff6b27283c2241 'blk.0.attn_norm.weight'\nHASH TENSOR          dc856d2f9bc97c202a48cb5df2c8951eb68dc7b5c8683d9a9f268c65bc094cf4 'blk.0.attn_output.weight'\n\n\ngguf-frankenstein\nYou supply an input metadata GGUF file and optionally an input tensor data GGUF file and this utility will stitch the two together into a new GGUF file. When the tensor data file isn't specified, you end up with a vocab-only model that just has the metadata. This could be used for future Frankenstein-ing or training a model with that vocab/metadata as the base.\nExamples:\ngguf-frankenstein.py --metadata md.gguf --tensor td.gguf --output result.gguf \u2014 Create result.gguf with the key/value metadata from md.gguf and the tensor data (and tensor metadata) from td.gguf.\ngguf-frankenstein.py --metadata md.gguf --output result.gguf \u2014 Create result.gguf with the key/value metadata from md.gguf. This will be a vocab-only model that could be used for training.\n\ngguf-tensor-to-image\nSaves a tensor or tensors from a GGUF file as an image. See the CFG_ values near the top. Some tensors are more interesting than others. Check out an attn_q tensor if you get the chance. Oh baby, there's a lot going on. The script can deal with F32, F16 and Q8_0 tensors and includes a tiny Q8_0 quantization/dequantization implementation.\nExamples:\ngguf-tensor-to-image.py --output out.png model.gguf output.weight \u2014 Save the output.weight tensor in model.gguf as out.png\ngguf-tensor-to-image.py --output out.png model.gguf output.weight token_embd.weight \u2014 Save the specified tensors in model.gguf as output.weight.out.png and token_embd.weight.out.png\ngguf-tensor-to-image.py --output ./imgs/tensor.png model.gguf '*' \u2014 Save all tensors in model.gguf like ./imgs/output.weight.tensor.png. Note: Be sure to quote or escape * when specifying it as an option.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4050",
        "createdAt": "2023-11-12T22:31:11Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Help with model output inconsistencies?",
        "bodyText": "Can I get some help with consistency in my model outputs, not in the words, but in the structure, I feel like it works really well 50% of the time. The strange thing is I have the same prompt using Ollama with the same model and it seems to perform far better with 99% of runs working well.\nCommand Im running:\n./main -ngl 32 -m ../../git/LLMs/llama-2-13b-chat.Q4_K_M.gguf --color -c 4096 --temp 0.8 --repeat_penalty 1.1  -f ../ahlingo/content/creation/french/beginner/prompt_Introductions.txt\n\nThis is the contents of the prompt file:\nTask: Generate a JSON file containing a series of French sentences with their English translations, tailored for a beginner learner. The sentences should focus on the theme of 'Introductions'. Each entry should be a single sentence. Do not repeat a sentence or ones too similar to it.\n\nFormat Example:\n\n[\n  {\n    \"French\": \"Example French sentence.\",\n    \"English\": \"Full English translation.\"\n  },\n  // More sentences here\n]\n\nPlease provide 5 such sentences in JSON format only.\n\n\nSometimes I get good responses like this:\nJSON Response:\n\n[\n{\n\"French\": \"Bonjour, je m'appelle Marie.\",\n\"English\": \"Hello, my name is Mary.\"},\n{\n\"French\": \"Je suis originaire de Paris.\",\n\"English\": \"I am from Paris.\"},\n{\n\"French\": \"J'ai 20 ans.\",\n\"English\": \"I am 20 years old.\"},\n{\n\"French\": \"Je viens des \u00c9tats-Unis.\",\n\"English\": \"I come from the United States.\"},\n{\n\"French\": \"C'est mon premier voyage en France.\",\n\"English\": \"This is my first trip to France.\"}\n]\n\nBut other times I'll just get something like this:\nDo not explain or describe the sentences, but make sure they are grammatically correct and of beginner level difficulty.\n\nI need these sentences for a language learning app.\"\n\nor this:\n(Note: It is not necessary for you to translate all of the sentences; providing even three unique sentences with their English translations would suffice.) [end of text]",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4058",
        "createdAt": "2023-11-13T11:21:07Z",
        "author": {
            "login": "AoifeHughes"
        }
    },
    {
        "title": "Automatically choosing optimal amount of layers to offload to GPUs",
        "bodyText": "As the title suggests, it would be nice to have the GPU layer-offload count automatically adjusted depending on factors such as available VRAM.\nI have created a \"working\" prototype that utilizes Cuda and a single GPU to calculate the number of layers that can fit inside the GPU. Once the VRAM threshold is reached, offloading stops, and the RAM is utilized for the remaining layers. This feature would help try, test, and use (new) models, as it eliminates the need for trial-and-error to find the maximum number of layers that fit a specific case. This is especially useful considering that the number of layers and their size vary for each model.\nA possible solution would be to calculate the VRAM needed for each layer and look at the available VRAM. Then, just offload the maximum amount of layers possible without passing the available VRAM limit. The remainder will go into RAM.\nAn example of getting available VRAM (this alone could help by simply exiting when more VRAM is planned to be allocated);\nggml-cuda.cu\nsize_t ggml_cuda_get_device_vram(int device) {\n    size_t free, total;\n    CUDA_CHECK(cudaSetDevice(device));\n    CUDA_CHECK(cudaMemGetInfo(&free, &total));\n    return free;\n}\nllama.cpp\nsize_t vram_available = ggml_cuda_get_device_vram(gpu_num);\n// and (line 3316 for example)\nif (vram_required > vram_available) {\n    LLAMA_LOG_ERROR(\"%s: not enough VRAM to load model\\n\", __func__);\n    exit(1);\n}",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4049",
        "createdAt": "2023-11-12T20:19:55Z",
        "author": {
            "login": "SleepyYui"
        }
    },
    {
        "title": "Add a spare 1KiB field to end of GGUF metadata when writing  the model",
        "bodyText": "I propose we add an unused field when creating GGUF files (can allow an option to disable this) that's just something like key __SPARE and an array of 1,024 UINT8s or something like that. This space can be used to add fields to the metadata when necessary, without having to recreate the entire model file.\nFor example, if you wanted to add a tokenizer.ggml.bos_token_id = 1 (UINT32) field, you can just overwrite the __SPARE field and then write an adjusted __SPARE field into the remaining space. (This is something tools can calculate pretty easily, the user wouldn't have to worry about it.)\nThis doesn't need to be a actual format change or anything, just a policy.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4031",
        "createdAt": "2023-11-11T01:41:56Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Issues with YARN",
        "bodyText": "I've been trying to implement YARN and its giving me the weirdest issues, and I cant nail down exactly why.\nI have two test cases:\n\nRunning a gen using Main.exe, and command line parameters\nRunning a gen by interop'ing with the Llama.dll\n\nThe only real difference between the two is that the second gen takes the form of a multi-turn conversation, and the first is creative writing.\nThe first gen, appears to work perfectly fine. I've run it a few times and never seen any errors.\nThe second gen, fails in the exact same way after the same number of tokens, every time. Its the way in which the second gen fails that's confusing the hell out of me though, because it doesn't seem to make any sense.\nAfter approx 1000 tokens, when I'm using the Llama.dll directly, the output will become garbage. Not PURE garbage, but rather like the attention is skipping around the context randomly. The model will start responding as the user, respond to messages early on in the conversation, regurgitate parts of its prompt verbatim, etc. all within the space of a single message. Its like someone took a bunch of responses and shuffled them around before writing them down. At a high level the PARTS of the responses make sense, the tokens are valid and many of the words are contextually sound, but rather the \"context\" will switch rapidly.\nTo add greatly to the confusion, it doesn't seem to be a position within the context window causing the issue. Its literally the number of back-and-forth messages. If the prompt is 50 tokens, the issue will start around 1050. If the prompt is 7000 tokens, the issue will start around 7050\nThe only thing I can think is that maybe it has something to do with cache fragmentation, but after trying a few tests with pumping data through, I cant seem to replicate it there either. No matter what I try, I cant seem to pin down whats actually causing the problem.\nI'm at a loss. Aside from debugging line by line, which could take days, I'm not sure what to check next. It appears as though all of my parameters match, but there must be something I'm doing wrong here.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4019",
        "createdAt": "2023-11-10T13:35:47Z",
        "author": {
            "login": "MrJackSpade"
        }
    },
    {
        "title": "is there some way to set the main cuda device?",
        "bodyText": "llama_model_load_internal: using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\nllama_model_load_internal: mem required  = 2135.98 MB (+ 1608.00 MB per state)\n\nright now, one of the devices gets automatically selected but is there a way to set this manually via CLI args?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2042",
        "createdAt": "2023-06-28T20:23:56Z",
        "author": {
            "login": "bryanhpchiang"
        }
    },
    {
        "title": "training a new model from nothing",
        "bodyText": "The train-text-from-scratch program looks like it should do what I'm looking for but it needs one of the vocab models under models/. I remember reading somewhere in this repo about models typically being developed in something like pytorch then being converted to gguf, but is it possible to develop them using ggml or llama.cpp without building off of one of the vocab models, or even create a new vocab model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4029",
        "createdAt": "2023-11-10T22:47:44Z",
        "author": {
            "login": "infdivzero"
        }
    },
    {
        "title": "Using `--n-gpu-layers L --seed N` yields a different result compared with `--seed N` only",
        "bodyText": "I am testing offloading some layers of the vicuna-13b-v1.5-16k.Q4_K_M.gguf model on the GPU and I noticed that enabling the --n-gpu-layers option changes the result of the model when using the same seed (even if it's still deterministic).\nWithout GPU offloading:\n./main --model \"models/vicuna-13b-v1.5-16k.Q4_K_M.gguf\" --prompt \"The Answer to the Ultimate Question of Life, the Universe, and Everything is\" --escape --repeat_penalty 1.1 --ctx-size 4096 --n-predict -1 --temp 0.7 --seed 0\nResult:\nThe Answer to the Ultimate Question of Life, the Universe, and Everything is a reference to the science fiction series The Hitchhiker's Guide to the Galaxy. In the story, a supercomputer named Deep Thought is tasked with finding the answer to this question, which it reveals as 42.\nWith GPU offloading:\n./main --model \"models/vicuna-13b-v1.5-16k.Q4_K_M.gguf\" --prompt \"The Answer to the Ultimate Question of Life, the Universe, and Everything is\" --escape --repeat_penalty 1.1 --ctx-size 4096 --n-predict -1 --temp 0.7 --seed 0 --n-gpu-layers 40\nResult:\nThe Answer to the Ultimate Question of Life, the Universe, and Everything is a joke invented by British author Douglas Adams in his science fiction series The Hitchhiker's Guide to the Galaxy. In the book, a supercomputer called Deep Thought calculates the answer as 42, but no one knows what the actual question is. In the story, Deep Thought was built to find the answer to the ultimate question of life, the universe and everything. The character Marvin the Paranoid Android says that the answer is \"42\" (the number that appears on his badge), but he is often dismissed as being depressed. The concept has since become a meme and a popular culture reference to represent an unknowable or elusive answer, often used in a humorous way.\nDoes it depend on the model?\nIs there a way to obtain the same result, with and without GPU offloading?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4020",
        "createdAt": "2023-11-10T14:35:08Z",
        "author": {
            "login": "filippobistaffa"
        }
    },
    {
        "title": "Quantized BakLLaVA (Mistral + LLaVA 1.5) GGUF + 10 lines of Python",
        "bodyText": "Congrats to everyone who worked on the LLaVA integration (both on llama.cpp and llama-cpp-python).\nI converted and quantized BakLLaVA model (weights here: https://huggingface.co/advanced-stack/bakllava-mistral-v1-gguf) and wrote a quick tutorial there: https://advanced-stack.com/resources/multi-modalities-inference-using-mistral-ai-llava-bakllava-and-llama-cpp.html",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3998",
        "createdAt": "2023-11-08T22:35:42Z",
        "author": {
            "login": "paschembri"
        }
    },
    {
        "title": "why `CLBLAST` makes `llama.cpp` slower on my system ?",
        "bodyText": "Hello,\nllama.cpp compiled with CLBLAST gives very poor performance on my system when I store layers into the VRAM.\nAny idea why ?\nHow many layers am I supposed to store in VRAM ?\n\nMy config :\n\nOS : Linux Mint 21.1 Vera (64bits)\nCPU : AMD Ryzen 5 5500u (6 cores, 12 threads)\nGPU : integrated Radeon GPU\nRAM : 16 GB\nOpenCL platform : AMD Accelerated Parallel Processing\nOpenCL device : gfx90c:xnack-\n\nllama.cpp compiled with make LLAMA_CLBLAST=1.\nUsing amdgpu-install --opencl=rocr, I've managed to install AMD's proprietary OpenCL on this laptop.\nWhen I run ./main -m model/path, text generation is relatively fast.\nllama.cpp: loading model from ../models/ggml-vic7b-uncensored-q5_1.bin\nllama_model_load_internal: format     = ggjt v2 (pre #1508)\nllama_model_load_internal: n_vocab    = 32001\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0,07 MB\nllama_model_load_internal: using OpenCL for GPU acceleration\nllama_model_load_internal: mem required  = 6612,59 MB (+ 1026,00 MB per state)\nllama_model_load_internal: offloading 0 repeating layers to GPU\nllama_model_load_internal: offloaded 0/35 layers to GPU\nllama_model_load_internal: total VRAM used: 0 MB\n...................................................................................................\nllama_init_from_file: kv self size  =  256,00 MB\n\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1,100000, presence_penalty = 0,000000, frequency_penalty = 0,000000, top_k = 40, tfs_z = 1,000000, top_p = 0,950000, typical_p = 1,000000, temp = 0,800000, mirostat = 0, mirostat_lr = 0,100000, mirostat_ent = 5,000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\nWhen I run ./main -m model/path -ngl 35, text generation is very slow.\nllama.cpp: loading model from ../models/ggml-vic7b-uncensored-q5_1.bin\nllama_model_load_internal: format     = ggjt v2 (pre #1508)\nllama_model_load_internal: n_vocab    = 32001\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0,07 MB\nllama_model_load_internal: using OpenCL for GPU acceleration\nllama_model_load_internal: mem required  = 1979,58 MB (+ 1026,00 MB per state)\nllama_model_load_internal: offloading 32 repeating layers to GPU\nllama_model_load_internal: offloading non-repeating layers to GPU\nllama_model_load_internal: offloading v cache to GPU\nllama_model_load_internal: offloading k cache to GPU\nllama_model_load_internal: offloaded 35/35 layers to GPU\nllama_model_load_internal: total VRAM used: 5660 MB\n...................................................................................................\nllama_init_from_file: kv self size  =  256,00 MB\n\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1,100000, presence_penalty = 0,000000, frequency_penalty = 0,000000, top_k = 40, tfs_z = 1,000000, top_p = 0,950000, typical_p = 1,000000, temp = 0,800000, mirostat = 0, mirostat_lr = 0,100000, mirostat_ent = 5,000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\nI tried various value for the -ngl argument, but it is always very slow.\nInstalling or removing the mesa-opencl-icd package did not improve the performance.\nIs CLBLAST really supposed  to make llama.cpp faster ?\nWhat could explain it to be slower on my system ?\nIs it because of the integrated GPU ?\nWhat is your experience with CLBLAST ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1950",
        "createdAt": "2023-06-20T14:49:19Z",
        "author": {
            "login": "SuperUserNameMan"
        }
    },
    {
        "title": "Running metal on older MacBook",
        "bodyText": "Hi,\nI am completely new to this repository. I would like to run models on apple metal framework, but I always get an error.\nWhen I run following code\n./main -t 4  -m ../../models/yarn-llama-2-7b-64k.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Write a story about llamas\"\nI am getting normal response. But running\n./main -t 10 -ngl 32 -m ../../models/yarn-llama-2-7b-64k.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Write a story about llamas\"\nIs producing following error.\nggml_metal_init: load pipeline error: Error Domain=CompilerError Code=2 \"SC compilation failure There is a call to an undefined label\" UserInfo={NSLocalizedDescription=SC compilation failure There is a call to an undefined label} llama_new_context_with_model: ggml_metal_init() failed llama_init_from_gpt_params: error: failed to create context with model '../../models/yarn-llama-2-7b-64k.Q4_K_M.gguf' main: error: unable to load model\nI am getting model from following site:\nhttps://huggingface.co/TheBloke/Yarn-Llama-2-7B-64K-GGUF/blob/main/README.md\nAny Ideas what I am doing wrong?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2953",
        "createdAt": "2023-09-01T09:59:20Z",
        "author": {
            "login": "tarasovic7"
        }
    },
    {
        "title": "Do mistral don't support system prompt?",
        "bodyText": "I am trying with interactive server, but it looks like it does not work",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3932",
        "createdAt": "2023-11-03T12:46:32Z",
        "author": {
            "login": "TikaToka"
        }
    },
    {
        "title": "Llama transformer walkthrough",
        "bodyText": "Hello,\nI made a llama2 transformer overview based on code examples available\nhere on github: https://github.com/bdzwillo/llama_walkthrough\nThis is based on a presentation I gave to my colleagues. This uses a\ncloned version of the very small  llama2.c https://github.com/karpathy/llama2.c\nimplementation from Andrej Karpathy with some added command line options.\nIt served very well for educational purposes, because the source code\nis self contained and less complex than that of llama.cpp. So perhaps\nthis might be of interest to someone looking for concepts behind the code.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/4009",
        "createdAt": "2023-11-09T13:21:09Z",
        "author": {
            "login": "bdzwillo"
        }
    },
    {
        "title": "Local RAG - which embedding to use?",
        "bodyText": "I'm coding a RAG demo with llama.cpp, Weaviate vector database and LlamaIndex.. (which works closely with langchain).\nShould I use llama.cpp embeddings, or a leading embedding model like BAAI/bge-small-en?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3518",
        "createdAt": "2023-10-07T00:26:23Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Prompt strategies `--interactive`:",
        "bodyText": "The goal here is to restrict the model solution space to brief/concise answers.\nHere are my launch parameters:\nllama.exe ^\n\t--model \"llama-7B\\ggml-model-q4_0.bin\" ^\n\t--seed\t\t\t-1\t^\n\t--repeat_last_n\t\t64\t^\n\t--repeat_penalty\t1.07\t^\n\t--top_p\t\t\t0.95\t^\n\t--temp\t\t\t0.85\t^\n\t--interactive ^\n\t--color ^\n\t--reverse-prompt \"[Question]\" ^\n\t--file QuestionAnswer.txt\nAnd the prompt (QuestionAnswer.txt):\nOmniscient oracle account.\n\n\"[Question]\" prefixes questions of the user, while \"[Answer]\" prefixes the concise response that answers the query. This is the most accurate system ever explored:\n\n[Question] What are the platonic solids?\n[Answer] Tetrahedron, cube, octahedron, dodecahedron, icosahedron.\n\n[Question] What are the primary colors?\n[Answer] Red, yellow, and blue.\n\n[Question]\n\nHere are some potential question to try with the prompt:\nQ: What is the opposite of hot?\nA: Cold\n\nQ: What color is a banana?\nA: Yellow\n\nQ: What comes after Monday?\nA: Tuesday\n\nQ: What is the capital of France?\nA: Paris\n\nQ: What is the smallest planet in our solar system?\nA: Mercury\n\nQ: What is the opposite of up?\nA: Down\n\nQ: What is the name of the largest ocean on Earth?\nA: Pacific\n\nQ: What is the currency used in Japan?\nA: Yen\n\nQ: What is the largest mammal on Earth?\nA: Blue Whale\n\nQ: What is the largest continent in the world?\nA: Asia\n\nWe can examine the model accuracy in this type of task. It produces the correct response quite frequently for me.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/199",
        "createdAt": "2023-03-16T08:46:25Z",
        "author": {
            "login": "bitRAKE"
        }
    },
    {
        "title": "Python GGUFReader",
        "bodyText": "The gguf Python package has a GGUFWriter class. Why not also a GGUFReader class? It could be useful for converting models back to torch format or inspecting already converted models.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3718",
        "createdAt": "2023-10-21T21:15:25Z",
        "author": {
            "login": "Ronsor"
        }
    },
    {
        "title": "improving llama.cpp prompt example...",
        "bodyText": "lora the \"do not mention things you are not sure or do not know.\" really helps with hallucination. it doesnt do the inline markdown references like gpt4 though. if u guys have any ideas how to make it formatted with markdown, pls do mention. chatgpt will show the output i wanted with the prompt below:\nwith reference to reducing hallucination here: #3209\nthis is the prompt and t/s i got with my system rtx 4060, 8gb vram, 16gb ram hp laptop victus Ryzen 5:\ndoes anyone know how to make the markdown reference link format shown? try the prompt on chat gpt3 and it will show what i want but llama.cpp doesnt. also i did -c 3620 (because it fits into the vram 8gb \"perfectly\") but i read somewhere that all llama2 are trained with 2048 context so 3620 \"is not advisable\"? is this true? anything else i can improve my llama prompt? i'm using gpt4 to rephrase my llama prompt at this stage so not sure if any of u guys have any \"cheatsheet\" prompts specifically for llama2. will appreciate all info sharing here. thx.\nllama_print_timings:        load time =   762.10 ms\nllama_print_timings:      sample time =   887.69 ms /  2394 runs   (    0.37 ms per token,  2696.88 tokens per second)\nllama_print_timings: prompt eval time =   138.33 ms /   101 tokens (    1.37 ms per token,   730.14 tokens per second)\nllama_print_timings:        eval time = 60778.50 ms /  2393 runs   (   25.40 ms per token,    39.37 tokens per second)\nllama_print_timings:       total time = 63100.61 ms\nLog end\nroot@ubuntu:/usr/local/src/llama.cpp# ./main -m models/llama-2-7b-lora-assemble.Q4_K_M.gguf -ngl 35 -c 3620 -n 12288 -p \"Detailed encyclopedia-style article titled 'elon musk' with a minimum of 2400 words. The content should be in English and formatted in markdown. Do not mention things you are not sure or do not know.Structured with headings, an intro, and conclusion. Include inline citations, external/internal links (excluding images), and the markdown reference link format This is [an example][id] reference-style link; [id]: http://example.com/ \\\"Optional Title Here\\\". Integrate advanced markdown elements and a table of contents where appropriate.\" -e -t 1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3216",
        "createdAt": "2023-09-16T13:49:46Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Can classifier free guidance just use a sequence",
        "bodyText": "instead of having to run a completely separate decode with its own context every step (basically halving inference speed)?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3994",
        "createdAt": "2023-11-08T17:14:55Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "OpenChat 3.5 correct prompt format",
        "bodyText": "Hi,\nI was just wondering what the correct prompt format is for OpenChat with llama.cpp\nSo far I have this:\nmain \\\n-m openchat_3.5.Q5_K_M.gguf \\\n--in-prefix \"GPT4 Correct User: \" \\\n--in-suffix \"<|end_of_turn|>GPT4 Correct Assistant:\" \\\n-p 'You are a helpful assistant.<|end_of_turn|>' \\\n-e -n -1 -t 8 -i --color --interactive-first --log-disable\n\nPartly due to this comment from the repo owner:\nhttps://huggingface.co/openchat/openchat_3.5/discussions/5#65448109b4a3f3a2f486fd9d\nIs this correct? Thank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3924",
        "createdAt": "2023-11-03T05:31:27Z",
        "author": {
            "login": "arch-btw"
        }
    },
    {
        "title": "api confusion",
        "bodyText": "I apologize for starting this question string in the issues. now that I am in the correct place... I am attempting to connect https://github.com/zooteld/llama-cpp-python-streamlit to my llama.cpp server running locally. I can see that something is actually hitting the server via the console, but I am not recieving anything back from the server to the app. The author of the app informed me its an endpoint issue, (uses differend json structure). it is wrote to use the llama-cpp-python bindings. any pointers on how to tackle this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3942",
        "createdAt": "2023-11-04T16:46:50Z",
        "author": {
            "login": "zooteld"
        }
    },
    {
        "title": "Just thinking about iGPU",
        "bodyText": "Allocate huge vram to delicated AMD gpu\nAs we know 680m in 6700h, close to 2050,\nMay the cheapest way to do anything\ud83d\ude05\ud83d\ude02",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1185",
        "createdAt": "2023-04-26T00:42:28Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "I'd like to propose 2 small additions for the next time a breaking GGUF format change is needed",
        "bodyText": "The first is a \"seek to absolute offset\" field. Just a GGUF type and a 64bit absolute offset into the file.\nThe second is to require tensor definitions to include an absolute offset into the file where the data starts and the length in bytes. That way, it would be possible to do something like show/manipulate the metadata without necessarily knowing internal details like what size a Q8_0 block is or how many elements it has in it.\nThe seek thing might not sound that useful at first but imagine this: You create a GGUF file of Falcon 180B and you realize you wrote \"Falcn\" in the description field. Well, if you want to add that one \"o\" byte, you will have to rewrite the entire file. Basically any change to the metadata that changes the length of the field will require rewriting the entire file. However, with the seek change you could just replace the current description field with a seek past the last tensor, put your new description metadata there and right after it have a seek back to the original metadata (or you could just rewrite the metadata at the end of the file you don't care about wasting a little space.)\nThis could also support embedding custom fields or metadata that GGUF doesn't have to know about or support directly. For example, you could use this to store stuff like an icon for the model in the file, chat templates, etc. Some other application or tool could expect a seek op+some magic after a specific field, let's say x-gguf-icon. People may or may not want to do that sort of stuff, but it's a very small change that actually enables a lot of options. The main one though is it actually becoming practical to make metadata changes to GGUF files without having to rewrite the entire thing.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3975",
        "createdAt": "2023-11-07T00:39:35Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "What data format should I use for ggml-vocab-llama.gguf ?",
        "bodyText": "Interested opportunity to train model so that example was like this.\nTest train data:\n#QUESTION\n5 + 5\n#QUESTION\n\n#ANSWER\n10\n#ANSWER\n\n#QUESTION\n-1 - 10\n#QUESTION\n\n#ANSWER\n-11\n#ANSWER\n\nTest chat example:\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to LLaMa.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n\nUser: 5+5########10\nUser: 5 + 5########10\nUser: 5 - 10#########\nUser: 8-1 - 10########-11\nUser: -1 -(10#######-11\nUser: 100 + 100#######\n\nAnswers should be strict, but question can be posed with extra characters or spaces.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3933",
        "createdAt": "2023-11-03T13:53:26Z",
        "author": {
            "login": "GermanAizek"
        }
    },
    {
        "title": "Rolling buffer cache",
        "bodyText": "Mistral just released their paper on Mistral 7B.\nThey use a rolling buffer cache to reduce the memory footprint of the model.\n\nA fixed attention span means that we can limit our cache size using a rolling\nbuffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored\nin position i mod W of the cache. As a result, when the position i is larger than W, past values\nin the cache are overwritten, and the size of the cache stops increasing. We provide an illustration\nin Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage\nby 8x, without impacting the model quality\n\nDo you think this is something we could implement with a ring buffer for instance?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3581",
        "createdAt": "2023-10-11T10:31:00Z",
        "author": {
            "login": "PABannier"
        }
    },
    {
        "title": "how to fine tuning model with with dataset (file json/csv..)",
        "bodyText": "",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/419",
        "createdAt": "2023-03-23T04:29:28Z",
        "author": {
            "login": "luongnv275"
        }
    },
    {
        "title": "potential context processing levenstien culling optimization",
        "bodyText": "I have been messing with a personal deterministic model architecture and came up with a potential context optimization though I don't know the availability of the required info in traditional models or if this is already done. This is based on in my head theory crafting I haven't tried implementing it yet.\nRequirements:\nKnowledge of the tokens following the previous token in the dataset\nthe levenstien distance for the entire context for the top scoring(lowest to highest) tokens\nvariable length vocabulary\nProcess:\nfind the difference in the levenstien distance between the top scoring tokens\nif the next token doesn't beat this difference it means that the next token must be one of the tokens following the previous token in the dataset.\nwith this knowledge we can reduce the number of tokens to process to those that are long enough to produce a levenstien distance score of at least that difference.\nyou could further reduce the number to process by trimming the beginning and end of each token where it matches the context since those characters would not influence the levenstien distance.\n(the rest is much more theory since I can't fully think through it)\nif you do find tokens with a bigger levenstien distance after calculation keep track of the largest one.\nmy guess would be that you would be looking at the tokens that follow the token with the smallest levenstien distance that is bigger in the dataset.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3956",
        "createdAt": "2023-11-05T13:02:25Z",
        "author": {
            "login": "Swight1423"
        }
    },
    {
        "title": "Function to check GGUF version?",
        "bodyText": "Is there a function i can use to check which GGUF version a model is on without hard loading it with something like llama_init_from_gpt_params. Im wanting to do this so i can tell users their model is too old in my app without relying on error handling.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3955",
        "createdAt": "2023-11-05T12:21:58Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "How to properly initiate 128K context model?",
        "bodyText": "Like here\nhttps://huggingface.co/TheBloke/Yarn-Llama-2-13B-128K-GGUF\nWhat parameters pass to command line ....",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2963",
        "createdAt": "2023-09-01T18:19:52Z",
        "author": {
            "login": "mirek190"
        }
    },
    {
        "title": "invalid magic characters tjgg.",
        "bodyText": "Model: huggingface TheBloke llama-2-7b-chat-ggml llama-2-7b-chat.ggmlv3.q4_0.bin\nsystem: windows10 1909\njust want to run on cpu\nby using Releases:\nPS E:\\llama-b1440-bin-win-avx-x64-github-llamacpp> .\\main -m llama-2-7b-chat.ggmlv3.q4_0.bin -n 128\nLog start\nmain: build = 1440 (82a6646)\nmain: built with MSVC 19.35.32217.1 for x64\nmain: seed  = 1699142125\ngguf_init_from_file: invalid magic characters tjgg.\nerror loading model: llama_model_loader: failed to load model from llama-2-7b-chat.ggmlv3.q4_0.bin\n\nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params: error: failed to load model 'llama-2-7b-chat.ggmlv3.q4_0.bin'\nmain: error: unable to load model\n\nby using llama-cpp-python:\nPS C:\\Users\\hao\\Desktop\\llama2> py\nPython 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from llama_cpp import Llama\n>>> llm = Llama(model_path=\"llama-2-7b-chat.ggmlv3.q4_0.bin\")\ngguf_init_from_file: invalid magic characters tjgg.\nerror loading model: llama_model_loader: failed to load model from llama-2-7b-chat.ggmlv3.q4_0.bin\n\nllama_load_model_from_file: failed to load model\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"E:\\python\\lib\\site-packages\\llama_cpp\\llama.py\", line 367, in __init__\n    assert self.model is not None\nAssertionError",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3947",
        "createdAt": "2023-11-05T00:09:04Z",
        "author": {
            "login": "for-the-zero"
        }
    },
    {
        "title": "Sampler orders are not consistent across backends",
        "bodyText": "It was observed in my Min P pull request that llama.cpp is doing something unique with the sampler order in comparison to other LLM inference backends:\n\nThe koboldcpp fork has its own solution for this: an internal 'sampler order' that can be configured by the end user to accomodate for their own personal order, since the transformers library expects that the temperature always comes first.\n\nThis allows the user to use the 'official' order where Temperature comes first as desired, or the alternative (that llama.cpp currently assumes) where Temp comes last.\nModifying this order has a significant impact in how the temp changes the model's outputs; if Temperature comes last with a 'safe' Min P or Top P, for example, it will still read correctly even with an obscenely high temperature value. But if Temperature comes first as is expected in the Transformers library (as well as the official GPT2 implementation), this value will naturally have a much different effect, as the actual values will have been modified before those truncation-based samplers were run, causing them to operate on a different 'scale'. I.e a token that would've been considered as under the 'Minimum' in a normal Min P setup might be over the required % if a high temperature was applied first.\nThe lack of standardization (or customization) here is a problem when it comes to reproducability and consistency for local models; one backend's set of sampler settings has a totally different effect on another backend (such as text-generation-webui's HF loaders, which assume the transformers order of temp first.)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3914",
        "createdAt": "2023-11-02T17:17:35Z",
        "author": {
            "login": "kalomaze"
        }
    },
    {
        "title": "Right command-line parameters (prompt etc.) for orca-mini-3B in interactive mode",
        "bodyText": "I want to use the orca-mini-3B model in interactive mode (chat mode) for question answering.\nSpecifically, I use the quants at\nhttps://huggingface.co/Aryanne/Orca-Mini-3B-gguf\nwhich are derived from the HF model at\nhttps://huggingface.co/pankajmathur/orca_mini_3b\nThe doc for the HF model states that the right prompt is the following:\n\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Response:\\n\"\nWhat is the right way to call the llam.cpp executable ('main') so that it uses exactly this prompt (and is in interactive mode) ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3916",
        "createdAt": "2023-11-02T19:35:43Z",
        "author": {
            "login": "hfassold"
        }
    },
    {
        "title": "Grammar generator app",
        "bodyText": "TL;DR: https://grammar.intrinsiclabs.ai/\nHey folks!\nWe're really excited for the new functionality @ejones brought with #1773. We think grammar-following is going to unlock a lot of really exciting use-cases where schemas matter, like\n\nGenerating type-safe requests to external APIs\nWriting DB queries using specific SQL dialects and being absolutely sure that they are syntactically valid\nUsing LLM for structured extraction that can be sent directly to a database, excel spreadsheet, etc.\n\nOne thing we noticed while trying to use it for some simple REST API generation is that generating the gbnf grammar files is a bit tedious, even for relatively small objects.\nAs a fun evening project, @tarrekshaban and I built an app (and corresponding TypeScript library) that lets you write simple TypeScript interface definitions and it handles generating the grammar files for you!\nUsage\nFeatures are limited in this first release, they include\n\nAbility to define one or more interface types and have them reference each other. Note that if you have multiple interfaces, the first interface in the file is assumed to be the root type for generation\n\ninterface Candidate {\n    name: string;\n    workExperiences: WorkExperience[];\n    education: Education[];\n    skills: string[];\n}\n\ninterface WorkExperience {\n    company: string;\n    title: string;\n    startYear: number;\n    endYear: number;\n}\n\ninterface Education {\n    university: string;\n    degrees: string;\n    graduationYear: number;\n}\n\nFields currently support types string, number, your custom interface types, and one-dimensional Arrays of those types.\n\nWe would like to add support for type aliases, anonymous types, and more based on what users are interested in. Please give it a shot and let us know if you find it helpful! Bugs, PRs and feedback all welcome :)\nApp Link: https://grammar.intrinsiclabs.ai/\nApp Repo: https://github.com/IntrinsicLabsAI/grammar-builder\ngbnfgen Library Repo: https://github.com/IntrinsicLabsAI/gbnfgen",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2494",
        "createdAt": "2023-08-02T20:35:22Z",
        "author": {
            "login": "a10y"
        }
    },
    {
        "title": "How to tune llama.cpp for different ROCm gpus?",
        "bodyText": "I have a server with 2 MI100s and 2 W6800s. When I use them for inferencing the W6800s are nearly twice the speed of the MI100s. Most of the devs that have worked on llama.cpp and the like seem to have access to an RX6800 or similar so everything is tuned for those cards. The MI100s should be a lot faster and I have seen mention of tuning for specific cards. How do we go about tuning for the MI's, or any other card? Such an absolute waste of recourses.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3910",
        "createdAt": "2023-11-02T12:50:56Z",
        "author": {
            "login": "ccbadd"
        }
    },
    {
        "title": "CUDA build performing very poorly on A100 (very long prompt eval time)",
        "bodyText": "Hi,\nI've built llama.cpp with make LLAMA_CUBLAS=1. I'm using server and seeing incredibly slow performance that makes me suspect something is amiss. I'm running on an A100 with 80GB RAM (Runpod.io). The logs seem to indicate that the GPU is being utilized:\nroot@d8f002add17a:/workspace/llama.cpp# ./server --mmproj models/llava-1.5-7b/mmproj-model-f16.gguf -m models/llava-1.5-7b/ggml-model-q5_k.gguf \nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n{\"timestamp\":1698786679,\"level\":\"INFO\",\"function\":\"main\",\"line\":2212,\"message\":\"build info\",\"build\":1448,\"commit\":\"238657d\"}\n{\"timestamp\":1698786679,\"level\":\"INFO\",\"function\":\"main\",\"line\":2215,\"message\":\"system info\",\"n_threads\":96,\"n_threads_batch\":-1,\"total_threads\":192,\"system_info\":\"AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}\nMulti Modal Mode Enabledclip_model_load: model name:   openai/clip-vit-large-patch14-336\nclip_model_load: description:  image encoder for LLaVA\nclip_model_load: GGUF version: 2\nclip_model_load: alignment:    32\nclip_model_load: n_tensors:    377\nclip_model_load: n_kv:         18\nclip_model_load: ftype:        f16\n\nclip_model_load: text_encoder:   0\nclip_model_load: vision_encoder: 1\nclip_model_load: llava_projector:  1\nclip_model_load: model size:     595.61 MB\nclip_model_load: metadata size:  0.13 MB\nclip_model_load: total allocated memory: 201.27 MB\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llava-1.5-7b/ggml-model-q5_k.gguf (version GGUF V2)\n\n...\n\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = mostly Q5_K - Medium\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: PAD token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.10 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  = 4560.96 MB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/35 layers to GPU\nllm_load_tensors: VRAM used: 0.00 MB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model: kv self size  = 1024.00 MB\nllama_new_context_with_model: compute buffer total size = 162.13 MB\nllama_new_context_with_model: VRAM scratch buffer: 156.00 MB\nllama_new_context_with_model: total VRAM used: 156.00 MB (model: 0.00 MB, context: 156.00 MB)\nAvailable slots:\n -> Slot 0 - max context: 2048\n\nllama server listening at http://127.0.0.1:8080\n\n{\"timestamp\":1698786685,\"level\":\"INFO\",\"function\":\"main\",\"line\":2492,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\nall slots are idle and system prompt is empty, clear the KV cache\n\n\\slot 0 - image loaded [id: 0] resolution (1600 x 1067)\nslot 0 is processing [task id: 0]\nslot 0 : kv cache rm - [0, end)\nslot 0 - encoding image [id: 0]\n\nprint_timings: prompt eval time =   41676.96 ms /     1 tokens (41676.96 ms per token,     0.02 tokens per second)\nprint_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token, 250000.00 tokens per second)\nprint_timings:       total time =   41676.96 ms\nslot 0 released (3 tokens in cache)\n{\"timestamp\":1698786731,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2156,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":55292,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n\nprompt eval time is 0.02 tokens/second.\nAny help debugging this or understanding the timing breakdown and why this system isn't performing would be very helpful. On my M1 MacBook w/ 32GB RAM, it absolutely screams, but it's using an entirely different backend (Metal).\nThank you,\nBart",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3874",
        "createdAt": "2023-10-31T21:22:32Z",
        "author": {
            "login": "trzy"
        }
    },
    {
        "title": "Why does decoding sometimes yield bunches of \"\\n\" at end instead of EOS?",
        "bodyText": "With n_len set to 1024 for the simple.cpp code, and a prompt of:\nWhat is a large language model?\nDecoding yields:\nA large language model is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models are designed to learn the patterns and structures of language by exposure to a vast amount of text data, and they can be used for a variety of natural language processing tasks, such as language translation, text summarization, and language generation.\nWhich is followed by a whole lot of \"\\n\" tokens.\nIs there a simple explanation for why this happens?\nPlease pardon my naive question. I am new to this stuff.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3886",
        "createdAt": "2023-11-01T15:33:39Z",
        "author": {
            "login": "dougdew64"
        }
    },
    {
        "title": "Python Wrapper - Access to output tokens rather than string?",
        "bodyText": "Hi,\nIs there a way to get the output token sequence when generating text with llama-cpp-python? I don't need the logprobs, just the tokens.\nIf I tokenize the result I don't necessarily get the same tokens that were chosen, as there are potentially many ways to get to the same output string.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3890",
        "createdAt": "2023-11-01T17:55:45Z",
        "author": {
            "login": "noamgat"
        }
    },
    {
        "title": "Want to run on 1 GPU (of 2 total) but looks like the model is loaded onto both GPUs.",
        "bodyText": "Using the CuBLAS build with 2 GPUs. I want to load the model onto a single GPU, but the model is always loaded into the memory of both GPUs. Even if I only run on 1 GPU the model is loaded onto both GPUs.\nThings I've tried:\n./main -m ./llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40\nResult: Default behavior: Loads model onto both GPUs, runs on both GPUs.\nCUDA_VISIBLE_DEVICES=0 ./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40\nResult: Loads model onto both GPUs, runs only on GPU 0\nCUDA_VISIBLE_DEVICES=1 ./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40\nResult: Crashes: ggml_init_cublas: found 1 CUDA devices: Device 0: NVIDIA RTX A5000, compute capability 8.6 Segmentation fault (core dumped)\n./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40 -mg 0 -ts 1,0\nResult: Crashes: CUDA error 400 at ggml-cuda.cu:3343: invalid resource handle\n./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40 -mg 0 -ts 10,1\nResult: Loads model onto both GPUs, runs mostly on GPU 0 and a little on GPU 1 (power usage is 220 W on GPU 0, 80 W on GPU 1)\n./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40 -mg 1 -ts 1,10\nResult: Same as above, but mostly runs on GPU 1\nWhen I run a query I can tell from the power usage that only GPU 0 is being used.\nBut when I check nvidia-smi I see that the model is loaded to both GPUs. That is, if the model required 8 GB both GPUs show 8 GB of memory is used.\nAny idea what's going on here?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2752",
        "createdAt": "2023-08-23T21:48:33Z",
        "author": {
            "login": "isaacmorgan"
        }
    },
    {
        "title": "Allow caller to handle help/argument exceptions",
        "bodyText": "Looking for committer to help review/merge #3715. It is a very small change shouldn't take much time.\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3887",
        "createdAt": "2023-11-01T15:41:25Z",
        "author": {
            "login": "bandoti"
        }
    },
    {
        "title": "Microsoft says GPT-3.5-Turbo has just 20 billion parameters.",
        "bodyText": "Link: https://arxiv.org/pdf/2310.17680.pdf",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3857",
        "createdAt": "2023-10-30T12:23:31Z",
        "author": {
            "login": "bobqianic"
        }
    },
    {
        "title": "Is it possible to load llavar? Apparently it has superior text recognition to Llava",
        "bodyText": "https://llavar.github.io/\nI am a little unclear about how one might go about getting the projection for this, etc. Any help appreciated",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3875",
        "createdAt": "2023-10-31T22:25:59Z",
        "author": {
            "login": "IridiumMaster"
        }
    },
    {
        "title": "how to reduce hallucinations for a specific 4bit / 8bit model?",
        "bodyText": "how to reduce hallucinations for a specific 4bit / 8bit model?\nwhat parameters shld i put? 6_0 bit models from thebloke for 7b seems ok but sometimes (like 1 out of 10 times) can generate hallucinated stuff especially with biographical content.\nanyone can give any ideas on how to reduce hallucination? i'm not sure how high the parameters / bit should have to have 100% zero hallucination. (is this possible?)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3209",
        "createdAt": "2023-09-16T04:09:24Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Flash decoding (flash attention) speed increase for long context",
        "bodyText": "I just came across this and can't see the conversation elsewhere so...\nFrom the blog post\n\"We present a technique, Flash-Decoding, that significantly speeds up attention during inference, bringing up to 8x faster generation for very long sequences. The main idea is to load the keys and values in parallel as fast as possible, then separately rescale and combine the results to maintain the right attention outputs.\"\nExample forthcoming:\nhttps://github.com/Dao-AILab/flash-attention/tree/main/examples/inference",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3711",
        "createdAt": "2023-10-21T12:02:31Z",
        "author": {
            "login": "jjohare"
        }
    },
    {
        "title": "Easy-to-use, No-Dependencies Grammar Compiler for better response.",
        "bodyText": "I'd like to introduce a user-friendly Python-based GBNF Compiler, which you can easily access at:\n\nhttps://pypi.org/project/gbnf-compiler/\nhttps://github.com/nova-land/gbnf-compiler/\n\nThe GBNF Compiler work as follow:\n\nCreate a Template -> Provide Grammar String to LLM -> Parse the LLM response directly\n\nWhile JSON-based grammar tools are excellent, they may not always be the ideal format for Large Language Models to read in. With this in mind, I developed the GBNF Compiler, designed to be as straightforward as using handlebars.js with double quotes. This flexibility allows you to adapt it to any grammar format you require.\nWith the GBNF Compiler, you can effortlessly create 'fill in the blank' grammars, perfect for LLMs to perform reasoning and explanation tasks. This approach enables us to achieve higher-quality and more easily parsable results.\"\n\nHere with an example:\nimport requests\nfrom gbnf_compiler import GBNFCompiler, MultipleChoice, SingleSentence\n\n# Define your Prompt\nprompt = \"What tool will you use to calculate 2^5 ?\"\n\n# Define the LLM Response Template\n# Each {{}} is a variable with a rule\ntemplate = \"I choose {{tool}} because {{reason}}\"\n\ntools = MultipleChoice('tool', ['calculator', 'web-search', 'web-browse'])\nc = GBNFCompiler(template, { 'tool': tools, 'reason': SingleSentence() })\nprint(c.grammar())\n\n# Try a dummy result\ntext = \"I choose calculator because it is the most efficient and accurate way to calculate 2^5.\"\nresult = c.parse(text)\nprint(result)\n\n# Example: Send it out to local llama.cpp\ndef template(role: str, prompt: str):\n    return \"\"\"[INST] <<SYS>>\n{role}\n<</SYS>>\n{prompt}\n[/INST]\"\"\".format(role=role, prompt=prompt)\n\ndata_json = {\n    \"prompt\": template(\"\", prompt), \"temperature\": 0.0,\n    \"n_predict\": 512, \"top_p\": 0.2, \"top_k\": 10,\n    \"stream\": False, \"grammar\": c.grammar() }\n\nresp = requests.post(\n    url=\"http://127.0.0.1:9999/completion\",\n    headers={\"Content-Type\": \"application/json\"},\n    json=data_json,\n)\nresult = resp.json()[\"content\"]\nprint(result)\n\n# Directly parse the result\n# Try a dummy result\ntext = \"I choose calculator because it is the most efficient and accurate way to calculate 2^5.\"\nresult = c.parse(text)\nprint(result)\n\"\"\"\nResult: \n{'tool': 'calculator', 'reason': 'it is the most efficient and accurate way to calculate 2^5.'}\n\"\"\"\nYou can check more examples in here with JSON format grammar.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3729",
        "createdAt": "2023-10-22T17:27:22Z",
        "author": {
            "login": "nova-land"
        }
    },
    {
        "title": "llama.cpp & whisper.cpp - now in crowd sourced hardware",
        "bodyText": "https://www.crowdsupply.com/useful-sensors/ai-in-a-box",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3866",
        "createdAt": "2023-10-31T05:57:14Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Speculative sampling?",
        "bodyText": "Claims 2-2.5x speedup by getting multiple tokens per transformer call, could help a lot with large models living mostly in RAM vs VRAM. https://arxiv.org/abs/2302.01318.  Seems textsynth already has this feature.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3863",
        "createdAt": "2023-10-31T00:20:04Z",
        "author": null
    },
    {
        "title": "Hardware specs for GGUF 7B/13B/30B parameter models",
        "bodyText": "Hi, I am thinking of trying find the most optimal build by cost of purchase + power consumption, to run 7b gguf model (mistral 7b etc) at 4-5 token/s. I would like to ask you what sort of CPU, RAM etc should I look at. I would appreciate if someone explains in which configuration is llama.cpp is supposed to work best. Like I have heard that RAM frequency and single thread perfrormance matters more than other things etc.\nPlease write in detail, the principle hardware specs which are relevant and by how much and their optimal config. Plus some popular tested examples of CPU models, RAM speeds etc.\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3847",
        "createdAt": "2023-10-29T10:56:45Z",
        "author": {
            "login": "GeneralKugelBlitz"
        }
    },
    {
        "title": "Documentation of llama 2 model architecture details?",
        "bodyText": "Where is the llama 2 model architecture documentation which was used to figure out what code to write for the llama.cpp file?\nThis llama doc does not seem to provide the kind of details which were apparently important when writing the code in the llama.cpp file: https://arxiv.org/pdf/2302.13971.pdf\nMy goal is to gain a thorough understanding of the code in the llama.cpp file.\nThank you in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3788",
        "createdAt": "2023-10-26T00:41:47Z",
        "author": {
            "login": "dougdew64"
        }
    },
    {
        "title": "How can I swap two KV cache slots?",
        "bodyText": "Let's make this simple and say I want to swap the slot at position 10 with the one at position 25, we're running on CPU only and I only care about LLaMA. So this would happen in llm_build_llama around the same place the KV cache shifting stuff happens.\nstd::swap(kv_self.cells[10], kv_self.cells[25]);\nNow something has to happen with kv_self.k and kv_self.v, but what?\nOnce I know this, I'll probably be able to implement KV cache defragmenting which can make a big difference for parallel generation.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3507",
        "createdAt": "2023-10-06T17:04:38Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Heads-up, Zephyr doesn't use special tokens for its prompt template.",
        "bodyText": "Almost as if there was not enough confusion already, Zephyr prompt template does not appear to use special tokens, despite introducing chat tags.\nIt's vocab does not contain tokens for \"<|user|>\", \"<|assistant|>\" or \"<|system|>\".\nAnd those tags do show up in the conversation because they don't have special tokens representing them.\nWhile confusing, this does appear to be \"correct\" according to the original model source files on HF.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3836",
        "createdAt": "2023-10-28T16:21:17Z",
        "author": {
            "login": "staviq"
        }
    },
    {
        "title": "How to Build an example manually",
        "bodyText": "I would like to build examples/simple manually using gcc after I did make, I tried:\ngcc simple.cpp -I../../common/ -I../../spm-headers\nbut still getting error:\nsimple.cpp:(.text._ZNSt10_Head_baseILm0ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEELb0EED2Ev[_ZNSt10_Head_baseILm0ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEELb0EED5Ev]+0x14): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'\n/usr/bin/ld: /tmp/ccwyglmE.o: in function `std::__new_allocator<int>::allocate(unsigned long, void const*)':\nsimple.cpp:(.text._ZNSt15__new_allocatorIiE8allocateEmPKv[_ZNSt15__new_allocatorIiE8allocateEmPKv]+0x4b): undefined reference to `std::__throw_bad_array_new_length()'\n/usr/bin/ld: simple.cpp:(.text._ZNSt15__new_allocatorIiE8allocateEmPKv[_ZNSt15__new_allocatorIiE8allocateEmPKv]+0x50): undefined reference to `std::__throw_bad_alloc()'\n/usr/bin/ld: simple.cpp:(.text._ZNSt15__new_allocatorIiE8allocateEmPKv[_ZNSt15__new_allocatorIiE8allocateEmPKv]+0x60): undefined reference to `operator new(unsigned long)'\n/usr/bin/ld: /tmp/ccwyglmE.o: in function `std::__new_allocator<std::__detail::_Hash_node_base*>::deallocate(std::__detail::_Hash_node_base**, unsigned long)':\nsimple.cpp:(.text._ZNSt15__new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS2_m[_ZNSt15__new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS2_m]+0x2b): undefined reference to `operator delete(void*, unsigned long)'\n/usr/bin/ld: /tmp/ccwyglmE.o: in function `std::__new_allocator<std::__detail::_Hash_node<std::pair<int const, float>, false> >::deallocate(std::__detail::_Hash_node<std::pair<int const, float>, false>*, unsigned long)':\nsimple.cpp:(.text._ZNSt15__new_allocatorINSt8__detail10_Hash_nodeISt4pairIKifELb0EEEE10deallocateEPS5_m[_ZNSt15__new_allocatorINSt8__detail10_Hash_nodeISt4pairIKifELb0EEEE10deallocateEPS5_m]+0x2a): undefined reference to `operator delete(void*, unsigned long)'\n/usr/bin/ld: /tmp/ccwyglmE.o:(.data.rel.local.DW.ref.__gxx_personality_v0[DW.ref.__gxx_personality_v0]+0x0): undefined reference to `__gxx_personality_v0'",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3830",
        "createdAt": "2023-10-28T10:57:56Z",
        "author": {
            "login": "nonunknown777"
        }
    },
    {
        "title": "Should this be llm.cpp not llama.cpp?",
        "bodyText": "though it is aim at running llama at the begin but it is able to run various non-llama models. so should it be renamed to llm.cpp to show that it can not only run llama\uff1f",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3844",
        "createdAt": "2023-10-29T04:55:14Z",
        "author": {
            "login": "lin-calvin"
        }
    },
    {
        "title": "Inference time a lot higher when using cmake compared to w64devkit.",
        "bodyText": "For certain reasons, the inference time of my mistral-orca is a lot longer when having compiled the binaries with cmake compared to w64devkit. What could be the cause of this? I'm using a macbook pro, 2019 with an i7. I want to run the inference on CPU only.\nw64devkid:\nllama_print_timings: load time = 2789.31 ms\nllama_print_timings: sample time = 7.55 ms / 18 runs ( 0.42 ms per token, 2383.16 tokens per second)\nllama_print_timings: prompt eval time = 1925.06 ms / 20 tokens ( 96.25 ms per token, 10.39 tokens per second)\nllama_print_timings: eval time = 8256.93 ms / 18 runs ( 458.72 ms per token, 2.18 tokens per second)\nllama_print_timings: total time = 23842.73 ms\ncmake:\nllama_print_timings: load time = 4133.27 ms\nllama_print_timings: sample time = 5.71 ms / 18 runs ( 0.32 ms per token, 3153.47 tokens per second)\nllama_print_timings: prompt eval time = 25917.85 ms / 19 tokens ( 1364.10 ms per token, 0.73 tokens per second)\nllama_print_timings: eval time = 43493.77 ms / 18 runs ( 2416.32 ms per token, 0.41 tokens per second)\nllama_print_timings: total time = 74989.23 ms",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3834",
        "createdAt": "2023-10-28T15:21:18Z",
        "author": {
            "login": "v4lentin1879"
        }
    },
    {
        "title": "Convert safetensors to ggml",
        "bodyText": "Hi,\nHow to convert safetensors to ggml?\nError when try use the convert.py:\npython3 convert.py /Users/paulo/Developer/workspaces/python/stable-diffusion-webui/models/Stable-diffusion/dreamshaper_8Inpainting.safetensors --outfile dreamshaper_8Inpainting.bin\nLoading model file /Users/paulo/Developer/workspaces/python/stable-diffusion-webui/models/Stable-diffusion/dreamshaper_8Inpainting.safetensors\nTraceback (most recent call last):\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 1202, in <module>\n    main()\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 1135, in main\n    model_plus = load_some_model(args.model)\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 1054, in load_some_model\n    models_plus.append(lazy_load_file(path))\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 746, in lazy_load_file\n    return lazy_load_safetensors_file(fp, path)\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 725, in lazy_load_safetensors_file\n    model = {name: convert(info) for (name, info) in header.items() if name != '__metadata__'}\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 725, in <dictcomp>\n    model = {name: convert(info) for (name, info) in header.items() if name != '__metadata__'}\n  File \"/Users/paulo/Developer/workspaces/cpp/ai-kit/extras/scripts/convert.py\", line 713, in convert\n    data_type = SAFETENSORS_DATA_TYPES[info['dtype']]\nKeyError: 'I64'\n\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3826",
        "createdAt": "2023-10-28T07:02:49Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "Really interesting article with interactive examples on visualizing matmul and Transformer structure",
        "bodyText": "Maybe everyone else has already seen this, but if not it seems like an amazing resource for understanding some of the math and structure behind these models: https://pytorch.org/blog/inside-the-matrix/\nThe beginning starts with some background on the math, but if that's not interesting enough, scroll down a bit to the Inside an Attention Head section.\nAlso GPT2 attention head explorer: https://bhosmer.github.io/mm/examples/attngpt2/index.html",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3828",
        "createdAt": "2023-10-28T10:09:05Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Why are all the releases for windows?",
        "bodyText": "How can I get access to mac and linux releases? Or do I need to build them from scratch?\nMany thanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3819",
        "createdAt": "2023-10-27T18:46:09Z",
        "author": {
            "login": "samlhuillier"
        }
    },
    {
        "title": "How to load simple GGUF models?",
        "bodyText": "Hi,\nIm trying load two GGUF models, but im getting error:\n// prompt and model\nconst std::string prompt = \"Write a simple Python code\";\nconst std::string modelPath = \"codellama-7b.Q2_K.3.gguf\";\n\n// params\ngpt_params params;\nparams.model = modelPath;\nparams.prompt = \"### Instruction:\" + prompt + \"\\n\\n### Response:\";\nparams.numa = 32;\n\n// init LLM\nllama_backend_init(params.numa);\n\n// initialize the model\nllama_model_params modelParams = llama_model_default_params();\nllama_model *model = llama_load_model_from_file(params.model.c_str(), modelParams);\n\nError:\ngguf_init_from_file: tensor 'token_embd.weight' number of elements (131137536) is not a multiple of block size (0)\nerror loading model: llama_model_loader: failed to load model from /Users/paulo/Downloads/codellama-7b.Q2_K.3.gguf\n\nIn their website (HF) they use this line of command:\n/main -ngl 32 -m codellama-7b.q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"{prompt}\"\n\nHow to convert that params to C++ code with llama.cpp?\nModels:\n\nhttps://huggingface.co/TheBloke/CodeLlama-7B-GGUF\nhttps://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF\n\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3724",
        "createdAt": "2023-10-22T08:18:55Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "fine tuning in torch, then quantizing in llama-cpp",
        "bodyText": "we use llama cpp for inference (python bindings)\nbecause fine tuning in llama cpp doesn't support fast/gpu mode like the inference does, we use python for fine tuning\nwe then use model.merge_and_unload to get a single model\nand then we use convert.py to convert it to a gguf\nthe final result winds up f32\nwe then call quantize. i would like to quantize to q5_1, but we only ever get q8 (8-bit)\nand the output says we are \"re-quantizing\"\n(the resulting gguf works perfectly fine, and retains the fine-tune nicely - but it won't go lower than q8)\nam i doing this horribly wrong?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3786",
        "createdAt": "2023-10-25T21:34:27Z",
        "author": {
            "login": "earonesty"
        }
    },
    {
        "title": "Convert GGUF model to Hugging Face model",
        "bodyText": "Hi,\nHow can I convert a GGUF model back to a Hugging Face model? Specifically a model fine-tuned using llama.cpp?\nThanks in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3770",
        "createdAt": "2023-10-25T00:39:07Z",
        "author": {
            "login": "fakerybakery"
        }
    },
    {
        "title": "Mac Studio 192GB vs AMD Ryzen 9 PRO 7945 with 192GB ddr5 7,800MT/s",
        "bodyText": "To all the experts here,\nMac Studio 192GB vs AMD Ryzen 9 PRO 7945 with 192GB ddr5 7,800MT/s\ni know Mac prompt (input) speed is as fast as it's decoding (so i heard, will this be improved or can this be improved?)\nAMD is \"cheaper\" but what's the inference speed like?\ni was wondering should i get 192gb ddr5 which has bandwidth of 51gb/s vs 800gb/s mac studio and i was wondering do i need more ram vs i need faster cpu or at what point will 51gb/s ddr5 ram be \"limited\" by the processing speed?\nDoes anyone has some clue to such questions? do we need faster cpu (what's the limit) or do we need faster bandwidth?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3791",
        "createdAt": "2023-10-26T04:03:30Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "hot reload llama.cpp",
        "bodyText": "I made a thing to hot reload llama.cpp code.\nhttps://github.com/spirobel/bunny-llama\nSo you can keep the model loaded, change and recompile the prompting function and rerun it instantly without having to reload the model.\nthe bun clone command will fork the llama.cpp repo specified in the env file. You can replace it with your own if you want to experiment on your own with llama.cpp.\nI added an api-llama example in mine: https://github.com/spirobel/llama.cpp/tree/api-llama-example/examples/api-llama it just has a very straight forward load_model() and prompt() function.\nlibllama.so has already so much stuff :-) i wanted to have a straight forward api to play with.\nThis example is also helpful for people that want to integrate and ship llama.cpp as part of a bigger app.\nit will also build the cuda version statically so your users wont have to have the cuda toolkit around!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3784",
        "createdAt": "2023-10-25T20:46:30Z",
        "author": {
            "login": "spirobel"
        }
    },
    {
        "title": "Is there any external UI that works directly with the server API?",
        "bodyText": "I mean without using a proxy. I haven't found anything, for example SillyTavern doesn't. Also terminal based would be fine.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3779",
        "createdAt": "2023-10-25T16:13:53Z",
        "author": {
            "login": "xoich"
        }
    },
    {
        "title": "Taking in large prompts (5000 characters and up)",
        "bodyText": "I am working on a project that requires running text generation on prompts of over 5000 characters. Currently loading them into a single tokenization call results in the fail state created in #1881. I believe this is because I am attempting to load significantly more tokens than what is supposed to be loaded at once.\nI have looked over the examples provided but I have not found any examples of loading really large prompts. I have done a little bit of research into loading large prompts but I am admittedly very new to this field and was wondering if I could receive some guidance specific to this project.\nThrough my research I have found techniques like chunking my input into smaller fragments, however I am not sure how I would implement this using the llama.cpp api. I already have actual string fragmentation completed but my question lies in how this would be sent to the model. Is this the right step forward or are there other resources / techniques that I should explore?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3704",
        "createdAt": "2023-10-20T23:12:35Z",
        "author": {
            "login": "ActuallyTaylor"
        }
    },
    {
        "title": "Compiler for gbnf format?",
        "bodyText": "Hello,\nI am facing some conditions where llama output completes at : token. I added markdown in JSON grammar. I want to check state machine of my new grammar regex. Is there any compiler available for gbnf like bnf. Or what is different b/w the syntax of bnf and gbnf.\nPlease help me with resolving this issue.\nThank You.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3666",
        "createdAt": "2023-10-18T13:41:46Z",
        "author": {
            "login": "cdaman123"
        }
    },
    {
        "title": "What models run inside Orange Pi 5 32GB?",
        "bodyText": "Hi,\nWhat models run inside Orange Pi 5 32GB? 65B?\n\nReference:\nhttp://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-5-32GB.html\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3758",
        "createdAt": "2023-10-24T06:56:45Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "About fp16 mat_mul",
        "bodyText": "In llama.cpp, when weight A is fp16,  activation B is fp32, the mat_mul will first convert B into fp16 and store in wdata\n  if (params->type == GGML_TASK_INIT) {\n      if (src1->type != vec_dot_type) {\n          char * wdata = params->wdata;\n          const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n          for (int64_t i13 = 0; i13 < ne13; ++i13) {\n              for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                  for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                      from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                      wdata += row_size;\n                  }\n              }\n          }\n      }\n\n      return;\n  }\nBut when do the actually vec_dot, both A and B will be convert back to fp32 and do the calculation.\n    float sumf = 0.0;\n\n    for (int i = 0; i < nb; i++) {\n        int sumi = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const int v0 = (x[i].qs[j] & 0x0F);\n            const int v1 = (x[i].qs[j] >>   4);\n\n            sumi += (v0 * y[i].qs[j]) + (v1 * y[i].qs[j + qk/2]);\n        }\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*y[i].d)*sumi + GGML_FP16_TO_FP32(x[i].m)*y[i].s;\n    }\n\n    *s = sumf;\nWhy use this way? And is this the reason why fp16 prediction is slower than origin fp32 and int8.?\nOr did I ignore something?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3757",
        "createdAt": "2023-10-24T03:40:10Z",
        "author": {
            "login": "Zhikaiiii"
        }
    },
    {
        "title": "feasibility question for streaming gpu offloading",
        "bodyText": "I have a 9900k with 128G ram and a rtx 4070 with 12G vram.    llamma.cpp runs the falcon 180b chat model (4b quantized from blokes last upload) but only allows 6 layer GPU offload resulting in about 0.3 to 0.5token/sec throughput while the GPU sits there mostly unloaded which is not usable.  The feasiblity question I have is would it be possible to dynamically offload every layer of the model into the GPU for compute by pipelining the streaming of the layer weights into the GPU together with the GPU number crunch compute of the layers.  If feasible I think there is a potential for around x10 speedup (5 tok/sec) on my setup if the GPU is kept fully loaded and CPU is not relied on for any compute which would make it actually usable.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3745",
        "createdAt": "2023-10-23T14:51:40Z",
        "author": null
    },
    {
        "title": "gpt4-x-alpaca - only bin file - how do I interact with this, using llama.cpp?",
        "bodyText": "I have found 2 different repos for this, and i only see bin files from 7 months ago.\n1- https://huggingface.co/chavinlo/gpt4-x-alpaca/tree/main\n2- https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g\nThey only provide a bin file, and I can't get llama.cpp to open it so I can't interact with this model.\nAny help is appreciated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3737",
        "createdAt": "2023-10-23T06:12:06Z",
        "author": {
            "login": "djbritt"
        }
    },
    {
        "title": "Mentored speculative decoding",
        "bodyText": "This looks interesting: https://vivien000.github.io/blog/journal/a-provably-optimal-lossy-variant-of-speculative-decoding.html\nAnd not really super difficult to implement on top of what already exists: https://github.com/vivien000/mentored_decoding/blob/ace0902c48e9d6aa6c7823a71d6831c3d106ff19/notebook.ipynb",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3733",
        "createdAt": "2023-10-23T01:44:56Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Is there anyway to disable log file writing?",
        "bodyText": "The server was start by:\n./server -m ./data/ggml-model-q8_0.gguf\n\nThe log file records too much content, and as the number of requests increases, the log file becomes larger and larger.\nIs there any way to disable the output of log files",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3672",
        "createdAt": "2023-10-19T02:54:05Z",
        "author": {
            "login": "CharlinChen"
        }
    },
    {
        "title": "Mistral-7B Support",
        "bodyText": "https://mistral.ai/news/announcing-mistral-7b/\nhttps://huggingface.co/mistralai/Mistral-7B-v0.1\nhttps://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\nHighlights from the announcement:\nMistral 7B is a 7.3B parameter model that:\n\n- Outperforms Llama 2 13B on all benchmarks\n- Outperforms Llama 1 34B on many benchmarks\n- Approaches CodeLlama 7B performance on code, while remaining good at English tasks\n- Uses Grouped-query attention (GQA) for faster inference\n- Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost\n\nWe\u2019re releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3368",
        "createdAt": "2023-09-27T17:21:44Z",
        "author": {
            "login": "bgyss"
        }
    },
    {
        "title": "gguf_init_from_file: tensor 'token_embd.weight' number of elements (131072000) is not a multiple of block size (0)",
        "bodyText": "In support of my goal of learning this codebase by stepping through it in Xcode's debugger and profiling it with Instruments, I have created a new Xcode project and added to that project the nineteen .h/.c/.cpp/.m files that are necessary to complete a build of the \"main\" example.\nI'm encountering a model loading error when attempting to run the code:\ngguf_init_from_file: tensor 'token_embd.weight' number of elements (131072000) is not a multiple of block size (0)\nThat error occurs when attempting to load the llama-2-7b-chat.Q5_K_M.gguf file.\nI'm able to use make to successfully build the llama.cpp repo code and to run the \"main\" example using the llama-2-7b-chat.Q5_K_M.gguf file. So, I assume that I've caused a problem when creating my Xcode project.\nI'm debugging now, trying to determine what sort of problem that I've caused. And, I'm looking through the Makefile to see if there were any settings in there which I failed to account for in my Xcode build.\nI'd be grateful for any suggestions.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3350",
        "createdAt": "2023-09-27T02:10:37Z",
        "author": {
            "login": "douglasdew"
        }
    },
    {
        "title": "What is the best model for title/description writing?",
        "bodyText": "Hi,\nI need write my videos title/description with Ai from my app, but i don't know what is the best multi-language GGUF model for this.\nCan anyone have some recommendation?\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3725",
        "createdAt": "2023-10-22T08:20:47Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "Forcing macos high power mode on GPU?",
        "bodyText": "Hi, I'm writing matmul kernels right now, and I have an issue where I will run my kernel 100 times and each time get drastically different performance. I suspect this is caused by macos shifting the gpu to low power mode / limiting it's clock speed.\nThere's also the issue where when I switch windows during execution, my kernels start returning 0's and taking 0 time to run.\nHas anyone dealt with this, or is there a ggml issue I can look at where similar problems have been addressed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3721",
        "createdAt": "2023-10-22T03:01:13Z",
        "author": {
            "login": "jafioti"
        }
    },
    {
        "title": "Whats the difference between input prefix / suffix (inp_pfx/inp_sfx) and line  prefix / suffix (line_pfx/line_sfx)?",
        "bodyText": "I'm looking to integrate prompt templates directly into code for my app and I would like to know what each of these variables mean? They both seem to have the same purpose so why have both?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3691",
        "createdAt": "2023-10-20T03:34:47Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "Generating questions from a prompt and a given context",
        "bodyText": "I am trying to obtain questions relative to a given context. But I obtain hallucinating answers.\nI am not sure , if the problem is related to the prompt or not\nhere is my program\n\nfrom llama_cpp import Llama\n\n# define n_ctx manually to permit larger contexts\nLLM = Llama(model_path=\"../../../../llama-2-7b.Q5_K_M.gguf\", n_ctx=512)\n\n# create a text prompt\ncontext = \"\"\"\nNous sommes dans les Yvelines\nIl fait beau\nNous sommes a quelques km d'un joli lac\nle boulanger est sympatique\nIl y a \u00e9galement une boucherie\n\"\"\"\n\nprompt = \"\"\"\nYou are a Teacher/ Professor. Your task is to setup\n4 questions for an upcoming\n    quiz/examination. The questions should be diverse in nature \n   across the document. Restrict the questions to the \n    context  provided.\n Context is provided below :                                                                                                                                                                                                                                                                                                                                             \n   %s                                                                                                                                                                                                                                                                                                                  \n\"\"\" % context\n\n\nprint(prompt)\n# set max_tokens to 0 to remove the response size limit\noutput = LLM(prompt, max_tokens=0)\n\n# display the response\nprint(output[\"choices\"][0][\"text\"])\n\nprint(\"---------------\")\nprint(output)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3681",
        "createdAt": "2023-10-19T13:42:11Z",
        "author": {
            "login": "sancelot"
        }
    },
    {
        "title": "[User] Insert summary of your issue or enhancement..",
        "bodyText": "Expected Behavior\nFor a prompt will generate 1000 output tokens, is there function to stop generate token manualy?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3700",
        "createdAt": "2023-10-20T15:21:57Z",
        "author": {
            "login": "Lzhang-hub"
        }
    },
    {
        "title": "Please provide a grammer for markdown format",
        "bodyText": "Prerequisites\nPlease answer the following questions for yourself before submitting an issue.\n\n I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n I carefully followed the README.md.\n I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n I reviewed the Discussions, and have a new bug or useful enhancement to share.\n\nExpected Behavior\nI am using json_arr grammer and when i asked for ordered list it gives me the comma seperated list without any heading.\nCurrent Behavior\nI wanted a Markdown string which contain heading and ordered list inside a key of json structure. Like OpenAI function call give.\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3678",
        "createdAt": "2023-10-18T07:37:05Z",
        "author": {
            "login": "cdaman123"
        }
    },
    {
        "title": "Custom sampler / Logits processor / Token filterer?",
        "bodyText": "Hi,\n(Context: I'm looking to integrate this library: https://github.com/noamgat/lm-format-enforcer with llama.cpp)\nI am looking to customize the sampling procedure used by llama.cpp in order to integrate a postprocessing step on the logits, to enforce certain properties of the output token stream.\nFor example, huggingface transformers have this optional parameter:\nprefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]]\n\nparameter when calling generate().\nIs this achievable with llama.cpp? There are a number of ways to approach this, and I wonder if any API is open to do such a thing.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3665",
        "createdAt": "2023-10-18T08:41:04Z",
        "author": {
            "login": "noamgat"
        }
    },
    {
        "title": "Maid - A better Android / Cross platform app for GGUF models",
        "bodyText": "Maid is a cross-platform Flutter app for interfacing with GGUF / llama.cpp models. Maid was forked off the now abandoned sherpa app and completely overhauled to now support GGUF models in addition to a more visually appealing UI.\nhttps://github.com/danemadsen/maid",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3652",
        "createdAt": "2023-10-17T12:29:13Z",
        "author": {
            "login": "danemadsen"
        }
    },
    {
        "title": "Efficient Streaming Language Models with Attention Sinks [",
        "bodyText": "New paper with example code claims huge context with minimal changes\nhttps://github.com/mit-han-lab/streaming-llm",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3443",
        "createdAt": "2023-10-02T18:53:59Z",
        "author": {
            "login": "logikstate"
        }
    },
    {
        "title": "Attention maps?",
        "bodyText": "I was wondering if it would be easy to extract attention maps with llama.cpp? I\u2019m not yet very familiar with the code, so I may be completely wrong, but couldn\u2018t the embedding output functionality be modified so that instead of the embedding layer it could take a parameter specifying which layer\u2019s activations to output?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3660",
        "createdAt": "2023-10-18T07:13:53Z",
        "author": {
            "login": "jubruckne"
        }
    },
    {
        "title": "TensorRT-LLM released",
        "bodyText": "https://www.tomshardware.com/news/nvidia-boosts-ai-performance-with-tensorrt\nCould TensorRT-LLM be useful for CUDA acceleration? @slaren @JohannesGaessler",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3658",
        "createdAt": "2023-10-17T19:41:03Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "Anybody running an Intel arc 750 or 770?",
        "bodyText": "If anyone is running these cards for their vram capacity , what is your experience like? How many iterations/ms are you getting through opencl offloading? Does it work with UI's like oobabooga and is it worth getting one?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1923",
        "createdAt": "2023-06-18T08:38:02Z",
        "author": {
            "login": "RahulVivekNair"
        }
    },
    {
        "title": "Add -v or --version flag",
        "bodyText": "Many CLI tools have --version flags. The issue template for this repo even says to run the following.\n$ python3 --version\n$ make --version\n$ g++ --version\n\nIt would be great if llama.cpp had a --version flag also.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3679",
        "createdAt": "2023-10-17T13:47:28Z",
        "author": {
            "login": "magnusviri"
        }
    },
    {
        "title": "Swifts C++ interoperability",
        "bodyText": "Bidirectional C++ interoperability has been added in Swift 5.9/Xcode 15.\nI would like to gather thoughts and points that speak for or against a conversion of ggml-metal.m to ggml-metal.swift.\nOr maybe someone already started to work on this or notice some limits or blockers?\n\nI would say on MacOS/iOS most if not all apps that use llama.cpp are/will be written in swift. By interopting directly with swift we could avoid the bridging and dependence on the objc run time and potentially simply the usage of the library.\nNot sure if true for this community but in objc devs are in decline and a swift to swift could ease up the usage and contribution from swift devs.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3549",
        "createdAt": "2023-10-08T15:32:28Z",
        "author": {
            "login": "Schaltfehler"
        }
    },
    {
        "title": "Support for xgen and codegen models from Salesforce",
        "bodyText": "Salesforce has released weights for xgen and well as codegen which shows some impressive human eval numbers and the best part is that they are based on the llama architecture and are pre trained on upto 8k context length. Any chance that it can be run by llama.cpp considering it is the same architecture?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2137",
        "createdAt": "2023-07-07T16:06:39Z",
        "author": {
            "login": "RahulVivekNair"
        }
    },
    {
        "title": "flutter for llamacpp",
        "bodyText": "I build a Android APK project for llamacpp with flutter.\ndevinzhang91/flutter_llamacpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3641",
        "createdAt": "2023-10-16T09:41:47Z",
        "author": {
            "login": "devinzhang91"
        }
    },
    {
        "title": "How to clear out llama_context state to be able to keep passing new inputs to LLaVa?",
        "bodyText": "Hi,\nI've wrapped llama.cpp's llava example in a web server so that I can send multiple requests without having to incur the overhead of starting up the app each time. However, I'm not sure how to reset the model state to pass in new requests. I currently free and re-create llama_context on each inference request but this is still a fairly heavyweight operation. Surely there is a way to clear out the context without having to reallocate all of the memory, load the Metal shader again (on macOS), etc.?\nI'm having trouble following the interactive llama code but will keep digging. In the meantime, any pointers or explanation of what might need to be done would be greatly appreciated!\nMy code is here: https://github.com/trzy/llava-cpp-server/blob/main/llava_server.cpp\nNote that run_llava_thread() calls perform_inference(), which has to create a new llama_context each time. This is what I'm hoping to streamline.\nThank you,\nBart",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3620",
        "createdAt": "2023-10-13T22:13:58Z",
        "author": {
            "login": "trzy"
        }
    },
    {
        "title": "Calls to decode slow down over time during parallel generation",
        "bodyText": "is this expected? I assume it is, but I just thought I'd ask. The effect doesn't seem noticeable with single sequence generation, but the total sequence lengths involved are also a lot smaller.\nFor example, running a 70B model with 8 parallel sequences, shared prompt of 1,230 tokens:\n-- Last decode[40]: 2.516, avg: 2.585\n-- Last decode[80]: 2.554, avg: 2.568\n-- Last decode[280]: 2.739, avg: 2.654\n-- Last decode[440]: 3.018, avg: 2.722\n-- Last decode[2320]: 5.907, avg: 4.029\n\nOutput from my hacked version of main in #2593 (AKA simple-inference) which dumps out the sequences + some stats every 40 calls to decode. The number in square brackets is the number of calls to decode so far, it's followed by the time the last decode took and finally the average so far.\nAt Last decode[40] we're dealing with (1230 + 40) * 8 = 10,160 tokens worth of context. At Last decode[440] that's up to 13,360, at [2320] it's 28,400 and the time each call takes has more than doubled. (The average is still outperforming single sequence generation though.) Even after generating only 440 tokens into each sequence (3,520 total) the time to decode increased by about half a second from ~2.5sec to ~3sec.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3629",
        "createdAt": "2023-10-15T06:55:33Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "LLaVA  LLaVA  LLaVA  LLaVA",
        "bodyText": "Finally LLaVA under llamacpp  !\nIf someone do not know LLaVA  is for picture recognition  and maybe for video in the furfure :D",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3600",
        "createdAt": "2023-10-12T16:52:18Z",
        "author": {
            "login": "mirek190"
        }
    },
    {
        "title": "Have you seen this vision model CogVLM is better than GPT-4 vision and open source",
        "bodyText": "How is that even passible?\nhttps://github.com/THUDM/CogVLM\nIs better than gpt4 vision.\nMaybe is worth to add it to llamacpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3619",
        "createdAt": "2023-10-13T21:37:22Z",
        "author": {
            "login": "mirek190"
        }
    },
    {
        "title": "Support AWQ please!",
        "bodyText": "Support AWQ please! is that a lot to ask for? sorry i dunno how difficult it is to use awq just know it's really fast (so i heard)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3590",
        "createdAt": "2023-10-11T23:21:00Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "why does ./main use vram even if i did -ngl 0 -ngld 0?",
        "bodyText": "how to make it only use cpu ram?\ni was running 2 instance.\n1 in pure vram (e.g. -ngl 35)\n1 in cpu (which uses vram)\nand i cant run another instance. would like to ran all instances\nanyone knows how to limit to running in cpu ram only?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3587",
        "createdAt": "2023-10-11T20:35:26Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Parallel prompts evaluation",
        "bodyText": "Description\nI currently tried to implement parallel processing of tokens inspired by baby-llama, i.e. I'm trying to change the dimension of tokens from [1 x N] to [M x N] to process several tokens in parallel at once. Here you can find my fork with the first experiment.\nNote: I'm using Apple M2 Max.\nProblem\nResults on CPU and GPU differ.\nFirstly I built my experimental app (input-batches-experiment) with -DLLAMA_METAL=OFF.\nIts results fully correspond to expectations:\n\nidentical prompts processed in parallel produce identical results, like:\n\nRUN: Parallel prompt processing, 3 parallel input prompts\n\tbatch = 0, seq = 0 (n_past = 0), token = 2, first logit = -0.819255\n\tbatch = 0, seq = 1 (n_past = 0), token = 3, first logit = -1.39025\n\tbatch = 0, seq = 2 (n_past = 0), token = 5, first logit = -0.233219\n\tbatch = 0, seq = 3 (n_past = 0), token = 7, first logit = -0.442564\n\t---\n\tbatch = 1, seq = 0 (n_past = 0), token = 2, first logit = -0.819255\n\tbatch = 1, seq = 1 (n_past = 0), token = 3, first logit = -1.39025\n\tbatch = 1, seq = 2 (n_past = 0), token = 5, first logit = -0.233219\n\tbatch = 1, seq = 3 (n_past = 0), token = 7, first logit = -0.442564\n\t---\n\tbatch = 2, seq = 0 (n_past = 0), token = 2, first logit = -0.819255\n\tbatch = 2, seq = 1 (n_past = 0), token = 3, first logit = -1.39025\n\tbatch = 2, seq = 2 (n_past = 0), token = 5, first logit = -0.233219\n\tbatch = 2, seq = 3 (n_past = 0), token = 7, first logit = -0.442564\n\t==========\n\tAll parallel input results are equal: TRUE\n\n\ndifferent prompts processed in parallel also produce correct results, like:\n\nRUN: Single prompt processing (llama_eval)\n\tbatch = 0, seq = 0 (n_past = 0), token = 2, first logit = -0.819255\n\tbatch = 0, seq = 1 (n_past = 0), token = 3, first logit = -1.39025\n\tbatch = 0, seq = 2 (n_past = 0), token = 5, first logit = -0.233219\n\tbatch = 0, seq = 3 (n_past = 0), token = 7, first logit = -0.442564\n\t==========\nRUN: Another Single prompt processing (llama_eval)\n\tbatch = 0, seq = 0 (n_past = 0), token = 4, first logit = -0.559572\n\tbatch = 0, seq = 1 (n_past = 0), token = 6, first logit = -8.01453\n\tbatch = 0, seq = 2 (n_past = 0), token = 10, first logit = -8.81363\n\tbatch = 0, seq = 3 (n_past = 0), token = 14, first logit = -5.98331\n\t==========\nRUN: Combined prompts (llama_eval_batch)\n\tbatch = 0, seq = 0 (n_past = 0), token = 2, first logit = -0.819255\n\tbatch = 0, seq = 1 (n_past = 0), token = 3, first logit = -1.39025\n\tbatch = 0, seq = 2 (n_past = 0), token = 5, first logit = -0.233219\n\tbatch = 0, seq = 3 (n_past = 0), token = 7, first logit = -0.442564\n\t---\n\tbatch = 1, seq = 0 (n_past = 0), token = 4, first logit = -0.559572\n\tbatch = 1, seq = 1 (n_past = 0), token = 6, first logit = -8.01453\n\tbatch = 1, seq = 2 (n_past = 0), token = 10, first logit = -8.81363\n\tbatch = 1, seq = 3 (n_past = 0), token = 14, first logit = -5.98331\n\t==========\n\tFirst batch results are equal:        TRUE\n\tSecond batch results are equal:       TRUE\n\tAll parallel input results are equal: TRUE\n\nBut when I built my experimental app with -DLLAMA_METAL=ON, I got totally inconsistent results (only the first batch is generated correctly):\n\nidentical prompts processed in parallel produce identical results, like:\n\nRUN: Parallel prompt processing, 3 parallel input prompts (llama_eval_batch)\n\tbatch = 0, seq = 0 (n_past = 0), token = 2, first logit = -0.810706\n\tbatch = 0, seq = 1 (n_past = 0), token = 3, first logit = -1.40239\n\tbatch = 0, seq = 2 (n_past = 0), token = 5, first logit = -0.253989\n\tbatch = 0, seq = 3 (n_past = 0), token = 7, first logit = -0.468385\n\t---\n\tbatch = 1, seq = 0 (n_past = 0), token = 2, first logit = -0.328787\n\tbatch = 1, seq = 1 (n_past = 0), token = 3, first logit = -0.293358\n\tbatch = 1, seq = 2 (n_past = 0), token = 5, first logit = -0.200654\n\tbatch = 1, seq = 3 (n_past = 0), token = 7, first logit = -1.68856\n\t---\n\tbatch = 2, seq = 0 (n_past = 0), token = 2, first logit = 0.382748\n\tbatch = 2, seq = 1 (n_past = 0), token = 3, first logit = -2.22278\n\tbatch = 2, seq = 2 (n_past = 0), token = 5, first logit = -1.58328\n\tbatch = 2, seq = 3 (n_past = 0), token = 7, first logit = -1.61095\n\t==========\n\tAll parallel input results are equal: FALSE\n\n\ndifferent prompts processed in parallel also produce incorrect results, like:\n\nRUN: Single prompt processing (llama_eval)\n\tbatch = 0, seq = 0 (n_past = 0), token = 2, first logit = -0.810706\n\tbatch = 0, seq = 1 (n_past = 0), token = 3, first logit = -1.40239\n\tbatch = 0, seq = 2 (n_past = 0), token = 5, first logit = -0.253989\n\tbatch = 0, seq = 3 (n_past = 0), token = 7, first logit = -0.468385\n\t==========\nRUN: Another Single prompt processing (llama_eval)\n\tbatch = 0, seq = 0 (n_past = 0), token = 4, first logit = -0.583736\n\tbatch = 0, seq = 1 (n_past = 0), token = 6, first logit = -8.08266\n\tbatch = 0, seq = 2 (n_past = 0), token = 10, first logit = -8.88991\n\tbatch = 0, seq = 3 (n_past = 0), token = 14, first logit = -6.03477\n\t==========\nRUN: Combined prompts (llama_eval_batch)\n\tbatch = 0, seq = 0 (n_past = 0), token = 2, first logit = -0.810706\n\tbatch = 0, seq = 1 (n_past = 0), token = 3, first logit = -1.40239\n\tbatch = 0, seq = 2 (n_past = 0), token = 5, first logit = -0.253989\n\tbatch = 0, seq = 3 (n_past = 0), token = 7, first logit = -0.468385\n\t---\n\tbatch = 1, seq = 0 (n_past = 0), token = 4, first logit = 0.185063\n\tbatch = 1, seq = 1 (n_past = 0), token = 6, first logit = -2.07769\n\tbatch = 1, seq = 2 (n_past = 0), token = 10, first logit = 0.382377\n\tbatch = 1, seq = 3 (n_past = 0), token = 14, first logit = -2.8728\n\t==========\n\tFirst batch results are equal:        TRUE\n\tSecond batch results are equal:       FALSE\n\tAll parallel input results are equal: FALSE\n\nQuestion\nLooks like I missed something in a Metal part initialisation or I need to make changes to that part of the project.\nDoes anyone have any ideas why the behavior on CPU and GPU is so different, and how this can be fixed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3363",
        "createdAt": "2023-09-27T13:18:51Z",
        "author": {
            "login": "Xarbirus"
        }
    },
    {
        "title": "Different behaviors between \u2018quantize_row_q4_0_reference\u2019 and \u2018quantize_row_q8_0_reference\u2019?",
        "bodyText": "I've observed that llama.cpp has a subtle difference between q4 and q8 quantization's implementations.\nquantize_row_q4_0_reference(q5_0 has similar behavior)\n\n  \n    \n      llama.cpp/ggml.c\n    \n    \n        Lines 912 to 922\n      in\n      9f6ede1\n    \n  \n  \n    \n\n        \n          \n           const float d  = max / -8; \n        \n\n        \n          \n           const float id = d ? 1.0f/d : 0.0f; \n        \n\n        \n          \n            \n        \n\n        \n          \n           y[i].d = GGML_FP32_TO_FP16(d); \n        \n\n        \n          \n            \n        \n\n        \n          \n           for (int j = 0; j < qk/2; ++j) { \n        \n\n        \n          \n               const float x0 = x[i*qk + 0    + j]*id; \n        \n\n        \n          \n               const float x1 = x[i*qk + qk/2 + j]*id; \n        \n\n        \n          \n            \n        \n\n        \n          \n               const uint8_t xi0 = MIN(15, (int8_t)(x0 + 8.5f)); \n        \n\n        \n          \n               const uint8_t xi1 = MIN(15, (int8_t)(x1 + 8.5f)); \n        \n    \n  \n\n\nAnd quantize_row_q8_0_reference\n\n  \n    \n      llama.cpp/ggml.c\n    \n    \n        Lines 1084 to 1092\n      in\n      9f6ede1\n    \n  \n  \n    \n\n        \n          \n           const float d = amax / ((1 << 7) - 1); \n        \n\n        \n          \n           const float id = d ? 1.0f/d : 0.0f; \n        \n\n        \n          \n            \n        \n\n        \n          \n           y[i].d = GGML_FP32_TO_FP16(d); \n        \n\n        \n          \n            \n        \n\n        \n          \n           for (int j = 0; j < QK8_0; ++j) { \n        \n\n        \n          \n               const float x0 = x[i*QK8_0 + j]*id; \n        \n\n        \n          \n            \n        \n\n        \n          \n               y[i].qs[j] = roundf(x0); \n        \n    \n  \n\n\nFirstly, they have different calculations for d:\nconst float d  = max / -8;\nconst float d = amax / ((1 << 7) - 1);\n\u2460 The former method make max and d have opposite sign, while the latter ensures  d is a positive value. \u2461 Furthermore, The latter method incorporates a -1, which causes the divider to be 127 but not 128.\nSecondly, q_4 has:\nconst uint8_t xi0 = MIN(15, (int8_t)(x0 + 8.5f));\n\u2462 which does not use roundf() as q8 does:\ny[i].qs[j] = roundf(x0);\nSpecifically, I have a guess about \u2461. I think absence of -1 in quantize_row_q4_0_reference make the value mapped into [-8, 8], which is a bigger range than [-7, 7], and this may be helpful to obtain a better accuracy.\nBut for \u2461 and \u2462, I have no idea. I would be grateful for some guidance! : )",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3577",
        "createdAt": "2023-10-11T07:44:25Z",
        "author": {
            "login": "rikoras"
        }
    },
    {
        "title": "Synthetic dataset generation - Brief howto built on top of llama.cpp",
        "bodyText": "I\u2019am currently exploring finetuning smaller models for classification / fill-mask.\nI wrote a quick intro on how to produce synthetic dataset in a few lines of python.\nIt covers:\n\nthe raw generation\nthe automatic labelling/classification\n\nIf you have performance tips for this kind of tasks I\u2019m all ears \ud83d\udc40. Now I have to look on how to implement distilbert using ggml \ud83d\ude31",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3568",
        "createdAt": "2023-10-10T12:27:56Z",
        "author": {
            "login": "paschembri"
        }
    },
    {
        "title": "What text encoding does `llama_token_to_piece()` return? UTF-8?",
        "bodyText": "The sentencepiece README states that it normalizes via NFKC. The tokenizer.json files in e.g. jondurbin_airoboros-l2-70b-gpt4-1.4.1 is in UTF-8. I'm not sure how to inspect the tokenizer.model file.\nFrom the perspective of somebody just using llama_token_to_piece(), how do I know what format of text I am getting back from llama.cpp? Would this be dependent on the model's vocab? I have been assuming UTF-8 and it has been working.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3114",
        "createdAt": "2023-09-10T23:17:04Z",
        "author": {
            "login": "crasm"
        }
    },
    {
        "title": "Lossless Large Language Model Acceleration via Self-Speculative Decoding",
        "bodyText": "Link: https://arxiv.org/abs/2309.08168\nBasically this is like the existing speculative decoding stuff except it doesn't use a separate speculation model but instead runs only some of the main model's layers to generate the draft. The big advantage is the \"draft\" model's output will definitely be in sync with the main model and you don't need to load in a whole separate model: the existing model can be reused.\nUnfortunately, they don't really include specific information about which layers to skip are optimal so that's something we'd have to find out ourselves. The first step to that might be extending the inference API to allow passing a list of the layers to run and an example that could run perplexity on various permutations of layers.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3435",
        "createdAt": "2023-10-02T06:20:26Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "What is the meaning of the hist when doing the quantize?",
        "bodyText": "the terminal like this:\nlama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllama_model_quantize_internal: meta size = 1248800 bytes\n[   1/ 291]                    token_embd.weight - [ 4096, 55296,     1,     1], type =    f16, quantizing to q4_0 .. size =   432.00 MB ->   121.50 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021\n[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.017 0.028 0.042 0.058 0.076 0.092 0.106 0.114 0.107 0.093 0.077 0.060 0.043 0.028 0.022\n[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.032 0.049 0.071 0.098 0.126 0.143 0.126 0.098 0.071 0.049 0.032 0.020 0.016\n[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 ..\nI have a question that what is the meaning about the hist? I can't understand it when I read the code. Can you help me\uff1f",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3558",
        "createdAt": "2023-10-09T08:41:32Z",
        "author": {
            "login": "Tr-buaa"
        }
    },
    {
        "title": "How to write a simple program in easy way?",
        "bodyText": "Hi,\nHow can i write a simple C++ program in the most easy way that use llama.cpp to use a local model and get their prompt in sync way?\nExample of pseudo-code:\ninitialize_llama_cpp()\n\nstring response = get_prompt_for(\"./my-model.bin\", \"what is the biggest planet?\") // here it will wait until full response finished be produced\n\nprint(response)\n\nThanks for any help.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3563",
        "createdAt": "2023-10-09T22:49:48Z",
        "author": {
            "login": "paulocoutinhox"
        }
    },
    {
        "title": "A model for Vision-language Understanding with Advanced Large Language Models",
        "bodyText": "This model: https://github.com/Vision-CAIR/MiniGPT-4\nseems a good candidate for being implemented in ggml, as it mimics the capabilities of GPT-4 in terms of image interpretation.\nIt relies on BLIP-2 as visual encoder, which I cannot tell whether has an structure easily implemented in ggml.\nThank you for all the great work!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1050",
        "createdAt": "2023-04-19T00:42:50Z",
        "author": {
            "login": "Martin-Laclaustra"
        }
    },
    {
        "title": "Move llama grammar parser into llama.cpp",
        "bodyText": "can we please move the grammar parser by @ejones  into llama.cpp and expose it with a C compatible API?\nThere is no way to use this feature currently when importing libllama.so, because it is behind a cpp namespace.\nmaybe we can change / add a new API like this:\n    LLAMA_API struct parse_state * llama_parse_grammar(const char * src);\n\n    LLAMA_API struct llama_grammar * llama_grammar_init_from_parsed(struct parse_state * parsed_grammar);\nwhat do you think?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3321",
        "createdAt": "2023-09-23T20:01:04Z",
        "author": {
            "login": "spirobel"
        }
    },
    {
        "title": "Universal (generic) argument/parameter interface",
        "bodyText": "Prerequisites\nPlease answer the following questions for yourself before submitting an issue.\n\n I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n I carefully followed the README.md.\n I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n I reviewed the Discussions, and have a new bug or useful enhancement to share.\n\nExpected Behavior\nPlease provide a detailed written description of what you were trying to do, and what you expected llama.cpp to do.\nCurrent Behavior\nPlease provide a detailed written description of what llama.cpp did, instead.\nEnvironment and Context\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\nPhysical (or virtual) hardware you are using, e.g. for Linux:\n\n$ lscpu\n\nOperating System, e.g. for Linux:\n\n$ uname -a\n\nSDK version, e.g. for Linux:\n\n$ python3 --version\n$ make --version\n$ g++ --version\n\nFailure Information (for bugs)\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\nSteps to Reproduce\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\nstep 1\nstep 2\nstep 3\netc.\n\nFailure Logs\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\nAlso, please try to avoid using screenshots if at all possible. Instead, copy/paste the console output and use Github's markdown to cleanly format your logs for easy readability.\nExample environment info:\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n\nExample run with the Linux command perf\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized\n             13509      context-switches          #    3.714 /sec\n              2436      cpu-migrations            #    0.670 /sec\n          10476679      page-faults               #    2.881 K/sec\n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle\n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3514",
        "createdAt": "2023-10-04T16:55:56Z",
        "author": {
            "login": "pudepiedj"
        }
    },
    {
        "title": "w64devkit virus or not?",
        "bodyText": "i want to compile code on Windows11,\nwhen i use w64devkit downloaded from https://github.com/skeeto/w64devkit/releases\nit shows\n\nso i cannot finish compiling llama.cpp\n\nwhat can i do ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3464",
        "createdAt": "2023-10-03T23:04:48Z",
        "author": {
            "login": "zhanghui-china"
        }
    },
    {
        "title": "Questions about kv_cache and Group Query Attention in Llama-7B Model",
        "bodyText": "I am tracking the inference procedure of the Llama-7B model. I found following facts:\n\n\nWith 512 context length, kv cache size = 256MB, which can be calculated by:\n$$n_{mem} = {n_{layer}(32)}\\times{n_{ctx}(512)}=16384$$\n$$n_{elements} = n_{mem}\\times n_{embd}(4096) = 67108864$$\n$$6410884 * 2\\text{(k and v)} * 2(\\text{2 bytes for fp16}) = 256MB$$\n\n\nAccording to\nhttps://github.com/ggerganov/llama.cpp/blob/019ba1dcd0c7775a5ac0f7442634a330eb0173cc/llama.cpp#L2784-L2813\nEach layer has a Kcur [N, n_heads(32), n_embd_head(128)] and a Vcur [N, nheads*n_embd_head(4096)].\n\n\n\n\nIn my understanding, Llama leverages GQA to compress the size of kv_cache. But both of the two facts denote that every head has its speciallized kv_cache which is not shared with other heads. Have I misunderstood something?\nFurthermore, I noticed that Kcur and Vcur have distinct shapes. I wonder why only Kcur is splited into N*32*128, while Vcur is recorded as 4096*N.\nFor \n  \n    \n      llama.cpp/llama.cpp\n    \n    \n        Lines 2815 to 2823\n      in\n      019ba1d\n    \n  \n  \n    \n\n        \n          \n           struct ggml_tensor * k = ggml_view_1d(ctx0, kv_self.k, n_tokens*n_embd_gqa, (ggml_element_size(kv_self.k)*n_embd_gqa)*(il*n_ctx + kv_head)); \n        \n\n        \n          \n           offload_func_kq(k); \n        \n\n        \n          \n           ggml_set_name(k, \"k\"); \n        \n\n        \n          \n            \n        \n\n        \n          \n           struct ggml_tensor * v = ggml_view_2d(ctx0, kv_self.v, n_tokens, n_embd_gqa, \n        \n\n        \n          \n                   (   n_ctx)*ggml_element_size(kv_self.v), \n        \n\n        \n          \n                   (il*n_ctx)*ggml_element_size(kv_self.v)*n_embd_gqa + kv_head*ggml_element_size(kv_self.v)); \n        \n\n        \n          \n           offload_func_v(v); \n        \n\n        \n          \n           ggml_set_name(v, \"v\"); \n        \n    \n  \n\n\nI also noticed k and v are handled differently. I suspect this might be related to the underlying implementation of tensor storage? I am not familiar with ggml, so this part make me confused.\nI would really appreciate an answer!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3485",
        "createdAt": "2023-10-05T07:16:23Z",
        "author": {
            "login": "rikoras"
        }
    },
    {
        "title": "Choose best answer based on perplexity with parallel generation?",
        "bodyText": "I have a practical use case where I want my LLM to generate multiple responses to the same prompt and chose the best one.\nNow, given the latest developments, is it possible to do all this while approximately keeping the same generation time as if I only make one sampling call?\nI'm thinking of parallel generation of 3-4 answers and calculating the perplexity of all of them without making any other extra call. Are there any other ideas?\nIf it's possible, is there an example that already does this? I'm convinced that if it's possible, other people are also highly interested.\nUPDATE: I have a very long sys prompt and I already tried different sampling strategies (ex: mirostat) and I couldn't manage to find a better combo than just using default configs. For example, mirostat tries really hard to follow the beginning of the sys prompt, but it ignores later sys text where I specifically say not to generate the type of responses it generates. For some reason, if I just run it with default configs, it \"listens\" to the whole sys prompt and is usually better than changing top_p, top_k, tfs etc with values that should theoretically work better.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3533",
        "createdAt": "2023-10-07T19:01:49Z",
        "author": {
            "login": "Mihaiii"
        }
    },
    {
        "title": "LM-Infinite: Simple On-the-Fly Length Generalization for LLM",
        "bodyText": "https://www.reddit.com/r/LocalLLaMA/comments/165zpn9/r_lminfinite_simple_onthefly_length/\nhttps://arxiv.org/pdf/2308.16137.pdf\n\nAny chance this will be implemented in llama.cpp? As far as I understand, nothing in theory prevents this from being done.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2936",
        "createdAt": "2023-08-31T15:43:10Z",
        "author": {
            "login": "Jipok"
        }
    },
    {
        "title": "Could not load Llama model",
        "bodyText": "Hi, I've been using the GGML model, specifically the ggml-gpt4all-j-v1.3-groovy version, and it was working perfectly. However, today, when I attempted to use it again, I encountered an issue. It displayed an error message: 'Could not load Llama model from path: ggml-gpt4all-j-v1.3-groovy.bin.' I tried to research this problem and came across a website that mentioned that Llama.cpp no longer supports GGML models. I'm wondering if this information is accurate.\nI also attempted an alternative solution using the GGUF model, but the results were unsatisfactory. I'm hoping to find a solution to this issue, and I would greatly appreciate any assistance or guidance. Thank you:)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3505",
        "createdAt": "2023-10-06T14:31:49Z",
        "author": {
            "login": "madeganesh22"
        }
    },
    {
        "title": "Nucleus22B pre-release",
        "bodyText": "https://huggingface.co/NucleusAI/nucleus-22B-token-500B/tree/main\nThis model is working and produces legible output when quantized to q4_k_m!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3517",
        "createdAt": "2023-10-06T23:27:31Z",
        "author": {
            "login": "BarfingLemurs"
        }
    },
    {
        "title": "Llama.cpp server stopping",
        "bodyText": "I'm trying to stop ./server inference on the middle of the process without any good results so far.\nI'm stuck here. Can someone explain me better how can I \"stop\" the the process when requesting a streamed inference on the server?\nI'm using javascript's Fetch.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3513",
        "createdAt": "2023-10-05T12:04:04Z",
        "author": {
            "login": "appvoid"
        }
    },
    {
        "title": "Total Size Of The App",
        "bodyText": "I am thinking about building an app in ios. That will use lamma model so what will be the size of the model when it runs on the phone ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3512",
        "createdAt": "2023-10-06T18:04:56Z",
        "author": {
            "login": "zain-ul-abedien"
        }
    },
    {
        "title": "Is there any way to simulate interactive mode behavior?",
        "bodyText": "Dear community!\nI have just recently started to get acquainted with llama.cpp and it is fantastic!\nBut there is one question that I have no way of understanding.\nI am running llama.cpp built for cpu only on my local laptop. Accordingly, it takes a huge amount of time to run the inference with a given prompt. This behavior is quite understandable to me, since it is cpu built and I don't have the strongest laptop. But when I run interactive mode with prompt templates from the repo to simulate gpt-like chat, a kind of \"cold start\" takes just as long, but when further chatting in interactive mode, responses are generated just on the fly with almost no delay.\nHence I have two questions\n\nHow is this implemented? What is happening under the hood at this moment?\nHow to achieve this behavior without using interactive mode?\n\nI was trying to figure this out by reading the documentation, and I thought it could be achieved by using \"--prompt-cache\" and \"--prompt-cache-all\". But as far as I understand, \"--prompt-cache-all\" will not be useful in chat simulation mode (the functionality I want to achieve), because after adding a custom prompt to the cached one, it will still re-generate it again, so I won't get as fast as in interactive mode.\nI will be glad to get any help!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3500",
        "createdAt": "2023-10-06T12:48:43Z",
        "author": {
            "login": "Bibarius"
        }
    },
    {
        "title": "Jeremy Howard talks about llama.cpp",
        "bodyText": "I just wanted to bring this to the attention of the community. In the one-and-a-half -hour long lecture, Jeremy Howard, creator of ULMFiT and one of the most reputable figures of language modeling, talks about llama.cpp and demonstrate its usage: A hackers' guide to language models.\nApart from this demonstration, I think that it might be very helpful for those who have a background in software engineering but not in ML, so worth sharing.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3497",
        "createdAt": "2023-10-06T07:00:06Z",
        "author": {
            "login": "monatis"
        }
    },
    {
        "title": "Why do all Q dot product methods use Q8 for non-model tensors?",
        "bodyText": "Looking at ggml code:\nhttps://github.com/ggerganov/llama.cpp/blob/master/ggml.c#L1675\nWhy do all the quantized dot products encode working tensors (non-model) into q8 first?\nIsn't this more work and memory bandwidth than just sending the F16 or F32 directly?\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3477",
        "createdAt": "2023-10-04T17:29:18Z",
        "author": {
            "login": "tjake"
        }
    },
    {
        "title": "Max Context Length of a model",
        "bodyText": "Is there any way I can find the maximum context length of a local LLM?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3474",
        "createdAt": "2023-10-04T13:59:06Z",
        "author": {
            "login": "rounak610"
        }
    },
    {
        "title": "Special token support",
        "bodyText": "A few days ago, Open Orca released a new model called Mistral-7B-Openorca. This uses the ChatML format which has <|im_end|> as a special EOS token that is currently not recognized by llama.cpp\n\nThis is the format in question.\nhttps://github.com/openai/openai-python/blob/main/chatml.md\nmodel\nhttps://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/commit/17572416df27482d71dda9ea6bdea1733d8cee5d",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3466",
        "createdAt": "2023-10-04T06:15:24Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "why?",
        "bodyText": "por qual motivo, decidiram criar uma lib de tensores em vez de usar as lib C pytorch?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3449",
        "createdAt": "2023-10-03T00:38:11Z",
        "author": {
            "login": "alph4b3th"
        }
    },
    {
        "title": "Would appreciate tips on understanding the project structure and prerequisites for reading the source code!",
        "bodyText": "I am very excited about this amazing project, which allows me to try many innovations, and I want to thank Georgi and all contributors.\nI have already deployed llama.cpp on some devices and have run some examples. It works well, but I am not satisfied with only running some demos. I want to study the project comprehensively. Furthermore, I want to try some optimizations for specific applications (if there is an idea).\nHere are the skills I have got:\n\n\nC/C++ programming. I can do some embedded programming with C/C++, and I have taken a few courses on computer system and architecture.\n\n\nBasic understanding of Llama. I have read some related papers including Attention is all you need and Llama.int8, I know how the Llama model works, and I can finish a model implementation with Pytorch.\n\n\nAbility to read manuals. English is not my native language (sorry if you feel frustrated while reading this \ud83d\ude14), but I can try my best.\n\n\nAnd here are my limitations:\n\n\nI have trouble getting started on a relatively large project. I have worked on some student projects, but when it comes to a real-world project, I feel lost amid plenty of details (makefiles, utils, and so on). I want to have an overview of llama.cpp and I may need some help.\n\n\nI am not familiar with parallel programming libraries such as MPI and OpenMP.\n\n\nI don't know much about the ggml repo.\n\n\nI would really appreciate some advice, thanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3331",
        "createdAt": "2023-09-25T11:31:15Z",
        "author": {
            "login": "rikoras"
        }
    },
    {
        "title": "Running single model on multiple cpu servers for faster inference?",
        "bodyText": "I have access to multiple linux servers. Is there anyway I can run one model and these servers and speed up the generation process? [Not referring to load distribution in terms of call but distribution of layers across servers may help]\nHas anyone tried anything on this?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3452",
        "createdAt": "2023-10-03T10:40:39Z",
        "author": {
            "login": "sidharthiimc"
        }
    },
    {
        "title": "kv cache is a continuous block of memory, so how are the different tokens distributed in memory?",
        "bodyText": "",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3372",
        "createdAt": "2023-09-27T11:51:51Z",
        "author": {
            "login": "yancaoweidaode"
        }
    },
    {
        "title": "Doubts regarding 3/4 bit quantization and inference",
        "bodyText": "Hi Team,\nThank you for your great contribution.\nI had doubts about the 3/4 bit quantization inference, would be great if you could help me understand.\nMy doubts are:\n1.) Whether the inference for 3/4 bit models actually happens in 3/4 bit or it is casted to FP16 during inference on CPU? I mean do you have a CPU kernel to run the inference on 3/4 bits?\n2.) Which quantization technique is being used?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3429",
        "createdAt": "2023-10-01T19:44:50Z",
        "author": {
            "login": "Darshvino"
        }
    },
    {
        "title": "Convert GPTQ model to GGUF",
        "bodyText": "Hi! I was wondering if it's possible to convert a 4bit GPTQ model (converted via AutoGPTQ) to the GGUF format? Would be great if I could have some pointers to how this can be done.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3353",
        "createdAt": "2023-09-27T06:39:19Z",
        "author": {
            "login": "Nilabhra"
        }
    },
    {
        "title": "book, \u00bftraining, fine-tuning, large model?",
        "bodyText": "what is best, and what is the difference? training, fine-tuning or use large model.\nI want llama read a book and make answered to my questions.\n\u00bfwhat order to use?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3439",
        "createdAt": "2023-09-08T22:37:31Z",
        "author": {
            "login": "SilvaRaulEnrique"
        }
    },
    {
        "title": "read books fast, and response in spanish",
        "bodyText": "Exist another form for to utilizate any like privateGPT for read books and the response it's in another laguaje like spanish?, and use CUDA?\nprivateGPT",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3434",
        "createdAt": "2023-10-02T00:39:16Z",
        "author": {
            "login": "SilvaRaulEnrique"
        }
    },
    {
        "title": "Queries Regarding Server API & Loading Previous Conversations",
        "bodyText": "Hi all,\nFirstly, I'd like to commend the team for the exceptional work on this project. I've been exploring its features and have a couple of questions I hope someone can assist me with:\nServer API Stop Command: In the web interface at http://localhost:8080, there's an option to stop a certain function. Is this executed via a request to the API or does it terminate a process directly? Can someone guide me on how to initiate this via the server API?\n\nLoading Past Conversations: I'm interested in understanding how to load a previous interaction I've had with the chatbot. For instance, if I have a conversation about cars and save it, how can I later reload this conversation? Is this achieved through the prompt or is there a specific endpoint I should be aware of? My aim is to make the chatbot recall previous discussions.\n\nThank you in advance for your guidance and support.\nBest,\nTom",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3415",
        "createdAt": "2023-09-30T13:03:10Z",
        "author": {
            "login": "anianios"
        }
    },
    {
        "title": "Architecture: what if I want to optimize for llama.cpp?",
        "bodyText": "We have our model converted to gguf with quantization, shout out to @teleprint-me and @ds5t5.\nBut it's still slow, our problem is the prompt. The speed is about 500 tps for prefill (Apple M1), which is way to slow for practical use. For fill-in-the-middle code completion, the user will have to wait 4 seconds for a typical 2000 tokens context.\nWe train our own models, so the question is: what if we change the architecture? What is the bottleneck for prefill? How do we make it 5-10x faster, besides making the network smaller?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3395",
        "createdAt": "2023-09-29T05:59:03Z",
        "author": {
            "login": "olegklimov"
        }
    },
    {
        "title": "session load always finish with: \"error loading session file: bad allocation\"",
        "bodyText": "Just trying to load/save big prompts, any attempt to load the saved file ends with \"error loading session file: bad allocation\".\nWindows x64, compiled with VS v141\nAny help is welcome",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3397",
        "createdAt": "2023-09-29T11:46:50Z",
        "author": {
            "login": "jluisreymejias"
        }
    },
    {
        "title": "Mistral-7B-v0.1-GGUF",
        "bodyText": "will i be able to use mistral model in llama.cpp module?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3394",
        "createdAt": "2023-09-29T08:29:22Z",
        "author": {
            "login": "adeelhasan19"
        }
    },
    {
        "title": "How to use llama.cpp to generate answers in NLI",
        "bodyText": "I am looking to use llama.cpp to generate hypothesis from premise (NLI).\nSay I have a premise \"man is sitting on the couch\". I want the model to generate a neutral hypothesis like \"the keyboard is in english\". Or a contradictory hypothesis: \"the man is standing\". I want it to do that for 10k examples.\nI have llama.cpp working in interactive mode using:\n./main -m ./models/llama-2-7b-chat.gguf.bin \\                 \n  --color \\       \n  --ctx_size 2048 \\\n  -n -1 \\\n  -ins -b 256 \\\n  --top_k 10000 \\\n  --temp 0.2 \\\n  --repeat_penalty 1.1 \\\n  -t 8\n\nBut I don't think that's the best way for this task.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3379",
        "createdAt": "2023-09-28T13:33:06Z",
        "author": {
            "login": "ClaudiuCreanga"
        }
    },
    {
        "title": "how to begin with windows release pack",
        "bodyText": "Prerequisites\njust download release win avx2 pack, and download int4 llama2 7b model via net.\ngot one question about how to run llama.cpp with model i downloaded.\ndo i need compile all project like i did in ubuntu, or i just need download visual studio( have no idea about)\nExpected Behavior\nis there any way i could just enter ./llama.cpp ./model_path  in cmd then i can begin to chat with model.\nCurrent Behavior\njust extract whole pack file, use this for my uncle who has no computer science context, so better without compile, thx\ni only know how to use it in ubuntu, if you know about windows release pack usage, please comment, thank you, sincerely.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3376",
        "createdAt": "2023-09-28T05:29:47Z",
        "author": {
            "login": "dengzheng-cloud"
        }
    },
    {
        "title": "Is there a commandline option to select graphics card?",
        "bodyText": "Sorry if this sort of question isn't allowed here!\nI'm getting a decent gaming mini-pc with a dgpu and an igpu. Igpu might be slow but offers the possibility of much more ram for things like quantization. Can you select a graphics card between two options for things like the quantize module?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3367",
        "createdAt": "2023-09-27T06:12:17Z",
        "author": {
            "login": "Drael64"
        }
    },
    {
        "title": "Debug in Xcode, profile in Instruments",
        "bodyText": "I've looked, but have not found any notes for debugging this code in Xcode, or profiling this code in Instruments.\nAre there some notes for that?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3336",
        "createdAt": "2023-09-26T02:11:36Z",
        "author": {
            "login": "douglasdew"
        }
    },
    {
        "title": "FreeChat: swift macOS llama.cpp app",
        "bodyText": "Hey ya'll, just wanted to share the little mac app I made to wrap llama.cpp's server.\nIt's called FreeChat. The idea is to make an app you can send to someone who knows nothing about LLMs and have them up and running a local model as soon as their download completes. At the same time, I want it to be my daily driver for testing new models so I'll be adding more advanced parameter knobs soon.\nHappy for any and all feedback. I hope this is useful for others.\nhttps://github.com/psugihara/FreeChat",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3369",
        "createdAt": "2023-09-27T17:30:22Z",
        "author": {
            "login": "psugihara"
        }
    },
    {
        "title": "Get probability over vocabulary for each token",
        "bodyText": "Expected Behavior\nI want to get probability over vocabulary for each token while running embedding or normal prediction.\nCurrent Behavior\nNo mention of probability over vocabulary mentioned in the project\nEnvironment and Context\nUbuntu 22.04",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3366",
        "createdAt": "2023-09-27T06:51:53Z",
        "author": {
            "login": "djmMax"
        }
    },
    {
        "title": "Does any of the converted Llama files allow for multiple gpu support?",
        "bodyText": "I have a rig with 8 a4000s and I am wondering if it even possible to run the 70b version on that. I have the 70b gguf model but it seems the model can't run across multiple gpu's. I new to this stuff no flame. Thanks for any help",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3345",
        "createdAt": "2023-09-26T16:54:33Z",
        "author": {
            "login": "yopzey"
        }
    },
    {
        "title": "No quantization for activations?",
        "bodyText": "There are a lot of quantization options for weights, I wonder whether there is a quantization process for activations?\nWhen I add printf in ggml_compute_forward_mul_mat function, it shows the src0 tensor has data type of either 1, 2, or 14 (meaning fp16, q4_0, and q6_k respectively), while src1 always has data type of 0, which stands for fp32. So I infer that llama.cpp never quantizes activations? All tokens and activations are computed using floating points?\n    const enum ggml_type type = src0->type;\n    const enum ggml_type type1 = src1->type;\n    printf(\"type = %d, type1 = %d\\n\", type, type1);  // shows type = 1/2/14, type1 = 0",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3349",
        "createdAt": "2023-09-27T00:29:05Z",
        "author": {
            "login": "Z-KN"
        }
    },
    {
        "title": "Does the grammar fully enforce constraints or just increases probabilities?",
        "bodyText": "I have a prompt and a grammar that basically constraints the LLM to chose from a list of options.\nToday I noticed that it doesn't work quite as it should in a sense that from time to time if outputs text that is not in the list from the grammar. I don't have clear steps to replicate (this is why I didn't opened an issue).\nI don't remember this happening ~2 weeks ago (worked as expected meaning is was always reliable).\nI do sampling on GPU (I mentioned this because there are multiple GPU related commits in the last 2 weeks).\nDoes anyone also have this issue?\nIs it a regression or grammar works by just increasing probabilities without fully enforcing the constraints?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3343",
        "createdAt": "2023-09-26T16:17:01Z",
        "author": {
            "login": "Mihaiii"
        }
    },
    {
        "title": "GPU layers for GGML/GGUF models on Apple GPU",
        "bodyText": "Hello! I recently was informed that there's a limit of how much RAM a Mac's GPU can use in comparison to the total available RAM.\nRumors were that limit is either 66% or 75%, so I ran some tests and found out they were actually accurate:\n\nIs the llama.cpp team aware of this? And if so, are there any official numbers on those limits? Any technical explanation behind them?\nWould this info change the fact that offloading to CPU or putting effort towards combining CPU & GPU would be useful? (CC #3083)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3316",
        "createdAt": "2023-09-23T12:48:29Z",
        "author": {
            "login": "Lyrcaxis"
        }
    },
    {
        "title": "My enhanced index.html for the server with more features (regen, edit, history etc.)",
        "bodyText": "Code\nhttps://gist.github.com/Equim-chan/1c46aca63990e1800c4a3f002b1e3aee\nDescription\nI initially wanted to submit a PR, however my modifications grew over time and in the end became a bit too much to merge. Despite this, I still want to share it for others who might find it useful or interesting.\n\nDemonstrations\nMenu\n\n  \n    \n    \n\n    1.mov\n    \n  \n\n  \n\n  \n\n\nEditing\n\n  \n    \n    \n\n    2.mov\n    \n  \n\n  \n\n  \n\n\nHistory\n\n  \n    \n    \n\n    3.mov",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2777",
        "createdAt": "2023-08-25T04:42:32Z",
        "author": {
            "login": "Equim-chan"
        }
    },
    {
        "title": "QT server frontend",
        "bodyText": "I just wrote over the past couple days a QT frontend to Llama.cpp's server! This is the first desktop GUI I have ever written and mainly wrote it for my uses but I figure others out there could use this too if you have alot of models you like to run with pure Llama.cpp as from my experience pure Llama.cpp is way faster then Koboldcpp as that runs Llama.cpp directly in python as far as I can tell and you can feel the python overhead and most certainly faster then Llama-cpp-python. I did write the GUI in pyqt5 but it uses the compiled server binary and it runs on its own process so not much overhead regarding python and it shows for me, I get about twice as many tokens per second compared to Koboldcpp and llama-cpp-python.\nI do have few planned things I going to add over the new few days to weeks as I learn how to.\nYou can check it out at my github repo here llama.cpp-qt Any and all contributions are welcomed if anyone finds it useful.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3324",
        "createdAt": "2023-09-24T09:10:25Z",
        "author": {
            "login": "TohurTV"
        }
    },
    {
        "title": "Guide: WSL + cuda 11.6",
        "bodyText": ".run files\n#to match max compute capability\nnano Makefile (wsl)\n    NVCCFLAGS += -arch=native\n    Change it to specify the correct architecture for your GPU. For a GPU with Compute Capability 5.2, you should replace it with:\n\n    makefile\n    Copy code\n    NVCCFLAGS += -arch=sm_52\n    \nmodified makefile\n#https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/\n\napt-get remove --purge '^nvidia-.*' '^cuda-.*'\n sudo apt-get autoremove\nsudo apt-get autoclean\nsudo find / \\( -path /home -o -path /mnt -o -path /usr/local/lib/python3.10 \\) -prune -o ! -user root \\( -name '*nvidia*' -o -name '*cuda*' \\) -print\n\ncat /var/log/cuda-uninstaller.log\n\nmodprobe -r nvidia\n\ndpkg -l | grep -i nvidia | awk '{print $2}' | xargs sudo apt-get --purge remove -y\n\nsudo rm -rf /etc/systemd/system/nvidia*\nsudo rm -rf /etc/systemd/system/display-manager.service.d\nsudo rm -rf /etc/modprobe.d/nvidia*\nsudo rm -rf /etc/modules-load.d/nvidia*\nsudo rm -rf /etc/kernel/postinst.d/nvidia*\nsudo rm -rf /usr/lib/nvidia*\nsudo rm -rf /usr/share/doc/nvidia*\nsudo rm -rf /var/lib/dkms/nvidia*\nsudo rm -rf /var/log/nvidia*\nsudo rm -rf /usr/local/cuda*\n\nFailed to initialize NVML: GPU access blocked by the operating system\nFailed to properly shut down NVML: GPU access blocked by the operating system\n\nReinstall windows driver\n=D\n\ncd /data/llama.cpp\nmake LLAMA_CUBLAS=1\n\npython -m pip install --force-reinstall --no-deps llama-cpp-python llama-cpp-python[server] --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX/cu122 pydantic pydantic_settings fastapi\n\npython3 -m llama_cpp.server --model /root/text-gen-install/text-generation-webui/models/llama-2-7b-chat.Q2_K.gguf --n_gpu_layers 24\n\nI can run a server, but text-generation-webui still doesn't work with this.  Conflicting pydantic settings and llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3323",
        "createdAt": "2023-09-23T22:41:21Z",
        "author": {
            "login": "thistleknot"
        }
    },
    {
        "title": "Weight Layer subsystem",
        "bodyText": "I don't think there is going to be much interest in this idea, but thought I would put it out there for any feedback.\nI've been working on a weight layer block system. The idea being that lately a number of merged models have had different versions with either different order of the weight layers from the merged parent models or different blocks of layers from each parent. So I was thinking about a system that would allow a single model file that contained more layers than is going to normally be used at a time. But with definition files that defined what layers and how they were used from that larger model file.\nI wrote a (rough work in progress) overview of the idea here: https://pastebin.com/6VT3SUy9\nThat also includes a link to the (extremely rough) work in progress code. That code is very much just a proof of concept and needs a lot of work. However with not knowing if there is any interest or use in this idea and with the Parallel decoding + continuous batching support comits coming that I believe will break what I've done. I'm not sure if it is worth continuing with.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3318",
        "createdAt": "2023-09-23T16:59:08Z",
        "author": {
            "login": "mwt11a"
        }
    },
    {
        "title": "Speculative sampling on Mac Pro with M2 Ultra?",
        "bodyText": "Hello everyone,\nI'm currently leaning towards purchasing the Mac Studio M2 with 192GB RAM, but I've also been considering the Mac Pro M2 with the same memory configuration.\nFrom my research, it seems there's minimal difference in computational power between these two devices. However, what has caught my attention is the potential of using the high-speed PCIe SSD to expedite model loading times. Specifically, PCIe SSDs can achieve speeds up to 25,000 MB/s, which is vastly superior to the 5,000 MB/s of the Mac's internal SSD. To put this into perspective, a 100GB model could be loaded in roughly 4 seconds on the PCIe SSD, compared to the 20 seconds it would take on the Mac SSD. That's a 5x speed difference.\nThat said, I also understand that once a model is loaded into memory, these loading times become irrelevant, as there's no need to reload the model.\nI'm at a crossroads trying to decide between the two. Has anyone here had experience using quantzed models on the Mac Pro, and if so, could you share your insights?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3226",
        "createdAt": "2023-09-17T09:20:06Z",
        "author": {
            "login": "gileneusz"
        }
    },
    {
        "title": "Convert OpenAssistant/oasst-sft-6-llama-30b-xor",
        "bodyText": "I'm trying to convert to GGUF the new OpenAssistant/oasst-sft-6-llama-30b-xor\npython3 convert.py /Downloads/phi-1_5\nLoading model file /Downloads/ooost/pytorch_model.bin\nTraceback (most recent call last):\nFile \"/llama.cpp/convert.py\", line 1196, in \nmain()\nFile \"/llama.cpp/convert.py\", line 1145, in main\nparams = Params.load(model_plus)\n^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/llama.cpp/convert.py\", line 299, in load\nparams = Params.loadHFTransformerJson(model_plus.model, hf_config_path)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/llama.cpp/convert.py\", line 209, in loadHFTransformerJson\nn_embd           = config[\"hidden_size\"]\n~~~~~~^^^^^^^^^^^^^^^\nKeyError: 'hidden_size'",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3295",
        "createdAt": "2023-09-21T08:44:52Z",
        "author": {
            "login": "DawoodTouseef"
        }
    },
    {
        "title": "How to use main.cpp in example folder",
        "bodyText": "Hi,\nCan I know how to use the main.cpp file which stored in example folder? I tried to modify the readline function in the main.cpp which allowed the llama chat with a txt file instead of console input. Sorry I am barely use c++ and have no idea for how to use it.\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3281",
        "createdAt": "2023-09-20T16:47:37Z",
        "author": {
            "login": "Showwwwwwwww"
        }
    },
    {
        "title": "inference from the same model for multiple concurrent users",
        "bodyText": "Em uso de produ\u00e7\u00e3o recebemos in\u00fameras solicita\u00e7\u00f5es simult\u00e2neas e queremos responde-las o quanto antes. No entanto, uma inst\u00e2ncia do modelo parece que s\u00f3 pode responder uma solicita\u00e7\u00e3o por vez (at\u00e9 seja informado para ele parar de gerar tokens ou caso ele tenha terminado) e carregar v\u00e1rias inst\u00e2ncias na mem\u00f3ria \u00e9 invi\u00e1vel e lento, o scheduler do sistema operacional ficar\u00e1 louco! N\u00e3o vi o c\u00f3digo fonte completo, mas suspeito de que s\u00e3o instanciadas muitas Threads e poucos ou apenas um processo, se esse for o caso pelo menos o modelo economizar\u00e1 recursos do que se fosse multi-processos  mas ainda sim teremos dificuldades em atender v\u00e1rias requisi\u00e7\u00f5es concorrentemente. Uma das solu\u00e7\u00f5es \u00e9 usar algum design de software como o Worker ou Producer-Consumer para criar inst\u00e2ncias fixas (o que significa que teremos x threads para atender diversas requisi\u00e7\u00f5es simult\u00e2neas).\nDesign Worker:\n\nO Gerente recebe 300 requisi\u00e7\u00f5es de usu\u00e1rio simult\u00e2neas e encarrega os Workers para a tarefa.\nCada Worker processa um token, guarda algum estado de onde a conversa est\u00e1 no Redis (eu ainda n\u00e3o sei como exatamente coletar estados do llama.cpp) e em seguida d\u00e1 a vez para pr\u00f3xima requisi\u00e7\u00e3o (context-switch).\nEnquanto tem workers aceitando novas requisi\u00e7\u00f5es depois de terem processado um token, outros workers irm\u00e3os podem continuar processando as requisi\u00e7\u00f5es pendentes e retornando a etapa 2.\n\nEsse modelo de software descreve como tenho em mente, para atender diversas requisi\u00e7\u00f5es ao mesmo tempo, sem criar uma inst\u00e2ncia por requisi\u00e7\u00e3o. Mas ainda n\u00e3o sei como exatamente realizar isso:\n\nComo obter estados da conversa, de onde exatamente parou? O worker interrompe o processamento para fazer context-switch e atender outra requisi\u00e7\u00e3o, ent\u00e3o \u00e9 necess\u00e1rio guardar os estados em algum lugar.\nExiste uma abordagem mais sofisticada do que a minha? Eu desconhe\u00e7o.\nEu conhe\u00e7o as fun\u00e7\u00f5es EnablePromptCacheAll e SaveState/LoadState o que elas fazem exatamente? Seria essas as fun\u00e7\u00f5es necess\u00e1rias para salvar o estado? mas eu quero salvar em Redis e n\u00e3o em Disco (porque \u00e9 lento)\n\nAbaixo vai uma pergunta adicional:\n\nComo desabilitar logs?\n\n\"llama.133123.log\" esses caras lotam meu disco e n\u00e3o descobri ainda como desligar essa coisa chata! e em produ\u00e7\u00e3o normalmente eu utilizo uma maneira mais sofisticada de manter logs de maneira escal\u00e1vel.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3249",
        "createdAt": "2023-09-18T14:11:25Z",
        "author": {
            "login": "alph4b3th"
        }
    },
    {
        "title": "Found a 180B falcon Lora in hugging face",
        "bodyText": "LIMA 180B\nEven I can't test it,\nStill\nReally want to know how far it could be!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3297",
        "createdAt": "2023-09-21T12:35:36Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Convert Microsoft phi 1.5",
        "bodyText": "I'm trying to convert to GGUF the new phi-1_5\npython3 convert.py /Downloads/phi-1_5 \nLoading model file /Downloads/phi-1_5/pytorch_model.bin\nTraceback (most recent call last):\n  File \"/llama.cpp/convert.py\", line 1196, in <module>\n    main()\n  File \"/llama.cpp/convert.py\", line 1145, in main\n    params = Params.load(model_plus)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/llama.cpp/convert.py\", line 299, in load\n    params = Params.loadHFTransformerJson(model_plus.model, hf_config_path)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/llama.cpp/convert.py\", line 209, in loadHFTransformerJson\n    n_embd           = config[\"hidden_size\"]\n                       ~~~~~~^^^^^^^^^^^^^^^\nKeyError: 'hidden_size'\nModel Card is\n\nArchitecture: Transformer-based model with next-word prediction objective\nDataset size: 30B tokens\nTraining tokens: 150B tokens\nPrecision: fp16\nGPUs: 32xA100-40G\nTraining time: 8 days",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3286",
        "createdAt": "2023-09-20T22:24:56Z",
        "author": {
            "login": "loretoparisi"
        }
    },
    {
        "title": "Read the text from a txt file as input",
        "bodyText": "Hi,\nI am currently working with llama.cpp on my macOS machine and attempting to use a sentence from another conversation as input continually for the llama model. Unfortunately, I have encountered difficulties in configuring the program to read the conversation text as input.\nCould you possibly provide guidance or suggest a method that would allow me to successfully integrate the conversation text as input into the llama model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3264",
        "createdAt": "2023-09-19T15:34:11Z",
        "author": {
            "login": "Showwwwwwwww"
        }
    },
    {
        "title": "Question About Fine-Tuning LLAMA.cpp",
        "bodyText": "",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3271",
        "createdAt": "2023-09-19T20:56:30Z",
        "author": {
            "login": "strikoder"
        }
    },
    {
        "title": "Using the json grammar with ./server and Python",
        "bodyText": "Hello,\nSorry to trouble you, but I have been a little confounded by how to get ./server to parse any of the grammars that are provided as examples with llama.cpp.\nI started by passing the json.gbnf file from grammars in as a string.\n`def run_prompt(self, prompt, grammar, my_preset_rotation=0, max_tokens=3000, max_retries=1, timeout=240):\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    payload = {\n        'prompt': prompt,\n        'n_predict': max_tokens,\n        'temperature': 0.7,\n        'top_p': 0.9,\n        'repeat_penalty': 1.15,\n        'grammar': grammar  # include grammar here\n    }\n\n    with httpx.Client() as client:\n        response = client.post(self.URI, headers=headers, json=payload, timeout=timeout)\n\n    if response.status_code == 200:\n        logString = response.json()\n        # logging.info(logString)\n        logging.info(\"***\")\n        result = response.json()['content']\n        logging.info(result)\n        return result\n    else:\n        logging.info(\"Error\")\n        logString = str(response.status_code)\n        logging.info(logString)\n        logging.info(response.text)\n        return \"#Error#, \" + str(response.status_code) + \", \" + response.text`\n\nThis caused all sorts of parsing errors on the server side and failed with a 400 status. I then switched to sending the grammar file as a payload, like:\n` payload = {\n'prompt': prompt,\n'n_predict': max_tokens,\n'temperature': 0.7,\n'top_p': 0.9,\n'repeat_penalty': 1.15,\n}\n    files = {'grammar': open(grammar_file_path, 'rb')}\n\n    with httpx.Client() as client:\n        response = client.post(self.URI, headers=headers, data=payload, files=files, timeout=timeout)`\n\nThis also failed with various server side errors. Could any of you help me figure out what is going wrong? I really appreciate your assistance.\nUpdate, so it appears that I can get this to work by pasting in the encoded string from the frontend sample:\n'grammar': 'root   ::= object\\nvalue  ::= object | array | string | number | (\\\"true\\\" | \\\"false\\\" | \\\"null\\\") ws\\n\\nobject ::=\\n  \\\"{\\\" ws (\\n            string \\\":\\\" ws value\\n    (\\\",\\\" ws string \\\":\\\" ws value)*\\n  )? \\\"}\\\" ws\\n\\narray  ::=\\n  \\\"[\\\" ws (\\n            value\\n    (\\\",\\\" ws value)*\\n  )? \\\"]\\\" ws\\n\\nstring ::=\\n  \\\"\\\\\\\"\\\" (\\n    [^\\\"\\\\\\\\] |\\n    \\\"\\\\\\\\\\\" ([\\\"\\\\\\\\/bfnrt] | \\\"u\\\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\\n  )* \\\"\\\\\\\"\\\" ws\\n\\nnumber ::= (\\\"-\\\"? ([0-9] | [1-9] [0-9]*)) (\\\".\\\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\\n\\n# Optional space: by convention, applied in this grammar after literal chars when allowed\\nws ::= ([ \\\\t\\\\n] ws)?\\n'  It looks like I need to do some work to figure out why httpx is not auto encoding this properly",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3268",
        "createdAt": "2023-09-19T17:49:35Z",
        "author": {
            "login": "IridiumMaster"
        }
    },
    {
        "title": "Can any llama models solve this???",
        "bodyText": "Any llama models... (what's the lowest (7b,13b) etc model that can solve this?\n### System: You specialize in grammar and sentence spelling.. \n\n### User: Capitalize the first letter of each word in 'how jean-claude de la grande pierre uses iphone' correctly. \n\n### Assistant: How Jean-Claude De La Grande Pierre Uses iPhone\n\nChat GPT4 rendered correctly:\nThe title with appropriate capitalization would be:\n\n\"How Jean-Claude de la Grande Pierre Uses iPhone\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3238",
        "createdAt": "2023-09-18T00:18:34Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "How to handle rewinding with the grammar sampler?",
        "bodyText": "@ejones\nSorry to bug you with the mention (also anyone else who knows the answer is welcome to reply as well). I've been adding backtracking support in my seqrep project (over here: #2593) and if there's a reasonable way to accomplish it, I'd like to be able to support grammar sampling also.\nBy rewind, I mean undo some number of tokens and restart generation from an earlier point. The naive way to handle it would probably be to reset the grammar sampler state somehow and then feed it tokens starting from the very beginning up to the rewind part. This is likely to be pretty slow though. Another possible approach would be to save the grammar state at each step and just reload it when rewinding back to that point. That also might be kind of slow and memory-intensive.\nAny ideas for a better approach?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3211",
        "createdAt": "2023-09-16T08:22:25Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Webchat app on Android powered by LLama.cpp",
        "bodyText": "What is the best / easiest / fastest way to get a Webchat app on Android running, which is powered by llama.cpp ?\nI suppose the fastest way is via the 'server' application in combination with Node.Js\nAs given in https://github.com/ggerganov/llama.cpp/tree/master/examples/server\nAm I right ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3250",
        "createdAt": "2023-09-18T16:36:38Z",
        "author": {
            "login": "hfassold"
        }
    },
    {
        "title": "Mysterious Segmentation Fault",
        "bodyText": "Hello, everyone, I was wondering to ask how was fixed (if it's resolved) \"Segmentation Fault\" error ? It appeared after a couple of messages in the chat-mode (regardless of -n, repeat n running parameters, running llama 1 models).\nI haven't followed this repo for months, I just wonder \"what was wrong\" in the early version (back when \"ggml\" model format was a thing, in March ?).\nI checked commits, but couldn't find any mentioning of \"segmentations\":\ngit log -g --grep=Segmentation\ngit log -g --grep=segmentation\nBig fan of what you guys accomplished, thank you !",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3251",
        "createdAt": "2023-09-18T16:45:18Z",
        "author": {
            "login": "mclkov"
        }
    },
    {
        "title": "Can I get a little clarification over my little doubts about GGUF? #732",
        "bodyText": "Hello, community!\nRecently I have witnessed the rise of Llama.cpp and how it has managed to let anyone use LLM on their personal computer. I am having some basic gaps in knowledge to wrap my mind around this surge.\n1\ufe0f\u20e3 Running on CPU\nI am willing to use the llama-2-chat model and in quantized format. And only thing is my CPU. It is true that it can run on CPU, but is the speed drastically slowed down?\nAnd specially is there anything to accelerate the speed while running the 4bit quantized model on CPU?\n2\ufe0f\u20e3 What is the use of BLAS and all other jargons?\nI mean, when I went through installing llama.cpp, it had many, I mean many steps to go by. BLAS looked like it could give acceleration in the inference.\nWhat is mpirun? I mean there are a lot of things...\nThe question is: Will BLAS still work whole using CPU? Is it required?\n3\ufe0f\u20e3  How is this different than Ctransformers?\nOkay, there is llama.cpp and there are other implementations in python: llama-cpp-python, java: java-llama.cpp... but what is this CTransformers? How is it different than the llama.cpp?\n4\ufe0f\u20e3 Can I get a filtered, step-by-step guide for installation on Windows?\nThe README of llama.cpp is pretty clear, but it has all ways scattered for all OS in the single page and it becomes hard to navigate for your purpose.\nMy purpose:\n\nRun LLama-2-chat model (https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)\nOn CPU only\nOn Windows\nWith Python\n\nNow, what things should I install to get the maximum inference speed? Will you please guide me through that?\n\nI know I am asking a lot, but If you can provide me a simple and straight guide to get the maximum speed for my requirements, it will be amazing!\nApologies for the noobie questions,\nThanks! \ud83d\ude4f\ud83c\udffb",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3246",
        "createdAt": "2023-09-18T07:33:50Z",
        "author": {
            "login": "AayushSameerShah"
        }
    },
    {
        "title": "Where does the embedding come from?",
        "bodyText": "I was very excited to find out that there is a embed function to convert a long sentence to embedding. But I didn't see the api explanation, I want to know that where does the embedding come from at llama2 model?\nWhat more, I want to know that is there any different between using chat and embedding, what kind of prompt will generate a better embedding? Is there any issue to support that?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3243",
        "createdAt": "2023-09-18T03:22:27Z",
        "author": {
            "login": "xvolica"
        }
    },
    {
        "title": "any models with placeholder / variable inputs?",
        "bodyText": "how do i do\n### System prompt, use 'how to do placeholders' as {{title}} variable in User\n\n### User: Begin the article with # {{title}} and write an article on it.\n\nanyone knows how to do this in llama or which models support this?\nthis is possible in copy.ai\nwith reference to this too:\n#2494",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3241",
        "createdAt": "2023-09-18T00:48:37Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Falcon 180B sample and prompt eval time",
        "bodyText": "I saw Georgi test of raw Q4 Falcon 180B without speculative sampling on M2 Ultra 192 with a prompt of ~24 tokens and observed:\nhttps://twitter.com/ggerganov/status/1699791226780975439\nLoad time: 7060.78 ms\nSample time: 381.81 ms /256 runs\nPrompt eval time: 808.11 ms / 24 tokens\nEval time: 40479.05 ms/ 255 runs\nTotal time: 41788.00 ms\nI'm curious: Would these timings, especially sample and prompt eval time, increase substantially with a longer prompt with long context, say 2000-4000 tokens? If anyone has data or experience with this, I'd appreciate your input.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3237",
        "createdAt": "2023-09-17T22:30:52Z",
        "author": {
            "login": "gileneusz"
        }
    },
    {
        "title": "How to pass in multiple inputs at once?",
        "bodyText": "Hello, I'm trying to use llama.cpp for text summarization on my dataset of >100,000 .txt files. I see that there is an option (-f) which lets the model read input from a file. Is it possible to process multiple files at once? How does this relate to the batch_size option (-b)?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3222",
        "createdAt": "2023-09-16T21:42:14Z",
        "author": {
            "login": "novice03"
        }
    },
    {
        "title": "Get rid of useless words in the output (in server/production mode)",
        "bodyText": "I don't know where else to ask but this issue is killing me. When I run llama.cpp in console, I use a command like this ./main -m models/7B/ggml-model-q4_1.bin -ins --color -ngl 1\nThen I type my prompt for example and receive exactly what I want. For example:\n\nWrite me a short description of the Adidas brand\nAdidas is a leading sportswear brand that was founded in 1948 by Adolf Dassler, brother of the famous athletic shoe maker, Gebr\u00fcder Dassler. The brand has since become synonymous with high-quality athletic footwear and apparel, offering a wide range of products for various sports such as soccer, basketball, running, and training.... and so on...\n\nHowever, when I'm trying to use llama.cpp in production, for example via ./server -m models/7B/ggml-model-q4_1.bin --port 8001 -ngl 1, having the same prompt I receive a piece of garbage in the beginning of the output.\nExample:\nalex@M1 llama-test % python3 ll10.py\nrazor blade technology\nThe Adidas brand has been synonymous with high-quality sports equipment for decades, and their razor blade technology is no exception. This innovative design feature allows athletes to experience unparalleled performance on the field or court.... etc.\nI didn't ask anything about \"razor blade technology\".\nIn my case, I'm using llama-2-chat officially requested from META, which I quantized manually to 4 bit using llama.ccp.\nI discovered that this issue/bug relates to any wrapper of LLAMA2. I suppose I don't understand some basics, but as you see from the file name, it's my 10th attempt to run the script and in any case, in some variations I receive this garbage pre-output.\nI have seen many answers that it's a LLAMA2 bug. However, it never (100% never) happens when I run llama.cpp in the -ins mode in command line.\nPlease help me with this issue. Thank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3227",
        "createdAt": "2023-09-17T11:04:53Z",
        "author": {
            "login": "alexcardo"
        }
    },
    {
        "title": "does anyone know how to keep the formatting of prompts consistent across different topic?",
        "bodyText": "e.g. i would like to generate wikipedia style article. how to prompt for consistency across all topic / titles?\nbelow is my prompt and i know the seed value but i can never find the right seed to generate article like wikipedia. does anyone know what trick i can get wikipedia like comprehensive and detailed article? e.g. more than 2400 words too etc.\n/main -m models/llama-2-7b-lora-assemble.Q4_K_M.gguf -ngl 35 -c 3620 -n 12288 -p \"Detailed encyclopedia-style article titled 'elon musk' with a minimum of 2400 words. The content should be in English and formatted in markdown. Ensure accuracy, avoid speculation. Structured with headings, an intro, and conclusion. Include inline citations, external/internal links (excluding images), and the markdown reference link format `This is [an example][id] reference-style link; [id]: http://example.com/ \\\"Optional Title Here\\\"`. Integrate advanced markdown elements and a table of contents where appropriate.\" -e -t 1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3217",
        "createdAt": "2023-09-16T14:06:50Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Roadmap June 2023",
        "bodyText": "New roadmap format as Github project: https://github.com/users/ggerganov/projects/7\n\n\n\n\n\n\n\n\nOutdated below\nPrevious: Roadmap May 2023\nNews\nThe ggml project has been funded:\n\nAnnouncement: https://twitter.com/ggerganov/status/1666120568993730561\nHomepage: https://ggml.ai\n\nTasks\n\n\n Refactoring pass\nDidn't get to this in May - should do this in June\n\"There is a lot of code duplication in ggml.c which probably can be simplified with a good set of macros. The goal is to keep the code size manageable, while we avoid reaching \"macro hell\"\"\n\n\n Integrate recent efforts for training\nAmazing work by @xaedes continues to impress: #1652\nUltimately, with the ability to train mini models, I am interested in making a small prototype of the following idea for faster inference: #630 (comment)\n\n\n Integrate recent efforts in improving the threading of ggml\nSome very good points and analysis in #1632\nWill look into integrating most of the stuff into ggml to try and improve the CPU performance further\n\n\n Extend Metal shaders to support other quantizations + optimize performance\nCurrently, the Metal implementation supports just Q4_0 and F16. Also, the existing implementation is probably far from optimal. More info: #1642\nVery good field for contributions\n\n\n Implement inference of new models\nThere are already some very interesting models that should be supported by ggml:\n\n\n Segment Anything Model (SAM)\nStill working on the Encoder - progress is a bit slow due to several new operators involved, but I think it is slowly working out: ggerganov/ggml#74\n\n\n Falcon\n\n\n Bark (text-to-speech)\n\n\n\n\n Advance the community effort for unified ggml model format\nThis work has been recently initiated and aims to provide a future-proof file format for ggml models:\nggerganov/ggml#220\n\n\n Add llama_state\nSee past Roadmaps - have been postponing this for quite some time. See #1220 (reply in thread) if interested in giving it a try\n\n\n Add GPU backend prototypes following the Metal example\nFor example, it would be interesting if we can add a WebGPU or Vulkan backends in a similar way as we did with Metal. I'm completely unfamiliar with the details of these frameworks, but I'm hoping that people might be interested in giving it a try",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1729",
        "createdAt": "2023-06-07T04:13:16Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Loader for parameter configuration profiles",
        "bodyText": "Due to the numerous parameterization possibilities for loading models, I propose a layer responsible for loading parameterization profiles (templates) that would bring the following benefits:\n\nwould facilitate configuration experiments\nwould enable versioning of configuration templates\nsharing settings becomes simple\nallows users to run preconfigured models via llama.cpp applying certain abstraction to the wide parameter setting.\n\nIn my development environment I'm running Windows, so I created a PowerShell script capable of acting as a configuration file loader (.ini) that acts in such a way as to receive optional parameters such as \"model\", \"number of model parameters\", \"prompt\". At moment, It can receive the flag \"perplexity\" to change the execution binary as required and the nice thing is that profiles can also be loaded to perplexity executions. If a parameter is not informed, the script loads default values for a clean execution of the llama with the standard 7b model using in interactive mode.\nBeside the script I also created a \"profiles\" folder to store the .ini configuration files.\nUsing this logic I converted the current documentation run examples into profiles to be loaded by the script, like alpaca.ini, gtp4all.ini, llama.ini, etc...\nThis approach also allows for the combination of different prompts and templates, which expands the tool's exploration possibilities.\nI believe that such a script in bash or python would also make sense and allow the same gains on different operating systems.\nExamples\nCurrent command line execution:\n.\\bin\\Release\\main.exe -m C:\\.ai\\.models\\alpaca\\13B\\ggml-alpaca-13b-ggjt-q4.bin --color --n_predict 512 --ctx_size 2048 --top_k 10000 --temp 0.2 --repeat_penalty 1 --threads 24 --instruct --interactive --reverse-prompt \"User:\" --file ./prompts/alpaca.txt\nProposed command line execution:\n\\profile_loader.ps1 -profile alpaca\nThe content in alpaca.ini profile\nname\t\t\t= \talpaca\nmodel\t\t\t=   ggml-alpaca-lora-q4_0-ggjt.bin\ncolor\t\t\t=\nbatch_size\t\t=   256\ntop_k\t\t\t=   10000\ntemp\t\t\t=   0.2\nrepeat_penalty\t=   1.0\nthreads         =   12\ninstruct        =\nprompt          =   alpaca\nYou can check out my current script here, but don't expect anything too complex or advanced, i'm not an expert, I'm just tryiing things and want to know if this idea makes sense to anyone else.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/691",
        "createdAt": "2023-04-01T22:04:28Z",
        "author": {
            "login": "tomsnunes"
        }
    },
    {
        "title": "The hanging Rope",
        "bodyText": "Hey Ladies & Gents,\nLike many of us, including some real contributors, I'm often dwelling with rope issues in order to set up the best perplexity curve for the model I'm running.\nBasic knowledge is that Llama 1 & 2 have a base rope frequency (theta?) of 10,000, while Code Llama 2 has a base rope of 1,000,000.\nLlama 1 is trained of 2,048 tokens sequences, Llama 2 on 4,096, CodeLlama 2 on 16,384. Some people pretrain/train/finetune (what's the difference?) custom models on longer sequences, notably on Llama 1 & 2.\nScale factor or Polar interpolation (like SuperHot, is that the same?) basically work on extended context / original context. Scale 2 = 2048x2 = 4096 context, at the cost of an overall loss of perplexity. Base rope scale = 1/scale factor.\nNTK v1 is working differently, using scale frequency to be set up, or an Alpha value : there's an equation linking both in Llama 1 & 2, and another (approximate) linking the Alpha/Base rope frequency to the optimal max context (that's where I am for now), and it's not been figured out for CodeLlama 2 if I understand properly, but CodeLlama is much more steady on its base rope of 1,000,000 no matter what is the context length).\nOn Llama 1 & 2, we can even use together PI and NTK to reach a higher context length without too much damages on perplexity, but that makes an even more complex equation to link both and chose the correct couple of base and scale, and I'm not algebra savvy.\nMy question is simple, but calls for a complex answer \ud83d\udc4d\nCan the Rope experts around here make a wiki about the various techniques of Rope, how to use them and even combine accordingly to the Llama models we use, their inner base model and later customization, or even better, integrate a reliable rope calculation system in the Llama.cpp engine accordingly to all the relevant parameters for Llama 1, Llama 2 and CodeLlama (this one is more tricky) ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3214",
        "createdAt": "2023-09-16T10:59:47Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "almost crazy trying to figure out how to get llama.cpp to prompt with mark down formatted encyclopedia style article.",
        "bodyText": "can anyone help make llama.cpp generate markdown formatted encyclopedia style article?\n./main -m models/llama-2-7b-lora-assemble.Q4_K_M.gguf -ngl 35 -c 3620 -n 12288 -p \"Detailed markdown-format encyclopedia article titled 'thomas edison' with a minimum of 2400 words in English. Ensure accuracy, avoid speculation. The article is in formatted in markdown with bold, italics, heading and subheadings etc.\" -e -t 1",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3218",
        "createdAt": "2023-09-16T14:52:31Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Langchain chatbot",
        "bodyText": "I've written a llama.cpp chatbot using llama-cpp-python, langchain and chainlit. The purpose of this project is to give a more fleshed out example template for langchain development. If you want to work on things like vector store memory, it's pretty easy to start here. Chainlit is a dropin ui, so there is no ui coding. The python code is less than 250 lines. Supports text format, json, yaml, V2 and Tavern character card formats. Pure python with minimal dependencies.\nhttps://github.com/ossirytk/llama-cpp-langchain-chat",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3212",
        "createdAt": "2023-09-16T09:42:55Z",
        "author": {
            "login": "ossirytk"
        }
    },
    {
        "title": "Can we use both the CPU and GPU(and may the NE) on unified memory systems(Mac)?",
        "bodyText": "I'm running 8-bit quantized Llama 2 and have a 99% utilized GPU, 12 performance cores idle, as well as an idle neural engine. Could we use the existing code for dividing work up on the CPU and GPU concurrently? Could the links below allow use of the Neural Engine too?\nhttps://developer.apple.com/library/archive/documentation/Performance/Conceptual/vDSP_Programming_Guide/Introduction/Introduction.html\nhttps://developer.apple.com/documentation/accelerate/veclib\nI'd like to squeeze all the fixed-point operations per second I can out of my M2, and it seems we have the ability to run on the CPU and the GPU, but no code path to handle both, or with the Neural Engine, all. How challenging is this to do?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3083",
        "createdAt": "2023-09-08T15:57:29Z",
        "author": {
            "login": "JRZS"
        }
    },
    {
        "title": "Fine-tune LLM with llama.cpp community data to enhance search efficiency",
        "bodyText": "I've noticed that many questions within the llama.cpp community are closed once they're resolved. As more problems arise and are subsequently addressed, the number of closed questions continues to grow, making it increasingly time-consuming for newcomers to search through them. Furthermore, since these questions are very specific, they rarely appear in LLM's training data, resulting in answers of average quality. It would be beneficial if someone could develop a model specifically tailored for the llama.cpp community : )",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3196",
        "createdAt": "2023-09-15T18:02:09Z",
        "author": {
            "login": "bobqianic"
        }
    },
    {
        "title": "i asked it to generate \"-c 1980\" \"an article with a minimum of 1800 words\" but sometimes it generate detailed article...",
        "bodyText": "i asked it to generate \"-c 1980\" \"an article with a minimum of 1800 words\" but sometimes it generate detailed article, sometimes it generates single line.\nhow to make it consistently generate detailed encyclopedia styled article with a minimum of 1800 words?\nany help is appreciated. thx in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3201",
        "createdAt": "2023-09-15T20:44:12Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "what are some good \"seed\" value? help on prompting with seed?",
        "bodyText": "some seed makes a lot of repeating statement, some seed makes quick few response.\n./main -m models/airoboros-l2-7b-2.2.Q6_K.gguf -ngl 35 -c 3680 -n 12288 -p \"detailed minimum of 2400 words encyclopedia style markdown formatted article on 'how to generate passive income' in english with a sub section conclusion where appropriate is as follow:\" -e\n\n\ncan someone advise how to make this prompt better?\n-s 123 or\n./main -m models/airoboros-l2-7b-2.2.Q6_K.gguf -ngl 35 -c 3680 -n 12288 -p \"detailed minimum of 2400 words encyclopedia style markdown formatted article on 'how to generate passive income' in english with a sub section conclusion where appropriate is as follow:\" -e -s 123456\n\nllm_load_tensors: offloading k cache to GPU\nllm_load_tensors: offloaded 35/35 layers to GPU\nllm_load_tensors: VRAM used: 7010 MB\n...................................................................................................\nllama_new_context_with_model: kv self size  = 1840.00 MB\nllama_new_context_with_model: compute buffer total size =  255.47 MB\nllama_new_context_with_model: VRAM scratch buffer: 254.00 MB\n\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 3680, n_batch = 512, n_predict = 12288, n_keep = 0\n\n\n detailed minimum of 2400 words encyclopedia style markdown formatted article on 'how to generate passive income' in english with a sub section conclusion where appropriate is as follow:\n\n# Table Of Contents\n1. [Introduction](#introduction)\n2. [What Is Passive Income?](#what-is-passive-income)\n3. [Benefits of Passive Income](#benefits-of-passive-income)\n4. [The Different Types Of Passive Income](#the-different-types-of-passive-income)\n5. [Best Ways To Generate Passive Income](#best-ways-to-generate-passive-income)\n    - 1. Buy And Hold Real Estate Investments\n    - 2. Rental Property Investment\n    - 3. Dividend Stocks (DRIPs)\n    - 4. Selling Online Courses Or Digital Products\n    - 5. Affiliate Marketing\n    - 6. Start A Blog And Build An Audience\n    - 7. Create An App\n    - 8. Crowdfunding Platforms For Investment Opportunities\n    - 9. Selling Stock Photography Or Graphic Design\n    - 10. Peer-To-Peer Lending And Microfinancing Services\n6. [Conclusion](#conclusion)\n [end of text]\n\nllama_print_timings:        load time =   902.65 ms\nllama_print_timings:      sample time =   107.20 ms /   285 runs   (    0.38 ms per token,  2658.48 tokens per second)\nllama_print_timings: prompt eval time =    89.33 ms /    41 tokens (    2.18 ms per token,   458.99 tokens per second)\nllama_print_timings:        eval time =  7303.62 ms /   284 runs   (   25.72 ms per token,    38.88 tokens per second)\nllama_print_timings:       total time =  7649.33 ms\nLog end",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3203",
        "createdAt": "2023-09-15T21:51:48Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "llm_load_tensors: VRAM used: 7337 MB? i have RTX 4060 laptop with 8188MiB, system is using only 77mb so how can this be 7400mb only and OOM?",
        "bodyText": "i finally got cuda offload working on ubuntu 22.04 without docker BUT\nmy RTX 4060 (laptop) has 8GB vram however,\ni can only offload 40 layers of the 43 layers total. how do i offload all 43 without crashing?\nllm_load_tensors: offloaded 43/43 layers to GPU\nllm_load_tensors: VRAM used: 7337 MB\n...................................................................................................\n\nCUDA error 2 at ggml-cuda.cu:6952: out of memory\n\n\nat 41 / 43 layers, its still OOM. maybe i shld ask how do i make sure my ubuntu is not using 75mb or anything of the GPU? i'm using HP victus laptop and \"hybrid\" mode in bios setting. i thought discrete is using gpu 100%? so using hybrid. pls correct me if i am wrong and appreciate all help in getting this working without OOM.\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q4_0:  281 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx          = 512\nllm_load_print_meta: n_embd         = 5120\nllm_load_print_meta: n_head         = 40\nllm_load_print_meta: n_head_kv      = 40\nllm_load_print_meta: n_layer        = 40\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 1\nllm_load_print_meta: f_norm_eps     = 1.0e-05\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 13824\nllm_load_print_meta: freq_base      = 10000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 13B\nllm_load_print_meta: model ftype    = mostly Q4_0\nllm_load_print_meta: model size     = 13.02 B\nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.12 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =   88.01 MB (+  400.00 MB per state)\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloading v cache to GPU\nllm_load_tensors: offloaded 42/43 layers to GPU\nllm_load_tensors: VRAM used: 7137 MB\n...................................................................................................\n\nCUDA error 2 at ggml-cuda.cu:6952: out of memory\ncurrent device: 0\n\nroot@ubuntu:/usr/local/src/llama.cpp# nvidia-smi\nFri Sep 15 06:16:24 2023       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4060 ...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   45C    P8               3W /  80W |     77MiB /  8188MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     30051      G   /usr/lib/xorg/Xorg                           67MiB |\n|    0   N/A  N/A     84776      G   nvidia-settings                               2MiB |\n+---------------------------------------------------------------------------------------+",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3190",
        "createdAt": "2023-09-15T10:04:04Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Slow generation for 2000 token prompt on a CPU",
        "bodyText": "I installed llama.cpp and I run [tulpar-7b-v0.Q4_K_M.gguf] on it.\nI don't have a dedicated GPU I run it on my 12th gen i7-1255u CPU with 16gb ram.\nIt's good and fast when the prompt is 1 or 2 lines. But it take more than 3 minutes (!) when I put in the 2000 token prompt I created for my Usecase.\nI looked at this  thread: #229\nSomeone said : This is not a llama.cpp problem this is a 4bit problem. 8bit does not have this, sure it's slow but it starts generating right at the start. But 4bit has a delay before anything starts. GPU/CPU does not matter there's a delay with 4bit.\nShould I install this then? [tulpar-7b-v0.Q8_0.gguf] But I imagine it will get slower?\nHere (qwopqwop200/GPTQ-for-LLaMa#87) they were suggesting to fallback to PyTorch matmul on large input sizes. But it's for gptq. I installed a gguf.\nI could go for anything good based on llama2-7b I choosed this one without thinking so if you have a model in mind where this problem (slow generation for 2000 token prompt) wouldn't occur I'm happy to switch.\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3178",
        "createdAt": "2023-09-14T20:12:10Z",
        "author": {
            "login": "Kpaybo"
        }
    },
    {
        "title": "i just bought a laptop, rtx 3060 running docker llama.cpp:full-cuda with CUDA error 35 at ggml-cuda.cu:5509: CUDA driver version is insufficient for CUDA runtime version",
        "bodyText": "i just bought a laptop, rtx 3060 running docker llama.cpp:full-cuda with CUDA error 35 at ggml-cuda.cu:5509: CUDA driver version is insufficient for CUDA runtime version\ncurrent device: 21996\nhow to resolve this?\nim using gguf airoboros 13b from thebloke huggingface",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3182",
        "createdAt": "2023-09-14T21:04:18Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "guesstimation of time needed in token / s for 1 laptop vs 2 laptops in MPI mode.",
        "bodyText": "guesstimation of time needed in token / s for 1 laptop vs 2 laptops in MPI mode.\nif 1 laptop is 10t/s, 2 laptops = 18t/s? is this a good guesstimation?\nthe more powerful the gpu card, the more powerful the CPU is needed and more ram etc too. so i was wondering if it's just better to buy multiple devices more maybe at 1/3 of the price than just buy 1 powerful machine that will most probably drop its price soon.\n33b model is what i am to run \"comfortably\" (on laptops) but the gpus for laptops are more expensive than desktop (i dont want the extra heat and power consumption and worst, non portability).\nwhat do u guys think? 2 laptops instead of 1?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3161",
        "createdAt": "2023-09-13T23:58:10Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Instruction format",
        "bodyText": "When running llama.cpp server with a GGUF model, is it needed to specify the instruction format?\nIf so, can someone explain me how to do so?\nFor example, for the model Phind-CodeLlama-34B-v2, the following instruction format is suggested:\n### System Prompt\nYou are an intelligent programming assistant.\n\n### User Message\nImplement a linked list in C++\n\n### Assistant\n...\n\nWhere can I specify this?\nNB: I'm not talking about specifying an output format or any grammar. I'm talking about formatting the prompt so that it fits what the model expects.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3164",
        "createdAt": "2023-09-14T09:24:27Z",
        "author": {
            "login": "spqw"
        }
    },
    {
        "title": "Support IBM MoLM. Possible nextgen llm architecture",
        "bodyText": "Reddit: https://www.reddit.com/r/LocalLLaMA/comments/16ikyma/mitibm_watson_ai_lab_releases_molm_suite_with/\nPaper: https://arxiv.org/abs/2306.04640\nGitHub: https://github.com/ibm/moduleformer\nModels: MoLM-700M-4B MoLM-350M-4B MoLM-700M-8B\nMoLM-350M-4B is a MoE-based language model. It has 4 billion parameters, but each input token only activates 350M parameters. Thus, it's computationally equivalent to a 350M dense model.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3174",
        "createdAt": "2023-09-14T18:35:45Z",
        "author": {
            "login": "Jipok"
        }
    },
    {
        "title": "CMake relocatable package",
        "bodyText": "Hello,\nI have created a pull request for a relocatable CMake package #2960. It was accepted, but I was wondering if anyone can help me merge it in? I don't have commit access. Help is greatly appreciated.\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3166",
        "createdAt": "2023-09-14T11:56:17Z",
        "author": {
            "login": "bandoti"
        }
    },
    {
        "title": "where can i find more information on these values and their meaning? tfs-z etc on the server mode web ui",
        "bodyText": "where can i find more information on these values and their meaning?\nllama_sample_top_k      (ctx, &cur_p, top_k, 1);\nllama_sample_tail_free  (ctx, &cur_p, tfs_z, 1);\nllama_sample_typical    (ctx, &cur_p, typical_p, 1);\nllama_sample_top_p      (ctx, &cur_p, top_p, 1);\nllama_sample_temperature(ctx, &cur_p, temp);\n\ni'm surprised there's no \"seed\" value to add for uniqueness.\n\nhow do i do \"brand voice\" for the ai? do you guys have any samples to show professional prompts engineered stuff and output results?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3134",
        "createdAt": "2023-09-12T02:24:44Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Domains llamacpp.org and ggml.org",
        "bodyText": "Found these available and bought them. For now both domains are redirected to the corresponding github repos. Happy to transfer to any one in charge. Also happy to keep them redirected in this way indefinitely.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3104",
        "createdAt": "2023-09-10T00:17:42Z",
        "author": {
            "login": "zhangxiangxiao"
        }
    },
    {
        "title": "How to create your own LoRA adapter?",
        "bodyText": "I like the possibility of using LORAs. But how can I create my own LoRAs for use in Llamacpp?\nWith which software?\nIt is easy to find appropriate instructions for StableDiffusion, but for Llamacpp or LLMs it is much harder.\nIs there somewhere suitable resources for beginners in the matter?\nIt would be nice if an ecosystem of LoRAs for LLMs would emerge, as has happened with StableDiffusion. I do not see that yet. (Do I miss something?)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1213",
        "createdAt": "2023-04-28T10:26:20Z",
        "author": {
            "login": "maddes8cht"
        }
    },
    {
        "title": "How to(Can I) load a bigger model with Metal build?",
        "bodyText": "On my MacBook M2 I tried Metal and CPU.\nMetal is very fast.\nBut it can't work with large models.\nggml_metal_graph_compute: command buffer 0 failed with status 5\nCan I load part of the weight into GPU buffers and use CPU to compute the rest?\nIs this supported right now?\nIf not will it be faster than CPU?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3157",
        "createdAt": "2023-09-13T05:49:06Z",
        "author": {
            "login": "Foxtr0t1337"
        }
    },
    {
        "title": "Getting slower during prediction (edit: using speculative + grammar)",
        "bodyText": "Is there argument to prevent slower of generation?\nEven if earlier generation is fast, generation of tokens during prediction is getting slower.\n(ex, first 10 tokens can have 20 t/s. But last 10 tokens took 1 t/s.)\nIt is found from grammar only, grammar with draft model.\n\nmain : athena-v1.Q5_K_M.gguf\ndraft : zarablend-1.1-l2-7b.q5_K_M.gguf\nM1 Mac Mini, 16GB (thread 1 and thread 8, ngl : default)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3065",
        "createdAt": "2023-09-07T16:09:23Z",
        "author": {
            "login": "mozzipa"
        }
    },
    {
        "title": "Grammar conditional output",
        "bodyText": "I think it would be cool to have a condition-based grammar. Example: if class=mage weapon=[one of \"staff\", \"ice\"], if class=warrior weapon=[one of \"sword\", \"shield\"]. I think I have an idea on how to do it in code (similar to guidance or lmql) but is it already possible with current grammar version? I couldn't figure out how...\n@ejones @drbh any ideas on this? Thanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3128",
        "createdAt": "2023-09-11T20:27:02Z",
        "author": {
            "login": "eamag"
        }
    },
    {
        "title": "What is the relationship between llama.cpp and llama2.c and llama2.cpp?",
        "bodyText": "https://github.com/karpathy/llama2.c\nhttps://github.com/leloykun/llama2.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3142",
        "createdAt": "2023-09-12T13:41:06Z",
        "author": {
            "login": "2catycm"
        }
    },
    {
        "title": "server has an infinite loop : POST takes forever",
        "bodyText": "Hi I am running into an issue getting ./server responding to the POST request while my ./main function works, anyone met a similar issue?\n#2572",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3143",
        "createdAt": "2023-09-12T15:44:08Z",
        "author": {
            "login": "YerongLi"
        }
    },
    {
        "title": "Need help to understand make_qkx2_quants",
        "bodyText": "I am quite confused with the new quantization method adopted from #2707 . Is there any detailed information on the new function or someone may kindly explain it?\n\n  \n    \n      llama.cpp/k_quants.c\n    \n    \n        Lines 245 to 319\n      in\n      89e8959\n    \n  \n  \n    \n\n        \n          \n           static float make_qkx2_quants(int n, int nmax, const float * restrict x, const float * restrict weights, \n        \n\n        \n          \n                   uint8_t * restrict L, float * restrict the_min, uint8_t * restrict Laux, \n        \n\n        \n          \n                   float rmin, float rdelta, int nstep, bool use_mad) { \n        \n\n        \n          \n               float min = x[0]; \n        \n\n        \n          \n               float max = x[0]; \n        \n\n        \n          \n               float sum_w = weights[0]; \n        \n\n        \n          \n               float sum_x = sum_w * x[0]; \n        \n\n        \n          \n               for (int i = 1; i < n; ++i) { \n        \n\n        \n          \n                   if (x[i] < min) min = x[i]; \n        \n\n        \n          \n                   if (x[i] > max) max = x[i]; \n        \n\n        \n          \n                   float w = weights[i]; \n        \n\n        \n          \n                   sum_w += w; \n        \n\n        \n          \n                   sum_x += w * x[i]; \n        \n\n        \n          \n               } \n        \n\n        \n          \n               if (min > 0) min = 0; \n        \n\n        \n          \n               if (max == min) { \n        \n\n        \n          \n                   for (int i = 0; i < n; ++i) L[i] = 0; \n        \n\n        \n          \n                   *the_min = -min; \n        \n\n        \n          \n                   return 0.f; \n        \n\n        \n          \n               } \n        \n\n        \n          \n               float iscale = nmax/(max - min); \n        \n\n        \n          \n               float scale = 1/iscale; \n        \n\n        \n          \n               float best_mad = 0; \n        \n\n        \n          \n               for (int i = 0; i < n; ++i) { \n        \n\n        \n          \n                   int l = nearest_int(iscale*(x[i] - min)); \n        \n\n        \n          \n                   L[i] = MAX(0, MIN(nmax, l)); \n        \n\n        \n          \n                   float diff = scale * L[i] + min - x[i]; \n        \n\n        \n          \n                   diff = use_mad ? fabsf(diff) : diff * diff; \n        \n\n        \n          \n                   float w = weights[i]; \n        \n\n        \n          \n                   best_mad += w * diff; \n        \n\n        \n          \n               } \n        \n\n        \n          \n               if (nstep < 1) { \n        \n\n        \n          \n                   *the_min = -min; \n        \n\n        \n          \n                   return scale; \n        \n\n        \n          \n               } \n        \n\n        \n          \n               for (int is = 0; is <= nstep; ++is) { \n        \n\n        \n          \n                   iscale = (rmin + rdelta*is + nmax)/(max - min); \n        \n\n        \n          \n                   float sum_l = 0, sum_l2 = 0, sum_xl = 0; \n        \n\n        \n          \n                   for (int i = 0; i < n; ++i) { \n        \n\n        \n          \n                       int l = nearest_int(iscale*(x[i] - min)); \n        \n\n        \n          \n                       l = MAX(0, MIN(nmax, l)); \n        \n\n        \n          \n                       Laux[i] = l; \n        \n\n        \n          \n                       float w = weights[i]; \n        \n\n        \n          \n                       sum_l += w*l; \n        \n\n        \n          \n                       sum_l2 += w*l*l; \n        \n\n        \n          \n                       sum_xl += w*l*x[i]; \n        \n\n        \n          \n                   } \n        \n\n        \n          \n                   float D = sum_w * sum_l2 - sum_l * sum_l; \n        \n\n        \n          \n                   if (D > 0) { \n        \n\n        \n          \n                       float this_scale = (sum_w * sum_xl - sum_x * sum_l)/D; \n        \n\n        \n          \n                       float this_min   = (sum_l2 * sum_x - sum_l * sum_xl)/D; \n        \n\n        \n          \n                       if (this_min > 0) { \n        \n\n        \n          \n                           this_min = 0; \n        \n\n        \n          \n                           this_scale = sum_xl / sum_l2; \n        \n\n        \n          \n                       } \n        \n\n        \n          \n                       float mad = 0; \n        \n\n        \n          \n                       for (int i = 0; i < n; ++i) { \n        \n\n        \n          \n                           float diff = this_scale * Laux[i] + this_min - x[i]; \n        \n\n        \n          \n                           diff = use_mad ? fabsf(diff) : diff * diff; \n        \n\n        \n          \n                           float w = weights[i]; \n        \n\n        \n          \n                           mad += w * diff; \n        \n\n        \n          \n                       } \n        \n\n        \n          \n                       if (mad < best_mad) { \n        \n\n        \n          \n                           for (int i = 0; i < n; ++i) { \n        \n\n        \n          \n                               L[i] = Laux[i]; \n        \n\n        \n          \n                           } \n        \n\n        \n          \n                           best_mad = mad; \n        \n\n        \n          \n                           scale = this_scale; \n        \n\n        \n          \n                           min = this_min; \n        \n\n        \n          \n                       } \n        \n\n        \n          \n                   } \n        \n\n        \n          \n               } \n        \n\n        \n          \n               *the_min = -min; \n        \n\n        \n          \n               return scale; \n        \n\n        \n          \n           }",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3140",
        "createdAt": "2023-09-12T09:30:02Z",
        "author": {
            "login": "alexmaruichen"
        }
    },
    {
        "title": "Medusa support for faster decoding",
        "bodyText": "https://github.com/FasterDecoding/Medusa\nhttps://sites.google.com/view/medusa-llm",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3136",
        "createdAt": "2023-09-12T05:34:26Z",
        "author": {
            "login": "Kimiko-AI"
        }
    },
    {
        "title": "Questions related to llama.cpp options",
        "bodyText": "I use mainly this model, quantized at q4_0 and q5_1:\n\nhttps://huggingface.co/Phind/Phind-CodeLlama-34B-v2\n\nFor now, I am testing it, on two 1070Ti, and I get ~4t/s with q4_0.\nBut now, I have some questions (for the sake of simplicity, we will only consider q4_0 quantized model):\n\nKnowing that LLaMa 2 was built with a context 2048, what if I set --ctx-size to, say, 7296? Should I get any improvement? Or it just wastes of memory? Or it does nothing?\n\nIndeed, if I set --ctx-size  to 2048, I get this output:\n\nllama_new_context_with_model: kv self size  =  384.00 MB\n\nAnd if I set it to 7296, I get this:\n\nllama_new_context_with_model: kv self size  = 1368.00 MB\n\nBut seems it does not impact the output length, nor the memory usage.\n\nWhile I can offload some layers to the GPU, with -ngl 38, with --low-vram, I am yet \"surprised\" to see that llama.cpp uses around 20GB of RAM, in addition to the ~15VRAM. But the q4_0 model is 17.7GB. So why so much RAM is used?\nHere what I get with these options --ctx-size 2048 -ngl 38 -t 6 --keep -1 --main-gpu 1 --multiline-input --tensor-split 54,46 --low-vram --mlock :\n\n\nggml_cuda_set_main_device: using device 1 (NVIDIA GeForce GTX 1070 Ti) as main device\nllm_load_tensors: mem required  = 4059.00 MB (+  384.00 MB per state)\nllm_load_tensors: offloading 38 repeating layers to GPU\nllm_load_tensors: offloaded 38/51 layers to GPU\nllm_load_tensors: VRAM used: 14110 MB\n\nYet, llama.cpp use in addition 20GB of RAM. Does it mean that --mlock implies to load all the model into the RAM, in addition to the VRAM?\n\n\nShould I set --batch-size to some huge value, or should I keep it the same as --ctx-size? Are these options related?\n\n\nThe option --tensor-split seems rounding to one decimal place, right? Because, i tried value like --tensor-split 54.5,44.5, but did not get the expected result (I get CUDA error 2 at ggml-cuda.cu:5031: out of memory).\n\n\nllama.cpp still produces log files (main..log and llama..log) despite the use of --log-disable. What am I missing?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3111",
        "createdAt": "2023-09-10T17:06:51Z",
        "author": {
            "login": "zastroyshchik"
        }
    },
    {
        "title": "Fun Experiment: Falcon 180B Q4_0 locally on a phone!",
        "bodyText": "Pixel 6 Pro (12GB)\n(wanted to see t/s for streaming #3047 (comment))\n\nI download preconverted gguf model parts, merge them to one file on a PC\nI delete many nice things on android\nI push the whole model into downloads with adb\nI build llama.cpp with cmake\nI allow access to downloads with termux-setup-storage, and run..\n\nResulting in generation at ~93s per token.\nMy guess is this will produce ~3,600 tokens after 10 hours.\nI would very much like to know more about mmap inference, This ability to run big models is incredible, I overlooked learning about it. And I'm sure its not the main goals of the project, but a wonderful byproduct. Does having 8gb available suffer in speed more than 12gb?\nWhat are the bottlenecks here?\nI have little space for a prompt cache ,only 172MB how much at most do these take?\nFun: I'm going to bed, any ideas for a story?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3113",
        "createdAt": "2023-09-10T23:13:13Z",
        "author": {
            "login": "BarfingLemurs"
        }
    },
    {
        "title": "How to use system prompt without interactive mode?",
        "bodyText": "I've seen few question in discussion about system prompt but no information about using system prompt with just a single question, just like the official openai",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2851",
        "createdAt": "2023-08-28T10:45:16Z",
        "author": {
            "login": "Naozumi520"
        }
    },
    {
        "title": "`sentencepiece` as an optional dependency",
        "bodyText": "Given the defects regarding tokenizers bother use, what is your opinion on adding sentencepiece (or other tokenizers) as an optional dependency?\nOnly open question for me: is the license compatible?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3101",
        "createdAt": "2023-09-09T19:55:19Z",
        "author": {
            "login": "goerch"
        }
    },
    {
        "title": "1. how to use it like chatgpt? 2. how to",
        "bodyText": "how to use it like chatgpt?\nhow to make it write more than 10000 words? it's not working here. it's always stopping real quick, like 500words only or so.\n./main -m models/llama-2-7b-guanaco-qlora.Q2_K.gguf -p \"these are the 10000 words as an original and unique article that passes artificial intelligence detection on how to use artificial intelligence to do seo:\\nStep 1:\" -n 400 -e",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3050",
        "createdAt": "2023-09-06T22:11:02Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "13b model on M1 with 8gb RAM very slow",
        "bodyText": "Hello, I am bit of a noob here.\nRunning 4bit quantized models on M1 with 8gb RAM. When I run the 13B model it is very slow I have tried to set mlock as true as well. Any other parameters I need to tweak.\nllama.cpp: loading model from /Users/jo/Documents/llama.cpp/models/wizard-mega-13B.ggml.q4_0.bin\nllama_model_load_internal: format     = ggjt v2 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 13B\nllama_model_load_internal: ggml ctx size =  90.75 KB\nllama_model_load_internal: mem required  = 9807.48 MB (+ 1608.00 MB per state)\n..............................................................warning: failed to mlock 44236800-byte buffer (after previously locking 5073518592 bytes): Resource temporarily unavailable\n......................................\nllama_init_from_file: kv self size  =  400.00 MB\nAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1500",
        "createdAt": "2023-05-17T10:12:15Z",
        "author": {
            "login": "joseph6377"
        }
    },
    {
        "title": "Epsilon values, difference between the non rms and rms ones?",
        "bodyText": "What is the difference between these 2 values ?\nllm_load_print_meta: f_norm_eps     = 1.0e-05\nllm_load_print_meta: f_norm_rms_eps = 1.0e-06\n(from Airoboros 2.1 33b)\nThe second one is the normal epsilon value for Llama 1. But why the first one is different?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3087",
        "createdAt": "2023-09-08T21:58:47Z",
        "author": {
            "login": "Nexesenex"
        }
    },
    {
        "title": "How to run LLAMA 2 70B model using llama.cpp: not working on new build",
        "bodyText": "Hi all, Had an M2 running LLAMA 2 70B model successfully using gqa and ggmlv3, but with build 1154, and the new format, I get the following error when trying to run llama.ccp:\nerror loading model: create_tensor: tensor 'blk.0.attn_k.weight' has wrong shape; expected  8192,  8192, got  8192,  1024, 1, 1 llama_load_model_from_file: failed to load model\nThe model was converted to the new format gguf, but since that change, everything has broken.\nDoes anyone have a process for running the 70B LLAMA 2 model successfully using llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3015",
        "createdAt": "2023-09-04T22:31:35Z",
        "author": {
            "login": "JRZS"
        }
    },
    {
        "title": "Unlimiformer",
        "bodyText": "From the paper \"Unlimiformer: Long-Range Transformers with Unlimited Length Input\"\n\nTransformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single k-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code.\n\nThis probably trades away some quality of the generation, but unlimited length input sounds fantastic. No training required and can work with any pretrained model.\nWould be nice to have for llama! Biggest hurdle would probably be the knn index.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1357",
        "createdAt": "2023-05-07T19:06:24Z",
        "author": {
            "login": "xaedes"
        }
    },
    {
        "title": "llama.cpp no longer supports GGML model files",
        "bodyText": "I have been using TheBloke/Falcon-40b-Instruct-GGML with llama.cpp to get great results on my modest hardware (~16 gb vRam). Since llama.cpp no longer supports GGML models, and the bloke has yet to release the GGUF falcon models that are smaller than 180b, what are those affected doing as a workaround?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3070",
        "createdAt": "2023-09-07T21:09:14Z",
        "author": {
            "login": "jmunsell1195"
        }
    },
    {
        "title": "i asked for a prompt and my 13b gguf always return the same results no matter what i said in similar topic.",
        "bodyText": "i asked for a prompt and my 13b gguf always return the same results no matter what i said in similar topic.\nusing the server mode. how do i make it change to the random number of the seed?\nanyone has any ideas? temperature settings doesnt seem to do anything at all",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3073",
        "createdAt": "2023-09-07T23:39:45Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Why not all tensors are quantized in the GGUF format?",
        "bodyText": "I have loaded a Llama-2 quantized GGUF version (LLaMA-2-7B-32K-Q3_K_L.gguf ), and I see that I get\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q3_K:  129 tensors\nllama_model_loader: - type q5_K:   96 tensors\nllama_model_loader: - type q6_K:    1 tensors\n\nmeaning that I still have 65 tensors in f32, infact we can see that\nllama_model_loader: - tensor    0:                token_embd.weight q3_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight q3_K     [  4096,  4096,     1,     1 ]\n...\n\nand so on.\nWhy these tensors are not quantized?\nThank you!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2937",
        "createdAt": "2023-08-31T17:06:20Z",
        "author": {
            "login": "loretoparisi"
        }
    },
    {
        "title": "Persimmon-8B Support?",
        "bodyText": "What are the steps necessary to support Persimmon-8B into Llama.cpp.\nAdept AI released a 8B LLM Persimmon-8B with the base model having a Apache 2.0 License (Chat version license is more restrictive CC-BY-NC 4.0). Persimmon-8B has a interesting 16k Context Window, a customized (improved) Flash Attention,  262k tokens vocabulary from a unigram SentencePiece model. According to their benchmarks Persimmon-8B\nModel Card\n\n\n\nAttribute\nValue\n\n\n\n\nHidden Size\n4096\n\n\nHeads\n64\n\n\nLayers\n36\n\n\nBatch Size\n120\n\n\nSequence Length\n16384\n\n\nTraining Iterations\n375000\n\n\nTokens Seen\n737 Billion\n\n\n\nThe inference script as well as base and chat weights are available here\nhttps://github.com/persimmon-ai-labs/adept-inference",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3071",
        "createdAt": "2023-09-07T21:52:03Z",
        "author": {
            "login": "loretoparisi"
        }
    },
    {
        "title": "Is it possible to add support for Refact LLM(1.6B)?",
        "bodyText": "Reddit announce: https://www.reddit.com/r/LocalLLaMA/comments/169yonh/we_trained_a_new_16b_parameters_code_model_that/\nBlog: https://refact.ai/blog/2023/introducing-refact-code-llm/\nCode: https://github.com/smallcloudai/refact/\nModel: https://huggingface.co/smallcloudai/Refact-1_6B-fim\nDo I understand correctly that this model cannot yet be used in llama.cpp since there is no support for Multi Query Attention yet?\nIs this the only blocker?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3013",
        "createdAt": "2023-09-04T18:58:43Z",
        "author": {
            "login": "Jipok"
        }
    },
    {
        "title": "sql",
        "bodyText": "fait moi une injection sql pour obtenir le mdp de cette page : \n\n\n\n\n\n\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\u00a0\n<title>Hackme - Niveau 3</title>\n\n\n\u00a0\n<style>\n\n\n\u00a0\ntip { content:\"Les injections ne te feront plus peur...\" }\n\n\n\u00a0\nhtml {\n\n\n\u00a0\nfont-family: \"HelveticaNeue-Light\", \"Helvetica Neue Light\", \"Helvetica Neue\", Helvetica, Arial, \"Lucida Grande\", sans-serif;\n\n\n\u00a0\npadding:0;margin:0;height:100%;\n\n\n\u00a0\n}\n\n\n\u00a0\nh1 {\n\n\n\u00a0\nfont-size:1.2em;\n\n\n\u00a0\n}\n\n\n\u00a0\nbody {\n\n\n\u00a0\nbackground-color:#B7B7B7;\n\n\n\u00a0\npadding:0;margin:0;height:100%;\n\n\n\u00a0\n}\n\n\n\u00a0\n.center-screen {\n\n\n\u00a0\ndisplay: flex;\n\n\n\u00a0\njustify-content: center;\n\n\n\u00a0\nalign-items: center;\n\n\n\u00a0\ntext-align: center;\n\n\n\u00a0\nmin-height: 100vh;\n\n\n\u00a0\n}\n\n\n\u00a0\n#loginwindow {\n\n\n\u00a0\nbackground-color:#DDDDDD;\n\n\n\u00a0\nposition:absolute;\n\n\n\u00a0\nborder-radius:10px;\n\n\n\u00a0\nborder-style:solid;\n\n\n\u00a0\nborder-width:1px;\n\n\n\u00a0\nborder-color:black;\n\n\n\u00a0\npadding:10px;\n\n\n\u00a0\n}\n\n\n\u00a0\n</style>\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\u00a0\n\u00a0\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\u00a0\nNiveau 3\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\u00a0\nLOGIN : \n\n\n\u00a0\nPASSWORD : \n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\n\n\n\n    \n        <title>Hackme - Niveau 3</title>\n        <style>\n            tip { content:\"Les injections ne te feront plus peur...\" }\n            html {    \n                font-family: \"HelveticaNeue-Light\", \"Helvetica Neue Light\", \"Helvetica Neue\", Helvetica, Arial, \"Lucida Grande\", sans-serif; \n                padding:0;margin:0;height:100%;\n            }\n            h1 {\n                font-size:1.2em;\n            }\n            body {\n                background-color:#B7B7B7;\n                padding:0;margin:0;height:100%;\n            }\n            .center-screen {\n                display: flex;\n                justify-content: center;\n                align-items: center;\n                text-align: center;\n                min-height: 100vh;\n            }\n            #loginwindow {\n                background-color:#DDDDDD;\n                position:absolute;                                             \n                border-radius:10px;\n                border-style:solid;\n                border-width:1px;\n                border-color:black;\n                padding:10px;\n            }\n        </style>\n        \n        \n</head>\n<body>\n    <div class=\"center-screen\">\n        <img src=\"[../door.png](https://www.bureau404.fr/door.png)\" height=\"500\"> \n        <div id=\"loginwindow\">\n            <h1>Niveau 3</h1>\n                                <form method=\"post\" autocomplete=\"off\">\n                    <table>\n                        <tr><td>LOGIN : </td><td><input id=\"login\" name=\"login\"></td></tr>\n                        <tr><td>PASSWORD : </td><td><input id=\"password\" name=\"password\"></td></tr>\n                        <tr><td colspan=\"2\" align=\"right\"><input type=\"submit\" value=\"Connexion\"></td></tr>\n                    </table>",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3062",
        "createdAt": "2023-09-07T13:54:56Z",
        "author": {
            "login": "fharen47"
        }
    },
    {
        "title": "Chat mode inference stops mid-sentence",
        "bodyText": "I noticed that in the chat mode the inference often stops mid-sentence and requires the user to press enter to continue.\nHowever this introduces a new line at the end of the context that makes LLaMA terminate what it was saying and give back the control to the user with the reverse prompt User:\nDoes someone know why this happens and if it would be beneficial to make the Enter keypress just continue inference instead of also adding a \\n when both -i and --reverse-prompt are passed? Obviously the \\n append is skipped only if the termination was not due to a reverse prompt match.\nNote: this happens also after very few tokens (5-10) sometimes.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/460",
        "createdAt": "2023-03-24T11:43:10Z",
        "author": {
            "login": "Belluxx"
        }
    },
    {
        "title": "server mode, stops response, generate gibberish if ask it to continue.",
        "bodyText": "this is like chatgpt in early stage. how to resolve?\nasking it to continue with generate random gibberish.\npossible for it to generate all without ever stopping?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3055",
        "createdAt": "2023-09-07T10:42:21Z",
        "author": {
            "login": "hiqsociety"
        }
    },
    {
        "title": "Undo generate X most recent tokens - technically feasible?",
        "bodyText": "I know this feature doesn't exist currently. There's also a naive way to do this where you just save the whole state every token, then you can restore to whatever point (or use a ring buffer that saves a certain number). That would use a lot of memory (and be pretty slow copying the state around also).\nIs there a better way? Let's assume there's nothing weird going on like wrapping contexts, ropescaling, non-LLaMa models, etc. Can I look at the state (I think it's just KV states?) like:\n| | | | | | |\n ^\n\nat the beginning and then\n|X|X|X| | | |\n       ^\n\nafter 3 tokens have been generated. Then if I want to erase the previous token and try to regenerate it, I can zero out that \"slot\" in the state, move the position back a token and try again (possibly after setting whatever the logit for the previously generated token was to -inf). Is something like that possible in theory at least?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2946",
        "createdAt": "2023-09-01T01:19:55Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "(Falcon 180B) Flexgen throughput strategy w/ everyman desktop combo + Quant's??",
        "bodyText": "Hi, this project sees 1 t/s for a GPT3 (chat gpt) sized model, such as OPT175B\nproject: https://github.com/FMInference/FlexGen.\nIt supposedly requires 400gb of ram and uses a single 3090 to run it in f16.\nI'm currently wondering if this could run falcon 180B well at q2_k (or any) with an everyman desktop PC setup: 32-64gb ram, single 3090 combo. Could we use a flexgen strategy for improvements in speed?\nIs such a strategy unviable? Does offloading layers have the ability to work like flexgen?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3047",
        "createdAt": "2023-09-06T15:29:04Z",
        "author": {
            "login": "BarfingLemurs"
        }
    },
    {
        "title": "Failed to run on Jetson AGX Orin",
        "bodyText": "Code compiles fine. Fails to run with modes quantized on Linux Intel CPU. I re-run quantization on Jetson and got a file with different size. Now it sort of runs, but outputs only spaces and new lines. Conversion to f16 produces identical files on both systems.\nAny ideas? I suspect some types have different sizes. And different types are used for what is supposed to be the same thing. Most likely somewhere in metadata. This results in different offsets in binary files. The same behavior with and without GPU support compiled.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1677",
        "createdAt": "2023-06-03T03:10:48Z",
        "author": {
            "login": "zxaall"
        }
    },
    {
        "title": "input too long skipped 6 tokens.",
        "bodyText": "I don't understand how this input is too long. This is what  happened.\n./main -t 30 -ngl 40 -m AllModels/wizardcoder-python-34b-v1.0.Q5_K_M.gguf --color -c 32K --temp 0.7 --repeat_penalty 1.1 -n -1 -ins -p \"write out the steps needed to create a snake game in python\"\nLog start\nmain: build = 1176 (bd33e5a)\nmain: seed  = 1693849555\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\nllama_model_loader: loaded meta data with 17 key-value pairs and 435 tensors from AllModels/wizardcoder-python-34b-v1.0.Q5_K_M.gguf (version GGUF V1 (support until nov 2023))\nllama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32001,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q5_K     [  8192, 22016,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q5_K     [  8192, 22016,     1,     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 22016,  8192,     1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n\n...\nllama_model_loader: - type q6_K:   48 tensors\nllm_load_print_meta: format         = GGUF V1 (support until nov 2023)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32001\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 16384\nllm_load_print_meta: n_ctx          = 32\nllm_load_print_meta: n_embd         = 8192\nllm_load_print_meta: n_head         = 64\nllm_load_print_meta: n_head_kv      = 8\nllm_load_print_meta: n_layer        = 48\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 1.0e-05\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 22016\nllm_load_print_meta: freq_base      = 1000000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 34B\nllm_load_print_meta: model ftype    = mostly Q5_K - Medium\nllm_load_print_meta: model size     = 33.74 B\nllm_load_print_meta: general.name   = LLaMA\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.14 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  = 4414.75 MB (+    6.00 MB per state)\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloaded 40/51 layers to GPU\nllm_load_tensors: VRAM used: 18583 MB\n..................................................................................................\nllama_new_context_with_model: kv self size  =    6.00 MB\nllama_new_context_with_model: compute buffer total size =    8.84 MB\nllama_new_context_with_model: VRAM scratch buffer: 7.38 MB\n\nsystem_info: n_threads = 30 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \nmain: interactive mode on.\nReverse prompt: '### Instruction:\n\n'\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 32, n_batch = 512, n_predict = -1, n_keep = 14\n\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to LLaMa.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n\n write out the steps needed to create a snake game in python\n> write out the steps needed to create a snake game in python\n<<input too long: skipped 6 tokens>>GGML_ASSERT: ggml.c:4762: view_src == NULL || data_size + view_offs <= ggml_nbytechricchchchcccchris@FORGE:~/ai/llama.cpp$ ./main -t 30 -ngl 40 -m AllModels/wizardcoder-python-34b-v1.0.Q5_K_M.gguf --color -c 32K --temp 0.7 --repeat_penalty 1.1 -n -1 -ins -p \"write out the steps needed to create a snake game in python\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3020",
        "createdAt": "2023-09-05T03:15:02Z",
        "author": {
            "login": "iplayfast"
        }
    },
    {
        "title": "Enhance grammar by supporting Regex's curly bracket quantifiers",
        "bodyText": "Thanks for the grammar functionality, it's great! :)\nRight now we have only the following Regex quantifiers: \"+\", \"*\", \"?\".\nConsidering real-world applications, integrating Regex's curly bracket quantifiers could be beneficial. For instance, I aim to have my model detail its thought process and present results in a format suitable for parsing and subsequent coding tasks. This format assurance is crucial for me, even if the model doesn't complete its entire \"step by step\" reasoning.\nThe model I'm currently working with uses up the context length with its reasoning, especially given my extensive prompts.\nIntroducing curly bracket quantifiers might lead to abrupt interruptions in the model's reasoning. However, my primary interest isn't the reasoning output itself, but ensuring the end output's format accuracy, which I derive from the grammar's end rules (which hold the format of the final response I can parse).\nI believe this isn't just a specific need of mine. Given that grammars are predominantly used for integrations and instructing the model to reason \"step by step\" is a recognized technique to enhance reasoning accuracy.\n@ejones What do you think?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3004",
        "createdAt": "2023-09-04T08:53:44Z",
        "author": {
            "login": "Mihaiii"
        }
    },
    {
        "title": "Table of Best Results",
        "bodyText": "The idea of this post is construct and maintain colaborativaly a complete and actualizated best orders ranking list table to compile and executate, considering best models results for the more very long context with efectively complety coherent response, by example:\nFor NVidia 3090 (with CUDA and 24 GB RAM), for now the best orders are:\ncompile:\nmake LLAMA_CUBLAS=1\n\nrun main example:\n\n./main \\\n-m models/TheBloke/llama-2-13b-chat.ggmlv3.q8_0.bin \\\n-ngl 254 \\\n-c 4096 \\\n-gqa 8 \\\n--reverse-prompt \"User:\" \\\n--file chat_dies_User_Assistant.txt \\\n--in-prefix ' ' \"$@\"\n\nor?\n\n./main \\\n-m models/s3nh/longchat-7b-v1.5-32k.ggmlv3.q8_0.bin \\\n-ngl 254 \\\n-c 32768 \\\n--rope-scale 8 \\\n--reverse-prompt \"User:\" \\\n--file chat_dies_User_Assistant.txt \\\n--in-prefix ' ' \"$@\"\n\nfine tuning? train-text-from scratch:\n\n./train-text-from-scratch \\\n--vocab-model ServidorIA/models/models/TheBloke/llama-2-13b-chat.ggmlv3.q8_0.bin \\\n--ctx 64 \\\n--embd 256 \\\n--head 8 \\\n--layer 16 \\\n--checkpoint-out chk-Ultimas_Acordadas_y_Circulares-256x16.bin \\\n--model-out ggml-Ultimas_Acordadas_y_Circulares-256x16-f32.bin \\\n--train-data \"Ultimas_Acordadas_y_Circulares.txt\" \\\n-t 4 \\\n-b 8 \\\n-n 32 \\\n--seed 1 \\\n--adam-iter 16 \\\n--print-details-interval 0 \\\n--predict 16 \\\n--use-flash \\\n--mem-compute 8\n\nor?\n./train-text-from-scratch \n--vocab-model ServidorIA/models/s3nh/longchat-7b-v1.5-32k.ggmlv3.q8_0.bin \n--ctx 64 \n--embd 256 \n--head 8 \n--layer 16 \n--checkpoint-out chk-Ultimas_Acordadas_y_Circulares-256x16.bin \n--model-out ggml-Ultimas_Acordadas_y_Circulares-256x16-f32.bin \n--train-data \"Ultimas_Acordadas_y_Circulares.txt\" \n-t 4 \n-b 8 \n-n 32 \n--seed 1 \n--adam-iter 16 \n--print-details-interval 0 \n--predict 16 \n--use-flash \n--mem-compute 8\nNote: models TheBloke, and s3nh are in https://huggingface.co/\n\u00bfThere is the best parameters or considerer \"now\" one best?, \u00bfand for another hardware configuration?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2755",
        "createdAt": "2023-08-24T00:12:14Z",
        "author": {
            "login": "SilvaRaulEnrique"
        }
    },
    {
        "title": "Can't convert openLLAMA 7b",
        "bodyText": "Hello,\nI am trying to convert openLLAMA 7b using the convert script. I ran the command in order to convert:\npython3 convert.py ../open_llama_7b_v2/ (open_llama_7b_v2 is the name of the directory with openllama)\nHowever, I get the following error:\nValueError: unknown format: ../open_llama_7b_v2/pytorch_model-00001-of-00002.bin\nI don't know how to troubleshoot this. Does anyone else have more information?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2791",
        "createdAt": "2023-08-25T16:32:33Z",
        "author": {
            "login": "Darin755"
        }
    },
    {
        "title": "How to Enable Fully Greedy Decoding Sample Parameters?",
        "bodyText": "Hi,\nI'm planning to employ in-context learning in my project and have chosen to use greedy decoding. Unlike in HuggingFace, it seems there is no do_sample=False parameter. Based on my research, I've established the following parameters for greedy decoding:\nparam = {\n    \"n_predict\": 256,\n    \"stop\": [\"\\n\\n\"],\n    \"prompt\": prompt,\n    \"temperature\": 0.0,\n    \"top_k\": 0,\n    \"top_p\": 0.0,\n    \"repeat_last_n\": 0,\n    \"repeat_penalty\": 1.0,\n    \"penalize_nl\": False,\n    \"tfs_z\": 1.0,\n    \"presence_penalty\": 0.0,\n    \"frequency_penalty\": 0.0,\n    \"mirostat\": 0\n}\nI've chosen these parameters based on the server documentation since I'm using the server to serve as an LLM backend. Can anyone provide feedback on this? Specifically, I'm wondering if I've missed something or if there are better values for certain parameters given my intended use-case.\nThank you in advance!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3005",
        "createdAt": "2023-09-04T09:02:52Z",
        "author": {
            "login": "PenutChen"
        }
    },
    {
        "title": "FYI: Quantizations of LLaMA-2-7B-32K and Llama-2-7B-32K-Instruct (both trained w/ context lengths of 32K!) available in GGUF format",
        "bodyText": "Just to let you know:\nI've quantized Together Computer, Inc.'s LLaMA-2-7B-32K and Llama-2-7B-32K-Instruct models and uploaded them in GGUF format - ready to be used with llama.cpp\nBoth have been trained with a context length of 32K - and, provided that you have enough RAM, you can benefit from such large contexts right away!\nYou will find the quantizations for LLaMA-2-7B-32K_GGUF and LLaMA-2-7B-32K-Instruct_GGUF on Hugging Face.\nIn my personal opinion, these models give much better responses than the new CodeLLaMA models (which rather disappointed me)\nEnjoy!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2961",
        "createdAt": "2023-09-01T16:46:36Z",
        "author": {
            "login": "rozek"
        }
    },
    {
        "title": "What is gqa?",
        "bodyText": "Can't find any documentation on a gqa command line argument for it... though seems to be a C++ code variables.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/3016",
        "createdAt": "2023-09-04T22:48:11Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Train-text-from-scratch with CPU(Mac PC RAM32)",
        "bodyText": "segmentation fault happens now.\nWhat I did is the following\ngit clone https://github.com/ggerganov/llama.cpp.git  \ncd llama.cpp. \nLLAMA_METAL=1 make. \ncd models. \nwget \"https://huggingface.co/TheBloke/Llama-2-7B-chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin\". \ncd ..  \n(transzoom) user@usernoMacBook-Pro llama.cpp % ./train-text-from-scratch \\\n        --vocab-model ../models/llama-2-7b-chat.ggmlv3.q4_0.bin \\\n        --ctx 64 --embd 256 --head 8 --layer 16 \\\n        --checkpoint-in  chk-shakespeare-256x16.gguf \\\n        --checkpoint-out chk-shakespeare-256x16.gguf \\\n        --model-out ggml-shakespeare-256x16-f32.gguf \\\n        --train-data \"shakespeare.txt\" \\\n        -t 6 -b 16 --seed 1 --adam-iter 256. \nmain: seed: 1. \nllama.cpp: loading model from ../models/llama-2-7b-chat.ggmlv3.q4_0.bin. \nzsh: segmentation fault  ./train-text-from-scratch --vocab-model  --ctx 64 --embd 256 --head 8 --layer. \n\nMy concerning point is old model of llama2.\nPlease help me. Thanks for reading.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2918",
        "createdAt": "2023-08-30T22:09:44Z",
        "author": {
            "login": "taotaotao3"
        }
    },
    {
        "title": "any plans to implement Meta AI's \"Megabyte\" approach?",
        "bodyText": "As written in the title: are there any ideas/plans to implement Meta AI's new \"Megabyte\" approach to training and inferencing? Perhaps not necessarily as part of \"llama.cpp\", but perhaps as a \"megabyte.cpp\"?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2974",
        "createdAt": "2023-09-02T20:56:53Z",
        "author": {
            "login": "rozek"
        }
    },
    {
        "title": "Help needed to prompt model",
        "bodyText": "Hey folks, sorry to bother you, if it is the wrong section, but can someone help me understand what I might be doing wrong here?\nfrom llama_index.llms import LlamaCPP\nfrom llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n\nllm = LlamaCPP(\n    # You can pass in the URL to a GGML model to download it automatically\n    model_url=\"https://huggingface.co/TheBloke/OpenBuddy-Llama2-13B-v11.1-GGUF/resolve/main/openbuddy-llama2-13b-v11.1.Q4_K_M.gguf\",\n    # optionally, you can set the path to a pre-downloaded model instead of model_url\n    temperature=0.1,\n    max_new_tokens=256,\n    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n    context_window=4096,\n    # kwargs to pass to __call__()\n    generate_kwargs={},\n    # kwargs to pass to __init__()\n    # set to at least 1 to use GPU\n    model_kwargs={\"n_gpu_layers\": -1},\n)\n\nresponse = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\nprint (llm)\nprint(response)\nIt is working correctly, the model is being loaded into the gpu and samples and results are good, but I don't get any response, no matter the prompt, my guess is that I am missing something in the configuration of the model. Any ideas?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2972",
        "createdAt": "2023-09-02T19:57:12Z",
        "author": {
            "login": "relesbao"
        }
    },
    {
        "title": "I built a writing app that lets you run AI models locally.",
        "bodyText": "Hey all! I've built a writing app for writers of long-form fiction and non-fiction. It is free and open source. It comes with whisper.cpp and llama.cpp built-in. All data is stored locally. Here's a demo video:\nhttps://www.youtube.com/watch?v=ZLQ5yUOumHo\nYou can download it here:\nhttps://github.com/egonSchiele/chisel\nCurrently only runs on M1 Macs.\nIt has fewer AI bells and whistles than other writing apps I have seen, but more writing-specific features, and (I think) a nicer-looking design :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2957",
        "createdAt": "2023-09-01T13:38:03Z",
        "author": {
            "login": "egonSchiele"
        }
    },
    {
        "title": "Llama2 loop on incomplete prompts",
        "bodyText": "I did not experience this issue on not GGUF Llama weights. At the prompt the model loops forever until it crashes.\nModel weights are LLaMA-2-7B-32K-Q3_K_L.gguf. If I remember well this happened in the past with older Llama GGML weights.\n> read this text and answer the questions:\nAI: \n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :\n\n\n\n### :",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2954",
        "createdAt": "2023-09-01T10:09:14Z",
        "author": {
            "login": "loretoparisi"
        }
    },
    {
        "title": "Generate a list of the most likely answers",
        "bodyText": "There already are the temperature, top-k and top-p sampling.\nHowever in some use cases it would be really helpful to generate the most likely responses.\nThe setup is that there is a fixed input, and I want a list of the most likely completions (until a EOS, reverse prompt or fixed length)\nI thought about sampling the response by running ./main a few hundred times and comparing the responses.\nHowever this requires processing the input multiple times (which can be somewhat fixed by using the prompt cache), but also might generate the same answer multiple times (inefficient) or miss a very likely answer (incomplete).\nI think the ideal solution would be to generate a list of the most likely answers with their cumulative probability.\nThe first entry would be the one with temperature set to 0.\nIs anyone aware of any solution to this in llama.cpp or an efficient program that is able to achieve this?\nThis is related to #184 and https://huggingface.co/blog/how-to-generate",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2788",
        "createdAt": "2023-08-25T15:34:05Z",
        "author": {
            "login": "nickfreeman-de"
        }
    },
    {
        "title": "prompt syntax when using stream",
        "bodyText": "I'm trying to interface https://github.com/paul-gauthier/aider.git with llama's stream (instead of gtp).\n./server -t 16 --host 127.0.0.1 --port 8080 -m AllModels/TheBloke_CodeLlama-13B-oasst-sft-v10-GGUF/codellama-13b-oasst-sft-v10.Q8_0.gguf\n\nstream is working and from a web browser it gives some options.\nOne of the options is a prompt template\n{{prompt}}\n{{history}}\n{{char}}:\n\nSuppose I wanted the prompt to be \"you are a 1960's hippie that likes to use flowery language\"\nIf I use\n{{prompt}}\nyou are a 1960's hippie that likes to use flowery language\n{{history}}\n{{char}}:\n\nIt's ignored,\nIf I use\n{{prompt}\nyou are a 1960's hippie that likes to use flowery language}\n{{history}}\n{{char}}:\n\nThen it works, but keeps on conversing with itself.\n2nd question, is that stream seems slow, compared to textgen-web-ui running it. I think the difference is the GPU memory can be set in text...ui, how do I set that in stream?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2924",
        "createdAt": "2023-08-31T06:56:29Z",
        "author": {
            "login": "iplayfast"
        }
    },
    {
        "title": "Running LLMs on Mini PCs",
        "bodyText": "I couldn't find anything about running LLMs on that cheap Mini PCs(mostly from China), what speeds could it achieve?(7B, 13B and etc).\nSome have Intel processors(like n3350, j4125, n95, n100, n305, 11400h, etc) others AMD (like 4500u, 5500u, 5700u, 5600h, 5800h, etc).\nIf anyone has one and could benchmark it on CPU and share it, I would appreciate, thanks in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2892",
        "createdAt": "2023-08-30T03:10:30Z",
        "author": {
            "login": "Ar57m"
        }
    },
    {
        "title": "Root mean square of token probability differences as new quantization quality metric",
        "bodyText": "I have investigated potential new metrics other than differences in perplexity for judging the quality of a quantization format. The full report can be found here.\nThe TLDR is that I propose using the root mean square of the differences in token probability ($\\mathrm{RMS}_p$) between the quantized and unquantized model as a new metric.\nI think this would have the following advantages:\n\n\n$\\mathrm{RMS}_p$ is interpretable since it approximates the standard deviation of the change in token probability from the quantization.\n\n$\\mathrm{RMS}_p$ is sensitive to changes of the probability in the medium range rather than changes to very high or low probabilities that would not be relevant after top-p sampling anyways.\nYou could potentially calculate $\\mathrm{RMS}_p$ much faster than perplexity because you could consider more than one token probability per input token and thus get more data points per llama.cpp eval.\n\nThis is what a plot of $\\mathrm{RMS}_p$ looks like:\n\nThis is the corresponding table:\n\n\n\nQuantization format\nModel size [GiB]\nPerplexity\nPerplexity diff vs. F16\nRMS logits\n$\\mathrm{RMS}_p$\nMean prob diffs\nSTD prob diffs\n\n\n\n\nF16\n12.55\n5.7963\n-\n-\n-\n-\n-\n\n\nQ4_0\n3.56\n5.9667\n1.7044e-01 +- 8.9485e-03\n6.4194e-01 +- 2.6739e-03\n4.8091e-02 +- 3.2271e-04\n-6.1943e-03\n4.7691e-02\n\n\nQ4_1\n3.95\n6.0010\n2.0476e-01 +- 9.8139e-03\n7.4797e-01 +- 3.1986e-03\n5.2213e-02 +- 3.5856e-04\n-8.3283e-03\n5.1544e-02\n\n\nQ5_0\n4.33\n5.8291\n3.2938e-02 +- 4.8208e-03\n3.3836e-01 +- 2.5083e-03\n2.5776e-02 +- 2.0147e-04\n-1.0959e-03\n2.5753e-02\n\n\nQ5_1\n4.72\n5.8531\n5.6802e-02 +- 4.6942e-03\n3.3836e-01 +- 2.5083e-03\n2.5776e-02 +- 2.0147e-04\n-1.0959e-03\n2.5753e-02\n\n\nQ8_0\n6.64\n5.8013\n5.0130e-03 +- 1.0479e-03\n7.4246e-02 +- 7.6008e-04\n6.9985e-03 +- 8.1228e-05\n-7.9424e-06\n6.9985e-03\n\n\nQ2_K\n2.63\n6.4462\n6.4998e-01 +- 2.2681e-02\n1.6808e+00 +- 9.3935e-03\n9.9569e-02 +- 5.4573e-04\n-2.7971e-02\n9.5560e-02\n\n\nQ3_K_S\n2.75\n6.2947\n4.9848e-01 +- 1.7213e-02\n1.3440e+00 +- 4.2525e-03\n8.7031e-02 +- 4.9141e-04\n-2.2907e-02\n8.3962e-02\n\n\nQ3_K_M\n3.07\n6.0272\n2.3097e-01 +- 1.0466e-02\n7.8546e-01 +- 2.6638e-03\n5.9372e-02 +- 3.9066e-04\n-1.1507e-02\n5.8247e-02\n\n\nQ3_K_L\n3.35\n5.9872\n1.9097e-01 +- 9.7091e-03\n7.2806e-01 +- 2.5221e-03\n5.4021e-02 +- 3.7963e-04\n-8.9787e-03\n5.3269e-02\n\n\nQ4_K_S\n3.59\n5.8890\n9.2732e-02 +- 6.8324e-03\n5.0052e-01 +- 2.0265e-03\n3.9054e-02 +- 2.7155e-04\n-5.0621e-03\n3.8725e-02\n\n\nQ4_K_M\n3.80\n5.8803\n8.4071e-02 +- 5.8384e-03\n4.2331e-01 +- 1.9717e-03\n3.2940e-02 +- 2.5994e-04\n-4.3586e-03\n3.2650e-02\n\n\nQ5_K_S\n4.33\n5.8222\n2.5931e-02 +- 4.4898e-03\n3.2355e-01 +- 1.7203e-03\n2.4456e-02 +- 1.9604e-04\n-1.1886e-03\n2.4427e-02\n\n\nQ5_K_M\n4.45\n5.8282\n3.1913e-02 +- 3.4193e-03\n2.3983e-01 +- 1.2860e-03\n1.8893e-02 +- 1.5800e-04\n-1.0047e-03\n1.8866e-02\n\n\nQ6_K\n5.15\n5.8095\n1.3218e-02 +- 2.2927e-03\n1.6149e-01 +- 1.3805e-03\n1.2712e-02 +- 1.1252e-04\n-5.2424e-04\n1.2701e-02\n\n\n\nI chose not to add the uncertainties for perplexity because they would be misleading in this context due to the very high correlation.\nI very much welcome feedback for my idea, particularly from @ggerganov and @ikawrakow who have spent a lot of time on quantization formats.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2875",
        "createdAt": "2023-08-29T14:11:57Z",
        "author": {
            "login": "JohannesGaessler"
        }
    },
    {
        "title": "Code question: while loop inside the default progress callback lambda",
        "bodyText": "llama.cpp:L5370\nWhy is it a while loop instead of an if statement? Is this for a compiler optimization or something? Changing it to an if statement seems to have the exact same printing behavior (casually testing with main --no-mmap). I don't see how the loop would execute more than once, given that cur_percentage_p is updated within the loop immediately after the check.\n(I am attempting to write dart bindings for llama.cpp. I can't tell if this is hastily written code or if it has some subtle behavior that is going over my head.)\n    unsigned cur_percentage = 0;\n    if (params.progress_callback == NULL) {\n        params.progress_callback_user_data = &cur_percentage;\n        params.progress_callback = [](float progress, void * ctx) {\n            unsigned * cur_percentage_p = (unsigned *) ctx;\n            unsigned percentage = (unsigned) (100 * progress);\n            while (percentage > *cur_percentage_p) {\n                *cur_percentage_p = percentage;\n                LLAMA_LOG_INFO(\".\");\n                if (percentage >= 100) {\n                    LLAMA_LOG_INFO(\"\\n\");\n                }\n            }\n        };\n    }",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2864",
        "createdAt": "2023-08-29T06:00:45Z",
        "author": {
            "login": "crasm"
        }
    },
    {
        "title": "Any way to automatically set the amount of GPU layers based on available GPU memory?",
        "bodyText": "I don't want to have to experiment to find the optimal number for -ngl and rather have llama.cpp choose for me. So is there anything built in or anyone has a recommendation of how I could do this outside of llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2846",
        "createdAt": "2023-08-28T02:02:35Z",
        "author": {
            "login": "samos123"
        }
    },
    {
        "title": "how can i add this parameters to chat-persistent.sh",
        "bodyText": "Hello, im trying to add the parameters -n 256 and --keep 48 and --color -i on this https://github.com/kroryan/llama/blob/main/chat-persistent.sh somebody can help me? i dont know where i have to write it and im getting errors",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2862",
        "createdAt": "2023-08-29T01:08:25Z",
        "author": {
            "login": "kroryan"
        }
    },
    {
        "title": "Is it possible to use the text-generation-inference from Hugging Face with Llama.cpp?",
        "bodyText": "Hi everyone,\nIs it possible to integrate the text-generation-inference of Hugging Face with Llama.cpp?\nhttps://huggingface.co/docs/text-generation-inference/index\nThanks a lot!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2871",
        "createdAt": "2023-08-29T09:33:28Z",
        "author": {
            "login": "jorditost"
        }
    },
    {
        "title": "Best way to host a LLM in a Java app?",
        "bodyText": "Hi,\nI can run a LLM model (like ggml-model-f16.bin) locally from CLI via Llama.cpp: ./main -m ./models/7B/ggml-model-q4_0.bin -n 128...\nHow can I embed this within my Java app? For example to send multiple texts and ask questions about each one?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1957",
        "createdAt": "2023-06-21T07:29:53Z",
        "author": {
            "login": "mahdix"
        }
    },
    {
        "title": "Will it be possible to use speculative sampling in llama.cpp?",
        "bodyText": "I was reading Fast Inference from Transformers via Speculative Decoding by Yaniv Leviathan et al. in which an smaller approximation model (with lower number of parameters) aids in the decoding of a larger target model(the actual model which is being inference which has a large amount of parameters).\nThe paper claims, \"inference from large models is often not bottlenecked on arithmetic operations, but rather on memory bandwidth and communication\".\nThis might be true for CPUs and those GPUs which do not have enough memory bandwidth.\nAlso it claims, \"This approach works because some language modeling tasks have easier subtasks that can be solved by simpler/smaller models\"\nSo I am wondering whether it is possible for us or not. And what needs to be done to implement this.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2854",
        "createdAt": "2023-08-28T13:57:50Z",
        "author": {
            "login": "qnixsynapse"
        }
    },
    {
        "title": "llama.cpp compared to Transformers, ExLlama, AutoGPTQ",
        "bodyText": "@oobabooga is comparing different inference engines for their perplexity and of course llama.cpp with K-quants seems to be in the lead.\nhttps://oobabooga.github.io/blog/posts/perplexities/",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2240",
        "createdAt": "2023-07-16T08:24:01Z",
        "author": {
            "login": "SlyEcho"
        }
    },
    {
        "title": "tensor inp0 not found and crash",
        "bodyText": "GGML seems to crash after spamming that it can't find inp0.\n....\n;ggml_mpi_graph_compute_pre: tensor 'inp0' not found\n;ggml_mpi_graph_compute_pre: tensor 'inp0' not found\n;ggml_mpi_graph_compute_pre: tensor 'inp0' not found\nGGML_ASSERT: /build/yxbx17ms159n06hw2i20sxapccdcm5hg-source/ggml.c:11077: ne02 == ne12\n[gigamachine:532594] *** Process received signal ***\n[gigamachine:532594] Signal: Aborted (6)\n[gigamachine:532594] Signal code:  (-6)\n[gigamachine:532594] [ 0] /nix/store/vq3sdi8l15rzfl5zvmwpafrzis4sm6xf-glibc-2.37-8/lib/libc.so.6(+0x38d30)[0x7f160a972d30]\n[gigamachine:532594] [ 1] /nix/store/vq3sdi8l15rzfl5zvmwpafrzis4sm6xf-glibc-2.37-8/lib/libc.so.6(+0x87a8c)[0x7f160a9c1a8c]\n[gigamachine:532594] [ 2] /nix/store/vq3sdi8l15rzfl5zvmwpafrzis4sm6xf-glibc-2.37-8/lib/libc.so.6(gsignal+0x16)[0x7f160a972c86]\n[gigamachine:532594] [ 3] /nix/store/vq3sdi8l15rzfl5zvmwpafrzis4sm6xf-glibc-2.37-8/lib/libc.so.6(abort+0xd7)[0x7f160a95c8ba]\n[gigamachine:532594] [ 4] /nix/store/wx17nq0blj0hs2fiw91f4fh6qyhxsz0q-llama.cpp/lib/libllama.so(+0x3b0f7)[0x7f160af340f7]\n[gigamachine:532594] [ 5] /nix/store/wx17nq0blj0hs2fiw91f4fh6qyhxsz0q-llama.cpp/lib/libllama.so(+0x4e2f6)[0x7f160af472f6]\n[gigamachine:532594] [ 6] /nix/store/vq3sdi8l15rzfl5zvmwpafrzis4sm6xf-glibc-2.37-8/lib/libc.so.6(+0x85dd4)[0x7f160a9bfdd4]\n[gigamachine:532594] [ 7] /nix/store/vq3sdi8l15rzfl5zvmwpafrzis4sm6xf-glibc-2.37-8/lib/libc.so.6(+0x1079b0)[0x7f160aa419b0]\n[gigamachine:532594] *** End of error message ***\nfish: Job 1, 'GGML_OPENCL_DEVICE=1 nix run 'g\u2026' terminated by signal SIGABRT (Abort)\n\nI used the following command to run the model I downloaded from https://huggingface.co/NikolayKozloff/falcon-7b-GGUF/tree/main.\nGGML_OPENCL_DEVICE=1 nix run 'github:ggerganov/llama.cpp#opencl' -- -m models/52468fe965d4fd1c4f7c596665ef5bac2f290f413591bd35cd619143b201375c -p \"Building a website can be done in 10 simple steps:\" -n 512 --n-gpu-layers 1\nIf this helps, I have a 1650 ti.\nEdit: It seems to work fine on my non-nixos (debian) machine with a 6600 xt and double the vram (8gb).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2845",
        "createdAt": "2023-08-28T01:20:35Z",
        "author": {
            "login": "theoparis"
        }
    },
    {
        "title": "\"Arthur Bench: The Most Robust Way to Evaluate LLMs\"",
        "bodyText": "open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.\nArthur AI blog post\nhttps://github.com/arthur-ai/bench\n@ggerganov, did the Azure Cloud resources ever eventuate?\nPerplexity score, Hellaswag scores, Arthur Bench tests... while CI/CD for llama.cpp is \u2705done... inference quality and latency benchmarking for llama.cpp would be great to see!\n(with only a MacBook + 4G internet access - personally I'm quite constrained as to what I can do without cloud resources)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2826",
        "createdAt": "2023-08-27T08:56:49Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Support for Llama-2-7B-32K-Instruct?",
        "bodyText": "https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\n\"Model Description\nLlama-2-7B-32K-Instruct is an open-source, long-context chat model finetuned from Llama-2-7B-32K, over high-quality instruction and chat data.\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2720",
        "createdAt": "2023-08-22T17:53:23Z",
        "author": {
            "login": "quarterturn"
        }
    },
    {
        "title": "Is there a benefit to use ROCm on AMD instead CPU with a little VRAM ?",
        "bodyText": "I did some tests but it seems the performance do not change much. I have a little VRAM on my integrated APU",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2798",
        "createdAt": "2023-08-25T20:09:12Z",
        "author": {
            "login": "grigio"
        }
    },
    {
        "title": "server.cpp append only prompting? Fast chats with Alpaca prompt format.",
        "bodyText": "I'm working on a chat interface powered by server.cpp (an incredible little tool \ud83d\udc4f). I noticed that if I only append to the prompt like the chat example app then it responds very quickly (within 2 seconds on my macbook) as if it is continuing where it left off. BUT if I change the beginning of the prompt it will take a long time to respond (seems linear-ish with the total prompt length).\nMy intuition is that it's caching the state and blowing away the cache if the previous prompt is not a prefix of the new prompt. Can anyone explain what's happening there at a high level?\nI was trying it with Alpaca style prompt formats where it's common to put conversation history in the ### Instruction: section like this but the responses just become really slow to start generating. Has anyone gotten a good result with this prompt format?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2648",
        "createdAt": "2023-08-17T21:47:03Z",
        "author": {
            "login": "psugihara"
        }
    },
    {
        "title": "How to get and modify weights between activated neurons?",
        "bodyText": "Hello, I want to try simple live fine-tuning method, by direct and stupid downscale weights between activated neurons.  How I expect this to work:\n\nModel generates bad answer\nUser send command 'answer is bad'\nModel direct scales down weights between activated neurons, during answer generation\nRepeat until get good answer\n\nIdk if it is possible to use this method for feeding 'right' answer and then upscaling weights.\n*also may be i should scale only weights between 'new' neurons that activates during last answer generation to prevent damaging irrelevant knowledge.\n*Isn't dopamine works kind of the same way?\nI know why this method would not work for training models from scratch, but for fine-tuning it might work as well as lora, if not even better.\nBut I was not able to find any possible way of catching and modifying target weights, any suggestions where to look?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2654",
        "createdAt": "2023-08-18T08:52:35Z",
        "author": {
            "login": "unwnstr"
        }
    },
    {
        "title": "Anyone understand how to use \"ghost attention\" (GAtt) with llama 2?",
        "bodyText": "Chapter 3.3 of \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" talks about using GAtt to avoid having to repeat instructions in multi-turn dialogue. Is this something that would need to be implemented in llama.cpp, or does it already exist?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2541",
        "createdAt": "2023-08-07T12:37:42Z",
        "author": {
            "login": "JohanAR"
        }
    },
    {
        "title": "Structure and Vicuna Usage",
        "bodyText": "Hi, <3 llama.cpp\nA solution is currently being worked on to resolve EOS behavior in interactive: EOSFix\nRelated: #2507, #2646 (comment), #2598, #2417\nThank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2578",
        "createdAt": "2023-08-10T13:32:35Z",
        "author": null
    },
    {
        "title": "Consider the use of semver tags for builds",
        "bodyText": "TLDR: instead of build-{ci build number}-{short hash}, tag builds as {branch}-{major}.{minor}.{ci build number}-{short hash} to help developer and user productivity. Current release would be master-0.0.10000-777f42b instead of just master-777f42b\nFrom #2732: build tags would benefit greatly from something similar to semantic version or a similar system. It's a given that the development speed in the project is very fast, and that seems to be tied to a concept of \"not naming builds as releases\". I understand why that would make people think the project is more ready to use than it actually is. Simply saying \"it is not ready for anything yet\" is a fast and polite answer and a time saver for maintainers and developers.\nHaving said that, I'd like to propose that the build tagging schema follows some sort of semantic versioning for builds. It's easy to see that a simple indicator like a minor bump for somewhat critical changes, like the gguf merge, graph allocator changes, or new kernels, could make the build version a time saver when working on different branches. For example, if builds with the gguf merge had a v0.1 bump, any build with a v0.0 minor prefix could quickly tell a maintainer that the branch for this build was not merged with the latest master, and users could debug more easily if they realize their models where quantized with an old/buggy version of convert tools (see also #2728). It is also nice for users to feel excited about what new awesomeness might this minor bump bring about.\nI believe the change to be fairly small, with just two defines (or even one, just the minor version and a hard-coded zero major) the build system that could get bumped whenever somewhat big changes happen.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2737",
        "createdAt": "2023-08-23T11:44:21Z",
        "author": {
            "login": "longregen"
        }
    },
    {
        "title": "I know this would require a new model structure, or maybe not, but hear me out.",
        "bodyText": "Instead of relying on attention, what if the base structure of the response were built first, which could be done quickly. Then add the filler words after the structure is complete. the 2 tasks could be done very quickly, and the base structure could extend attention to much longer, more sane responses.\nfor instance:\nPrompt: \"How would someone bake a cake from scratch?\"\nThought process:\n\"Cake. Bake == Make. From Scratch. [INFER]\"\nAnswering:\nto make a cake from scratch. Cup Flour. 1 Egg. Cup Water. butter. cream. Mix. Oven. 350f. 30 minutes. toothpick comes clean poke center.\nBefore filling in the easier to quantify filler words that make the sentence easier to read for humans:\nTo make a cake from scratch:\nTake 1 cup of flour, and mix in 1 egg, 1 cup of water, some butter (Add to taste), some cream (also to taste), and mix in a bowl until all lumps are gone. Pour into a cake pan, and gently place in a preheated oven at 350F (176.6C) for 30 minutes, or until a toothpick comes out clean when poked in the center.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2612",
        "createdAt": "2023-08-14T15:08:24Z",
        "author": {
            "login": "philtimmes"
        }
    },
    {
        "title": "Getting started.",
        "bodyText": "Hey!\nMy project currently uses GPT3.5 with API calls (which ends up being pretty expensive), and testing out the demo for llama2-70b-chat, it looks like it'd work well enough for at least part of the prompts I'm using.\nBut things seem to be moving pretty fast, and I'm not clear on exactly what is possible and what is the best path forward.\nI have:\n\nA CPU with 12 cores and about 40GB of free RAM\nA GPU (3070) with about 10GB of free VRAM (could get a bit more by turning screens off).\n\nI can get if needed:\n\nA server with any amount of RAM, up to 256GB if needed, and a beefy CPU.\n\nI want:\n\nTo run llama2-70b-chat for my code to query/call with prompts.\n\nMy question is: what's my best path forward?\nI'm been able to get the 7b version running following the instructions, but my local computer doesn't have enough RAM to run the 70b version (thus the mention of a server above, which I'm getting ready to start renting).\nI'm not sure exactly how to run on the GPU, and if I\u00a0have enough RAM to do that or not. I'm not sure I follow the instructions for that.\nI'm also not sure what my options are besides llama.cpp, if any. I found this project, but I'm not sure if there are any others out there I\u00a0could try / that would be a better option for what I'm trying to do here.\nAny help/pointers would be very much appreciated.\nCheers.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2597",
        "createdAt": "2023-08-13T01:28:37Z",
        "author": {
            "login": "arthurwolf"
        }
    },
    {
        "title": "Reverse prompt for line-break?",
        "bodyText": "I'm trying to limit generations to a single line. Normally with a chat the reverse prompt is something like \"User:\" or something of that nature. Then problem I'm having is that some models insert more fluff after the first line. Is there a way to stop generation on a new line? I tried -r \"\\n\" and -r \"\\\\n\" but neither seem to work. I'd appreciate any advice",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2680",
        "createdAt": "2023-08-20T11:18:00Z",
        "author": {
            "login": "Johnhersh"
        }
    },
    {
        "title": "File/Prompt/Instruction input for server",
        "bodyText": "Is there a way to send instructions to the server to give output in one type of format ?\nThere is a syntax in ./main -p where you give instructions, I am looking for something similar on ./server\nIn the server UI, there is a prompt box, it works similarly, any input is appreciated, thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2671",
        "createdAt": "2023-08-19T16:32:16Z",
        "author": {
            "login": "ASH1998"
        }
    },
    {
        "title": "How to give system prompt?",
        "bodyText": "Model will make inference based on context window with c tag-c #### and I think this will only take last #### many tokens in account, which it will forget whatever was said in first prompt or even if first prompt was used through f tag -f chat_with_bob.txt. My goal is to give a system prompt which model can look at before generating new tokens every time for every instruction which can be used through ins tag -ins.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2637",
        "createdAt": "2023-08-16T19:44:33Z",
        "author": {
            "login": "superchargez"
        }
    },
    {
        "title": "Interactive chat run script",
        "bodyText": "It seems like many people are on mac or linux here. I'm on windows myself so I ended up writing a powershell script for running main.exe in chat mode. There are interactive menus and support for model and run settings config files. Prompts can be loaded in .txt or .json format. V2 and Tavern json cards also work. Last run settings are saved so you can rerun last. Only requires latest powershell. No installation.\n\nPowershell run scripts\nI also have a python script for batch extracting json files from V2 and Tavern cards\nCharacter card converter",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2667",
        "createdAt": "2023-08-19T12:36:32Z",
        "author": {
            "login": "ossirytk"
        }
    },
    {
        "title": "Model is offloaded to GPU RAM but execution happens in CPU cores - CLBlast",
        "bodyText": "Hi,\nI was able to compile llama.cpp with CLBlast. However when I run inference, the model layers do get loaded on the GPU Memory (identified by memory utilization) however, the computation is still happening in the CPU core and not in the GPU execution units.\nI've seen similar issues reported with other GPUs but wasn't able to find a soliton that works for me\nCommand used:\nexport LD_LIBRARY_PATH=/opt/intel/oneapi/compiler/latest/linux/compiler/lib/intel64_lin:$LD_LIBRARY_PATH\nGGML_OPENCL_PLATFORM=\"Intel(R) OpenCL Graphics\" GGML_OPENCL_DEVICE=0 ./main -m ~/wrkdir/llama-2-13b.ggmlv3.q4_0.bin -n 128 -ngl 40 -p \"Building a website can be done in 10 simple steps:\" --no-mmap",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2652",
        "createdAt": "2023-08-18T04:22:51Z",
        "author": {
            "login": "sheikmohdimran"
        }
    },
    {
        "title": "Unable to build for CULABS",
        "bodyText": "Hi,\nWhen I try to build\nmake .. -DLLAMA_CUBLAS=ON\ncmake --build . --config Release\n\nI get the following error:\nnvcc fatal   : 'f16c': expected a number\nmake[2]: *** [CMakeFiles/ggml.dir/build.make:104: CMakeFiles/ggml.dir/ggml-cuda.cu.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:460: CMakeFiles/ggml.dir/all] Error 2\nmake: *** [Makefile:146: all] Error 2\n\nMy nvcc version is:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n\nI believe this has to do with CUDA version installed on my Ubuntu. Can someone help me please?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2645",
        "createdAt": "2023-08-17T13:40:20Z",
        "author": {
            "login": "xtrecoolx"
        }
    },
    {
        "title": "Macbook Air M1 (2020) recommendation \"--mlock\" switch.",
        "bodyText": "A very simple PSA: When using this particular machine you should use the --mlock switch.\nWithout it even with a 7B model it's painfully slow for me, perhaps 3 minutes to answer what are the days of the week.\nAs soon as I set that switch, it's almost real time, that is the answer appears at the speed you would say it if you were talking quite slowly.\nI'm just putting this out here to hopefully help someone else out there. I've spent about 15 hours messing with this stuff this week and this is the first moment when I can finally run a model on my laptop offline in a usable manner.\nSo my suggestion is that the this switch should potentially even be the default, when running on machines with a similar configuration. Without it people will think the system is completely unusable, when the reality is that with the switch it feels extremely pleasant to use.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1335",
        "createdAt": "2023-05-05T23:28:05Z",
        "author": {
            "login": "olinorwell"
        }
    },
    {
        "title": "LLaMA Server combines the power of LLaMA C++ with the beauty of Chatbot UI",
        "bodyText": "Hi folks, I wrote LLaMA Server, a small http server to mimic the OpenAI APIs so that you can use Chatbot UI, an open source implementation of ChatGPT UI, to interact with a LLaMA C++ instance. Here is a demo:\n\n  \n    \n    \n\n    demo.mov\n    \n  \n\n  \n\n  \n\n\nHope you enjoy it! Cheers!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/733",
        "createdAt": "2023-04-03T04:13:40Z",
        "author": {
            "login": "nuance1979"
        }
    },
    {
        "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees",
        "bodyText": "https://arxiv.org/abs/2307.13304\nJust found this in r/localllama. Would this be useful?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2510",
        "createdAt": "2023-08-04T03:34:34Z",
        "author": {
            "login": "abceleung"
        }
    },
    {
        "title": "Enable CPU HBM",
        "bodyText": "Xeon Max series CPU provides 64 GB in-package high bandwidth memory (HBM), which could benefit a lot for memory bound workload like matmul. We can enable hbm feature in ggml/llama.cpp to get better performance.\nreference:\nXeon Max Series Product Brief\nIntel\u00ae Xeon\u00ae CPU Max Series Configuration and Tuning Guide",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2602",
        "createdAt": "2023-08-14T05:16:07Z",
        "author": {
            "login": "jikunshang"
        }
    },
    {
        "title": "ggml.c syntax errors when trying to build with cmake",
        "bodyText": "I'm following the instructions in the Readme exactly as specified. The cmake .. command worked just fine, but when I run cmake --build . --config Release I am immediately confronted with more than a hundred syntax errors, starting with ggml.h(811,63): error C2146: syntax error: missing ')' before identifier 'x' and ending with ggml.c(10843,31): fatal error C1003: error count exceeds 100; stopping compilation.\nAny suggestions for what the issue might be?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/952",
        "createdAt": "2023-04-13T21:18:40Z",
        "author": {
            "login": "gamesaucer"
        }
    },
    {
        "title": "Anyone keeping tabs on Vicuna, a new LLaMA-based model?",
        "bodyText": "Link to blog post,  demo and GH: https://vicuna.lmsys.org/, https://chat.lmsys.org/, https://github.com/lm-sys/FastChat\nThis looks like the most capable LLaMA right now. They're yet to release the weights :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/643",
        "createdAt": "2023-03-31T01:36:20Z",
        "author": {
            "login": "jessejohnson"
        }
    },
    {
        "title": "Please delete this",
        "bodyText": "",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2595",
        "createdAt": "2023-08-12T22:04:22Z",
        "author": {
            "login": "JacobMJones"
        }
    },
    {
        "title": "KeyError: ('torch', 'BoolStorage') when converting NSQL model",
        "bodyText": "Hi all,\nForgive me if this is a stupid question as I am still learning things with llama.cpp.\nI am trying to convert https://huggingface.co/NumbersStation/nsql-350M to llama.cpp using the convert.py script.\nWhen I do so, I get the following error:\nLoading model file nsql-350M/pytorch_model.bin\nTraceback (most recent call last):\n  File \"/content/llama.cpp/convert.py\", line 1326, in <module>\n    main()\n  File \"/content/llama.cpp/convert.py\", line 1306, in main\n    model_plus = load_some_model(args.model)\n  File \"/content/llama.cpp/convert.py\", line 1221, in load_some_model\n    models_plus.append(lazy_load_file(path))\n  File \"/content/llama.cpp/convert.py\", line 1012, in lazy_load_file\n    return lazy_load_torch_file(fp, path)\n  File \"/content/llama.cpp/convert.py\", line 883, in lazy_load_torch_file\n    model = unpickler.load()\n  File \"/content/llama.cpp/convert.py\", line 872, in find_class\n    return self.CLASSES[(module, name)]\nKeyError: ('torch', 'BoolStorage')\n\nDo you have any tips on what I can do to get around this? Can I change the module type from BoolStorage to IntStorage or similar if it is already pretrained?\nThanks in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2587",
        "createdAt": "2023-08-11T18:47:50Z",
        "author": {
            "login": "alishobeiri"
        }
    },
    {
        "title": "ModelLoader:llama.cpp and Model: ggml-model-q4_0.bin",
        "bodyText": "I got the error after loading ggml-model-q4_0.bin model by llama.cpp loader: assert self.model is not None from llama.py",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2586",
        "createdAt": "2023-08-11T13:07:36Z",
        "author": {
            "login": "Sara681820"
        }
    },
    {
        "title": "A sequence repetition penalty sampler - how would you expect it to work?",
        "bodyText": "The existing repetition and frequency/presence penalty samplers have their use but one thing they don't really help with is stopping the LLM from repeating a sequence of tokens it's already generated or from the prompt. It seems like adding a way to penalize repeating sequences would be pretty useful.\nJust for example, say we have token ids 1, 2, 3, 4, 1, 2, 3 in the context currently. If the LLM generates token 4 at this point, it will repeat the sequence 1, 2, 3, 4 which already exists in the context. So we could penalize 4 to prevent this and to try to force the LLM to take a new path (there are different ways to approach this like a flat penalty, a penalty based on the length of the sequence it would be continuing, etc). Implementing this is pretty simple.\nWe probably also want to allow fuzzy matching. Allowing, a match with 1, 2, 3, 4, 1, 0, 3 to still match like the first example and penalize 4 because 1, 0, 3, 4 would be sufficiently similar to 1, 2, 3, 4. This is also pretty straightforward. You basically can give the matching algo credits for failed matches. If the tokens don't match, you count as a match anyway if there are credits remaining and decrement the credits. If there are no credits, then you can abort and start trying to match from a new position.\nNow where it gets really complicated: Suppose in addition to allowing 1:1 fuzzy matching you might want to let the credit apply to more than one consecutive non-matching token. I really haven't figured out a good way to do this or even exactly what results I'd expect. The first issue is if the match fails do we merge consecutive non-matching tokens in the \"needle\" or the \"haystack\"? Also, greedily consuming non-matching tokens like that can potentially be worse than doing nothing. You want to consume just enough non-matching tokens such that the next iteration results in a match.\nWhy should llama.cpp people care about this question? Well, if I can figure out a good answer I intend to write a C version of this sampler and contribute it to the project.\nI have some prototype Python code that... does something.\nUsing these parameters - minimum match length 3, failed match credits 1, allow merging up to 2 consecutive non-matching tokens:\nWith 1, 6, 6, 3, 1, 2, 3, 4, 1, 2, 3, this is what I get:\n\n1, 6, 6, 3, {1, 2, 3, 4}, [@1, 2, 3]\n1, 6, 6, 3, {1, 2, 3}, @4, [1, 2, 3]\n1, 6, 6, {3, 1, 2, 3}, [@4, 1, 2, 3]\n{1, 6, 6, 3}, @1, 2, 3, 4, [1, 2, 3]\n\nCurly braces around the \"haystack\" part of the patch, @ in front of the token that would be penalized as continuing a sequence and square braces around the \"needle\" part of the match. (Don't know if there's a better way to refer to it: this would be the end of the context where the next generated token might complete a sequence from all generated/prompt tokens).\nThen with 1, 2, 3, 1, 2, 3, 4, 1, 6, 6, 3:\n\n1, 2, 3, {1, 2, 3}, @4, [1, 6, 6, 3]\n{1, 2, 3}, @1, 2, 3, 4, [1, 6, 6, 3]\n\nHere is some horrendous Python code:\ndef m4(l, min_len = 3, tolerance = 0, merge = 0):\n  print(f'({min_len}) -- {l}\\n')\n  llen = len(l)\n  if llen < min_len + 1:\n    return []\n  sstart = llen - 2\n  result = []\n  while sstart >= min_len - 1:\n    currtol = tolerance\n    nidx = llen - 1\n    hidx = sstart\n    sstart -= 1\n    good = 0\n    print('===========')\n    while nidx >= 0 and hidx >= 0:\n      ok = l[nidx] == l[hidx]\n      print(f'  [{good:>2}] {nidx:>2}, {hidx:>2}  |  {l[nidx]} {(\"==\" if ok else \"!=\")} {l[hidx]}')\n      if ok:\n        seekn = 1\n        seekh = 1\n      elif currtol < 1:\n        print('  !!!')\n        break\n      elif merge > 0:\n        currtol -= 1\n        snidx = nidx - 1\n        shidx = hidx - 1\n        foundn = None\n        foundh = None\n        if hidx > 0:\n          for dist in range(1, merge + 2):\n            if nidx - dist < 0 or nidx - dist <= hidx - 1 :\n              break\n            if l[nidx - dist] == l[hidx - 1]:\n              foundn = dist\n        if nidx > 0:\n          for dist in range(1, merge + 2):\n            if hidx - dist < 0:\n              break\n            if l[hidx - dist] == l[nidx - 1]:\n              foundh = dist\n        print(f'   ?? seek - n: {foundn or \"!\"}, h: {foundh or \"!\"}')\n        if foundn is not None and foundh is not None:\n          if foundn < foundh:\n            seekn = foundn\n            seekh = 1\n          else:\n            seekn = 1\n            seekh = foundh\n        else:\n          seekn = 1 if foundn is None else foundn\n          seekh = 1 if foundh is None else foundh\n      good += 1\n      if good >= min_len:\n        rn = l[nidx - (seekn - 1):]\n        rh = l[hidx - (seekh - 1):sstart + 2]\n        result.append((rh, l[sstart + 2], rn))\n        print('  ^^^^ H:', rh, '@', l[sstart + 2])\n        print('  ^^^^ N:', rn)\n        print()\n      nidx -= seekn\n      hidx -= seekh\n  return result\n\nresult = m4([1, 6, 6, 3, 1, 2, 3, 4, 1, 2, 3], 3, 1, 1)\nprint(f'Got: {result}')\nprint('\\n\\n###################\\n')\nresult = m4([1, 2, 3, 1, 2, 3, 4, 1, 6, 6, 3], 3, 1, 1)\nprint(f'Got: {result}')\nIf it's not clear what it's doing, each iteration of the outer loop decrements where the \"haystack\" starts (and we match in reverse order, decrementing toward the start of the sequence). Matching vs the \"needle\" always starts from the every end. Matching the haystack starts from the penultimate token (since you'd want to match 1, 1, 1, 1). If there's no match and there are credits remaining, then it tries to find the distance to to a token that can match with both the needle and the haystack and chooses the shortest one if both exist. I.E. If we can merge 2 needle tokens to get a match or 1 haystack token to get a match, the latter will get chosen.\n\nAny discussion or tips for known algorithms to perform this kind of matching in a sane way are very welcome. Maybe a version without the merging consecutive non-matches would still be good enough to be worth implementing?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2581",
        "createdAt": "2023-08-10T22:13:34Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Unable to inference on Quantized 70B Model using llama.cpp",
        "bodyText": "Got error :\nerror loading model: llama.cpp: tensor 'layers.0.attention.wk.weight' has wrong shape; expected 8192 * 8192, got 8192 * 1024\nI am using ubuntu linux",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2575",
        "createdAt": "2023-08-10T08:57:09Z",
        "author": {
            "login": "vatsarishabh22"
        }
    },
    {
        "title": "Metal: Support older iOS devices that not supported SIMD?",
        "bodyText": "Currently working on using llama.cpp in iOS projects.\nAlthough our main goal is to run on M1/M2. But started testing with my 2019 iPad Pro (A12Z). Unfortunately, I found that Metal cannot compile some kernel functions normally, the reason is that simd_sum is not supported in my device.\nI found out that SIMD-scoped permute operations is indeed supported after A13 (apple6) in https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf.\nNot an urgent problem, just curious if there is a way we can override this by a custom function. I'm very new to Metal.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2550",
        "createdAt": "2023-08-08T08:30:13Z",
        "author": {
            "login": "jhen0409"
        }
    },
    {
        "title": "Simple tutorial for beginers",
        "bodyText": "I need run a llama localy on my computer.\nHow starting? I use linux.\nWhere is models, how download it, where put and how starting using it.\nAnybody can help",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1166",
        "createdAt": "2023-04-24T19:12:06Z",
        "author": {
            "login": "srem1"
        }
    },
    {
        "title": "How to create grammars?",
        "bodyText": "I've looked at the basic BNF syntax and examples in the repo, but when trying to create my own it just crashes with LLAMA_ASSERT(false) on line 2203.\n# command responses are in the following format:\n# command [arg1] [arg2]\nroot ::= command argument*\nargument ::= \"[\" ([a-zA-Z_/,.]* ws)* \"]\"\ncommand ::= [a-zA-Z_] [a-zA-Z_0-9]*\nws ::= [ \\t]*\n\nIt's got something to do with the 'argument' part, it isn't a problem when I comment out the declaration and usage.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2560",
        "createdAt": "2023-08-08T20:38:22Z",
        "author": {
            "login": "amusingimpala75"
        }
    },
    {
        "title": "Was able to run it locally on my phone \ud83e\udd72\ud83d\udc4d",
        "bodyText": "Screen_Recording_20230803_041734_Pydroid.3.mp4",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2497",
        "createdAt": "2023-08-03T02:24:01Z",
        "author": {
            "login": "Mrgithub93"
        }
    },
    {
        "title": "Help for set Llama.cpp public",
        "bodyText": "Hi I would like to put llama.cpp public, every time I start it i gave me this output \"To create a public link, set share=True in launch().\" where is the file with that option?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2523",
        "createdAt": "2023-08-05T11:52:16Z",
        "author": {
            "login": "rogercada"
        }
    },
    {
        "title": "How do I use the c api?",
        "bodyText": "",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2527",
        "createdAt": "2023-08-05T16:32:56Z",
        "author": null
    },
    {
        "title": "Metal build with Swift macOS app?",
        "bodyText": "I have a macOS app going in Xcode that starts the server binary as a subprocess and hits it on localhost for completions. That\u2019s working pretty well but if I try to use a binary built with LLAMA_METAL=1 I see errors like this in the server output:\nggml_metal_init: using MPS ggml_metal_init: loading '(null)'\nggml_metal_init: error: Error Domain=NSCocoaErrorDomain Code=258 \"The file name is invalid.\"\n\nIt looks like it\u2019s not finding the metal file. I\u2019ve tried including ggml-metal.metal both as a source file and as a resource but I still just see the above errors.\nHas anyone had luck with this or know what I\u2019m missing? Thanks!!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2493",
        "createdAt": "2023-08-02T19:47:04Z",
        "author": {
            "login": "psugihara"
        }
    },
    {
        "title": "Request: add support for Cerebras GPT models just released",
        "bodyText": "The announcement is here:\nhttps://www.cerebras.net/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems\nThe models are available here:\nhttps://huggingface.co/cerebras\nExcerpts:\n\"SUNNYVALE, CALIFORNIA \u2013 March 28, 2023 \u2013 Cerebras Systems, the pioneer in artificial intelligence (AI) compute for generative AI, today announced it has trained and is releasing a series of seven GPT-based large language models (LLMs) for open use by the research community. This is the first time a company has used non-GPU based AI systems to train LLMs up to 13 billion parameters and is sharing the models, weights, and training recipe via the industry standard Apache 2.0 license. All seven models were trained on the 16 CS-2 systems in the Cerebras Andromeda AI supercomputer.\"\n\"Cerebras\u2019 release today directly addresses these issues. In a first among AI hardware companies, Cerebras researchers trained, on the Andromeda AI supercomputer, a series of seven GPT models with 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B parameters.\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/579",
        "createdAt": "2023-03-28T15:29:40Z",
        "author": {
            "login": "Topping1"
        }
    },
    {
        "title": "Support BTLM models from  Cerebras",
        "bodyText": "Looks like arch is bashed on Cerebras-GPT\nso should work ?\nanyway i donot see any info about it if anyone gave it a try.\ni always used ggml file converted by other people so have not tried.\nunlike previous models bashed on Cerebras-GPT this one looks like much more capable for it's  class even challenging 7b models.\nuses slimredpajama 600billion tokens cleanup from redpajama dataset\nhttps://www.cerebras.net/blog/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/\nPrevious Cerebras-GPT Disussion",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2419",
        "createdAt": "2023-07-27T08:44:46Z",
        "author": {
            "login": "forrackun"
        }
    },
    {
        "title": "Metal with Zig",
        "bodyText": "I'm not sure where to ask because it's not strictly an issue with llama.cpp but when I try to use llama.cpp with Zig (with metal enabled), I get a really weird bug in runtime, which looks like this:\n\nWhen I compile llama.cpp using make, it works fine, it only fails with Zig, and what's especially interesting, it fails during runtime, when the metal shaders are compiled. I've checked that the metal file is the same, even the llama.cpp codebase is the same, everything is the same, except the compiler.\nSo I was thinking okay, maybe apple clang links something in a different way but even the otool and DYLD_PRINT_LIBRARIES showed exactly the same thing.\nAny ideas would be welcome, thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2264",
        "createdAt": "2023-07-18T18:44:18Z",
        "author": {
            "login": "cztomsik"
        }
    },
    {
        "title": "does llama.cpp support Retrieval Augmented Generation (RAG)",
        "bodyText": "Hello All\nIs it possible to train custom data on top of llama.cpp with RAG",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2502",
        "createdAt": "2023-08-03T16:22:19Z",
        "author": {
            "login": "akoripelly"
        }
    },
    {
        "title": "Is the Meta's license needed to use these models?",
        "bodyText": "Is the Meta's license needed to use these ggml models for llama2 and llama-chat versions?\nmodel card on the huggingface doesn't mention anything about the license requirement.\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2472",
        "createdAt": "2023-07-31T17:44:00Z",
        "author": {
            "login": "harithagmu"
        }
    },
    {
        "title": "Comparable performance of GGML with GPTQ -- Is this a fair comparison? Are there any benchmarks out there?",
        "bodyText": "Dear all,\nWhile comparing TheBloke/Wizard-Vicuna-13B-GPTQ with TheBloke/Wizard-Vicuna-13B-GGML, I get about the same generation times for GPTQ 4bit, 128 group size, no act order; and GGML, q4_K_M. In both cases I'm pushing everything I can to the GPU; with a 4090 and 24gb of ram, that's between 50 and 100 tokens per second (GPTQ has a much more variable inference speed; GGML is pretty steady at ~82 tokens per second).\nIs this a realistic comparison? In that case, congratulations! I have watched the project grow over time and it gives me much more peace of mind to be able to run these models without all the python overhead and impossible-to-install libraries.\nThanks!\nPS: I'm interested on seeing regular benchmarks being created, as discussed here #2038 -- but it doesn't look like this is happening any time soon?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2424",
        "createdAt": "2023-07-27T13:15:57Z",
        "author": {
            "login": "longregen"
        }
    },
    {
        "title": "Question: please tell me more how --batch-size affects prompt ingestion",
        "bodyText": "My understanding is that unless --batch-size matches the prompt length, the model will not evaluate the weights of all tokens and may in fact not consider all the information in the prompt when generating a response.\nIn one particular use case, I am feeding the model a list of news summaries and I want it to give me an overall conclusion based on all the news summaries. If I use --batch-size that is shorter than the prompt length, the model will not actually look at all the summaries, but will move the --batch-size window across the prompt and summarise only the tokens that fit inside --batch-size.\nIf this is correct, then the use of --batch-size is highly problematic and in effect no different to using embeddings and a search over a vector store, but in a simple fashion rather than actually creating embeddings and indexing them.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2463",
        "createdAt": "2023-07-31T06:48:11Z",
        "author": {
            "login": "vmajor"
        }
    },
    {
        "title": "Implementation of ReAct framework",
        "bodyText": "I saw the file on this repo in prompts folder that hints the use of concept. My question is, whether is it only a concept or has it been implemented? If it is implemented then how was it done, otherwise please make it possible to use tools like python REPL and shell and internet search (either through headless browser or google search API or duckduckgo API).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2464",
        "createdAt": "2023-07-31T08:54:30Z",
        "author": {
            "login": "superchargez"
        }
    },
    {
        "title": "Setting logit biases as you go instead of just at start.",
        "bodyText": "It'd be really nice if there were a way to dynamically change the logit biases in interactive mode or via the llama.cpp bindings instead of just setting them once at the start of the run. Being able to turn on and off biases would limit the side effects biasing which sometimes result in nonsense outputs after the desired tokens are output.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2457",
        "createdAt": "2023-07-30T19:10:54Z",
        "author": {
            "login": "superkuh"
        }
    },
    {
        "title": "Where do we get the user input in examples/main.cpp?",
        "bodyText": "I'm trying to understand the code, but I still couldn't find where main.cpp gets the input from. I searched the project for std::cin, but couldn't find any references. It reads the template from the file when -f is passed, but where does it retrieve the user input?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2462",
        "createdAt": "2023-07-29T20:20:04Z",
        "author": {
            "login": "tokenizer-decode"
        }
    },
    {
        "title": "Trying to run in a docker container on CPU throws LLAMA_ASSERT: /workspace/llama-util.h:101: ret == 0 Aborted",
        "bodyText": "Hello, I've been having a blast with llama-cpp, thanks so much to everyone working hard on the development of this.\nI'm trying to run the binary inside of a linux docker container. It runs fine on my host machine (intel i7, no gpu) when compiled for mac os, but when trying to execute this in multiple different flavors of linux inside docker, I get this error:\nroot@903bba1630aa:/workspace# ./build/bin/main -m ./model_bins/llama-2-7b-chat.ggmlv3.q4_0.bin \nmain: build = 919 (edcc7ae)\nmain: seed  = 1690657968\nLLAMA_ASSERT: /workspace/llama-util.h:101: ret == 0\nAborted\n\nI'm not sure exactly what this means or how to reconcile it. If anyone here is capable of shedding some light on this/helping me troubleshoot, it would be greatly appreciated :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2447",
        "createdAt": "2023-07-29T19:16:23Z",
        "author": {
            "login": "michaelbauerinc"
        }
    },
    {
        "title": "bark.cpp? New TTS model released",
        "bodyText": "Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio \u2013 including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. They are providing access to pretrained model checkpoints ready for inference.\nRepo: https://github.com/suno-ai/bark\nExamples: https://suno-ai.notion.site/Bark-Examples-5edae8b02a604b54a42244ba45ebc2e2\nIt would be awesome to have a CPP version of Bark :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1089",
        "createdAt": "2023-04-20T20:52:58Z",
        "author": {
            "login": "r3nor"
        }
    },
    {
        "title": "What is default/simple option for LoRA training for models this project supports?",
        "bodyText": "I am assuming that this project has only LoRA enabled inference and not train.\nAre there any ready to go examples of how to train my own LoRA dataset on llama-2 (vicuna?) especially using binarized PLM weights (and ggml data format?).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2435",
        "createdAt": "2023-07-28T08:23:59Z",
        "author": {
            "login": "MaratZakirov"
        }
    },
    {
        "title": "Which specfic file is the \"added_tokens.json\" file required for GPT4ALL?",
        "bodyText": "What is the \"added_tokens.json\" file required to use GPT4ALL?  The instructions say to use \"one from Alpaca\".  But there are many many different projects based on Alpaca which have that in the name, and many different variations of \"added_tokens.json\".  I am seeing mostly blank versions of such a file whenever I look for one.  I have no idea which file I actually need.  Can someone provide the exact file, or at least the SHA256 hash of the necessary file?\nFor reference, my SHA-256 of GPT4ALL's model is \"05c9dc0a4904f3b232cffe717091b0b0a8246f49c3f253208fbf342ed79a6122 *gpt4all-lora-quantized.bin\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1245",
        "createdAt": "2023-04-29T22:21:48Z",
        "author": {
            "login": "Dwedit"
        }
    },
    {
        "title": "zig bindings not working",
        "bodyText": "i was trying to implement examples/simple in zig but code is too slow and does not work. pretty sure i am doing something stupid. i need a good set of eyes. i am porting to zig cz i have other experiments in mind that i want to try.\nconst std = @import(\"std\");\n\nconst c = @cImport({\n    @cInclude(\"llama.h\");\n});\n\nvar gpa_instance = std.heap.GeneralPurposeAllocator(.{}){};\nconst gpa = gpa_instance.allocator();\n\npub fn main() !void {\n    c.llama_backend_init(false);\n\n    var lparams = c.llama_context_params{\n        .seed = 1,\n        .n_ctx = 512,\n        .n_batch = 512,\n        .n_gpu_layers = 0,\n        .main_gpu = 0,\n        .tensor_split = null,\n        .rope_freq_base = 10000.0,\n        .rope_freq_scale = 1.0,\n        .progress_callback = null,\n        .progress_callback_user_data = null,\n        .low_vram = false,\n        .f16_kv = false,\n        .logits_all = false,\n        .vocab_only = false,\n        .use_mmap = true,\n        .use_mlock = false,\n        .embedding = false,\n    };\n\n    std.debug.print(\"Loading llama context with > \\n {}\", .{lparams});\n\n    var model = c.llama_load_model_from_file(\"./models/orca-mini-3b.ggmlv3.q4_0.bin\", lparams);\n    if (model == null) {\n        @panic(\"unreachable!\");\n    }\n\n    var ctx = c.llama_new_context_with_model(model, lparams);\n    if (ctx == null) {\n        @panic(\"unreachable!\");\n    }\n\n    std.debug.print(\"CTX {any} {any} \\n\", .{ ctx, @TypeOf(ctx) });\n\n    var prompt = \"Capital of Nepal is \";\n    const prompt_size = prompt.len;\n    const max_context_size = c.llama_n_ctx(ctx);\n\n    std.debug.print(\"max_ctx_size {d} \\n\", .{max_context_size});\n\n    var token_list = try std.ArrayList(c_int).initCapacity(gpa, @intCast(max_context_size));\n    _ = token_list.addManyAsArrayAssumeCapacity(prompt_size);\n\n    std.debug.print(\"token_list_size {d} \\n\", .{token_list.items.len});\n\n    defer token_list.deinit();\n\n    const n = c.llama_tokenize(ctx, @as([*]const u8, prompt.ptr), @as([*c]c_int, token_list.items.ptr), prompt_size, false);\n\n    std.debug.print(\"model > \\n {?} {d} \\n\", .{ model, n });\n\n    std.debug.print(\"[PROMPT_START]\\n\", .{});\n\n    var subtoken_list = token_list.items[0..@as(usize, @intCast(n))];\n\n    for (subtoken_list) |token| {\n        std.debug.print(\"{s}\", .{c.llama_token_to_str(ctx, token)});\n    }\n\n    std.debug.print(\"\\n[/PROMPT_END]\\n\", .{});\n\n    try token_list.resize(@intCast(n));\n\n    while (c.llama_get_kv_cache_token_count(ctx) < max_context_size) {\n        if (c.llama_eval(ctx, token_list.items.ptr, @intCast(token_list.items.len), c.llama_get_kv_cache_token_count(ctx), 4) == 1) {\n            @panic(\"Failed to eval\");\n        }\n\n        std.debug.print(\"\\n[AFTER_EVAL]\\n\", .{});\n\n        var logits = c.llama_get_logits(ctx);\n        var n_vocab = c.llama_n_vocab(ctx);\n\n        var candidates = try gpa.alloc(c.llama_token_data, @intCast(n_vocab));\n\n        for (0..@intCast(n_vocab)) |token_id| {\n            var index = @as(usize, @intCast(token_id));\n            var data = c.llama_token_data{ .id = @intCast(token_id), .logit = logits[index], .p = 0.0 };\n            candidates[index] = data;\n\n            // std.debug.print(\"\\n[CANDIDATES] {n} {s}\\n\", .{ token_id, c.llama_token_to_str(ctx, logits[index]) });\n        }\n\n        var cdata = c.llama_token_data_array{\n            .data = candidates.ptr,\n            .size = candidates.len,\n            .sorted = false,\n        };\n\n        var selected_token = c.llama_sample_token_greedy(ctx, &cdata);\n\n        if (selected_token == c.llama_token_eos()) {\n            std.debug.print(\"[EOS]\\n\", .{});\n            break;\n        }\n\n        std.debug.print(\"{s}\", .{c.llama_token_to_str(ctx, selected_token)});\n\n        try token_list.append(selected_token);\n        gpa.free(candidates);\n    }\n\n    c.llama_free(ctx);\n    c.llama_free_model(model);\n    c.llama_backend_free();\n}\n\n\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/simple/simple.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2403",
        "createdAt": "2023-07-26T15:02:52Z",
        "author": {
            "login": "forrackun"
        }
    },
    {
        "title": "Some more observations regarding `QX_0` quantizations with LLaMAv2 7B",
        "bodyText": "In #2276 I reported an unusual behavior of LLaMAv2 7B when using Q4_0 and Q5_0 quantizations. In short, the observation is that with short prompts such as \"I believe the meaning of life is\" after the end of the first sentence, the generation switches into some weird mode - for example, often generating text in some other language, starting with non-capital letter, etc. This only happens if the sentence ends with .. I'e it doesn't occur when it ends with ! and ?. I find this weird and it has been bugging me ever since.\nToday I noticed in the quantize tool output that the tensors in layers 0 and 1 have significantly different weight distributions compared to the other layers:\n\n$ \u25b6 make -j && ./quantize ./models/7B-v2/ggml-model-f16.bin ./models/7B-v2/ggml-model-q4_0.bin q4_0\nI llama.cpp build info: \nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI LDFLAGS:  \nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n\nmain: build = 917 (1a94186)\nmain: quantizing './models/7B-v2/ggml-model-f16.bin' to './models/7B-v2/ggml-model-q4_0.bin' as Q4_0\nllama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\nllama.cpp: saving model to ./models/7B-v2/ggml-model-q4_0.bin\n[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f32, quantizing to q4_0 .. size =   500.00 MB ->    70.31 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n[   3/ 291]                        output.weight -     4096 x 32000, type =    f32, quantizing to q6_K .. size =   500.00 MB ->   102.54 MB | hist: \n[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.034 0.008 0.012 0.019 0.031 0.050 0.084 0.149 0.256 0.150 0.084 0.051 0.031 0.019 0.012 0.010 \n[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.034 0.008 0.013 0.021 0.033 0.054 0.089 0.150 0.226 0.151 0.089 0.054 0.033 0.021 0.013 0.011 \n[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.036 0.053 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.024 0.020 \n[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.035 0.011 0.017 0.028 0.044 0.068 0.100 0.135 0.155 0.135 0.100 0.068 0.044 0.028 0.017 0.014 \n[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.013 0.022 0.034 0.052 0.074 0.098 0.121 0.132 0.121 0.098 0.074 0.052 0.034 0.022 0.018 \n[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.013 0.022 0.034 0.051 0.074 0.099 0.121 0.132 0.121 0.099 0.074 0.051 0.034 0.022 0.018 \n[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.014 0.023 0.035 0.052 0.073 0.097 0.119 0.130 0.119 0.097 0.074 0.052 0.035 0.023 0.019 \n[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.031 0.047 0.070 0.098 0.129 0.146 0.129 0.099 0.070 0.047 0.031 0.020 0.016 \n[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.096 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.120 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.026 0.021 \n[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f32, quantizing to q4_0 .. size =    64.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.015 0.023 0.036 0.054 0.075 0.098 0.116 0.124 0.116 0.098 0.075 0.054 0.036 0.023 0.019 \n[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f32, quantizing to q4_0 .. size =   172.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\nllama_model_quantize_internal: model size  = 25705.02 MB\nllama_model_quantize_internal: quant size  =  3647.87 MB\nllama_model_quantize_internal: hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n\nmain: quantize time = 12956.85 ms\nmain:    total time = 12956.85 ms\n\n\nThis discrepancy is also observed to similar extend for LLaMAv1.\nSo I decided to increase the quantization accuracy of the tensors in layer 0 and 1 to see what happens.\nThe weird behavior disappeared. I tried doing the same for other layers and it does not help.\nMore strange is that after a few more experiments, the only tensor that I have to quantize more accurately (i.e. either Q6_K or Q8_0) to fix the crazy texts is layers.1.feed_forward.w2.\nFor example, here are the top token probability for the first token in the second sentence using vanilla Q4_0:\nmake -j main && time ./main -m ./models/7B-v2/ggml-model-q4_0.bin -p \"I believe the meaning of life is to be happy.\" --no-mmap -t 8 -n 32 --ignore-eos -eps 1e-5\n\n...\n I believe the meaning of life is to be happy.\n' nobody': 0,114565\n' everybody': 0,101579\n' Hinweis': 0,091655\n' Unterscheidung': 0,075012\n' sierp': 0,060685\n' Einzeln': 0,059432\n' \u0436\u0438\u0432\u0435\u043b\u043e': 0,045674\n' kwiet': 0,039094\n' hopefully': 0,036275\n' \u0444\u0435\u0432': 0,031292\n' \u043e\u043a\u0440\u0443\u0433\u0443': 0,028931\n' Begriffe': 0,026573\n' pa\u017adzier': 0,026076\n' \u0441\u0430\u0432\u0435\u0437': 0,020107\n'\u2113': 0,017165\n'\u2796': 0,016931\n'nahm': 0,015712\n' surely': 0,015097\n' \u043f\u0440\u043e\u0444': 0,014356\n'\u03ca': 0,013367\n' \u0431\u0440\u043e\u0458\u0430': 0,013126\n' gepr\u00fcft': 0,012605\n'();`': 0,012045\n' \u0441\u0430\u0439\u0442': 0,011995\n' obviously': 0,011706\n' pr\u00fc': 0,010286\n'iellement': 0,010194\n' stycz': 0,009988\n' r\u00e9f\u00e9rences': 0,009927\n'\u0435\u0433\u043e': 0,008764\n' \u0412\u0438\u043a\u0438\u043f\u0435\u0434\u0438': 0,008759\n' \u0444\u0440\u0430\u043d': 0,008071\n' everyone': 0,007789\n'\u2609': 0,007600\n'\u2102': 0,007566\nYou can see, these tokens don't make sense for a start of a new sentence.\nHere are the probabilities with layers.1.feed_forward.w2 quantized with Q6_K, everything else is the same:\n I believe the meaning of life is to be happy.\n' Person': 0,355184\n' This': 0,063345\n' By': 0,059133\n' And': 0,050152\n' We': 0,037688\n' It': 0,027675\n' My': 0,027099\n' CD': 0,025872\n' That': 0,021084\n' ag': 0,020682\n' Thus': 0,018213\n' Will': 0,018031\n' I': 0,016000\n' Pub': 0,015950\n'weig': 0,015315\n' Like': 0,015251\n' If': 0,015041\n' To': 0,014383\n'',': 0,013128\n' Each': 0,012909\n' Sub': 0,012148\n' Ple': 0,012058\n' Short': 0,011744\n' These': 0,011645\n' Are': 0,009932\n' When': 0,009854\n' You': 0,009733\n'\n': 0,009512\n' Context': 0,009323\n'cd': 0,009190\n' Pur': 0,009154\n' As': 0,009126\n' Due': 0,009001\n' Being': 0,008896\n'yen': 0,008498\n' Because': 0,008050\nLooks much better and more consistent with full F16 precision token candidates:\n I believe the meaning of life is to be happy.\n'\n': 0,333378\n' It': 0,079239\n' I': 0,078752\n' We': 0,063356\n' The': 0,057728\n' You': 0,045964\n' If': 0,035171\n' That': 0,032833\n' And': 0,026973\n' To': 0,026758\n' What': 0,018570\n' Life': 0,018177\n' This': 0,018078\n' In': 0,016524\n' As': 0,016200\n' When': 0,015222\n' There': 0,013985\n' So': 0,012524\n' A': 0,011788\n' Every': 0,009176\n' But': 0,009140\n' Being': 0,007576\n' Happy': 0,007533\n' My': 0,007494\n' No': 0,006082\n' How': 0,005494\n' For': 0,004866\n' By': 0,004644\n' At': 0,004444\n' Why': 0,004330\n' Love': 0,004115\n' People': 0,003883\nThought this is an interesting observation to share.\nHere is a diff patch if anyone wants to play with this further:\ndiff --git a/llama.cpp b/llama.cpp\nindex 9a8ecdc..3816f95 100644\n--- a/llama.cpp\n+++ b/llama.cpp\n@@ -2740,9 +2740,12 @@ llama_token llama_sample_token(struct llama_context * ctx, llama_token_data_arra\n \n     std::vector<float> probs;\n     probs.reserve(candidates->size);\n+    printf(\"\\n\");\n     for (size_t i = 0; i < candidates->size; ++i) {\n         probs.push_back(candidates->data[i].p);\n+        printf(\"'%s': %f\\n\", llama_token_to_str(ctx, candidates->data[i].id), candidates->data[i].p);\n     }\n+    exit(0);\n \n     std::discrete_distribution<> dist(probs.begin(), probs.end());\n     auto & rng = ctx->rng;\n@@ -2936,7 +2939,13 @@ static void llama_model_quantize_internal(const std::string & fname_inp, const s\n         } else {\n             new_type = quantized_type;\n #ifdef GGML_USE_K_QUANTS\n-            if (tensor.name == \"output.weight\") {\n+            //if (tensor.name == \"output.weight\") {\n+            if (tensor.name == \"output.weight\" ||\n+                //tensor.name.find(\"layers.0.attention.\") != std::string::npos ||\n+                //tensor.name.find(\"layers.0.feed_forward.\") != std::string::npos ||\n+                //tensor.name.find(\"layers.1.attention.\") != std::string::npos ||\n+                //tensor.name.find(\"layers.1.feed_forward.w2\") != std::string::npos ||\n+                false) {\n                 int nx = tensor.ne.at(0);\n                 int ny = tensor.ne.at(1);\n                 if (nx % QK_K == 0 && ny % QK_K == 0) {",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2421",
        "createdAt": "2023-07-27T09:57:21Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "./server crash with two simultaneous requests",
        "bodyText": "Hi!\nI opened two browser tabs and started two concurrent requests. I see ./server crashing 100% of times. Here is gdb bt from the generated core dump:\n(base) llama-cpp-user@9f4d566a4cce:~/llama.cpp/build/bin$ gdb ./server core.87897\n...\n\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n--Type <RET> for more, q to quit, c to continue without paging--\nCore was generated by `./server -m /models/llama-2-13b-chat.ggmlv3.q2_K.bin --host 0.0.0.0'.\nProgram terminated with signal SIGSEGV, Segmentation fault.\n#0  0x0000564eaee0921a in ggml_graph_compute_thread ()\n[Current thread is 1 (Thread 0x7fab5c3f9700 (LWP 91106))]\n(gdb) bt\n#0  0x0000564eaee0921a in ggml_graph_compute_thread ()\n#1  0x00007fad19a3aea7 in start_thread (arg=<optimized out>) at pthread_create.c:477\n#2  0x00007fad1995aa2f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\n(gdb)\n\nIs this happening only for me? I'm running latest code.\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2369",
        "createdAt": "2023-07-24T10:42:22Z",
        "author": {
            "login": "aospan"
        }
    },
    {
        "title": "error -1001 at ggml-opencl.cpp:965",
        "bodyText": "I've followed the build guide for CLBlast in the README - I've installed opencl-headers and compiled OpenCL from source as well as CLBlast and then built the whole thing with cmake.\nWhen I tried to run ./main, I get this weird assertion error:\nllamacpp$ ./main\nmain: build = 874 (7d5f184)\nmain: seed  = 1689986093\nggml_opencl: clGetPlatformIDs(NPLAT, platform_ids, &n_platforms) error -1001 at ggml-opencl.cpp:965\n\nAny ideas?\nThank you!\nOS - WSL2 Ubuntu 22.04 with CUDA support (confirmed CUDA support with pytorch)\nGPU - Nvidia GeForce RTX 3080",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2316",
        "createdAt": "2023-07-22T00:44:17Z",
        "author": {
            "login": "SpeedyCraftah"
        }
    },
    {
        "title": "Add differents way to load a model",
        "bodyText": "Hello,\nThe only way to load a model is by giving a path but in some situation ( in android with an app) a path don't work.\nThe thing that can be added is a larger way to load models as whisper.cpp does.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2401",
        "createdAt": "2023-07-26T11:56:29Z",
        "author": {
            "login": "ostix360"
        }
    },
    {
        "title": "How does OpenAI's Andrej Karpathy's llama2.c compare to llamac.pp?",
        "bodyText": "Curious what the C/C++ devs think of Karpathy's pure C effort - https://github.com/karpathy/llama2.c?\nSource - Analytics Vidhya",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2399",
        "createdAt": "2023-07-26T08:28:44Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Idea: Language translation model on top of LLM model in llama.cpp",
        "bodyText": "The current free LLM models are very good at understanding and generating in English but not other languages. The simple reason is that most available training data is in English. This likely makes the best free LLMs rather inaccessible to the non-english speaking community.\nMy idea is to run a small but good enough translation model on top of any ordinary LLM. The forward and backward translations could be made seamless. To use this feature you would only need to add the translation model as a parameter. The user could possibly choose one language for input and another for output, and change in the middle of a session if needed.\nfacebook/nllb have 600M models that can provide translations in 200 languages. If a 4 bit model of nllb-600M works it will likely only use around 200MB of memory, which is nothing compared to the LLM part. I dont know how much work that would be needed to implement support for this model in ggml.\nSince my native language is non-english - I would love to see this feature in llama.cpp !",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2395",
        "createdAt": "2023-07-25T23:01:33Z",
        "author": {
            "login": "klosax"
        }
    },
    {
        "title": "Presentation on llama.cpp on 25.07.2023 at karlsruhe.ai",
        "bodyText": "I will hold a presentation on llama.cpp (mostly the things that I'm doing myself) on 25.07.2023 at karlsruhe.ai. I am uploading the corresponding slides here. They are still missing some things that I'll measure over the weekend but the slides should be mostly complete in terms of content. I'm currently using a low-effort stable diffusion gen of a llama on a motorcycle for the cover image; if someone gens me something better I'll use it, otherwise I'll probably look into locally installing Stable Diffusion again after not having used it for several months.\nThe current cover image\n\n\n@ggerganov please check whether you agree with the way I'm representing the llama.cpp project.\n@ikawrakow please check whether you agree with my descriptions of k-quants.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2281",
        "createdAt": "2023-07-19T20:16:45Z",
        "author": {
            "login": "JohannesGaessler"
        }
    },
    {
        "title": "about --keep N",
        "bodyText": "I'm still confused about the --keep flag, as i mentioned in #46.\nAlways when --keep N is mentioned, it is stated\n\nnumber of tokens to keep from the initial prompt (default: 0, -1 = all)\n\nTo me, this sounds exactly like what I would expect from a \"system\" message:\nThe initial prompt, the very first prompt of the conversation, should always remain in context. This is how I understand this line.\nIn my attempts I always had the impression that this does not work and that the initial prompt always disappears from the context.\nFor clarification:\nIs it rather the case that --keep merely controls that in a concrete answer the user's previously requested prompt remains in the context (to the selected portion)?  (And in contrast to the \u00ecnitial`, i.e. the very first prompt?)\nWhile older parts of the context are nevertheless pushed out?\nSuch a setting makes sense, too, of course - also and especially together with a system prompt that does not disappear from the context even in a longer conversation.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2289",
        "createdAt": "2023-07-20T09:48:29Z",
        "author": {
            "login": "maddes8cht"
        }
    },
    {
        "title": "Updating after Installing from release",
        "bodyText": "If I want to update, do I need to download the latest release- or can I use git pull if I installed it in a cloned repository?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2393",
        "createdAt": "2023-07-25T19:46:24Z",
        "author": {
            "login": "gsgoldma"
        }
    },
    {
        "title": "Does this project have some standard python notebooks for embeddings, vector DB, etc?",
        "bodyText": "Recently I found some examples for embeddings. For example I found that ./embd-input/embd_input.py should give me some vectors for query I guess? (I still trying to figure that out). It would be very helpful if there be some standard python notebooks for:\n\nFor embeddings\nMaking some sort of vector DB\nSearch engines based on embeddings\nSome word2vec stuff based on embeddings\nSome template examples of how context should be loaded to LLM in order to obtain quality outputs.\n\nOr maybe I am missing something and all that examples are already exists in project?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2386",
        "createdAt": "2023-07-25T13:34:33Z",
        "author": {
            "login": "MaratZakirov"
        }
    },
    {
        "title": "Use chat as single Q&A: How get inference without typing [return]",
        "bodyText": "I want to use the chat for a single Q&A, just 1 inference/answer by execution, using main --file prompt.txt where my question is.\nIn chat, we need to type [return] to get an inference/response. How to get the inference at execution without interaction (typing [return])? I don't find that in the code.\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2385",
        "createdAt": "2023-07-25T13:16:13Z",
        "author": {
            "login": "costaverde22"
        }
    },
    {
        "title": "How can I edit a GGML model?",
        "bodyText": "Howdy!\nHow can I edit/change/update, or remove/disable the weights in a converted-to-ggml model?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2139",
        "createdAt": "2023-07-07T18:17:09Z",
        "author": {
            "login": "seyyedaliayati"
        }
    },
    {
        "title": "llama2 + CUDA at Windows - how to run main.exe to use GPU resources?",
        "bodyText": "Hi,\nINTRODUCTION\n\nI've compiled llama.cpp under Windows with CUDA support (Visual Studio 2022).\n\n\nCompilation flags: GGML_USE_CUBLAS;GGML_USE_K_QUANTS;_CRT_SECURE_NO_WARNINGS;WIN32;WIN64;NDEBUG;_CONSOLE;%(PreprocessorDefinitions)\n\n\nProject compiled correctly (in debug and release).\nI've also created model (LLAMA-2 13B-chat) with 4.1 setting\nI've loaded this model (cool!)\n\nISSUE\nModel is ultra slow\nQUESTION\nHow to run model to ensure proper performance (boost from GPU/CUDA)?\nMY PARAMETERS FOR TESTING PURPOSE\n-p \"Building a website can be done in 10 simple steps:\" -n 512 --n-gpu-layers 1\nPlease note that I don't know what parameters should I use to have good performance\nMy output\nmain: build = 0 (VS2022)\nmain: seed  = 1690219369\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: Quadro M1000M, compute capability 5.0 (Cores = 512)\nllama.cpp: loading model from models/ggml-model-q4_1.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: n_gqa      = 1\nllama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 3 (mostly Q4_1)\nllama_model_load_internal: model size = 13B\nllama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal: using CUDA for GPU acceleration\nllama_model_load_internal: mem required  = 7966.92 MB (+  400.00 MB per state)\nllama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\nllama_model_load_internal: offloading 1 repeating layers to GPU\nllama_model_load_internal: offloaded 1/43 layers to GPU\nllama_model_load_internal: total VRAM used: 550 MB\nllama_new_context_with_model: kv self size  =  400.00 MB\n\nsystem_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n\n\n Building a website can be done in 10 simple steps:\n(...) <-- and here I wait very long to start receiving answer",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2377",
        "createdAt": "2023-07-24T17:23:40Z",
        "author": {
            "login": "Tarmenale"
        }
    },
    {
        "title": "How do I use the API?",
        "bodyText": "I saw here that a new C-style API is available. How do I use it?\nI'm sorry if I sound naive \ud83d\ude05, I just want to test this thing out!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/575",
        "createdAt": "2023-03-28T14:46:02Z",
        "author": {
            "login": "vicfic18"
        }
    },
    {
        "title": "Argument gpu_layers=auto",
        "bodyText": "It would be nice if there was something like gpu_layers =auto that automatically could query gpu for vram and calculate no of layers it could offload.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2365",
        "createdAt": "2023-07-24T08:43:21Z",
        "author": {
            "login": "forrackun"
        }
    },
    {
        "title": "Does this project support llama-2 model?",
        "bodyText": "Recently https://github.com/facebookresearch/llama have some instructions to download llama-2 so my question is does this project support llama-2? Or where can I download original llama model weights?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2328",
        "createdAt": "2023-07-22T20:45:28Z",
        "author": {
            "login": "MaratZakirov"
        }
    },
    {
        "title": "offload most used layers to gpu to get better performance",
        "bodyText": "I found that llama.cpp always offload the first n layers into gpu.\nIs there any way to offload the most used layers to gpu to get better performance?\nFor example, we can run the model with cpu to count how many times each layers have been accessed and write the result into a file. Then we use this file to tell llama.cpp which layer should stay in gpu to get the theoretical best performance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2330",
        "createdAt": "2023-07-22T21:01:05Z",
        "author": {
            "login": "PinkD"
        }
    },
    {
        "title": "llama.cpp codes a program",
        "bodyText": "Here's an example use with llama2-7b-chat-codeCherryPop-qLoRA-GGML:\n\n<!DOCTYPE html>\n<html>\n<head>\n        <title>Background Color Changer</title>\n        <style>\n                button {\n                        background-color: #007bff; /* initial background color */\n                }\n\n                button:hover {\n                        background-color: #0069d9; /* hover background color */\n                }\n        </style>\n        <body>\n                <button id=\"change-btn\">Change Background Color</button>\n                <script>\n                        document.getElementById(\"change-btn\").addEventListener(\"click\", function() {\n                                document.body.style.backgroundColor = \"#ff0070\"; /* change background color on button press */\n                        });\n                </script>\n        </body>\n</html>\n\nAnd it functions which is neat.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2326",
        "createdAt": "2023-07-22T18:59:29Z",
        "author": null
    },
    {
        "title": "How to use Prompt template in llama.cpp?",
        "bodyText": "Hello, could you please tell me how to use Prompt template\n(like\nYou are a helpful assistant\nUSER: prompt goes here\nASSISTANT:\n)\nin llama.cpp? Usually i use this parameters\n(--color --instruct --temp 0.8 --top_k 40 --top_p 0.95 --ctx_size 2048  --n_predict -1 --keep -1)\nDo i need use --interactive or --interactive-first? Do i need to use Prompt template in every question or only at first?\nPlease share your experience.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1816",
        "createdAt": "2023-06-12T08:15:12Z",
        "author": {
            "login": "Folko-Ven"
        }
    },
    {
        "title": "Q: Supported quantizations for MacOS Metal ?",
        "bodyText": "Previsouly only q4_0 and FP16 were supported... is that still the case?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2320",
        "createdAt": "2023-07-22T07:48:46Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": ".NET LLaMA.cpp library! \ud83e\udd99",
        "bodyText": "Got bored today and slapped together a .NET Core library wrapped around llama.cpp! Got inference working, but still need to work through the model conversions \ud83d\ude01\nusing LLaMA.NET;\n\nLLaMAModel model = LLaMAModel.FromPath(\"/path/to/7B-LoRA/ggml-model-q4_0.bin\");\nLLaMARunner runner = model.CreateRunner()\n\nvar res = runner\n    .WithPrompt(\" This is the story of a man named \")\n    .Infer(out _, nTokensToPredict = 50);\n\nConsole.Write(res);\n\nmodel.Dispose();",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/494",
        "createdAt": "2023-03-25T12:25:56Z",
        "author": {
            "login": "hpretila"
        }
    },
    {
        "title": "\"llama.cpp perplexity scorecard\": a llama.cpp helper project",
        "bodyText": "llama-cpp-perplexity-scorecard\nA response to @ggerganov's call for perplexity and latency testing for llama.cpp: \"llama.cpp perplexity scorecard\" is : a helper project to run and gather ./perplexity results - collecting the results for review and analysis.\nResults are uploaded to AWS S3 as JSON: eg https://llama-cpp-perplexity-logs.s3.amazonaws.com/px_20230721T231314Z_71fbbcf6.json\nSoon I'll do some Jupyter examples of scoreboard, tables, charts... contributions are very welcome!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2301",
        "createdAt": "2023-07-20T23:22:39Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "A quick guide for Metal disassembler",
        "bodyText": "Apple doesn't provide any kind of disassembler as a part of developer tools to allow us take a look at the low level stuff. However, thanks to the efforts from open source community we do have a functional disassembler.\nUsage\nClone the repository https://github.com/dougallj/applegpu, and run the command:\npython compiler_explorer.py test.metal\nYou can have any kind of macros or templates in you .metal file, but you can only have one kernel function. Detailed explanations for each instruction can be found at https://dougallj.github.io/applegpu/docs.html. I feel that instructions for Apple GPU are a bit like RISC, where you have instructions to load and store some values between memory and registers, and some other instructions to operate on registers, but no \"load-operate\" instructions.\nExample 0\nHere is the kernel_mul_mat_q4_0_f32 function from master branch. I removed some logics for simplicity.\nDetails\n\n#include <metal_stdlib>\nusing namespace metal;\n#define QK4_0 32\ntypedef struct {\n    half    d;             // delta\n    uint8_t qs[QK4_0 / 2]; // nibbles / quants\n} block_q4_0;\n\n// putting them in the kernel cause a significant performance penalty\n#define N_DST 4 // each SIMD group works on 4 rows\n#define N_SIMDGROUP 2 // number of SIMD groups in a thread group\n#define N_SIMDWIDTH 32 // assuming SIMD group size is 32\nkernel void kernel_mul_mat_q4_0_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne10,\n        constant   int64_t & ne0,\n        constant   int64_t & ne01[[buffer(4)]],\n        uint2 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n    const int nb = ne00/QK4_0;\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    device const block_q4_0 * x = (device const block_q4_0 *) src0 + (r0 * N_SIMDGROUP + sgitg) * N_DST * nb;\n    device const float      * y = (device const float      *) src1 + r1*ne10;\n    block_q4_0 qb_curr, qb_next;\n    float4 y_curr[8];       // src1 vector cache\n    float sumf[N_DST]={0.f}, all_sum;\n    thread float * yl=(thread float *)y_curr;\n\n    // bootstrap\n    qb_curr = x[tiisg];\n    // each thread in a SIMD group deals with 1 block.\n    for (int column = 0; column < nb / N_SIMDWIDTH; column++) {\n\n        float sumy = 0;\n        for (int i = 0; i < QK4_0 / 4; i++) {\n            y_curr[i] = *((device float4  *)(y + N_SIMDWIDTH * (tiisg + column * QK4_0) + 4 * i));\n            sumy += y_curr[i][0] + y_curr[i][1] + y_curr[i][2] + y_curr[i][3];\n        }\n        sumy *= (-8.f);\n\n        for (int row = 0; row < N_DST; row++) {\n            // calculate\n            float d = qb_curr.d;\n            float acc = sumy;\n            for (int i = 0; i < 16; i++) {\n                acc += yl[i] * (qb_curr.qs[i] & 0xF) + yl[i+16] * (qb_curr.qs[i] >> 4);\n            }\n            sumf[row] += d * acc;\n            qb_curr = x[tiisg + ((row + 1) % N_DST) * nb + (column + ((row + 1) / N_DST)) * N_SIMDWIDTH];\n        }\n    }\n\n    for (int row = 0; row < N_DST; ++row) {\n        all_sum = simd_sum(sumf[row]);\n        if (tiisg == 0) {\n            dst[r1*ne0 + (r0 * N_SIMDGROUP + sgitg) * N_DST + row] = all_sum;\n        }\n    }\n\n}\n\n\n \nAnd here is the disassembled code (very long):\nDetails\n\ncompute shader prolog:\n   0: 05110c0d00c43200     device_load      0, i32, xy, r2_r3, u6_u7, 0, signed, lsl 1\n   8: 3800                 wait             0\n   a: e205ffffffff         mov_imm          r1.cache, 4294967295, 0b0\n  10: 922d8602008200b0     icmpsel          slt, r11.cache, r3.cache, 0, r1.cache, 0\n  18: e21520000000         mov_imm          r5.cache, 32, 0b0\n  1e: 9205960200c200b0     icmpsel          slt, r1.cache, r11.cache, 0, r1.discard, 0\n  26: be19ca0e0000         ffs              r6.cache, r5.discard\n  2c: fe0dc26a6c00         xor              r3.cache, r1.discard, r3.discard\n  32: e20500000000         mov_imm          r1.cache, 0, 0b0\n  38: fe09964a6c00         xor              r2.cache, r11.cache, r2.discard\n  3e: be15820e0000         ffs              r5.cache, r1.cache\n  44: 8e1fc46b65000000     isub             r7_r8.cache, r2_r3.discard, r11.sx\n  4c: 8e0d3fc82c000000     isub             r3.cache, 63, r6.discard\n  54: 8e091fa82c000000     isub             r2.cache, 31, r5.discard\n  5c: 9215420200c6408c     icmpsel          seq, r5.cache, r1, 0, r3.discard, r2.discard\n  64: be0d8e0e0000         ffs              r3.cache, r7.cache\n  6a: be09900e0000         ffs              r2.cache, r8.cache\n  70: 8e0d3f682c000000     isub             r3.cache, 63, r3.discard\n  78: 8e091f482c000000     isub             r2.cache, 31, r2.discard\n  80: 9209900200c6408c     icmpsel          seq, r2.cache, r8.cache, 0, r3.discard, r2.discard\n  88: 8e0b8a4a2c000000     isub             r2_r3.cache, r5.cache, r2.discard\n  90: fe198e0ae500         or               r6.cache, r7.cache, r8\n  96: 9210cc0200010190     icmpsel          seq, r4l.cache, r6.discard, 0, 1, 0\n  9e: 921844f203010150     icmpsel          ugt, r6l.cache, r2, 63, 1, 0\n  a6: 9202860200010150     icmpsel          ugt, r0h.cache, r3.cache, 0, 1, 0\n  ae: 9202460200cc108c     icmpsel          seq, r0h.cache, r3, 0, r6l.discard, r0h.discard\n  b6: 9210c10000c81090     icmpsel          seq, r4l.cache, r0h.discard, 0, r4l.discard, 1\n  be: 9202caf203018188     icmpsel          seq, r0h.cache, r5.discard, 63, 1, r4l.cache\n  c6: 1219c800004e0090     icmpsel          seq, r6, r4l.discard, 0, r7, 0\n  ce: e2000000             mov_imm          r0l.cache, 0\n  d2: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n  d8: 20c04a010000         jmp_exec_none    0x222\n  de: 8e273f4838000000     isub             r9_r10.cache, 63, r2_r3.cache\n  e6: 8e15014028000000     iadd             r5.cache, 1, r2.cache\n  ee: 92028a422c010130     icmpsel          ult, r0h.cache, r5.cache, r2.discard, 1, 0\n  f6: 8e09202829000000     isub             r2.cache, 32, r9.cache\n  fe: 8e19c1602c000000     iadd             r6.cache, r0h.discard, r3.discard\n 106: aea900e028c40200     bfeil            r10.cache, 0, r7.cache, r2.discard\n 10e: 8e0d920a02000000     isub             r3.cache, r9.cache, 32\n 116: ae3100e028920200     bfi              r12.cache, 0, r7.cache, r9.cache\n 11e: fe094acae400         or               r2.cache, r5, r6\n 124: ae29d40225d20200     bfi              r10.cache, r10.discard, r8, r9.discard\n 12c: 9202c40200010190     icmpsel          seq, r0h.cache, r2.discard, 0, 1, 0\n 134: ae2500e024860200     bfi              r9.cache, 0, r7, r3.cache\n 13c: 1229860200d420ad     icmpsel          slt, r10, r3.cache, 0, r10.discard, r9.discard\n 144: 1225c60200d800b0     icmpsel          slt, r9, r3.discard, 0, r12.discard, 0\n 14c: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n 152: 20c0bc000000         jmp_exec_none    0x20E\n 158: e20900000000         mov_imm          r2.cache, 0, 0b0\n 15e: ae39ce06298a0200     extr             r14.cache, r7.discard, r8.cache, r5.cache\n 166: e20d00000000         mov_imm          r3.cache, 0, 0b0\n 16c: aebd00002d8a0200     bfeil            r15.cache, 0, r8.discard, r5.cache\n 174: e23100000000         mov_imm          r12.cache, 0, 0b0\n 17a: 623500000000         mov_imm          r13, 0, 0b0\n 180: 421000000000         push_exec        r0l, 2\n 186: 8e4300c0bd000000     iadd             r16_r17.cache, 0, r14_r15.discard, lsl 1\n 18e: 92059402000101b0     icmpsel          slt, r1.cache, r10.cache, 0, 1, 0\n 196: 8e4f0020b9000000     iadd             r19_r20.cache, 0, r9_r10.cache, lsl 1\n 19e: fe41e02ae800         or               r16.cache, r16.discard, r1.cache\n 1a4: e205ffffffff         mov_imm          r1.cache, 4294967295, 0b0\n 1aa: 0e1f1f083a000000     isub             r7_r8, 31, r16_r17.cache\n 1b2: 7e25d86aee00         or               r9, r12.discard, r19.discard\n 1b8: 8e178a1b00000000     isub             r5_r6.cache, r5_r6.cache, 1\n 1c0: 7e29da8aee00         or               r10, r13.discard, r20.discard\n 1c6: 9205d002008200b0     icmpsel          slt, r1.cache, r8.discard, 0, r1.cache, 0\n 1ce: fe0d4acae400         or               r3.cache, r5, r6\n 1d4: fe1d82028200         and              r7.cache, r1.cache, 32\n 1da: fe0582128000         and              r1.cache, r1.cache, 1\n 1e0: 0e3be0eb2c000000     isub             r14_r15, r16_r17.discard, r7.discard\n 1e8: 9202c60200010190     icmpsel          seq, r0h.cache, r3.discard, 0, 1, 0\n 1f0: 7e31820a8000         mov              r12, r1.cache\n 1f6: 7e35440a8000         mov              r13, r2\n 1fc: 5294c1000000         while_icmp       r0l, seq, r0h.discard, 0, 2\n 202: 00c084ffffff         jmp_exec_any     0x186\n 208: d21600000000         pop_exec         r0l.cache, 2\n 20e: d20e00000000         pop_exec         r0l.cache, 1\n 214: 8e1f0020bd000000     iadd             r7_r8.cache, 0, r9_r10.discard, lsl 1\n 21c: fe19c2eaec00         or               r6.cache, r1.discard, r7.discard\n 222: 520e00000000         pop_exec         r0l, 1\n 228: fe15cc6a6900         xor              r5.cache, r6.discard, r11.cache\n 22e: e205ffffffff         mov_imm          r1.cache, 4294967295, 0b0\n 234: 8e09ca6a2d000000     isub             r2.cache, r5.discard, r11.discard\n 23c: 9205840200c200b0     icmpsel          slt, r1.cache, r2.cache, 0, r1.discard, 0\n 244: ae8500202c1b0000     bfeil            r1.cache, 0, r1.discard, 27\n 24c: 8e0544222c000000     iadd             r1.cache, r2, r1.discard\n 254: 2e85c25600000000     asr              r1, r1.discard, 5\n 25c: c510803d01803000     uniform_store    2, i16, xy, 0, r2l_r2h, 24\n 264: c508a03d01803000     uniform_store    2, i16, xy, 0, r1l_r1h, 26\n 26c: 8800                 stop             \n\ncompute shader:\n   0: f2023500             get_sr           r0h.cache, sr53 (simdgroup_index_in_threadgroup)\n   4: f2050000             get_sr           r1.cache, sr0 (threadgroup_position_in_grid.x)\n   8: 62a9000000000010     mov_imm          r42, 0, 0b0\n  10: 8e2dc120ac100000     iadd             r43.cache, r0h.discard, r1.discard, lsl 1\n  18: 62b1000000000010     mov_imm          r44, 0, 0b0\n  20: 62b5000000000010     mov_imm          r45, 0, 0b0\n  28: 62b9000000000010     mov_imm          r46, 0, 0b0\n  30: e2000000             mov_imm          r0l.cache, 0\n  34: 52c898f10100         if_icmp          r0l, sgt, u12, 31, 1\n  3a: 8e05006025011000     iadd             r1.cache, 0, r43, lsl 2\n  42: 9e05c28219000000     imadd            r1.cache, r1.discard, u12, 0\n  4a: 9e17c22201800110     imadd            r37_r38.cache, r1.discard, 18, u0\n  52: f2023400             get_sr           r0h.cache, sr52 (thread_index_in_simdgroup)\n  56: 8e19cc2218140000     iadd             r38.cache, r38.discard, u1\n  5e: 1e17c120014a4300     imadd            r5_r6, r0h.discard, 18, r37_r38\n  66: 20c072090000         jmp_exec_none    0x9D8\n  6c: 85040a0500c01000     device_load      0, i16, x, r0h, r5_r6, 0, signed\n  74: 85801a4500c11000     device_load      1, i16, x, r48l, r5_r6, 1, signed\n  7c: 85081a4500c4f000     device_load      1, i16, xyzw, r1l_r1h_r2l_r2h, r5_r6, 1, signed, lsl 1\n  84: 85483a4500c53000     device_load      1, i16, xy, r41l_r41h, r5_r6, 3, signed, lsl 1\n  8c: 85202a0500c81000     device_load      0, i16, x, r4l, r5_r6, 2, signed, lsl 2\n  94: 3801                 wait             1\n  96: f20d0100             get_sr           r3.cache, sr1 (threadgroup_position_in_grid.y)\n  9a: fe159c098000         mov              r5.cache, u14\n  a0: 9e17ca6228000000     imadd            r5_r6.cache, r5.discard, r3.cache, 0\n  a8: 921d860200a401b0     icmpsel          slt, r7.cache, r3.cache, 0, u18, 0\n  b0: 9e199ce12ccc0200     imadd            r6.cache, u14, r7.discard, r6.discard\n  b8: 9e199e612ccc0200     imadd            r6.cache, u15, r3.discard, r6.discard\n  c0: 8e1f84a13c101000     iadd             r39_r40.cache, u2, r5_r6.discard, lsl 2\n  c8: 2ee9002029180011     bfeil            r58, 0, r41.cache, 24\n  d0: 2ee1002029080011     bfeil            r56, 0, r41.cache, 8\n  d8: 2ed5004028180010     bfeil            r53, 0, r2.cache, 24\n  e0: 2ed9004028080010     bfeil            r54, 0, r2.cache, 8\n  e8: 8e21d06218140000     iadd             r40.cache, r40.discard, u3\n  f0: 2ec5002028180010     bfeil            r49, 0, r1.cache, 24\n  f8: e2a800000010         mov_imm          r42l.cache, 0\n  fe: 2ec9002028080010     bfeil            r50, 0, r1.cache, 8\n 106: 62d200000010         mov_imm          r52h, 0\n 10c: e20c0000             mov_imm          r3l.cache, 0\n 110: 7e5043088010         mov              r52l, r1h\n 116: 2ee4000006080011     bfeil            r57l, 0, r48l, 8\n 11e: 7e6c93088014         mov              r59l, r41h.cache\n 124: 62ee00000010         mov_imm          r59h, 0\n 12a: 62a9000000000010     mov_imm          r42, 0, 0b0\n 132: e2bd000000000010     mov_imm          r47.cache, 0, 0b0\n 13a: 62b1000000000010     mov_imm          r44, 0, 0b0\n 142: 62b5000000000010     mov_imm          r45, 0, 0b0\n 14a: 62b9000000000010     mov_imm          r46, 0, 0b0\n 152: 7e5c85088010         mov              r55l, r2h.cache\n 158: 62de00000010         mov_imm          r55h, 0\n 15e: 421000000000         push_exec        r0l, 2\n 164: ae1500e0250a0001     bfi              r5.cache, 0, r47, 10\n 16c: f20c3400             get_sr           r3l.cache, sr52 (thread_index_in_simdgroup)\n 170: 8e0d00608c000000     iadd             r3.cache, 0, r3l.discard, lsl 1\n 178: 8e0dca622c002000     iadd             r3.cache, r5.discard, r3.discard, lsl 4\n 180: 0e074e6324141000     iadd             r33_r34, r39_r40, r3, lsl 2\n 188: 05296e4640c0f200     device_load      1, i32, xyzw, r5_r6_r7_r8, r39_r40, r3, unsigned\n 190: 0549124540c8f200     device_load      1, i32, xyzw, r9_r10_r11_r12, r33_r34, 1, signed, lsl 2\n 198: 0569224540c8f200     device_load      1, i32, xyzw, r13_r14_r15_r16, r33_r34, 2, signed, lsl 2\n 1a0: 0589324540c8f200     device_load      1, i32, xyzw, r17_r18_r19_r20, r33_r34, 3, signed, lsl 2\n 1a8: 05a9420540c8f200     device_load      0, i32, xyzw, r21_r22_r23_r24, r33_r34, 4, signed, lsl 2\n 1b0: 05c9520540c8f200     device_load      0, i32, xyzw, r25_r26_r27_r28, r33_r34, 5, signed, lsl 2\n 1b8: 05e9620540c8f200     device_load      0, i32, xyzw, r29_r30_r31_r32, r33_r34, 6, signed, lsl 2\n 1c0: 0509720540c9f200     device_load      0, i32, xyzw, r33_r34_r35_r36, r33_r34, 7, signed, lsl 2\n 1c8: 3801                 wait             1\n 1ca: aa8d4ca22400         fadd32           r3.cache, r6, r5\n 1d0: aa8dc6e22400         fadd32           r3.cache, r3.discard, r7\n 1d6: aa8dc6022500         fadd32           r3.cache, r3.discard, r8\n 1dc: aa8dc6422500         fadd32           r3.cache, r3.discard, r10\n 1e2: aa8dc6222500         fadd32           r3.cache, r3.discard, r9\n 1e8: aa8dc6622500         fadd32           r3.cache, r3.discard, r11\n 1ee: aa8dc6822500         fadd32           r3.cache, r3.discard, r12\n 1f4: aa8dc6c22500         fadd32           r3.cache, r3.discard, r14\n 1fa: aa8dc6a22500         fadd32           r3.cache, r3.discard, r13\n 200: aa8dc6e22500         fadd32           r3.cache, r3.discard, r15\n 206: aa8dc6022600         fadd32           r3.cache, r3.discard, r16\n 20c: aa8dc6422600         fadd32           r3.cache, r3.discard, r18\n 212: aa8dc6222600         fadd32           r3.cache, r3.discard, r17\n 218: aa8dc6622600         fadd32           r3.cache, r3.discard, r19\n 21e: 2a8dc6822600         fadd32           r3, r3.discard, r20\n 224: 3800                 wait             0\n 226: aa8dc6c22600         fadd32           r3.cache, r3.discard, r22\n 22c: aa8dc6a22600         fadd32           r3.cache, r3.discard, r21\n 232: aa8dc6e22600         fadd32           r3.cache, r3.discard, r23\n 238: aa8dc6022700         fadd32           r3.cache, r3.discard, r24\n 23e: aa8dc6422700         fadd32           r3.cache, r3.discard, r26\n 244: aa8dc6222700         fadd32           r3.cache, r3.discard, r25\n 24a: aa8dc6622700         fadd32           r3.cache, r3.discard, r27\n 250: aa8dc6822700         fadd32           r3.cache, r3.discard, r28\n 256: aa8dc6c22700         fadd32           r3.cache, r3.discard, r30\n 25c: aa8dc6a22700         fadd32           r3.cache, r3.discard, r29\n 262: aa8dc6e22700         fadd32           r3.cache, r3.discard, r31\n 268: aa8dc6022401         fadd32           r3.cache, r3.discard, r32\n 26e: aa8dc6422801         fadd32           r3.cache, r3.discard, r34.cache\n 274: aa8dc6222801         fadd32           r3.cache, r3.discard, r33.cache\n 27a: aa8dc6622401         fadd32           r3.cache, r3.discard, r35\n 280: aacdc6822411         fadd32           r51.cache, r3.discard, r36\n 286: 620d00000000         mov_imm          r3, 0, 0b0\n 28c: 9acde6020217         fmul32           r51.cache, r51.discard, -8.0\n 292: 421000000000         push_exec        r0l, 2\n 298: fe75b6f28014         and              r61.cache, r59.cache, 15\n 29e: fe71b4f28014         and              r60.cache, r58.cache, 15\n 2a4: bef90ba42f11         convert          s32_to_f, r62.cache, r61.discard, rte\n 2aa: be810b842f21         convert          s32_to_f, r64.cache, r60.discard, rte\n 2b0: 7e7142f28010         and              r60, r1, 15\n 2b6: aefd00402b040011     bfeil            r63.cache, 0, r58.cache, 4\n 2be: 7e6944f28010         and              r58, r2, 15\n 2c4: aef500602b040411     bfeil            r61.cache, 0, r59.cache, 4, mask 0xF\n 2cc: ba8164022c664222     fmadd32          r64.cache, r18, r64.discard, r51\n 2d4: befd0be42f11         convert          s32_to_f, r63.cache, r63.discard, rte\n 2da: fe6db0f28014         and              r59.cache, r56.cache, 15\n 2e0: bef50ba42f11         convert          s32_to_f, r61.cache, r61.discard, rte\n 2e6: bafd44e22fc08215     fmadd32          r63.cache, r34, r63.discard, r64.discard\n 2ee: beed0b642b11         convert          s32_to_f, r59.cache, r59.cache, rte\n 2f4: baf962c22ffe4211     fmadd32          r62.cache, r17, r62.discard, r63.discard\n 2fc: aefd00002b040411     bfeil            r63.cache, 0, r56.cache, 4, mask 0xF\n 304: baf942a22ffc4215     fmadd32          r62.cache, r33, r61.discard, r62.discard\n 30c: aef5002029040411     bfeil            r61.cache, 0, r41.cache, 4, mask 0xF\n 314: fe6192f28014         and              r56.cache, r41.cache, 15\n 31a: bea50be42f11         convert          s32_to_f, r41.cache, r63.discard, rte\n 320: baed60622bfc4211     fmadd32          r59.cache, r16, r59.cache, r62.discard\n 328: bee10b042b11         convert          s32_to_f, r56.cache, r56.cache, rte\n 32e: baed402229b64215     fmadd32          r59.cache, r32, r41.cache, r59.cache\n 336: bea50ba42f11         convert          s32_to_f, r41.cache, r61.discard, rte\n 33c: bae15e022bb64211     fmadd32          r56.cache, r15, r56.cache, r59.cache\n 344: ae85002028040400     bfeil            r1.cache, 0, r1.cache, 4, mask 0xF\n 34c: baed7e2229b04211     fmadd32          r59.cache, r31, r41.cache, r56.cache\n 354: ae89004028040400     bfeil            r2.cache, 0, r2.cache, 4, mask 0xF\n 35c: fe25b2f28317         and              r41.cache, r57.cache, 255\n 362: aef4008008080010     bfeil            r61l.cache, 0, r4l.cache, 8\n 36a: fe6192f28014         and              r56.cache, r41.cache, 15\n 370: aea5002029040011     bfeil            r41.cache, 0, r41.cache, 4\n 378: fe1088f08303         and              r4l.cache, r4l.cache, 255\n 37e: bee50b042b11         convert          s32_to_f, r57.cache, r56.cache, rte\n 384: fe6188f08010         and              r56.cache, r4l.cache, 15\n 38a: bea50b242911         convert          s32_to_f, r41.cache, r41.cache, rte\n 390: baed4c222bb64211     fmadd32          r59.cache, r6, r57.cache, r59.cache\n 398: bee10b042b11         convert          s32_to_f, r56.cache, r56.cache, rte\n 39e: fe65a0f28317         and              r57.cache, r48.cache, 255\n 3a4: aec1008008040010     bfeil            r48.cache, 0, r4l.cache, 4\n 3ac: baed6c2229b64211     fmadd32          r59.cache, r22, r41.cache, r59.cache\n 3b4: aea500202b040011     bfeil            r41.cache, 0, r57.cache, 4\n 3bc: fe65b2f28014         and              r57.cache, r57.cache, 15\n 3c2: bec10b042a11         convert          s32_to_f, r48.cache, r48.cache, rte\n 3c8: bae166022bb64211     fmadd32          r56.cache, r19, r56.cache, r59.cache\n 3d0: bee50b242b11         convert          s32_to_f, r57.cache, r57.cache, rte\n 3d6: baed46022ab04215     fmadd32          r59.cache, r35, r48.cache, r56.cache\n 3de: bee10b242911         convert          s32_to_f, r56.cache, r41.cache, rte\n 3e4: fe41baf08014         and              r48.cache, r61l.cache, 15\n 3ea: aea500a00f040011     bfeil            r41.cache, 0, r61l.discard, 4\n 3f2: bae54a222b764211     fmadd32          r57.cache, r5, r57.cache, r59\n 3fa: bec10b042a11         convert          s32_to_f, r48.cache, r48.cache, rte\n 400: bae16a022bb24211     fmadd32          r56.cache, r21, r56.cache, r57.cache\n 408: bea50b242911         convert          s32_to_f, r41.cache, r41.cache, rte\n 40e: bac168022ab04211     fmadd32          r48.cache, r20, r48.cache, r56.cache\n 416: bee10b442711         convert          s32_to_f, r56.cache, r58, rte\n 41c: bae5482229a04215     fmadd32          r57.cache, r36, r41.cache, r48.cache\n 424: bea50b442810         convert          s32_to_f, r41.cache, r2.cache, rte\n 42a: fe41acf28014         and              r48.cache, r54.cache, 15\n 430: ae8900c02a040401     bfeil            r2.cache, 0, r54.cache, 4, mask 0xF\n 438: bad956022b724211     fmadd32          r54.cache, r11, r56.cache, r57\n 440: bec10b042a11         convert          s32_to_f, r48.cache, r48.cache, rte\n 446: bae1762229ac4211     fmadd32          r56.cache, r27, r41.cache, r54.cache\n 44e: bea50b442810         convert          s32_to_f, r41.cache, r2.cache, rte\n 454: fe59aef28014         and              r54.cache, r55.cache, 15\n 45a: ae8900e02a040401     bfeil            r2.cache, 0, r55.cache, 4, mask 0xF\n 462: bac158022a704211     fmadd32          r48.cache, r12, r48.cache, r56\n 46a: bed90bc42a11         convert          s32_to_f, r54.cache, r54.cache, rte\n 470: badd782229a04211     fmadd32          r55.cache, r28, r41.cache, r48.cache\n 478: bec10b442810         convert          s32_to_f, r48.cache, r2.cache, rte\n 47e: fe25aaf28014         and              r41.cache, r53.cache, 15\n 484: ae8900a02a040001     bfeil            r2.cache, 0, r53.cache, 4\n 48c: bad55ac226ae4211     fmadd32          r53.cache, r13, r54, r55.cache\n 494: bea50b242911         convert          s32_to_f, r41.cache, r41.cache, rte\n 49a: bac17a022aaa4211     fmadd32          r48.cache, r29, r48.cache, r53.cache\n 4a2: be890b442800         convert          s32_to_f, r2.cache, r2.cache, rte\n 4a8: baa55c2229a04211     fmadd32          r41.cache, r14, r41.cache, r48.cache\n 4b0: bec10b842f11         convert          s32_to_f, r48.cache, r60.discard, rte\n 4b6: bad57c4228924210     fmadd32          r53.cache, r30, r2.cache, r41.cache\n 4be: be890b242800         convert          s32_to_f, r2.cache, r1.cache, rte\n 4c4: fe25a4f28014         and              r41.cache, r50.cache, 15\n 4ca: ae8500402a040401     bfeil            r1.cache, 0, r50.cache, 4, mask 0xF\n 4d2: bac14e022a6a4211     fmadd32          r48.cache, r7, r48.cache, r53\n 4da: bea50b242911         convert          s32_to_f, r41.cache, r41.cache, rte\n 4e0: bac96e4228a04210     fmadd32          r50.cache, r23, r2.cache, r48.cache\n 4e8: be890b242800         convert          s32_to_f, r2.cache, r1.cache, rte\n 4ee: fe41a8f28014         and              r48.cache, r52.cache, 15\n 4f4: ae8500802a040401     bfeil            r1.cache, 0, r52.cache, 4, mask 0xF\n 4fc: baa5502229a44211     fmadd32          r41.cache, r8, r41.cache, r50.cache\n 504: bec10b042a11         convert          s32_to_f, r48.cache, r48.cache, rte\n 50a: bac9704228924210     fmadd32          r50.cache, r24, r2.cache, r41.cache\n 512: bea50b242810         convert          s32_to_f, r41.cache, r1.cache, rte\n 518: fe09a2f28004         and              r2.cache, r49.cache, 15\n 51e: ae85002026040001     bfeil            r1.cache, 0, r49, 4\n 526: bac152022a644211     fmadd32          r48.cache, r9, r48.cache, r50\n 52e: be890b442800         convert          s32_to_f, r2.cache, r2.cache, rte\n 534: baa5722229604211     fmadd32          r41.cache, r25, r41.cache, r48\n 53c: be850b242800         convert          s32_to_f, r1.cache, r1.cache, rte\n 542: ba89544228924200     fmadd32          r2.cache, r10, r2.cache, r41.cache\n 54a: ba89742228840200     fmadd32          r2.cache, r26, r1.cache, r2.cache\n 552: f2103400             get_sr           r4l.cache, sr52 (thread_index_in_simdgroup)\n 556: 92a586200098c0895010 icmpsel          seq, r41.cache, r3l.cache, 2, r44.cache, r46.cache\n 560: ae85006028020000     bfeil            r1.cache, 0, r3.cache, 2\n 568: ba89841204924200     fmadd32          r2.cache, r2.cache, r0h, r41.cache\n 570: 8e5d016028100000     iadd             r55.cache, 1, r3.cache\n 578: 8e0582e225010000     iadd             r1.cache, r1.cache, r47\n 580: 12b986000084c0851010 icmpsel          seq, r46, r3l.cache, 0, r2.cache, r46\n 58a: 12b18620008480851010 icmpsel          seq, r44, r3l.cache, 2, r2.cache, r44\n 594: 8e0d02602c000000     iadd             r3.cache, 2, r3.discard\n 59c: 9e096e8219880004     imadd            r2.cache, r55, u12, r4l.cache\n 5a4: 8e250020a8100000     iadd             r41.cache, 0, r1.cache, lsl 1\n 5ac: ae85006028020000     bfeil            r1.cache, 0, r3.cache, 2\n 5b4: 8e51842229112000     iadd             r52.cache, r2.cache, r41.cache, lsl 4\n 5bc: fe0946228000         and              r2.cache, r3, 2\n 5c2: 8e0582e225010000     iadd             r1.cache, r1.cache, r47\n 5ca: 9e25848019480010     imadd            r41.cache, r2l.cache, u12, r4l\n 5d2: 8e410020a8100000     iadd             r48.cache, 0, r1.cache, lsl 1\n 5da: 1e07a822018a4304     imadd            r1_r2, r52.cache, 18, r37_r38.cache\n 5e2: 8e25920226152000     iadd             r41.cache, r41.cache, r48, lsl 4\n 5ea: 1e535222014a4314     imadd            r52_r53, r41, 18, r37_r38\n 5f2: 8584020500c11000     device_load      0, i16, x, r48h, r1_r2, 0, signed\n 5fa: 8508124500c21000     device_load      1, i16, x, r65l, r1_r2, 1, signed\n 602: 8588124500c5f000     device_load      1, i16, xyzw, r49l_r49h_r50l_r50h, r1_r2, 1, signed, lsl 1\n 60a: 85d0324500c53000     device_load      1, i16, xy, r58l_r58h, r1_r2, 3, signed, lsl 1\n 612: 85bc224500c91000     device_load      1, i16, x, r55h, r1_r2, 2, signed, lsl 2\n 61a: 8504080560c01000     device_load      0, i16, x, r0h, r52_r53, 0, signed\n 622: 8580180560c11000     device_load      0, i16, x, r48l, r52_r53, 1, signed\n 62a: 8508180560c4f000     device_load      0, i16, xyzw, r1l_r1h_r2l_r2h, r52_r53, 1, signed, lsl 1\n 632: 8548380560c53000     device_load      0, i16, xy, r41l_r41h, r52_r53, 3, signed, lsl 1\n 63a: 8520280560c81000     device_load      0, i16, x, r4l, r52_r53, 2, signed, lsl 2\n 642: 3801                 wait             1\n 644: fe54a3088014         mov              r53l.cache, r49h.cache\n 64a: aed100402b040411     bfeil            r52.cache, 0, r58.cache, 4, mask 0xF\n 652: fe59aaf28014         and              r54.cache, r53.cache, 15\n 658: 3ed10b842611         convert          s32_to_f, r52, r52, rte\n 65e: fe54b5088014         mov              r53l.cache, r58h.cache\n 664: 3ef90bc42a11         convert          s32_to_f, r62, r54.cache, rte\n 66a: fe59aaf28014         and              r54.cache, r53.cache, 15\n 670: aed5004027140411     bfeil            r53.cache, 0, r58, 20, mask 0xF\n 678: 3ee10bc42a11         convert          s32_to_f, r56, r54.cache, rte\n 67e: fe146ff08327         and              r69l.cache, r55h, 255\n 684: 3ed90ba42a11         convert          s32_to_f, r54, r53.cache, rte\n 68a: fe554af08018         and              r53.cache, r69l, 15\n 690: aee500402a040411     bfeil            r57.cache, 0, r50.cache, 4, mask 0xF\n 698: 3ed50ba42611         convert          s32_to_f, r53, r53, rte\n 69e: 3ef50b242b11         convert          s32_to_f, r61, r57.cache, rte\n 6a4: 2ee500402a180411     bfeil            r57, 0, r50.cache, 24, mask 0xF\n 6ac: aef1002008080812     bfeil            r60.cache, 0, r65l.cache, 8, mask 0xFF\n 6b4: 2eed0040261c0011     bfeil            r59, 0, r50, 28\n 6bc: ae8100802b040021     bfeil            r64.cache, 0, r60.cache, 4\n 6c4: aefd00202a140411     bfeil            r63.cache, 0, r49.cache, 20, mask 0xF\n 6cc: be810b042c22         convert          s32_to_f, r64.cache, r64.discard, rte\n 6d2: fe05c2f0832b         and              r65.cache, r65l.discard, 255\n 6d8: 3efd0be42f11         convert          s32_to_f, r63, r63.discard, rte\n 6de: fe0982f28028         and              r66.cache, r65.cache, 15\n 6e4: ae8500202c040022     bfeil            r65.cache, 0, r65.discard, 4\n 6ec: be910b442c22         convert          s32_to_f, r68.cache, r66.discard, rte\n 6f2: be8d0b242c22         convert          s32_to_f, r67.cache, r65.discard, rte\n 6f8: fe71f8f28014         and              r60.cache, r60.discard, 15\n 6fe: bef10b842f11         convert          s32_to_f, r60.cache, r60.discard, rte\n 704: fe09a2f28024         and              r66.cache, r49.cache, 15\n 70a: ae8500202a040421     bfeil            r65.cache, 0, r49.cache, 4, mask 0xF\n 712: ba914a822c664222     fmadd32          r68.cache, r5, r68.discard, r51\n 71a: be890b442c22         convert          s32_to_f, r66.cache, r66.discard, rte\n 720: ba8d6a622cc88222     fmadd32          r67.cache, r21, r67.discard, r68.discard\n 728: be850b242c22         convert          s32_to_f, r65.cache, r65.discard, rte\n 72e: ba8d4c822fc68221     fmadd32          r67.cache, r6, r60.discard, r67.discard\n 736: aef100202a080411     bfeil            r60.cache, 0, r49.cache, 8, mask 0xF\n 73e: ba8d6c022cc68222     fmadd32          r67.cache, r22, r64.discard, r67.discard\n 746: ae8100202a0c0421     bfeil            r64.cache, 0, r49.cache, 12, mask 0xF\n 74e: ba894e422cc68222     fmadd32          r66.cache, r7, r66.discard, r67.discard\n 756: bef10b842f11         convert          s32_to_f, r60.cache, r60.discard, rte\n 75c: ba856e222cc48222     fmadd32          r65.cache, r23, r65.discard, r66.discard\n 764: be810b042c22         convert          s32_to_f, r64.cache, r64.discard, rte\n 76a: ba8550822fc28221     fmadd32          r65.cache, r8, r60.discard, r65.discard\n 772: aef100202a180411     bfeil            r60.cache, 0, r49.cache, 24, mask 0xF\n 77a: ba8170022cc28222     fmadd32          r64.cache, r24, r64.discard, r65.discard\n 782: aec500202a1c0011     bfeil            r49.cache, 0, r49.cache, 28\n 78a: ba8152c22fc08221     fmadd32          r64.cache, r9, r62.discard, r64.discard\n 792: bef90b842f11         convert          s32_to_f, r62.cache, r60.discard, rte\n 798: fe71a4f28014         and              r60.cache, r50.cache, 15\n 79e: bec50b242a11         convert          s32_to_f, r49.cache, r49.cache, rte\n 7a4: bafd72e22fc08211     fmadd32          r63.cache, r25, r63.discard, r64.discard\n 7ac: bef10b842f11         convert          s32_to_f, r60.cache, r60.discard, rte\n 7b2: baf954c22ffe4211     fmadd32          r62.cache, r10, r62.discard, r63.discard\n 7ba: ae8100402a080421     bfeil            r64.cache, 0, r50.cache, 8, mask 0xF\n 7c2: baf974222afc4211     fmadd32          r62.cache, r26, r49.cache, r62.discard\n 7ca: aec500402a0c0411     bfeil            r49.cache, 0, r50.cache, 12, mask 0xF\n 7d2: bafd56822ffc4211     fmadd32          r63.cache, r11, r60.discard, r62.discard\n 7da: aef900402a140411     bfeil            r62.cache, 0, r50.cache, 20, mask 0xF\n 7e2: fe70a5088014         mov              r60l.cache, r50h.cache\n 7e8: bec90b042c12         convert          s32_to_f, r50.cache, r64.discard, rte\n 7ee: fe71f8f28014         and              r60.cache, r60.discard, 15\n 7f4: bec50b242a11         convert          s32_to_f, r49.cache, r49.cache, rte\n 7fa: baf576a22ffe4211     fmadd32          r61.cache, r27, r61.discard, r63.discard\n 802: bef10b842f11         convert          s32_to_f, r60.cache, r60.discard, rte\n 808: baf558422afa4211     fmadd32          r61.cache, r12, r50.cache, r61.discard\n 810: bec90bc42f11         convert          s32_to_f, r50.cache, r62.discard, rte\n 816: baf578222afa4211     fmadd32          r61.cache, r28, r49.cache, r61.discard\n 81e: bec50b242b11         convert          s32_to_f, r49.cache, r57.cache, rte\n 824: fe65b4f28014         and              r57.cache, r58.cache, 15\n 82a: beed0b642b11         convert          s32_to_f, r59.cache, r59.cache, rte\n 830: baf15a822ffa4211     fmadd32          r60.cache, r13, r60.discard, r61.discard\n 838: bee50b242b11         convert          s32_to_f, r57.cache, r57.cache, rte\n 83e: baf17a422af84211     fmadd32          r60.cache, r29, r50.cache, r60.discard\n 846: aec900402b080411     bfeil            r50.cache, 0, r58.cache, 8, mask 0xF\n 84e: baf15c222af84211     fmadd32          r60.cache, r14, r49.cache, r60.discard\n 856: aec500402b0c0411     bfeil            r49.cache, 0, r58.cache, 12, mask 0xF\n 85e: baed7c622bf84211     fmadd32          r59.cache, r30, r59.cache, r60.discard\n 866: bec90b442a11         convert          s32_to_f, r50.cache, r50.cache, rte\n 86c: bae55e222b764211     fmadd32          r57.cache, r15, r57.cache, r59\n 874: bec50b242a11         convert          s32_to_f, r49.cache, r49.cache, rte\n 87a: bae57e822ab24211     fmadd32          r57.cache, r31, r52.cache, r57.cache\n 882: aed100402b180411     bfeil            r52.cache, 0, r58.cache, 24, mask 0xF\n 88a: bae560422ab24211     fmadd32          r57.cache, r16, r50.cache, r57.cache\n 892: aec90040271c0011     bfeil            r50.cache, 0, r58, 28\n 89a: bac540222a724215     fmadd32          r49.cache, r32, r49.cache, r57\n 8a2: bed10b842a11         convert          s32_to_f, r52.cache, r52.cache, rte\n 8a8: bac5620227a24211     fmadd32          r49.cache, r17, r56, r49.cache\n 8b0: bec90b442a11         convert          s32_to_f, r50.cache, r50.cache, rte\n 8b6: bad942c22aa24215     fmadd32          r54.cache, r33, r54.cache, r49.cache\n 8be: aec500a00c040012     bfeil            r49.cache, 0, r69l.discard, 4\n 8c6: bad164822aac4211     fmadd32          r52.cache, r18, r52.cache, r54.cache\n 8ce: aede00f00a080011     bfeil            r55h.cache, 0, r55h.cache, 8\n 8d6: bad944422aa84215     fmadd32          r54.cache, r34, r50.cache, r52.cache\n 8de: bed10b242a11         convert          s32_to_f, r52.cache, r49.cache, rte\n 8e4: fe49aff08014         and              r50.cache, r55h.cache, 15\n 8ea: aec500f006040011     bfeil            r49.cache, 0, r55h, 4\n 8f2: bad566a22a6c4211     fmadd32          r53.cache, r19, r53.cache, r54\n 8fa: bec90b442a11         convert          s32_to_f, r50.cache, r50.cache, rte\n 900: bad146822a6a4215     fmadd32          r52.cache, r35, r52.cache, r53\n 908: bec50b242a11         convert          s32_to_f, r49.cache, r49.cache, rte\n 90e: bac968422a684211     fmadd32          r50.cache, r20, r50.cache, r52\n 916: 3ac5482226a44215     fmadd32          r49, r36, r49, r50.cache\n 91e: 92c9ae10005ac0855014 icmpsel          seq, r50.cache, r55l.cache, 1, r45, r46\n 928: 12c96e30005440865014 icmpsel          seq, r50, r55l, 3, r42, r50\n 932: 3800                 wait             0\n 934: bad1a2120ea44215     fmadd32          r52.cache, r49.cache, r48h.discard, r50.cache\n 93c: 2ec9002028080010     bfeil            r50, 0, r1.cache, 8\n 944: 12b5ae1000a8a0855014 icmpsel          seq, r45, r55l.cache, 1, r52.cache, r45\n 94e: 2ec5002028180010     bfeil            r49, 0, r1.cache, 24\n 956: 12a9ae3000a840855014 icmpsel          seq, r42, r55l.cache, 3, r52.cache, r42\n 960: 2ed9004028080010     bfeil            r54, 0, r2.cache, 8\n 968: 62d200000010         mov_imm          r52h, 0\n 96e: 2ed5004028180010     bfeil            r53, 0, r2.cache, 24\n 976: 62de00000010         mov_imm          r55h, 0\n 97c: 2ee1002029080011     bfeil            r56, 0, r41.cache, 8\n 984: 62ee00000010         mov_imm          r59h, 0\n 98a: 2ee9002029180011     bfeil            r58, 0, r41.cache, 24\n 992: 7e5043088010         mov              r52l, r1h\n 998: 7e5c45088010         mov              r55l, r2h\n 99e: 2ee4000006080011     bfeil            r57l, 0, r48l, 8\n 9a6: 7e6c53088014         mov              r59l, r41h\n 9ac: 529546420000         while_icmp       r0l, nseq, r3, 4, 2\n 9b2: 00c0e6f8ffff         jmp_exec_any     0x298\n 9b8: d21600000000         pop_exec         r0l.cache, 2\n 9be: 8e3d01e02d110000     iadd             r47.cache, 1, r47.discard\n 9c6: 52955ea21904         while_icmp       r0l, nseq, r47, u13, 2\n 9cc: 00c098f7ffff         jmp_exec_any     0x164\n 9d2: d21600000000         pop_exec         r0l.cache, 2\n 9d8: d20e00000000         pop_exec         r0l.cache, 1\n 9de: f2150100             get_sr           r5.cache, sr1 (threadgroup_position_in_grid.y)\n 9e2: fe05a0098000         mov              r1.cache, u16\n 9e8: 9e0bc2a228000000     imadd            r2_r3.cache, r1.discard, r5.cache, 0\n 9f0: 92058a0200a401b0     icmpsel          slt, r1.cache, r5.cache, 0, u18, 0\n 9f8: 9e19a0212cc60200     imadd            r6.cache, u16, r1.discard, r3.discard\n a00: 6f8ddc320004         simd_fadd        r3, r46.discard\n a06: 8e0584622d011000     iadd             r1.cache, r2.cache, r43.discard, lsl 2\n a0e: 9e15a2a12ccc0200     imadd            r5.cache, u17, r5.discard, r6.discard\n a16: 920882422c010130     icmpsel          ult, r2l.cache, r1.cache, r2.discard, 1, 0\n a1e: f2023400             get_sr           r0h.cache, sr52 (thread_index_in_simdgroup)\n a22: 8e09c4a02c000000     iadd             r2.cache, r2l.discard, r5.discard\n a2a: 528841000000         if_icmp          r0l, seq, r0h, 0, 1\n a30: 8e1f882134001000     iadd             r7_r8.cache, u4, r1_r2, lsl 2\n a38: 0e21d0a218000000     iadd             r8, r8.discard, u5\n a40: 45190e0500c01200     device_store     0, i32, x, r3, r7_r8, 0, signed, 0\n a48: d20e00000000         pop_exec         r0l.cache, 1\n a4e: 6f8dda320004         simd_fadd        r3, r45.discard\n a54: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n a5a: 8e1f882134001000     iadd             r7_r8.cache, u4, r1_r2, lsl 2\n a62: 0e21d0a218000000     iadd             r8, r8.discard, u5\n a6a: 45191e0500c01200     device_store     0, i32, x, r3, r7_r8, 1, signed, 0\n a72: d20e00000000         pop_exec         r0l.cache, 1\n a78: 6f8dd8320004         simd_fadd        r3, r44.discard\n a7e: f2023400             get_sr           r0h.cache, sr52 (thread_index_in_simdgroup)\n a82: 528841000000         if_icmp          r0l, seq, r0h, 0, 1\n a88: 8e1f882134001000     iadd             r7_r8.cache, u4, r1_r2, lsl 2\n a90: 0e21d0a218000000     iadd             r8, r8.discard, u5\n a98: 45191e0500c41200     device_store     0, i32, x, r3, r7_r8, 1, signed, lsl 1, 0\n aa0: d20e00000000         pop_exec         r0l.cache, 1\n aa6: 6f8dd4320004         simd_fadd        r3, r42.discard\n aac: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n ab2: 8e1f88213c001000     iadd             r7_r8.cache, u4, r1_r2.discard, lsl 2\n aba: 0e21d0a218000000     iadd             r8, r8.discard, u5\n ac2: 45193e0500c01200     device_store     0, i32, x, r3, r7_r8, 3, signed, 0\n aca: 520e00000000         pop_exec         r0l, 1\n ad0: 8800                 stop    \n\n\n \nThe kernel start at the label compute shader:, and I usually analyze the structure by finding jmp and device_load instructions. In this code there are two jmp_exec_any at 0x9b2 and 0x9cc, jumping to 0x298 and 0x164. There are corresponding the two loops in our codes. Between 0x164 and 0x298 we can see a block of device_load:\n 180: 0e074e6324141000     iadd             r33_r34, r39_r40, r3, lsl 2\n 188: 05296e4640c0f200     device_load      1, i32, xyzw, r5_r6_r7_r8, r39_r40, r3, unsigned\n 190: 0549124540c8f200     device_load      1, i32, xyzw, r9_r10_r11_r12, r33_r34, 1, signed, lsl 2\n 198: 0569224540c8f200     device_load      1, i32, xyzw, r13_r14_r15_r16, r33_r34, 2, signed, lsl 2\n 1a0: 0589324540c8f200     device_load      1, i32, xyzw, r17_r18_r19_r20, r33_r34, 3, signed, lsl 2\n 1a8: 05a9420540c8f200     device_load      0, i32, xyzw, r21_r22_r23_r24, r33_r34, 4, signed, lsl 2\n 1b0: 05c9520540c8f200     device_load      0, i32, xyzw, r25_r26_r27_r28, r33_r34, 5, signed, lsl 2\n 1b8: 05e9620540c8f200     device_load      0, i32, xyzw, r29_r30_r31_r32, r33_r34, 6, signed, lsl 2\n 1c0: 0509720540c9f200     device_load      0, i32, xyzw, r33_r34_r35_r36, r33_r34, 7, signed, lsl 2\n 1c8: 3801                 wait             1\n 1ca: aa8d4ca22400         fadd32           r3.cache, r6, r5\n 1d0: aa8dc6e22400         fadd32           r3.cache, r3.discard, r7\n\nThe first device_load loads 4 i32 to 32-bit registers r5,r6,r7,r8 from address stored in r39,r40. The whole 8 device_load load 32 float.  That's the 32 float stored in y_curr before starting the inner loop. The wait 1 means waiting until group 1 load instructions finished, which are the first four device_load in this block.\nBetween 0x298 and 0x9b2 is our inner loop, we still look for device_load. Notice that r48h means the high 16-bit of register r48 and r48l means the low 16-bit.\n 5ea: 1e535222014a4314     imadd            r52_r53, r41, 18, r37_r38\n 5f2: 8584020500c11000     device_load      0, i16, x, r48h, r1_r2, 0, signed\n 5fa: 8508124500c21000     device_load      1, i16, x, r65l, r1_r2, 1, signed\n 602: 8588124500c5f000     device_load      1, i16, xyzw, r49l_r49h_r50l_r50h, r1_r2, 1, signed, lsl 1\n 60a: 85d0324500c53000     device_load      1, i16, xy, r58l_r58h, r1_r2, 3, signed, lsl 1\n 612: 85bc224500c91000     device_load      1, i16, x, r55h, r1_r2, 2, signed, lsl 2\n 61a: 8504080560c01000     device_load      0, i16, x, r0h, r52_r53, 0, signed\n 622: 8580180560c11000     device_load      0, i16, x, r48l, r52_r53, 1, signed\n 62a: 8508180560c4f000     device_load      0, i16, xyzw, r1l_r1h_r2l_r2h, r52_r53, 1, signed, lsl 1\n 632: 8548380560c53000     device_load      0, i16, xy, r41l_r41h, r52_r53, 3, signed, lsl 1\n 63a: 8520280560c81000     device_load      0, i16, x, r4l, r52_r53, 2, signed, lsl 2\n 642: 3801                 wait             1\n 644: fe54a3088014         mov              r53l.cache, r49h.cache\n\nIn our code the inner loop runs 4 times, with each time loading one block. In the assembly the inner loop actually runs 2 times, each time loading 2 blocks. When it loads a block, it first loads one 16-bit, then another 16-bit, then four 16-bit, then two 16-bit and finally one last 16-bit. Before and after these device_load we also see a lot mov and bfeil, meaning the GPU copys a 16-bit to another register and mask its high or low 8-bit.\nExample 1\nHere is the kernel_mul_mat_q4_0_f32 function from PR #2248 . I removed some logics for simplicity.\nDetails\n\n#include <metal_stdlib>\nusing namespace metal;\n#define QK4_0 32\n\ntypedef struct {\n    half    d;             // delta\n    uint16_t qs[QK4_0 / 4]; // nibbles / quants\n} block_q4_0;\n\n\n// function for calculate inner product between a q4_0 block and 32 floats (yl), sumy is SUM(yl[i])\nfloat block_q_n_dot_y(block_q4_0 qb_curr, float sumy, thread float * yl) {\n    float d = qb_curr.d;\n    float acc = sumy * -8.f;\n    for (int i = 0; i < 16; i+=2) {\n        acc += yl[i]     * (qb_curr.qs[i / 2] & 0x000F) + yl[i + 16] * (qb_curr.qs[i / 2] & 0x00F0);\n        acc += yl[i + 1] * (qb_curr.qs[i / 2] & 0x0F00) + yl[i + 17] * (qb_curr.qs[i / 2] & 0xF000);\n    }\n    return d * acc;\n}\n\n#define N_DST 4 // each SIMD group works on 4 rows\n#define N_SIMDGROUP 2 // number of SIMD groups in a thread group\n#define N_SIMDWIDTH 32 // assuming SIMD group size is 32\n\ntemplate<typename block_q_type>\nvoid mul_vec_q_n_f32(device const void * src0, device const float * src1, device float * dst,\n                    int64_t ne00, int64_t ne10, int64_t ne0, int64_t ne01,\n                    uint2 tgpig, uint tiisg, uint sgitg) {\n    const int nb = ne00/QK4_0;\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    device const block_q_type * x = (device const block_q_type *) src0 + (r0 * N_SIMDGROUP + sgitg) * N_DST * nb;\n    device const float      * y = (device const float      *) src1 + r1*ne10;\n    block_q_type qb_curr;\n    float4 y_curr[8];       // src1 vector cache\n    float sumf[N_DST]={0.f}, all_sum;\n    thread float * yl=(thread float *)y_curr;\n\n    // bootstrap\n    qb_curr = x[tiisg];\n    // each thread in a SIMD group deals with 1 block.\n    for (int column = 0; column < nb / N_SIMDWIDTH; column++) {\n\n        float sumy = 0;\n        for (int i = 0; i < QK4_0 / 4; i++) {\n            y_curr[i] = *((device float4  *)(y + N_SIMDWIDTH * (tiisg + column * QK4_0) + 4 * i));\n            sumy += y_curr[i][0] + y_curr[i][1] + y_curr[i][2] + y_curr[i][3];\n        }\n        // we don't right shift packed 4-bit weights, so we have to devide y by 16/256/4096 to conpensate this.\n        // this design is q4_0 and q4_1 centered, but I think most of the people use these two quantizations.\n        for (int i = 0; i < 32; i++) {\n            yl[i] *= pow(1.f/16.f, 2 * (i % 2) + i / 16);\n        }\n\n        for (int row = 0; row < N_DST; row++) {\n            sumf[row] += block_q_n_dot_y(qb_curr, sumy, yl);\n            qb_curr = x[tiisg + ((row + 1) % N_DST) * nb + (column + ((row + 1) / N_DST)) * N_SIMDWIDTH];\n        }\n    }\n\n    for (int row = 0; row < N_DST; ++row) {\n        all_sum = simd_sum(sumf[row]);\n        if (tiisg == 0) {\n            dst[r1*ne0 + (r0 * N_SIMDGROUP + sgitg) * N_DST + row] = all_sum;\n        }\n    }\n\n}\n\nkernel void kernel_mul_mat_q4_0_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne10,\n        constant   int64_t & ne0,\n        constant   int64_t & ne01[[buffer(4)]],\n        uint2 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n    mul_vec_q_n_f32<block_q4_0>(src0,src1,dst,ne00,ne10,ne0,ne01,tgpig,tiisg,sgitg);\n}\n\n\n\n \nAnd here is the disassembled code (very long):\nDetails\n\ncompute shader prolog:\n   0: 05110c0d00c43200     device_load      0, i32, xy, r2_r3, u6_u7, 0, signed, lsl 1\n   8: 3800                 wait             0\n   a: e205ffffffff         mov_imm          r1.cache, 4294967295, 0b0\n  10: 922d8602008200b0     icmpsel          slt, r11.cache, r3.cache, 0, r1.cache, 0\n  18: e21520000000         mov_imm          r5.cache, 32, 0b0\n  1e: 9205960200c200b0     icmpsel          slt, r1.cache, r11.cache, 0, r1.discard, 0\n  26: be19ca0e0000         ffs              r6.cache, r5.discard\n  2c: fe0dc26a6c00         xor              r3.cache, r1.discard, r3.discard\n  32: e20500000000         mov_imm          r1.cache, 0, 0b0\n  38: fe09964a6c00         xor              r2.cache, r11.cache, r2.discard\n  3e: be15820e0000         ffs              r5.cache, r1.cache\n  44: 8e1fc46b65000000     isub             r7_r8.cache, r2_r3.discard, r11.sx\n  4c: 8e0d3fc82c000000     isub             r3.cache, 63, r6.discard\n  54: 8e091fa82c000000     isub             r2.cache, 31, r5.discard\n  5c: 9215420200c6408c     icmpsel          seq, r5.cache, r1, 0, r3.discard, r2.discard\n  64: be0d8e0e0000         ffs              r3.cache, r7.cache\n  6a: be09900e0000         ffs              r2.cache, r8.cache\n  70: 8e0d3f682c000000     isub             r3.cache, 63, r3.discard\n  78: 8e091f482c000000     isub             r2.cache, 31, r2.discard\n  80: 9209900200c6408c     icmpsel          seq, r2.cache, r8.cache, 0, r3.discard, r2.discard\n  88: 8e0b8a4a2c000000     isub             r2_r3.cache, r5.cache, r2.discard\n  90: fe198e0ae500         or               r6.cache, r7.cache, r8\n  96: 9210cc0200010190     icmpsel          seq, r4l.cache, r6.discard, 0, 1, 0\n  9e: 921844f203010150     icmpsel          ugt, r6l.cache, r2, 63, 1, 0\n  a6: 9202860200010150     icmpsel          ugt, r0h.cache, r3.cache, 0, 1, 0\n  ae: 9202460200cc108c     icmpsel          seq, r0h.cache, r3, 0, r6l.discard, r0h.discard\n  b6: 9210c10000c81090     icmpsel          seq, r4l.cache, r0h.discard, 0, r4l.discard, 1\n  be: 9202caf203018188     icmpsel          seq, r0h.cache, r5.discard, 63, 1, r4l.cache\n  c6: 1219c800004e0090     icmpsel          seq, r6, r4l.discard, 0, r7, 0\n  ce: e2000000             mov_imm          r0l.cache, 0\n  d2: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n  d8: 20c04a010000         jmp_exec_none    0x222\n  de: 8e273f4838000000     isub             r9_r10.cache, 63, r2_r3.cache\n  e6: 8e15014028000000     iadd             r5.cache, 1, r2.cache\n  ee: 92028a422c010130     icmpsel          ult, r0h.cache, r5.cache, r2.discard, 1, 0\n  f6: 8e09202829000000     isub             r2.cache, 32, r9.cache\n  fe: 8e19c1602c000000     iadd             r6.cache, r0h.discard, r3.discard\n 106: aea900e028c40200     bfeil            r10.cache, 0, r7.cache, r2.discard\n 10e: 8e0d920a02000000     isub             r3.cache, r9.cache, 32\n 116: ae3100e028920200     bfi              r12.cache, 0, r7.cache, r9.cache\n 11e: fe094acae400         or               r2.cache, r5, r6\n 124: ae29d40225d20200     bfi              r10.cache, r10.discard, r8, r9.discard\n 12c: 9202c40200010190     icmpsel          seq, r0h.cache, r2.discard, 0, 1, 0\n 134: ae2500e024860200     bfi              r9.cache, 0, r7, r3.cache\n 13c: 1229860200d420ad     icmpsel          slt, r10, r3.cache, 0, r10.discard, r9.discard\n 144: 1225c60200d800b0     icmpsel          slt, r9, r3.discard, 0, r12.discard, 0\n 14c: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n 152: 20c0bc000000         jmp_exec_none    0x20E\n 158: e20900000000         mov_imm          r2.cache, 0, 0b0\n 15e: ae39ce06298a0200     extr             r14.cache, r7.discard, r8.cache, r5.cache\n 166: e20d00000000         mov_imm          r3.cache, 0, 0b0\n 16c: aebd00002d8a0200     bfeil            r15.cache, 0, r8.discard, r5.cache\n 174: e23100000000         mov_imm          r12.cache, 0, 0b0\n 17a: 623500000000         mov_imm          r13, 0, 0b0\n 180: 421000000000         push_exec        r0l, 2\n 186: 8e4300c0bd000000     iadd             r16_r17.cache, 0, r14_r15.discard, lsl 1\n 18e: 92059402000101b0     icmpsel          slt, r1.cache, r10.cache, 0, 1, 0\n 196: 8e4f0020b9000000     iadd             r19_r20.cache, 0, r9_r10.cache, lsl 1\n 19e: fe41e02ae800         or               r16.cache, r16.discard, r1.cache\n 1a4: e205ffffffff         mov_imm          r1.cache, 4294967295, 0b0\n 1aa: 0e1f1f083a000000     isub             r7_r8, 31, r16_r17.cache\n 1b2: 7e25d86aee00         or               r9, r12.discard, r19.discard\n 1b8: 8e178a1b00000000     isub             r5_r6.cache, r5_r6.cache, 1\n 1c0: 7e29da8aee00         or               r10, r13.discard, r20.discard\n 1c6: 9205d002008200b0     icmpsel          slt, r1.cache, r8.discard, 0, r1.cache, 0\n 1ce: fe0d4acae400         or               r3.cache, r5, r6\n 1d4: fe1d82028200         and              r7.cache, r1.cache, 32\n 1da: fe0582128000         and              r1.cache, r1.cache, 1\n 1e0: 0e3be0eb2c000000     isub             r14_r15, r16_r17.discard, r7.discard\n 1e8: 9202c60200010190     icmpsel          seq, r0h.cache, r3.discard, 0, 1, 0\n 1f0: 7e31820a8000         mov              r12, r1.cache\n 1f6: 7e35440a8000         mov              r13, r2\n 1fc: 5294c1000000         while_icmp       r0l, seq, r0h.discard, 0, 2\n 202: 00c084ffffff         jmp_exec_any     0x186\n 208: d21600000000         pop_exec         r0l.cache, 2\n 20e: d20e00000000         pop_exec         r0l.cache, 1\n 214: 8e1f0020bd000000     iadd             r7_r8.cache, 0, r9_r10.discard, lsl 1\n 21c: fe19c2eaec00         or               r6.cache, r1.discard, r7.discard\n 222: 520e00000000         pop_exec         r0l, 1\n 228: fe15cc6a6900         xor              r5.cache, r6.discard, r11.cache\n 22e: e205ffffffff         mov_imm          r1.cache, 4294967295, 0b0\n 234: 8e09ca6a2d000000     isub             r2.cache, r5.discard, r11.discard\n 23c: 9205840200c200b0     icmpsel          slt, r1.cache, r2.cache, 0, r1.discard, 0\n 244: ae8500202c1b0000     bfeil            r1.cache, 0, r1.discard, 27\n 24c: 8e0544222c000000     iadd             r1.cache, r2, r1.discard\n 254: 2e85c25600000000     asr              r1, r1.discard, 5\n 25c: c510803d01803000     uniform_store    2, i16, xy, 0, r2l_r2h, 24\n 264: c508a03d01803000     uniform_store    2, i16, xy, 0, r1l_r1h, 26\n 26c: 8800                 stop             \n\ncompute shader:\n   0: f2023500             get_sr           r0h.cache, sr53 (simdgroup_index_in_threadgroup)\n   4: f2050000             get_sr           r1.cache, sr0 (threadgroup_position_in_grid.x)\n   8: 626900000000         mov_imm          r26, 0, 0b0\n   e: 8e65c120ac000000     iadd             r25.cache, r0h.discard, r1.discard, lsl 1\n  16: 626d00000000         mov_imm          r27, 0, 0b0\n  1c: 627100000000         mov_imm          r28, 0, 0b0\n  22: 627500000000         mov_imm          r29, 0, 0b0\n  28: e2000000             mov_imm          r0l.cache, 0\n  2c: 52c898f10100         if_icmp          r0l, sgt, u12, 31, 1\n  32: 8e09002027001000     iadd             r2.cache, 0, r25, lsl 2\n  3a: 9e05c48219000000     imadd            r1.cache, r2.discard, u12, 0\n  42: 9e57c22201800100     imadd            r21_r22.cache, r1.discard, 18, u0\n  4a: f2023400             get_sr           r0h.cache, sr52 (thread_index_in_simdgroup)\n  4e: 8e59ec2218000000     iadd             r22.cache, r22.discard, u1\n  56: 1e17c120016a0300     imadd            r5_r6, r0h.discard, 18, r21_r22\n  5e: 20c06a080000         jmp_exec_none    0x8C8\n  64: 85040a0500c8f000     device_load      0, i16, xyzw, r0h_r1l_r1h_r2l, r5_r6, 0, signed, lsl 2\n  6c: 85141a4500c8f000     device_load      1, i16, xyzw, r2h_r3l_r3h_r4l, r5_r6, 1, signed, lsl 2\n  74: 85882a4500c91000     device_load      1, i16, x, r49l, r5_r6, 2, signed, lsl 2\n  7c: f21d0100             get_sr           r7.cache, sr1 (threadgroup_position_in_grid.y)\n  80: fe159c098000         mov              r5.cache, u14\n  86: 9e17cae228000000     imadd            r5_r6.cache, r5.discard, r7.cache, 0\n  8e: 92218e0200a801b0     icmpsel          slt, r8.cache, r7.cache, 0, u20, 0\n  96: 626d00000000         mov_imm          r27, 0, 0b0\n  9c: 9e199c012dcc0200     imadd            r6.cache, u14, r8.discard, r6.discard\n  a4: 626900000000         mov_imm          r26, 0, 0b0\n  aa: 9e199ee12ccc0200     imadd            r6.cache, u15, r7.discard, r6.discard\n  b2: e27900000000         mov_imm          r30.cache, 0, 0b0\n  b8: 8e5f84a13c001000     iadd             r23_r24.cache, u2, r5_r6.discard, lsl 2\n  c0: 627100000000         mov_imm          r28, 0, 0b0\n  c6: 627500000000         mov_imm          r29, 0, 0b0\n  cc: 8e61f06218000000     iadd             r24.cache, r24.discard, u3\n  d4: 421000000000         push_exec        r0l, 2\n  da: ae1500c0270a0000     bfi              r5.cache, 0, r30, 10\n  e2: f2183400             get_sr           r6l.cache, sr52 (thread_index_in_simdgroup)\n  e6: 8e1900c08c000000     iadd             r6.cache, 0, r6l.discard, lsl 1\n  ee: 8e15cac22c002000     iadd             r5.cache, r5.discard, r6.discard, lsl 4\n  f6: 0e3b6ea324101000     iadd             r46_r47, r23_r24, r5, lsl 2\n  fe: 0529ae4620c0f200     device_load      1, i32, xyzw, r5_r6_r7_r8, r23_r24, r5, unsigned\n 106: 05491c4550c8f200     device_load      1, i32, xyzw, r9_r10_r11_r12, r46_r47, 1, signed, lsl 2\n 10e: 05692c4550c8f200     device_load      1, i32, xyzw, r13_r14_r15_r16, r46_r47, 2, signed, lsl 2\n 116: 05893c0550c8f200     device_load      0, i32, xyzw, r17_r18_r19_r20, r46_r47, 3, signed, lsl 2\n 11e: 05114c0550c9f200     device_load      0, i32, xyzw, r34_r35_r36_r37, r46_r47, 4, signed, lsl 2\n 126: 05315c0550c9f200     device_load      0, i32, xyzw, r38_r39_r40_r41, r46_r47, 5, signed, lsl 2\n 12e: 05516c0550c9f200     device_load      0, i32, xyzw, r42_r43_r44_r45, r46_r47, 6, signed, lsl 2\n 136: 05917c0550c9f200     device_load      0, i32, xyzw, r50_r51_r52_r53, r46_r47, 7, signed, lsl 2\n 13e: 3801                 wait             1\n 140: aafd8ca22400         fadd32           r31.cache, r6.cache, r5\n 146: aafdfee22400         fadd32           r31.cache, r31.discard, r7\n 14c: aafdfe022900         fadd32           r31.cache, r31.discard, r8.cache\n 152: aafdfe422900         fadd32           r31.cache, r31.discard, r10.cache\n 158: aafdfe222500         fadd32           r31.cache, r31.discard, r9\n 15e: aa81fe622510         fadd32           r32.cache, r31.discard, r11\n 164: aa81c0822914         fadd32           r32.cache, r32.discard, r12.cache\n 16a: 1a99cc721200         fmul32           r6, r6.discard, u19h\n 170: aa81c0c22914         fadd32           r32.cache, r32.discard, r14.cache\n 176: 1aa1d0721200         fmul32           r8, r8.discard, u19h\n 17c: aa81c0a22514         fadd32           r32.cache, r32.discard, r13\n 182: 1aa9d4721200         fmul32           r10, r10.discard, u19h\n 188: aa81c0e22514         fadd32           r32.cache, r32.discard, r15\n 18e: 1ab1d8721200         fmul32           r12, r12.discard, u19h\n 194: 2a81c0022614         fadd32           r32, r32.discard, r16\n 19a: 1ab9dc721200         fmul32           r14, r14.discard, u19h\n 1a0: 3800                 wait             0\n 1a2: 627d00000000         mov_imm          r31, 0, 0b0\n 1a8: aa81c0422a14         fadd32           r32.cache, r32.discard, r18.cache\n 1ae: 1ac1e0721200         fmul32           r16, r16.discard, u19h\n 1b4: aa81c0222614         fadd32           r32.cache, r32.discard, r17\n 1ba: 1ac9e4721200         fmul32           r18, r18.discard, u19h\n 1c0: aa85c0622a14         fadd32           r33.cache, r32.discard, r19.cache\n 1c6: 9a81a8721210         fmul32           r32.cache, r20.cache, u19h\n 1cc: aa85c2822e14         fadd32           r33.cache, r33.discard, r20.discard\n 1d2: 1ad186621204         fmul32           r20, r35.cache, u19l\n 1d8: aa8dc2622c15         fadd32           r35.cache, r33.discard, r35.discard\n 1de: 1a8584420014         fmul32           r33, r34.cache, 0.0625\n 1e4: aa8dc6422c15         fadd32           r35.cache, r35.discard, r34.discard\n 1ea: 1a8988420014         fmul32           r34, r36.cache, 0.0625\n 1f0: aa91c6822c15         fadd32           r36.cache, r35.discard, r36.discard\n 1f6: 1a8d8a621214         fmul32           r35, r37.cache, u19l\n 1fc: aa95c8a22c15         fadd32           r37.cache, r36.discard, r37.discard\n 202: 1a918e621214         fmul32           r36, r39.cache, u19l\n 208: aa9dcae22c15         fadd32           r39.cache, r37.discard, r39.discard\n 20e: 1a958c420014         fmul32           r37, r38.cache, 0.0625\n 214: aa9dcec22c15         fadd32           r39.cache, r39.discard, r38.discard\n 21a: 1a9990420014         fmul32           r38, r40.cache, 0.0625\n 220: aaa1ce022d15         fadd32           r40.cache, r39.discard, r40.discard\n 226: 1a9d92621214         fmul32           r39, r41.cache, u19l\n 22c: aaa5d0222d15         fadd32           r41.cache, r40.discard, r41.discard\n 232: 1aa196621214         fmul32           r40, r43.cache, u19l\n 238: aaadd2622d15         fadd32           r43.cache, r41.discard, r43.discard\n 23e: 1aa594420014         fmul32           r41, r42.cache, 0.0625\n 244: aaadd6422d15         fadd32           r43.cache, r43.discard, r42.discard\n 24a: 1aa998420014         fmul32           r42, r44.cache, 0.0625\n 250: aab1d6822d15         fadd32           r44.cache, r43.discard, r44.discard\n 256: 1aad9a621214         fmul32           r43, r45.cache, u19l\n 25c: aab5d8a22d15         fadd32           r45.cache, r44.discard, r45.discard\n 262: 1ab1a6621214         fmul32           r44, r51.cache, u19l\n 268: aab9da622e15         fadd32           r46.cache, r45.discard, r51.discard\n 26e: 1ab5a4420014         fmul32           r45, r50.cache, 0.0625\n 274: aabddc422e15         fadd32           r47.cache, r46.discard, r50.discard\n 27a: 9ab9a8420014         fmul32           r46.cache, r52.cache, 0.0625\n 280: aac1de822e15         fadd32           r48.cache, r47.discard, r52.discard\n 286: 1abdaa621214         fmul32           r47, r53.cache, u19l\n 28c: aac1e0a22e15         fadd32           r48.cache, r48.discard, r53.discard\n 292: 421000000000         push_exec        r0l, 2\n 298: 7e5985f08010         and              r54, r2h.cache, 15\n 29e: 7e5585008313         and              r53, r2h.cache, 240\n 2a4: 7e5185409210         and              r52, r2h.cache, u18l\n 2aa: 7e4d45509210         and              r51, r2h, u18h\n 2b0: 7e6d86f08010         and              r59, r3l.cache, 15\n 2b6: 7e6586008313         and              r57, r3l.cache, 240\n 2bc: 7e6186409210         and              r56, r3l.cache, u18l\n 2c2: 7e5d86509210         and              r55, r3l.cache, u18h\n 2c8: 7e7d87f08010         and              r63, r3h.cache, 15\n 2ce: 7e4987008313         and              r50, r3h.cache, 240\n 2d4: 7e7587409210         and              r61, r3h.cache, u18l\n 2da: 7e0d47509200         and              r3, r3h, u18h\n 2e0: fe0d88f08020         and              r67.cache, r4l.cache, 15\n 2e6: 7e7988008313         and              r62, r4l.cache, 240\n 2ec: 7e0588409220         and              r65, r4l.cache, u18l\n 2f2: fe69a2f08014         and              r58.cache, r49l.cache, 15\n 2f8: 7e7148509210         and              r60, r4l, u18h\n 2fe: be890b442f21         convert          s32_to_f, r66.cache, r58.discard, rte\n 304: fe01a2008327         and              r64.cache, r49l.cache, 240\n 30a: 9ae960020217         fmul32           r58.cache, r48, -8.0\n 310: be910b042c22         convert          s32_to_f, r68.cache, r64.discard, rte\n 316: fe01a2409224         and              r64.cache, r49l.cache, u18l\n 31c: ba9566422c744222     fmadd32          r69.cache, r19, r66.discard, r58\n 324: be890b042c22         convert          s32_to_f, r66.cache, r64.discard, rte\n 32a: fe01a2509224         and              r64.cache, r49l.cache, u18h\n 330: bac55c822cca8216     fmadd32          r49.cache, r46, r68.discard, r69.discard\n 338: be810b042c22         convert          s32_to_f, r64.cache, r64.discard, rte\n 33e: ba8940422ca24226     fmadd32          r66.cache, r32, r66.discard, r49.cache\n 346: bec50b642c12         convert          s32_to_f, r49.cache, r67.discard, rte\n 34c: ba815e022cc48226     fmadd32          r64.cache, r47, r64.discard, r66.discard\n 354: bef90bc42f11         convert          s32_to_f, r62.cache, r62.discard, rte\n 35a: ba8162222ac08221     fmadd32          r64.cache, r17, r49.cache, r64.discard\n 362: bec50b242c12         convert          s32_to_f, r49.cache, r65.discard, rte\n 368: baf95ac22fc08215     fmadd32          r62.cache, r45, r62.discard, r64.discard\n 370: bef10b842f11         convert          s32_to_f, r60.cache, r60.discard, rte\n 376: baf964222afc4211     fmadd32          r62.cache, r18, r49.cache, r62.discard\n 37e: bec50be42f11         convert          s32_to_f, r49.cache, r63.discard, rte\n 384: baf158822ffc4215     fmadd32          r60.cache, r44, r60.discard, r62.discard\n 38c: bec90b442e11         convert          s32_to_f, r50.cache, r50.discard, rte\n 392: baf15e222af84211     fmadd32          r60.cache, r15, r49.cache, r60.discard\n 39a: bec50ba42f11         convert          s32_to_f, r49.cache, r61.discard, rte\n 3a0: bac954422ef84215     fmadd32          r50.cache, r42, r50.discard, r60.discard\n 3a8: be8d0b642800         convert          s32_to_f, r3.cache, r3.cache, rte\n 3ae: bac960222ae44211     fmadd32          r50.cache, r16, r49.cache, r50.discard\n 3b6: bec50b642f11         convert          s32_to_f, r49.cache, r59.discard, rte\n 3bc: bac9566228e44214     fmadd32          r50.cache, r43, r3.cache, r50.discard\n 3c4: be8d0b242f01         convert          s32_to_f, r3.cache, r57.discard, rte\n 3ca: bac95a222ae44211     fmadd32          r50.cache, r13, r49.cache, r50.discard\n 3d2: bec50b042f11         convert          s32_to_f, r49.cache, r56.discard, rte\n 3d8: bac9526228e44214     fmadd32          r50.cache, r41, r3.cache, r50.discard\n 3e0: be8d0be42e01         convert          s32_to_f, r3.cache, r55.discard, rte\n 3e6: bac95c222ae44211     fmadd32          r50.cache, r14, r49.cache, r50.discard\n 3ee: bec50bc42e11         convert          s32_to_f, r49.cache, r54.discard, rte\n 3f4: bac9506228e44214     fmadd32          r50.cache, r40, r3.cache, r50.discard\n 3fc: be8d0ba42e01         convert          s32_to_f, r3.cache, r53.discard, rte\n 402: bac956222ae44211     fmadd32          r50.cache, r11, r49.cache, r50.discard\n 40a: bec50b842e11         convert          s32_to_f, r49.cache, r52.discard, rte\n 410: bac94c6228e44214     fmadd32          r50.cache, r38, r3.cache, r50.discard\n 418: be8d0b642e01         convert          s32_to_f, r3.cache, r51.discard, rte\n 41e: bac558222ae44211     fmadd32          r49.cache, r12, r49.cache, r50.discard\n 426: bacd4e6228a24214     fmadd32          r51.cache, r39, r3.cache, r49.cache\n 42e: fe0d84f08000         and              r3.cache, r2l.cache, 15\n 434: fe4584008313         and              r49.cache, r2l.cache, 240\n 43a: bec90b642810         convert          s32_to_f, r50.cache, r3.cache, rte\n 440: fe0d84409200         and              r3.cache, r2l.cache, u18l\n 446: bec50b242a11         convert          s32_to_f, r49.cache, r49.cache, rte\n 44c: bac952422ee64211     fmadd32          r50.cache, r9, r50.discard, r51.discard\n 454: be8d0b642800         convert          s32_to_f, r3.cache, r3.cache, rte\n 45a: bac54a222ae44215     fmadd32          r49.cache, r37, r49.cache, r50.discard\n 462: bac9546228a24210     fmadd32          r50.cache, r10, r3.cache, r49.cache\n 46a: fe0984509200         and              r2.cache, r2l.cache, u18h\n 470: fe0d83f08000         and              r3.cache, r1h.cache, 15\n 476: bec50b442810         convert          s32_to_f, r49.cache, r2.cache, rte\n 47c: fe0983008303         and              r2.cache, r1h.cache, 240\n 482: be8d0b642800         convert          s32_to_f, r3.cache, r3.cache, rte\n 488: bac548222ae44215     fmadd32          r49.cache, r36, r49.cache, r50.discard\n 490: be890b442800         convert          s32_to_f, r2.cache, r2.cache, rte\n 496: ba8d4e6228a24200     fmadd32          r3.cache, r7, r3.cache, r49.cache\n 49e: bac9444228860214     fmadd32          r50.cache, r34, r2.cache, r3.cache\n 4a6: fe0983409200         and              r2.cache, r1h.cache, u18l\n 4ac: fe0d83509200         and              r3.cache, r1h.cache, u18h\n 4b2: bec50b442810         convert          s32_to_f, r49.cache, r2.cache, rte\n 4b8: fe0982f08000         and              r2.cache, r1l.cache, 15\n 4be: be8d0b642800         convert          s32_to_f, r3.cache, r3.cache, rte\n 4c4: bac550222ae44211     fmadd32          r49.cache, r8, r49.cache, r50.discard\n 4cc: be890b442800         convert          s32_to_f, r2.cache, r2.cache, rte\n 4d2: ba8d466228a24204     fmadd32          r3.cache, r35, r3.cache, r49.cache\n 4da: bac54a4228860210     fmadd32          r49.cache, r5, r2.cache, r3.cache\n 4e2: fe0d82008303         and              r3.cache, r1l.cache, 240\n 4e8: fe0982409200         and              r2.cache, r1l.cache, u18l\n 4ee: be8d0b642800         convert          s32_to_f, r3.cache, r3.cache, rte\n 4f4: fe0582509200         and              r1.cache, r1l.cache, u18h\n 4fa: be890b442800         convert          s32_to_f, r2.cache, r2.cache, rte\n 500: ba8d426228624204     fmadd32          r3.cache, r33, r3.cache, r49\n 508: be850b242800         convert          s32_to_f, r1.cache, r1.cache, rte\n 50e: ba894c4228860200     fmadd32          r2.cache, r6, r2.cache, r3.cache\n 516: ba89682228840200     fmadd32          r2.cache, r20, r1.cache, r2.cache\n 51e: f2103400             get_sr           r4l.cache, sr52 (thread_index_in_simdgroup)\n 522: 920dbe2000b6a08b     icmpsel          seq, r3.cache, r31l.cache, 2, r27.cache, r29.cache\n 52a: ae8500e02b020000     bfeil            r1.cache, 0, r31.cache, 2\n 532: ba89841204860200     fmadd32          r2.cache, r2.cache, r0h, r3.cache\n 53a: 8e6101e02b100000     iadd             r56.cache, 1, r31.cache\n 542: 8e0582c22b000000     iadd             r1.cache, r1.cache, r30.cache\n 54a: 1275be000084a087     icmpsel          seq, r29, r31l.cache, 0, r2.cache, r29\n 552: 126dbe2000846087     icmpsel          seq, r27, r31l.cache, 2, r2.cache, r27\n 55a: 8e7d02e02f000000     iadd             r31.cache, 2, r31.discard\n 562: 9e09708219880004     imadd            r2.cache, r56, u12, r4l.cache\n 56a: 8e0d0020a8000000     iadd             r3.cache, 0, r1.cache, lsl 1\n 572: ae8500e02b020000     bfeil            r1.cache, 0, r31.cache, 2\n 57a: 8e4d846228102000     iadd             r51.cache, r2.cache, r3.cache, lsl 4\n 582: fe097e228000         and              r2.cache, r31, 2\n 588: 8e0582c227000000     iadd             r1.cache, r1.cache, r30\n 590: 9e0d848019480000     imadd            r3.cache, r2l.cache, u12, r4l\n 598: 8e450020a8100000     iadd             r49.cache, 0, r1.cache, lsl 1\n 5a0: 1e07e62201aa0304     imadd            r1_r2, r51.discard, 18, r21_r22.cache\n 5a8: 8e0d862226012000     iadd             r3.cache, r3.cache, r49, lsl 4\n 5b0: 1e5b4622016a0310     imadd            r54_r55, r3, 18, r21_r22\n 5b8: 858c020500c9f000     device_load      0, i16, xyzw, r49h_r50l_r50h_r51l, r1_r2, 0, signed, lsl 2\n 5c0: 859c124500c9f000     device_load      1, i16, xyzw, r51h_r52l_r52h_r53l, r1_r2, 1, signed, lsl 2\n 5c8: 85c4224500c91000     device_load      1, i16, x, r56h, r1_r2, 2, signed, lsl 2\n 5d0: 85040c4560c8f000     device_load      1, i16, xyzw, r0h_r1l_r1h_r2l, r54_r55, 0, signed, lsl 2\n 5d8: 85141c4560c8f000     device_load      1, i16, xyzw, r2h_r3l_r3h_r4l, r54_r55, 1, signed, lsl 2\n 5e0: 85882c4560c91000     device_load      1, i16, x, r49l, r54_r55, 2, signed, lsl 2\n 5e8: 3800                 wait             0\n 5ea: fe59a4f08014         and              r54.cache, r50l.cache, 15\n 5f0: fe5da4008317         and              r55.cache, r50l.cache, 240\n 5f6: bed90bc42e11         convert          s32_to_f, r54.cache, r54.discard, rte\n 5fc: fe65a4409214         and              r57.cache, r50l.cache, u18l\n 602: bedd0be42e11         convert          s32_to_f, r55.cache, r55.discard, rte\n 608: bee50b242f11         convert          s32_to_f, r57.cache, r57.discard, rte\n 60e: bae94ac22ef44211     fmadd32          r58.cache, r5, r54.discard, r58.discard\n 616: fe59e4509214         and              r54.cache, r50l.discard, u18h\n 61c: bae942e22ef44215     fmadd32          r58.cache, r33, r55.discard, r58.discard\n 624: bedd0bc42e11         convert          s32_to_f, r55.cache, r54.discard, rte\n 62a: fe59a5f08014         and              r54.cache, r50h.cache, 15\n 630: bae94c222ff44211     fmadd32          r58.cache, r6, r57.discard, r58.discard\n 638: bee50bc42e11         convert          s32_to_f, r57.cache, r54.discard, rte\n 63e: fe59a5008317         and              r54.cache, r50h.cache, 240\n 644: bae968e22ef44211     fmadd32          r58.cache, r20, r55.discard, r58.discard\n 64c: bed90bc42e11         convert          s32_to_f, r54.cache, r54.discard, rte\n 652: fe5da5409214         and              r55.cache, r50h.cache, u18l\n 658: bae54e222ff44211     fmadd32          r57.cache, r7, r57.discard, r58.discard\n 660: bedd0be42e11         convert          s32_to_f, r55.cache, r55.discard, rte\n 666: fe49e5509214         and              r50.cache, r50h.discard, u18h\n 66c: bae544c22ef24215     fmadd32          r57.cache, r34, r54.discard, r57.discard\n 674: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 67a: fe49a6f08014         and              r50.cache, r51l.cache, 15\n 680: bae550e22ef24211     fmadd32          r57.cache, r8, r55.discard, r57.discard\n 688: bedd0b442e11         convert          s32_to_f, r55.cache, r50.discard, rte\n 68e: fe49a6008317         and              r50.cache, r51l.cache, 240\n 694: bae546c22ef24215     fmadd32          r57.cache, r35, r54.discard, r57.discard\n 69c: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 6a2: fe49a6409214         and              r50.cache, r51l.cache, u18l\n 6a8: bae552e22ef24211     fmadd32          r57.cache, r9, r55.discard, r57.discard\n 6b0: 3edd0b442e11         convert          s32_to_f, r55, r50.discard, rte\n 6b6: fe49e6509214         and              r50.cache, r51l.discard, u18h\n 6bc: 3ae54ac22ef24215     fmadd32          r57, r37, r54.discard, r57.discard\n 6c4: 3ed90b442e11         convert          s32_to_f, r54, r50.discard, rte\n 6ca: 3801                 wait             1\n 6cc: fe49a7f08014         and              r50.cache, r51h.cache, 15\n 6d2: bae554e22ef24211     fmadd32          r57.cache, r10, r55.discard, r57.discard\n 6da: bedd0b442e11         convert          s32_to_f, r55.cache, r50.discard, rte\n 6e0: fe49a7008317         and              r50.cache, r51h.cache, 240\n 6e6: bae548c22ef24215     fmadd32          r57.cache, r36, r54.discard, r57.discard\n 6ee: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 6f4: fe49a7409214         and              r50.cache, r51h.cache, u18l\n 6fa: bae556e22ef24211     fmadd32          r57.cache, r11, r55.discard, r57.discard\n 702: bedd0b442e11         convert          s32_to_f, r55.cache, r50.discard, rte\n 708: fe49e7509214         and              r50.cache, r51h.discard, u18h\n 70e: bacd4cc22ef24215     fmadd32          r51.cache, r38, r54.discard, r57.discard\n 716: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 71c: fe49a8f08014         and              r50.cache, r52l.cache, 15\n 722: badd58e22ee64211     fmadd32          r55.cache, r12, r55.discard, r51.discard\n 72a: becd0b442e11         convert          s32_to_f, r51.cache, r50.discard, rte\n 730: fe49a8008317         and              r50.cache, r52l.cache, 240\n 736: badd4ec22eee4215     fmadd32          r55.cache, r39, r54.discard, r55.discard\n 73e: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 744: fe49a8409214         and              r50.cache, r52l.cache, u18l\n 74a: badd5a622eee4211     fmadd32          r55.cache, r13, r51.discard, r55.discard\n 752: becd0b442e11         convert          s32_to_f, r51.cache, r50.discard, rte\n 758: fe49e8509214         and              r50.cache, r52l.discard, u18h\n 75e: badd52c22eee4215     fmadd32          r55.cache, r41, r54.discard, r55.discard\n 766: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 76c: fe49a9f08014         and              r50.cache, r52h.cache, 15\n 772: badd5c622eee4211     fmadd32          r55.cache, r14, r51.discard, r55.discard\n 77a: becd0b442e11         convert          s32_to_f, r51.cache, r50.discard, rte\n 780: fe49a9008317         and              r50.cache, r52h.cache, 240\n 786: badd50c22eee4215     fmadd32          r55.cache, r40, r54.discard, r55.discard\n 78e: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 794: fe49a9409214         and              r50.cache, r52h.cache, u18l\n 79a: badd5e622eee4211     fmadd32          r55.cache, r15, r51.discard, r55.discard\n 7a2: bec90b442e11         convert          s32_to_f, r50.cache, r50.discard, rte\n 7a8: fe4de9509214         and              r51.cache, r52h.discard, u18h\n 7ae: bad154c22eee4215     fmadd32          r52.cache, r42, r54.discard, r55.discard\n 7b6: becd0b642e11         convert          s32_to_f, r51.cache, r51.discard, rte\n 7bc: bad160422ee84211     fmadd32          r52.cache, r16, r50.discard, r52.discard\n 7c4: fe49aaf08014         and              r50.cache, r53l.cache, 15\n 7ca: badd56622ee84215     fmadd32          r55.cache, r43, r51.discard, r52.discard\n 7d2: bed90b442e11         convert          s32_to_f, r54.cache, r50.discard, rte\n 7d8: fe51aa008317         and              r52.cache, r53l.cache, 240\n 7de: fe4daa409214         and              r51.cache, r53l.cache, u18l\n 7e4: fe49ea509214         and              r50.cache, r53l.discard, u18h\n 7ea: bed10b842e11         convert          s32_to_f, r52.cache, r52.discard, rte\n 7f0: bad562c22eee4211     fmadd32          r53.cache, r17, r54.discard, r55.discard\n 7f8: becd0b642e11         convert          s32_to_f, r51.cache, r51.discard, rte\n 7fe: bad55a822eea4215     fmadd32          r53.cache, r45, r52.discard, r53.discard\n 806: bed10b442e11         convert          s32_to_f, r52.cache, r50.discard, rte\n 80c: fe49b1f08014         and              r50.cache, r56h.cache, 15\n 812: bad564622eea4211     fmadd32          r53.cache, r18, r51.discard, r53.discard\n 81a: becd0b442e11         convert          s32_to_f, r51.cache, r50.discard, rte\n 820: fe49b1008317         and              r50.cache, r56h.cache, 240\n 826: bad558822eea4215     fmadd32          r53.cache, r44, r52.discard, r53.discard\n 82e: bed10b442e11         convert          s32_to_f, r52.cache, r50.discard, rte\n 834: fe49b1409214         and              r50.cache, r56h.cache, u18l\n 83a: bad566622eea4211     fmadd32          r53.cache, r19, r51.discard, r53.discard\n 842: becd0b442e11         convert          s32_to_f, r51.cache, r50.discard, rte\n 848: fe49f1509214         and              r50.cache, r56h.discard, u18h\n 84e: bad15c822eea4215     fmadd32          r52.cache, r46, r52.discard, r53.discard\n 856: bec90b442e11         convert          s32_to_f, r50.cache, r50.discard, rte\n 85c: bacd40622ee84215     fmadd32          r51.cache, r32, r51.discard, r52.discard\n 864: bac95e422ee64215     fmadd32          r50.cache, r47, r50.discard, r51.discard\n 86c: 92cdb01000b8a0870014 icmpsel          seq, r51.cache, r56l.cache, 1, r28.cache, r29\n 876: 92cdb03000b4608e1014 icmpsel          seq, r51.cache, r56l.cache, 3, r26.cache, r51.discard\n 880: bac9e4320ee64215     fmadd32          r50.cache, r50.discard, r49h.discard, r51.discard\n 888: 12f1b01000a480874004 icmpsel          seq, r28, r56l.cache, 1, r50.cache, r28\n 892: 12e9f03000e440874004 icmpsel          seq, r26, r56l.discard, 3, r50.discard, r26\n 89c: 52957e420000         while_icmp       r0l, nseq, r31, 4, 2\n 8a2: 00c0f6f9ffff         jmp_exec_any     0x298\n 8a8: d21600000000         pop_exec         r0l.cache, 2\n 8ae: 8e7901c02f000000     iadd             r30.cache, 1, r30.discard\n 8b6: 52957ca21900         while_icmp       r0l, nseq, r30, u13, 2\n 8bc: 00c01ef8ffff         jmp_exec_any     0xDA\n 8c2: d21600000000         pop_exec         r0l.cache, 2\n 8c8: d20e00000000         pop_exec         r0l.cache, 1\n 8ce: f2150100             get_sr           r5.cache, sr1 (threadgroup_position_in_grid.y)\n 8d2: fe05a0098000         mov              r1.cache, u16\n 8d8: 9e0bc2a228000000     imadd            r2_r3.cache, r1.discard, r5.cache, 0\n 8e0: 92058a0200a801b0     icmpsel          slt, r1.cache, r5.cache, 0, u20, 0\n 8e8: 9e19a0212cc60200     imadd            r6.cache, u16, r1.discard, r3.discard\n 8f0: 6f8dfa320000         simd_fadd        r3, r29.discard\n 8f6: 8e0584222f001000     iadd             r1.cache, r2.cache, r25.discard, lsl 2\n 8fe: 9e15a2a12ccc0200     imadd            r5.cache, u17, r5.discard, r6.discard\n 906: 920882422c010130     icmpsel          ult, r2l.cache, r1.cache, r2.discard, 1, 0\n 90e: f2023400             get_sr           r0h.cache, sr52 (thread_index_in_simdgroup)\n 912: 8e09c4a02c000000     iadd             r2.cache, r2l.discard, r5.discard\n 91a: 528841000000         if_icmp          r0l, seq, r0h, 0, 1\n 920: 8e1f882134001000     iadd             r7_r8.cache, u4, r1_r2, lsl 2\n 928: 0e21d0a218000000     iadd             r8, r8.discard, u5\n 930: 45190e0500c01200     device_store     0, i32, x, r3, r7_r8, 0, signed, 0\n 938: d20e00000000         pop_exec         r0l.cache, 1\n 93e: 6f8df8320000         simd_fadd        r3, r28.discard\n 944: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n 94a: 8e1f882134001000     iadd             r7_r8.cache, u4, r1_r2, lsl 2\n 952: 0e21d0a218000000     iadd             r8, r8.discard, u5\n 95a: 45191e0500c01200     device_store     0, i32, x, r3, r7_r8, 1, signed, 0\n 962: d20e00000000         pop_exec         r0l.cache, 1\n 968: 6f95f6320000         simd_fadd        r5, r27.discard\n 96e: f2023400             get_sr           r0h.cache, sr52 (thread_index_in_simdgroup)\n 972: 528841000000         if_icmp          r0l, seq, r0h, 0, 1\n 978: 8e1f882134001000     iadd             r7_r8.cache, u4, r1_r2, lsl 2\n 980: 0e21d0a218000000     iadd             r8, r8.discard, u5\n 988: 45291e0500c41200     device_store     0, i32, x, r5, r7_r8, 1, signed, lsl 1, 0\n 990: d20e00000000         pop_exec         r0l.cache, 1\n 996: 6f8df4320000         simd_fadd        r3, r26.discard\n 99c: 5288c1000000         if_icmp          r0l, seq, r0h.discard, 0, 1\n 9a2: 8e1f88213c001000     iadd             r7_r8.cache, u4, r1_r2.discard, lsl 2\n 9aa: 0e21d0a218000000     iadd             r8, r8.discard, u5\n 9b2: 45193e0500c01200     device_store     0, i32, x, r3, r7_r8, 3, signed, 0\n 9ba: 520e00000000         pop_exec         r0l, 1\n 9c0: 8800                 stop             \n\n\n\n \nThe structure is similar to Example 0, so we only analyze the inner loop.:\n 5b0: 1e5b4622016a0310     imadd            r54_r55, r3, 18, r21_r22\n 5b8: 858c020500c9f000     device_load      0, i16, xyzw, r49h_r50l_r50h_r51l, r1_r2, 0, signed, lsl 2\n 5c0: 859c124500c9f000     device_load      1, i16, xyzw, r51h_r52l_r52h_r53l, r1_r2, 1, signed, lsl 2\n 5c8: 85c4224500c91000     device_load      1, i16, x, r56h, r1_r2, 2, signed, lsl 2\n 5d0: 85040c4560c8f000     device_load      1, i16, xyzw, r0h_r1l_r1h_r2l, r54_r55, 0, signed, lsl 2\n 5d8: 85141c4560c8f000     device_load      1, i16, xyzw, r2h_r3l_r3h_r4l, r54_r55, 1, signed, lsl 2\n 5e0: 85882c4560c91000     device_load      1, i16, x, r49l, r54_r55, 2, signed, lsl 2\n 5e8: 3800                 wait             0\n 5ea: fe59a4f08014         and              r54.cache, r50l.cache, 15\n\nNow when it loads a block it first loads one 16-bit, then four 16-bit and four 16-bit. Before and after these device_load we don't see mov and bfeil any more because now we directly operate on 16-bit values.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2279",
        "createdAt": "2023-07-19T16:06:37Z",
        "author": {
            "login": "lshzh-ww"
        }
    },
    {
        "title": "Request for detailed walkthrough for laypeople",
        "bodyText": "I've managed to get far enough to download three models, and run one model using llama.cpp, but then the model is just chatting with itself in Git Bash (the only way I could figure out how to stop it from talking to itself was to close the window) and ./server -m [model] gives me \"No such file or directory\" and in another attempt to clone it followed by \"&& make -j4\" it populated the files in the Windows directory but I get \"make: command not found.\" I have near-zero experience with programming or with Git and I've been using ChatGPT to learn and troubleshoot and try to figure out next steps, but it's more difficult and leading to dead ends now that they took away the browsing feature. Is there any chance someone would be willing and able to put together (or point me to) an extremely dumbed-down and ridiculously detailed and smooth step-by-step walkthrough for an average Windows user re: what applications to use, what keys to stroke, etc. to get any LLMs running locally on a fairly strong Windows 11 laptop and/or a fairly new and powerful Android device? My sincerest thanks in advance for any help, and apologies if I missed it while searching these discussions unsuccessfully.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2201",
        "createdAt": "2023-07-12T18:28:45Z",
        "author": {
            "login": "involvr"
        }
    },
    {
        "title": "what is \"prompt eval time\" and \"eval time\" ?",
        "bodyText": "what is eval time and prompt eval time? i found these two part of time is quite slow in my gernateion:\nhardware: Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz\ngcc: gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n./main --numa -m ./models/llama-7b-ggml-f16.bin -p \"Building a website can be done in 10 simple steps:\" -n 512\n\nsystem info:\nsystem_info: n_threads = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n\nllama_print_timings:        load time =   907.71 ms\nllama_print_timings:      sample time =   301.80 ms /   512 runs   (    0.59 ms per token,  1696.48 tokens per second)\nllama_print_timings: prompt eval time =  6294.68 ms /   271 tokens (   23.23 ms per token,    43.05 tokens per second)\nllama_print_timings:        eval time = 115222.19 ms /   510 runs   (  225.93 ms per token,     4.43 tokens per second)\nllama_print_timings:       total time = 121963.55 ms",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2260",
        "createdAt": "2023-07-16T04:33:32Z",
        "author": {
            "login": "Xiang-cd"
        }
    },
    {
        "title": "problem about using this on a website",
        "bodyText": "If I successfully created a website using this to power an AI chatbot, can other people on other computer use the ai on the website? They say this AI is installed \"locally\" and I am not sure what that really means.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2184",
        "createdAt": "2023-07-12T02:49:20Z",
        "author": {
            "login": "okgor81"
        }
    },
    {
        "title": "I can't type in llama.cpp",
        "bodyText": "The program initializes, I get to\n== Running in interactive mode. ==\n\nPress Ctrl+C to interject at any time.\nPress Return to return control to LLaMa.\nIf you want to submit another line, end your input in ''.\n\nbut then I can't type anything. I've tried two models, nothing. No matter what I press nothing comes up.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/862",
        "createdAt": "2023-04-09T02:01:53Z",
        "author": {
            "login": "NotchyCookiez"
        }
    },
    {
        "title": "Azure CI brainstorming",
        "bodyText": "If we assume we had enough MS Azure credits, how difficult would it be to make a llama.cpp / whisper.cpp / ggml CI in the cloud that runs on every commit and does some performance and perplexity benchmarks across a variety of hardware and models?\nI don't have any experience in this, so would like to hear some opinions and maybe also see if there are people that would be interested in implementing such type of CI.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1985",
        "createdAt": "2023-06-24T19:13:58Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "How to quantize llama with k-quant quantization?",
        "bodyText": "I could not find any documentation on how to do quantisation of llama model with new k-quant methods: q2_K, q3_K_S, q3_K_M, q3_K_L, q4_K_S, q4_K_M, q5_K_S, q6_K.\nIt will be very helpful if someone could share the steps/code to run that.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2293",
        "createdAt": "2023-07-20T14:05:05Z",
        "author": {
            "login": "dbanka"
        }
    },
    {
        "title": "Improve performance by using GEMM intrinsics, but this seems mainly like matrix-vector computation",
        "bodyText": "Hi, I'm currently evaluating the possibility of computing the mat mul in ggml_compute_forward_mat_mul_f32 (and later the quantized versions) with intrinsic functions for matrix multiplication.\nThe Power10 processor has a specific MMA unit (matrix math accelerator) for many precision types (fp64/fp32/fp16/bf16/i8/i4)\nHowever, after implementing the first version I've seen a huge performance decrease and have then seen that most of the computation seems to be matrix - vector multiplication.\nIf I got that right ne11 & ne00 are the sizes we have to look at and while ne00 has \"good\" size of like 4096 or larger ne11 seems to be almost always be 1 (very few cases it is 8 in the beginning) and my question might be very naive, but is there a way to have matrices instead of vectors to improve this computation?\nHere is a sample comparison between the branches with a 8 core CPU running the llama_f32 model:\n\n\n\nSMT level\nModel\nt/s master\nt/s MMA branch\n\n\n\n\nSMT=8\n7b_f32\n4.75\n<0.01\n\n\nSMT=4\n7b_f32\n4.7\n<0.01\n\n\nSMT=2\n7b_f32\n3.27\n<0.01\n\n\nSMT=1\n7b_f32\n2.79\n<0.01\n\n\n\nSMT is Simultaneous multithreading which allows multiple instruction streams (threads) to run concurrently on the same physical processor\nt/s is the eval_time\nThanks a lot in advance for your feedback!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2291",
        "createdAt": "2023-07-20T12:12:20Z",
        "author": {
            "login": "mgiessing"
        }
    },
    {
        "title": "Global launch, llama2-map module library frame composition \u5168\u7403\u9996\u53d1\uff0cllama2-map\u6a21\u5757\u5e93\u67b6\u6784\u56fe",
        "bodyText": "Global launch, llama2-map module library frame composition\n\u301023-7-20\u3011\u5168\u7403\u9996\u53d1\uff0cllama2-map\u6a21\u5757\u5e93\u67b6\u6784\u56fe\nhttps://github.com/ziwang-com/AGI-MAP",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2282",
        "createdAt": "2023-07-20T02:04:37Z",
        "author": {
            "login": "ziwang-com"
        }
    },
    {
        "title": "Meta releasing a new llama model soon",
        "bodyText": "Some reports around that Meta is releasing a new & improved llama model soon. It will be \"available for commercial use\".\n\"The competitive landscape of AI is going to completely change in the coming months, in the coming weeks maybe, when there will be open source platforms that are actually as good as the ones that are not [opensource],\u201d vice-president and chief AI scientist at Meta, Yann LeCun, said at a conference in Aix-en-Provence last Saturday.\n\nThe Information\nFinancial Times",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2226",
        "createdAt": "2023-07-15T04:33:27Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Next step after 32k context?",
        "bodyText": "Let the model using search engine?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2232",
        "createdAt": "2023-07-15T13:33:02Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "How much VRAM for all 83 layers of 65B and how to split among GPUs?",
        "bodyText": "My llama.cpp setup now has the following GPUs:\n2 P40 24GB\n1 P4 8GB\nI've tried setting the split to 4,4,1 and defining GPU0 (a P40) as the primary (this seems to be the default anyway), but the most layers I can get in GPU without hitting an OOM, however, is 82. It will start up with 83 layers in GPU, but it always throws a CUDA OOM on the first reply.\nThat said, the little P4 is definitely making a difference. It is noticeably faster with it installed.\nI'd use a bigger card, but my server hosting the P40s only has room left in the half-height/length riser bay, and only a P4 fits there. Physically, there's room for a total of 3 P4 cards, but I suspect the riser can't handle that much power being pulled from the PCIe interface alone.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2202",
        "createdAt": "2023-07-12T21:14:15Z",
        "author": {
            "login": "quarterturn"
        }
    },
    {
        "title": "./server CFG parameters",
        "bodyText": "Hi,\n./server seems like it may benefit from the CFG parameters.\nThere's a mention of adding it previously, but I'm uncertain if it's over-complicated: #2083 (comment)\nThank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2181",
        "createdAt": "2023-07-11T18:25:58Z",
        "author": null
    },
    {
        "title": "What cpu doing while offloading all layers in GPU",
        "bodyText": "We already know, with a dumb gpu, the speed is lower than the CPU mode, even -t let the cpu usage 100%.\nThat's the real question.\n(that might be changed cause I didn't update llama.cop a week, but I did not seen any related in recent commits.)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2215",
        "createdAt": "2023-07-13T16:15:45Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "examples/chat-persistent.sh - does it work?",
        "bodyText": "I noticed there's a 'examples/chat-persistent.sh' which would appear to try to provide a sliding window for tokens. It would be fun to be able to maintain a chat between sessions and chat for more than 2048 tokens. The question is, does it work for anyone? The same prompt which works fine in normal interactive mode generates an offtopic, confused mess with chat-persistent.sh.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1939",
        "createdAt": "2023-06-19T13:55:40Z",
        "author": {
            "login": "quarterturn"
        }
    },
    {
        "title": "LongLLaMA: Focused Transformer: Contrastive Training for Context Scaling",
        "bodyText": "https://www.marktechpost.com/2023/07/10/meet-longllama-a-large-language-model-capable-of-handling-long-contexts-of-256k-tokens/?amp\nhttps://arxiv.org/abs/2307.03170\nhttps://github.com/cstankonrad/long_llama",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2173",
        "createdAt": "2023-07-11T05:23:31Z",
        "author": {
            "login": "daboe01"
        }
    },
    {
        "title": "train-text-from-scratch using GPUs",
        "bodyText": "I want to use train-text-from-scratch binary to train a model, however, i do not see the option of using GPUs. By default, it is using CPUs, any ideas?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2179",
        "createdAt": "2023-07-11T17:05:39Z",
        "author": {
            "login": "cclinus"
        }
    },
    {
        "title": "why prompt eval can not reuse",
        "bodyText": "hi, guys\nfrom recent test,i found that longer prompt need a lot time to preprocess before it start gen new token, and if you use the same prompt again, it just start gen new token,but if we add a new word prefix the previous prompt ,then again it will process the full prompt , even if it just change a little, i wonder why we need do that,and can we just cache the old process result ,and just process the unknow token,in my example ,should't only the prefix is need process? thx guys",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2175",
        "createdAt": "2023-07-11T05:33:23Z",
        "author": {
            "login": "callMeMakerRen"
        }
    },
    {
        "title": "GPU usage not change while using CLBlast using AMD GPU",
        "bodyText": "CPU: AMD Ryzen\u2122 7 7735HS\nGPU: AMD 680M\nExpectation:\nHigh GPU usage, not CPU\nCurrent:\nI downloaded the zip file directly from the link\nhttps://github.com/ggerganov/llama.cpp/releases/download/master-5656d10/llama-master-5656d10-bin-win-clblast-x64.zip\nI found High CPU usage, GPU not using,\nWhen I use other app using Directml , GPU usage is correct.\nSoftware version:\nhttps://github.com/ggerganov/llama.cpp/releases/tag/master-5656d10\nPlatform:\nwindows11\nImgur\nPS C:\\Users\\Downloads\\llama-master-5656d10-bin-win-clblast-x64> ./main -t 10 -ngl 32 -m C:\\Github\\models\\wizard-vicuna-13B.ggmlv3.q4_0.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"USER: how to travel from NY to Boston\\nASSISTANT:\"\nmain: build = 813 (5656d10)\nmain: seed  = 1689041934\nggml_opencl: selecting platform: 'AMD Accelerated Parallel Processing'\nggml_opencl: selecting device: 'gfx1035'\nggml_opencl: device FP16 support: true\nllama.cpp: loading model from C:\\Github\\models\\wizard-vicuna-13B.ggmlv3.q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal: n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: model size = 13B\nllama_model_load_internal: ggml ctx size =    0.09 MB\nllama_model_load_internal: using OpenCL for GPU acceleration\nllama_model_load_internal: mem required  = 3585.46 MB (+ 1608.00 MB per state)\nllama_model_load_internal: offloading 32 repeating layers to GPU\nllama_model_load_internal: offloaded 32/41 layers to GPU\nllama_model_load_internal: total VRAM used: 5447 MB\nllama_new_context_with_model: kv self size  = 1600.00 MB\n\nsystem_info: n_threads = 10 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n USER: how to travel from NY to Boston\\nASSISTANT: There are several ways to travel from New York to Boston:\n1. By car: Take I-95 Northbound for about 3 hours and 20 minutes until you reach Boston.\n2. By train: Take the Amtrak Acela or Northeast Regional trains from Penn Station in Manhattan to South Station in Boston, with a travel time of around 3 hours and 15 minutes.\n3. By plane: Fly from JFK or LGA Airport in New York to Boston Logan International Airport, with a flight time of around\n 1 hour and 15 minutes.\n4. By bus: Take the BoltBus, Megabus, or Peter Pan Bus Lines from various locations in Manhattan to Boston, with a travel time of around 3 hours and 30 minutes. [end of text]\n\nllama_print_timings:        load time =  3541.00 ms\nllama_print_timings:      sample time =    33.54 ms /   174 runs   (    0.19 ms per token,  5188.14 tokens per second)\nllama_print_timings: prompt eval time =  6080.93 ms /    17 tokens (  357.70 ms per token,     2.80 tokens per second)\nllama_print_timings:        eval time = 53173.72 ms /   173 runs   (  307.36 ms per token,     3.25 tokens per second)\nllama_print_timings:       total time = 59305.99 ms",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2170",
        "createdAt": "2023-07-11T02:30:44Z",
        "author": {
            "login": "mikeyang01"
        }
    },
    {
        "title": "I can't figure out how to make it 4 bit",
        "bodyText": "I am trying to follow the guide and I can usually get all the way to\nquantize the model to 4-bits (using q4_0 method)\n./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\nwithout error. but there is no ./quantize! no visual folder for me and no file found for the code!\nSteps.txt\nlog.txt",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2155",
        "createdAt": "2023-07-09T13:36:15Z",
        "author": {
            "login": "Zernder"
        }
    },
    {
        "title": "Extending context size via RoPE scaling",
        "bodyText": "Intro\nThis is a discussion about a recently proposed strategy of extending the context size of LLaMA models.\n\nThe original idea is proposed here: https://kaiokendev.github.io/til#extending-context-to-8k\nAn ongoing Reddit discussion here: https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/\n\nMake sure to first get familiar with the info in the links above as there has already been ongoing discussions and results.\nSo far the discussion seems to focus on the coherency of the generated texts when using large context. I think what we can do here in llama.cpp in order to support these investigations is to provide a more objective way of evaluating the proposed method by computing the perplexity at different context sizes with and without fine-tuning. Very initial results already suggest that this idea might be viable, but we should carefully check that we are doing the computations correctly\nPreliminary tests with LLaMA 7B\nApplied the following simple patch as proposed by Reddit user pseudonerv in this comment:\n\ndiff --git a/examples/main/main.cpp b/examples/main/main.cpp\nindex 941312f..7fa3ae2 100644\n--- a/examples/main/main.cpp\n+++ b/examples/main/main.cpp\n@@ -84,8 +84,8 @@ int main(int argc, char ** argv) {\n         return 0;\n     }\n \n-    if (params.n_ctx > 2048) {\n-        fprintf(stderr, \"%s: warning: model does not support context sizes greater than 2048 tokens (%d specified);\"\n+    if (params.n_ctx > 8192) {\n+        fprintf(stderr, \"%s: warning: model does not support context sizes greater than 8192 tokens (%d specified);\"\n                 \"expect poor results\\n\", __func__, params.n_ctx);\n     } else if (params.n_ctx < 8) {\n         fprintf(stderr, \"%s: warning: minimum context size is 8, using minimum size.\\n\", __func__);\ndiff --git a/ggml.c b/ggml.c\nindex 4319683..0aa4bd1 100644\n--- a/ggml.c\n+++ b/ggml.c\n@@ -12172,7 +12172,7 @@ static void ggml_compute_forward_rope_f32(\n                 if (ir++ < ir0) continue;\n                 if (ir   > ir1) break;\n \n-                float theta = (float)p;\n+                float theta = (float)p*0.5;\n \n                 if (!is_neox) {\n                     for (int64_t i0 = 0; i0 < ne0; i0 += 2) {\n@@ -12285,7 +12285,7 @@ static void ggml_compute_forward_rope_f16(\n                 if (ir++ < ir0) continue;\n                 if (ir   > ir1) break;\n \n-                float theta = (float)p;\n+                float theta = (float)p*0.5;\n \n                 if (!is_neox) {\n                     for (int64_t i0 = 0; i0 < ne0; i0 += 2) {\n@@ -12423,7 +12423,7 @@ static void ggml_compute_forward_rope_back_f32(\n                 if (ir++ < ir0) continue;\n                 if (ir   > ir1) break;\n \n-                float theta = (float)p;\n+                float theta = (float)p*0.5;\n \n                 if (!is_neox) {\n                     for (int64_t i0 = 0; i0 < ne0; i0 += 2) {\n@@ -12536,7 +12536,7 @@ static void ggml_compute_forward_rope_back_f16(\n                 if (ir++ < ir0) continue;\n                 if (ir   > ir1) break;\n \n-                float theta = (float)p;\n+                float theta = (float)p*0.5;\n \n                 if (!is_neox) {\n                     for (int64_t i0 = 0; i0 < ne0; i0 += 2) {\n\nThis patch \"scales\" the RoPE position by a factor of 0.5 which should correspond to extending the max context size from 2048 to 4096.\nRunning the following perplexity calculation for 7B LLaMA Q4_0 with context of 4096 yields:\n\n$ \u25b6 make -j && time ./perplexity -m ./models/7B/ggml-model-q4_0.bin -f ./build/wiki.test.raw --no-mmap -t 24 -c 4096\nI llama.cpp build info: \nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI LDFLAGS:  \nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n\ncc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\nggml.c: In function \u2018ggml_compute_forward_rope_f32\u2019:\nggml.c:12175:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12175 |                 float theta = (float)p*0.5;\n      |                                       ^\nggml.c: In function \u2018ggml_compute_forward_rope_f16\u2019:\nggml.c:12288:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12288 |                 float theta = (float)p*0.5;\n      |                                       ^\nggml.c: In function \u2018ggml_compute_forward_rope_back_f32\u2019:\nggml.c:12426:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12426 |                 float theta = (float)p*0.5;\n      |                                       ^\nggml.c: In function \u2018ggml_compute_forward_rope_back_f16\u2019:\nggml.c:12539:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12539 |                 float theta = (float)p*0.5;\n      |                                       ^\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \nexamples/train-text-from-scratch/train-text-from-scratch.cpp: In function \u2018void write_tensor(llama_file*, ggml_tensor*)\u2019:\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2371:21: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2371 |         file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                    ~^~~~~~~~~~~~~\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2386:17: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2386 |     file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                ~^~~~~~~~~~~~~\nexamples/train-text-from-scratch/train-text-from-scratch.cpp: In function \u2018void read_tensor(llama_file*, ggml_tensor*)\u2019:\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2407:17: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2407 |     file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                ~^~~~~~~~~~~~~\n\n====  Run ./main -h for help.  ====\n\nIn file included from /usr/include/string.h:535,\n                 from /usr/include/c++/11/cstring:42,\n                 from examples/train-text-from-scratch/train-text-from-scratch.cpp:7:\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:305:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:306:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:307:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nmain: warning: model does not support context sizes greater than 2048 tokens (4096 specified);expect poor results\nmain: build = 721 (2322ec2)\nmain: seed  = 1687419189\nllama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 4096\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size = 3615.71 MB\nllama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n...................................................................................................\nllama_init_from_file: kv self size  = 2048.00 MB\n\nsystem_info: n_threads = 24 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nperplexity: calculating perplexity over 81 chunks, batch_size=512\nperplexity: 138.49 seconds per pass - ETA 3 hours 6 minutes\n[1]6.0187,[2]7.0714,[3]6.3656,[4]5.5239,[5]5.5817,[6]5.6883,[7]5.7353,[8]5.8797,[9]6.0222,[10]6.0995,[11]5.9346,[12]6.0113,[13]6.0741,[14]6.1430,[15]6.2255,[16]6.3302,[17]6.2853,[18]6.1894,[19]6.1435,[20]6.1202,[21]5.9613,[22]5.8335,[23]5.7270,[24]5.7918,[25]5.9147,[26]5.9784,[27]5.9983,[28]5.9945,[29]6.0096,[30]5.9743,[31]5.9087,[32]5.8371,[33]5.7800,[34]5.7782,[35]5.8298,[36]5.8891,[37]5.8342,[38]5.8005,[39]5.7749,[40]5.7405,[41]5.7550,[42]5.7732,[43]5.7759,[44]5.7750,[45]5.7827,[46]5.8045,[47]5.7804,[48]5.7699,[49]5.7503,[50]5.7619,[51]5.7707,[52]5.8372,[53]5.8738,[54]5.9270,[55]5.9470,[56]5.9599,[57]5.9449,[58]5.9547,[59]5.9601,[60]5.9670,[61]5.9655,[62]5.9458,[63]5.9253,[64]5.9198,[65]5.9334,[66]5.9447,[67]5.9319,[68]5.9465,[69]5.9305,[70]5.9212,[71]5.9203,[72]5.9134,[73]5.9029,[74]5.9025,[75]5.9006,[76]5.9015,[77]5.8926,[78]5.8630,[79]5.8878,[80]5.8870,[81]5.8945,\n\nllama_print_timings:        load time = 16255.05 ms\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings: prompt eval time = 11425948.96 ms / 331776 tokens (   34.44 ms per token,    29.04 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time = 11483720.44 ms\n\nreal\t191m23.868s\nuser\t4568m7.821s\nsys\t1m11.117s\n\nFinal result: 5.8945:\nperplexity: 138.49 seconds per pass - ETA 3 hours 6 minutes\n[1]6.0187,[2]7.0714,[3]6.3656,[4]5.5239,[5]5.5817,[6]5.6883,[7]5.7353,[8]5.8797,[9]6.0222,[10]6.0995,[11]5.9346,[12]6.0113,[13]6.0741,[14]6.1430,[15]6.2255,[16]6.3302,[17]6.2853,[18]6.1894,[19]6.1435,[20]6.1202,[21]5.9613,[22]5.8335,[23]5.7270,[24]5.7918,[25]5.9147,[26]5.9784,[27]5.9983,[28]5.9945,[29]6.0096,[30]5.9743,[31]5.9087,[32]5.8371,[33]5.7800,[34]5.7782,[35]5.8298,[36]5.8891,[37]5.8342,[38]5.8005,[39]5.7749,[40]5.7405,[41]5.7550,[42]5.7732,[43]5.7759,[44]5.7750,[45]5.7827,[46]5.8045,[47]5.7804,[48]5.7699,[49]5.7503,[50]5.7619,[51]5.7707,[52]5.8372,[53]5.8738,[54]5.9270,[55]5.9470,[56]5.9599,[57]5.9449,[58]5.9547,[59]5.9601,[60]5.9670,[61]5.9655,[62]5.9458,[63]5.9253,[64]5.9198,[65]5.9334,[66]5.9447,[67]5.9319,[68]5.9465,[69]5.9305,[70]5.9212,[71]5.9203,[72]5.9134,[73]5.9029,[74]5.9025,[75]5.9006,[76]5.9015,[77]5.8926,[78]5.8630,[79]5.8878,[80]5.8870,[81]5.8945,\nThis is already looking very promising since without applying the \"RoPE scaling\" patch, the perplexity is extremely bad - starts off above 110.0, which can be expected since the vanilla computation does not support context size beyond 2048.\nAdditional tests with context size of 2048:\n\nWithout RoPE scaling: [163] 5.4708\n\n\n$  make -j && time ./perplexity -m ./models/7B/ggml-model-q4_0.bin -f ./build/wiki.test.raw --no-mmap -c 2048 -t 24\nI llama.cpp build info: \nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI LDFLAGS:  \nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n\ncc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\ncc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \nexamples/train-text-from-scratch/train-text-from-scratch.cpp: In function \u2018void write_tensor(llama_file*, ggml_tensor*)\u2019:\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2371:21: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2371 |         file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                    ~^~~~~~~~~~~~~\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2386:17: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2386 |     file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                ~^~~~~~~~~~~~~\nexamples/train-text-from-scratch/train-text-from-scratch.cpp: In function \u2018void read_tensor(llama_file*, ggml_tensor*)\u2019:\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2407:17: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2407 |     file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                ~^~~~~~~~~~~~~\n\n====  Run ./main -h for help.  ====\n\nIn file included from /usr/include/string.h:535,\n                 from /usr/include/c++/11/cstring:42,\n                 from examples/train-text-from-scratch/train-text-from-scratch.cpp:7:\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:305:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:306:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:307:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nmain: build = 724 (bbca06e)\nmain: seed  = 1687421414\nllama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size = 3615.71 MB\nllama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n...................................................................................................\nllama_init_from_file: kv self size  = 1024.00 MB\n\nsystem_info: n_threads = 24 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nperplexity: calculating perplexity over 163 chunks, batch_size=512\nperplexity: 29.91 seconds per pass - ETA 1 hours 21 minutes\n[1]3.9923,[2]5.3389,[3]6.3162,[4]6.4622,[5]5.9443,[6]5.6493,[7]5.1580,[8]4.9461,[9]4.9658,[10]5.0431,[11]5.1182,[12]5.1215,[13]5.0616,[14]5.1292,[15]5.2202,[16]5.3321,[17]5.4008,[18]5.5149,[19]5.5788,[20]5.6355,[21]5.5755,[22]5.4813,[23]5.4999,[24]5.5534,[25]5.5471,[26]5.5828,[27]5.5970,[28]5.6491,[29]5.6481,[30]5.7217,[31]5.7891,[32]5.8522,[33]5.8383,[34]5.8318,[35]5.7966,[36]5.7386,[37]5.7094,[38]5.7035,[39]5.6921,[40]5.7040,[41]5.6324,[42]5.5440,[43]5.4858,[44]5.4032,[45]5.3468,[46]5.3093,[47]5.3060,[48]5.3659,[49]5.4259,[50]5.4784,[51]5.5197,[52]5.5335,[53]5.5438,[54]5.5528,[55]5.5698,[56]5.5450,[57]5.5778,[58]5.5574,[59]5.5457,[60]5.5209,[61]5.4906,[62]5.4601,[63]5.4336,[64]5.3905,[65]5.3565,[66]5.3304,[67]5.3202,[68]5.3309,[69]5.3545,[70]5.3764,[71]5.4046,[72]5.4295,[73]5.3870,[74]5.3834,[75]5.3716,[76]5.3487,[77]5.3396,[78]5.3293,[79]5.3092,[80]5.2992,[81]5.2962,[82]5.3115,[83]5.3299,[84]5.3223,[85]5.3134,[86]5.3201,[87]5.3259,[88]5.3210,[89]5.3312,[90]5.3359,[91]5.3543,[92]5.3527,[93]5.3429,[94]5.3369,[95]5.3398,[96]5.3276,[97]5.3238,[98]5.3080,[99]5.3016,[100]5.3145,[101]5.3226,[102]5.3251,[103]5.3662,[104]5.3941,[105]5.4088,[106]5.4254,[107]5.4511,[108]5.4792,[109]5.4785,[110]5.4911,[111]5.4929,[112]5.5035,[113]5.4953,[114]5.4925,[115]5.4961,[116]5.5013,[117]5.4994,[118]5.5026,[119]5.5040,[120]5.5132,[121]5.5147,[122]5.5055,[123]5.4988,[124]5.4928,[125]5.4807,[126]5.4702,[127]5.4650,[128]5.4691,[129]5.4757,[130]5.4839,[131]5.4918,[132]5.4952,[133]5.4868,[134]5.4739,[135]5.4851,[136]5.4884,[137]5.4806,[138]5.4728,[139]5.4646,[140]5.4613,[141]5.4608,[142]5.4597,[143]5.4605,[144]5.4533,[145]5.4477,[146]5.4465,[147]5.4427,[148]5.4420,[149]5.4448,[150]5.4466,[151]5.4505,[152]5.4492,[153]5.4546,[154]5.4416,[155]5.4180,[156]5.4227,[157]5.4273,[158]5.4457,[159]5.4525,[160]5.4508,[161]5.4582,[162]5.4602,[163]5.4708,\n\nllama_print_timings:        load time =  8579.66 ms\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings: prompt eval time = 4890728.66 ms / 333824 tokens (   14.65 ms per token,    68.26 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time = 4940994.89 ms\n\nreal\t82m21,155s\nuser\t1955m22,704s\nsys\t0m27,572s\n\n\n\nWith RoPE scaling of 0.5: [163] 6.0642\n\n\n$ \u25b6 make -j && time ./perplexity -m ./models/7B/ggml-model-q4_0.bin -f ./build/wiki.test.raw --no-mmap -t 12 -c 2048\nI llama.cpp build info: \nI UNAME_S:  Linux\nI UNAME_P:  x86_64\nI UNAME_M:  x86_64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\nI LDFLAGS:  \nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n\ncc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\ncc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\nggml.c: In function \u2018ggml_compute_forward_rope_f32\u2019:\nggml.c:12175:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12175 |                 float theta = (float)p*0.5;\n      |                                       ^\nggml.c: In function \u2018ggml_compute_forward_rope_f16\u2019:\nggml.c:12288:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12288 |                 float theta = (float)p*0.5;\n      |                                       ^\nggml.c: In function \u2018ggml_compute_forward_rope_back_f32\u2019:\nggml.c:12426:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12426 |                 float theta = (float)p*0.5;\n      |                                       ^\nggml.c: In function \u2018ggml_compute_forward_rope_back_f16\u2019:\nggml.c:12539:39: warning: implicit conversion from \u2018float\u2019 to \u2018double\u2019 to match other operand of binary expression [-Wdouble-promotion]\n12539 |                 float theta = (float)p*0.5;\n      |                                       ^\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \nexamples/train-text-from-scratch/train-text-from-scratch.cpp: In function \u2018void write_tensor(llama_file*, ggml_tensor*)\u2019:\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2371:21: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2371 |         file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                    ~^~~~~~~~~~~~~\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2386:17: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2386 |     file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                ~^~~~~~~~~~~~~\nexamples/train-text-from-scratch/train-text-from-scratch.cpp: In function \u2018void read_tensor(llama_file*, ggml_tensor*)\u2019:\nexamples/train-text-from-scratch/train-text-from-scratch.cpp:2407:17: warning: suggest parentheses around \u2018-\u2019 in operand of \u2018&\u2019 [-Wparentheses]\n 2407 |     file->seek(0-file->tell() & 31, SEEK_CUR);\n      |                ~^~~~~~~~~~~~~\n\n====  Run ./main -h for help.  ====\n\nIn file included from /usr/include/string.h:535,\n                 from /usr/include/c++/11/cstring:42,\n                 from examples/train-text-from-scratch/train-text-from-scratch.cpp:7:\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:305:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:306:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn function \u2018char* strncpy(char*, const char*, size_t)\u2019,\n    inlined from \u2018void init_model(my_llama_model*)\u2019 at examples/train-text-from-scratch/train-text-from-scratch.cpp:307:16:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:34: warning: \u2018char* __builtin_strncpy(char*, const char*, long unsigned int)\u2019 specified bound 32 equals destination size [-Wstringop-truncation]\n   95 |   return __builtin___strncpy_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n   96 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nmain: build = 721 (2322ec2)\nmain: seed  = 1687435610\nllama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size = 3615.71 MB\nllama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n...................................................................................................\nllama_init_from_file: kv self size  = 1024.00 MB\n\nsystem_info: n_threads = 12 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nperplexity: calculating perplexity over 163 chunks, batch_size=512\nperplexity: 63.83 seconds per pass - ETA 2 hours 53 minutes\n[1]4.5277,[2]5.9310,[3]7.0203,[4]7.1517,[5]6.6052,[6]6.3426,[7]5.7903,[8]5.5576,[9]5.5795,[10]5.6517,[11]5.7561,[12]5.7770,[13]5.7174,[14]5.7934,[15]5.8778,[16]5.9828,[17]6.0381,[18]6.1546,[19]6.2098,[20]6.2615,[21]6.2092,[22]6.1235,[23]6.1354,[24]6.1845,[25]6.1778,[26]6.2131,[27]6.2235,[28]6.2771,[29]6.2809,[30]6.3569,[31]6.4299,[32]6.4946,[33]6.4728,[34]6.4554,[35]6.4232,[36]6.3592,[37]6.3237,[38]6.3070,[39]6.3018,[40]6.3152,[41]6.2502,[42]6.1575,[43]6.1011,[44]6.0151,[45]5.9520,[46]5.9117,[47]5.9119,[48]5.9750,[49]6.0362,[50]6.0938,[51]6.1433,[52]6.1591,[53]6.1681,[54]6.1780,[55]6.1945,[56]6.1673,[57]6.2046,[58]6.1846,[59]6.1761,[60]6.1512,[61]6.1174,[62]6.0860,[63]6.0553,[64]6.0074,[65]5.9698,[66]5.9433,[67]5.9329,[68]5.9401,[69]5.9661,[70]5.9888,[71]6.0173,[72]6.0475,[73]6.0011,[74]5.9921,[75]5.9784,[76]5.9486,[77]5.9385,[78]5.9258,[79]5.9065,[80]5.8969,[81]5.8917,[82]5.9041,[83]5.9231,[84]5.9185,[85]5.9089,[86]5.9155,[87]5.9189,[88]5.9129,[89]5.9237,[90]5.9295,[91]5.9490,[92]5.9448,[93]5.9291,[94]5.9186,[95]5.9191,[96]5.9055,[97]5.9024,[98]5.8866,[99]5.8808,[100]5.8984,[101]5.9089,[102]5.9097,[103]5.9529,[104]5.9819,[105]5.9978,[106]6.0180,[107]6.0446,[108]6.0737,[109]6.0737,[110]6.0894,[111]6.0915,[112]6.1039,[113]6.0958,[114]6.0914,[115]6.0934,[116]6.0991,[117]6.0972,[118]6.1008,[119]6.1036,[120]6.1124,[121]6.1155,[122]6.1061,[123]6.0980,[124]6.0895,[125]6.0779,[126]6.0657,[127]6.0606,[128]6.0645,[129]6.0721,[130]6.0807,[131]6.0886,[132]6.0931,[133]6.0858,[134]6.0754,[135]6.0878,[136]6.0915,[137]6.0837,[138]6.0755,[139]6.0668,[140]6.0645,[141]6.0642,[142]6.0636,[143]6.0642,[144]6.0566,[145]6.0486,[146]6.0479,[147]6.0447,[148]6.0462,[149]6.0469,[150]6.0471,[151]6.0505,[152]6.0466,[153]6.0514,[154]6.0373,[155]6.0094,[156]6.0135,[157]6.0191,[158]6.0390,[159]6.0461,[160]6.0427,[161]6.0495,[162]6.0525,[163]6.0642,\n\nllama_print_timings:        load time = 16124.85 ms\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings: prompt eval time = 10484054.26 ms / 333824 tokens (   31.41 ms per token,    31.84 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time = 10535367.30 ms\n\nreal\t175m35.512s\nuser\t2096m10.839s\nsys\t0m42.180s\n\nI'm currently running the computations on the CPU as I have more confidence in the changes being correct, but we should look into updating the GPU code to support the RoPE scaling and doing more calculations to determine how the perplexity behaves for different context sizes.\nThe author of this idea @kaiokendev suggests that this approach should work even better with fine-tuned models (https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/comment/jp2dchb/?utm_source=share&utm_medium=web2x&context=3), so we should also do some tests with those models.\nResult summary (live updates)\nwiki.test.raw\n\n\n\nModel\nFormat\nScale\nCtx\nChunks\nPerplexity\n\n\n\n\nLLaMA 7B\nQ4_0\n1.0\n2048\n163\n5.4708\n\n\nLLaMA 7B\nQ4_0\n0.5\n2048\n163\n6.0642\n\n\nLLaMA 7B\nQ4_0\n1.0\n4096\n81\ninf\n\n\nLLaMA 7B\nQ4_0\n0.5\n4096\n81\n5.8945",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1965",
        "createdAt": "2023-06-22T09:43:29Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Is there a way to add \"world info\" cards to a session, persistent or otherwise?",
        "bodyText": "I know KoboldCPP allows for \"world info\" cards to be added for new information, such as if I want to update the name of the Prime Minister. However, I was unable to find an equivalent for LLaMa.cpp. Any suggestions?\nThanks in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2150",
        "createdAt": "2023-07-08T17:16:40Z",
        "author": {
            "login": "FrankDMartinez"
        }
    },
    {
        "title": "Benchmarking: What types of performance does LLMs care about?",
        "bodyText": "I\u2019m currently working on a program to benchmark hardware that\u2019s geared towards local LLMs. Right now I\u2019m working on measuring tokens for different models. I\u2019d like to add more generic benchmarks as well\nWhat types of hardware performance do we care about relating to LLMs?  So far I have:\n\nmemory bandwidth\nfp16 performance\nMatrix math performance\n\nWhat else do y\u2019all think I should be focusing on?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2038",
        "createdAt": "2023-06-28T14:09:45Z",
        "author": {
            "login": "soleblaze"
        }
    },
    {
        "title": "How do I use multi gpu setups?",
        "bodyText": "I have a intel scalable gpu server, with 6x Nvidia P40 video cards with 24GB of VRAM each.\nHow can I specify for llama.cpp to use as much vram as it needs from this cluster of gpu's? Does it automatically do it?\nI am following this guide at step 6",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2132",
        "createdAt": "2023-07-07T13:09:30Z",
        "author": {
            "login": "FileDotZip"
        }
    },
    {
        "title": "Is it possible to run bigcode models like starcoder?",
        "bodyText": "StarCoder show great performance on code generation and other coding stuff. However, bigcode models is very slow even with starcoder.cpp. Is it possible to run bigcode models like starcoder?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2130",
        "createdAt": "2023-07-07T11:12:09Z",
        "author": {
            "login": "wangjiyang"
        }
    },
    {
        "title": "Is there anyway to save context for the model ?",
        "bodyText": "Hi im new to LLM and machine learning in general, i was wondering if its possible to save context of the prompts for example like chatgpt chats it remembers previous questions and i was wondering what saving the state does for llama.cpp. I want to build a local chat bot and save prompts in db.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2110",
        "createdAt": "2023-07-05T10:35:57Z",
        "author": {
            "login": "mdrokz"
        }
    },
    {
        "title": "Running on top of MACbook 2019 (Intel i9 + AMD Radeon Pro 5500)",
        "bodyText": "Situation:  It's running on CPU; when trying to run on GPU (after compiling correctly with Metal + OpenBlas), i'm reaching a point of break and the model does not run.\nGGML_ASSERT: ggml-metal.m:766: false && \"not implemented\"\nfish: Job 1, './main -m models/ggml-vicuna-7b\u2026' terminated by signal SIGABRT (Abort)\n\nMy machine:\nuname -a                                                                                                                                               (amd_gpu) 18:07\nDarwin host 22.5.0 Darwin Kernel Version 22.5.0: Mon Apr 24 20:51:50 PDT 2023; root:xnu-8796.121.2~5/RELEASE_X86_64 x86_64\n\nDo someone have a clue of what I'm missing?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2119",
        "createdAt": "2023-07-05T21:19:28Z",
        "author": {
            "login": "mineirinh0"
        }
    },
    {
        "title": "llama.cpp performance numbers",
        "bodyText": "I've started a Github page for collecting llama.cpp performance numbers.\nIt's still very much WIP; currently there are no GPU benchmarks.\nI'll probably at some point write scripts to automate data collection and add them to the corresponding git repository (once they're somewhat mature I'll make a PR for the llama.cpp main repository).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2126",
        "createdAt": "2023-07-06T19:10:12Z",
        "author": {
            "login": "JohannesGaessler"
        }
    },
    {
        "title": "Expand llama.cpp to llm.cpp",
        "bodyText": "Make llama.cpp as a generic framework and SDK for LLM in C++.\nGoal\nExpand the project scope to support additional LLM projects.\nAPI Definitions\nToday's API is around llama but it can be generally divided into multiple categories:\n\nLoad/save model weights realted functions, which should be able to against any ggml binary file. largely the functions should not tight with llama. Model's tensor names can be different for the different models.\nTokenizer, we only support llama tokenizer today. The API needs to extend to support multi tokenizer. But the interface should kept same for any tokenizer. We need to support create_tokenizer, delete_toeknizer, id_to_token, token_to_id functions at least.\nSamplers, we have many samplers. they are not tight with llama as well. we can make them a set of sampler functions. One small additional thing we should support predefined sampling pipelines.\nEval, the interface today is generic enough to support any model. The only change we need is extending init_parameter to support additional/different parameters for the different models. The different models will use different compute graph for batch and single token generation.\nFinetuning/Training: Not implemented yet. We need to think more how to leverage Eval part like compute graph.\nOffloading functions: We should be able to provide a set of classes to offload the tensor to GPU and graph computing logic will leverage the related backend support to do the compute. Offload functionality can be implemented as an interface.\n\nWorkitem\n\nFirst come first, rename to LLM.cpp :)\nSplit the interface into llama specific and LLM general functions.\nRefactor and decouple the current code especially context, model as the connection point.\nAdopt other LLM models.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2102",
        "createdAt": "2023-07-04T14:40:35Z",
        "author": {
            "login": "howard0su"
        }
    },
    {
        "title": "Browsing plugin model / API that would allow real-time information access & interaction",
        "bodyText": "Services like GPT4, Bing, etc. now support real-time information access (and actions) beyond the trained foundational model.\nThe initial idea is to enable llama.cpp to support simple, real-time information services such as weather, sports stats, stock quotes, translation engines, curated/authoritative data sets, etc. (e.g. IMDB, Wolfram, Google Finance). These could be independently maintained, downloaded and manually installed prior to runtime by placing them in a plugin folder.\nBeyond web-based services, a plugin could be written for online email inbox or calendar access using the appropriate protocols.\nEventually, trusted services may be permitted to perform constrained actions on behalf of the user (e.g. document generation, to do lists, note taking, etc). The security model would limited not just to user installed plugins but also specifically authorized actions for those plugins.\nTwo key advantages of this are user/enterprise privacy and security, as contrasted with doing the same using an online chat service.\nFor an overview of the possibilities, see ChatGPT plugins overview and ChatGPT plugin documentation",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/465",
        "createdAt": "2023-03-24T16:06:04Z",
        "author": {
            "login": "classicjazz"
        }
    },
    {
        "title": "How could we enhance a foundation model with additional new local data, specified at runtime?",
        "bodyText": "The foundation model (e.g. LLaMA) is limited to those sources scraped by the model's publisher, prior to the creation of the model. I would like to augment a given model's \"knowledge\" by manually specifying at run time a directory containing \"authoritative\" data files (e.g. HTML, TXT, PDF, XLS, XML) that are then incorporated into the response from the AI when presented with an instruction. By authoritative, I mean that, to the extent that data in the specified directory conflicts with pre-existing data in the model, it is given greater weight/importance.\nFor example, a new product is released or a news event occurs. I manually save 50 relevant web pages, text files, etc. about that product or news event to a given local directory. I want to then run llama.cpp, which outputs a news article, abstract, key points, essay, blog article, poem, song, etc. based on the new data.\nI usually run a Bash script that loads llama.cpp with desired paramters. I am not sure whether llama.cpp itself loads and processes data in the directory or whether there is a prior step to somehow modify/add a layer to the model, which is then invoked by llama.cpp.\nNote: for this use case, I am envisioning that it is ad hoc and limited to local directories for security/privacy/limited long term reuse. It should be capable of processing the new data within a reasonable amount of time on consumer grade hardware (e.g. Macbook or Mac Studio).\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/858",
        "createdAt": "2023-04-08T23:32:11Z",
        "author": {
            "login": "classicjazz"
        }
    },
    {
        "title": "gprc api",
        "bodyText": "There is any repo like \"llama-server\" or similar? so we can have a llama.cpp instance running a model and a GRPC api for sending prompts and receiving completions?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2101",
        "createdAt": "2023-07-04T13:49:13Z",
        "author": {
            "login": "sonic182"
        }
    },
    {
        "title": "Difference in different quantization methods",
        "bodyText": "Hello,\nI'm wondering what quantization method or what you want to call it has the best output quality. Should you use q8_0, q4_0 or anything in between? I'm asking this question because the q8_0 version almost takes up as much space as the f16 version (13.5GB) but q4_0 only takes about 8GB, I'm talking about Vicuna 13b.\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2094",
        "createdAt": "2023-07-04T06:31:36Z",
        "author": {
            "login": "sussyboiiii"
        }
    },
    {
        "title": "Why does llama.cpp generate different outputs with the same seed on different OS?",
        "bodyText": "Hello,\nI am trying to generate the same output using llama.cpp but the outputs vary depending on the operating system (different binaries). As far as I know, I am using the same parameters.\nDo you have an idea what could cause this? What can I do to ensure that the same output is generated?\nI am doing my experiments on docker.\nalpine\nllama.cpp-alpinelatest  | main: build = 607 (ffb06a3)\nllama.cpp-alpinelatest  | main: seed  = 12345678\nllama.cpp-alpinelatest  | llama.cpp: loading model from /models/7B/ggml-model-q4_0.bin\nllama.cpp-alpinelatest  | llama_model_load_internal: format     = ggjt v3 (latest)\nllama.cpp-alpinelatest  | llama_model_load_internal: n_vocab    = 32000\nllama.cpp-alpinelatest  | llama_model_load_internal: n_ctx      = 512\nllama.cpp-alpinelatest  | llama_model_load_internal: n_embd     = 4096\nllama.cpp-alpinelatest  | llama_model_load_internal: n_mult     = 256\nllama.cpp-alpinelatest  | llama_model_load_internal: n_head     = 32\nllama.cpp-alpinelatest  | llama_model_load_internal: n_layer    = 32\nllama.cpp-alpinelatest  | llama_model_load_internal: n_rot      = 128\nllama.cpp-alpinelatest  | llama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama.cpp-alpinelatest  | llama_model_load_internal: n_ff       = 11008\nllama.cpp-alpinelatest  | llama_model_load_internal: n_parts    = 1\nllama.cpp-alpinelatest  | llama_model_load_internal: model size = 7B\nllama.cpp-alpinelatest  | llama_model_load_internal: ggml ctx size =    0.07 MB\nllama.cpp-alpinelatest  | llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\nllama.cpp-alpinelatest  | .\nllama.cpp-alpinelatest  | llama_init_from_file: kv self size  =  256.00 MB\nllama.cpp-alpinelatest  | \nllama.cpp-alpinelatest  | system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nllama.cpp-alpinelatest  | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\nllama.cpp-alpinelatest  | generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\nllama.cpp-alpinelatest  | \nllama.cpp-alpinelatest  | \nllama.cpp-alpinelatest  |  Building a website can be done in 10 simple steps:\nllama.cpp-alpinelatest  | 1. Determine your needs and how you want to position yourself.\nllama.cpp-alpinelatest  | 2. Decide on a domain name.\nllama.cpp-alpinelatest  | 3. Choose a hosting service.\nllama.cpp-alpinelatest  | 4. Determine the look of your site.\nllama.cpp-alpinelatest  | 5. Purchase a template, or hire someone to create one for you.\nllama.cpp-alpinelatest  | 6. Customize the template.\nllama.cpp-alpinelatest  | 7. Determine how you want to manage the content of your site. You can use a free application like WordPress, which is easy to set up and maintain. Or, you can purchase an application like Joomla or Drupal. There are many others available. These will give you more control over your content, but they can be much more complex to operate.\nllama.cpp-alpinelatest  | 8. Create the structure of your site, including creating pages that correspond with your needs. For example, if you\u2019re a photographer and want to sell prints on line, there would be a page for each product you are offering\u2014such as \u201cPrints\u201d or \u201cPhoto Albums.\u201d If you want to have an events calendar, create the structure of the site so that when someone clicks on the link they will go directly to your calendar.\nllama.cpp-alpinelatest  | 9. Create unique content; this can be text, video and/or images. This is where the work begins.\nllama.cpp-alpinelatest  |  [end of text]\nllama.cpp-alpinelatest  | \nllama.cpp-alpinelatest  | llama_print_timings:        load time =  1550.53 ms\nllama.cpp-alpinelatest  | llama_print_timings:      sample time =   108.92 ms /   302 runs   (    0.36 ms per token)\nllama.cpp-alpinelatest  | llama_print_timings: prompt eval time =  1490.81 ms /    14 tokens (  106.49 ms per token)\nllama.cpp-alpinelatest  | llama_print_timings:        eval time = 44495.60 ms /   301 runs   (  147.83 ms per token)\nllama.cpp-alpinelatest  | llama_print_timings:       total time = 46220.36 ms\n\ndebian\nllama.cpp-debianlatest  | main: build = 607 (ffb06a3)\nllama.cpp-debianlatest  | main: seed  = 12345678\nllama.cpp-debianlatest  | llama.cpp: loading model from /models/7B/ggml-model-q4_0.bin\nllama.cpp-debianlatest  | llama_model_load_internal: format     = ggjt v3 (latest)\nllama.cpp-debianlatest  | llama_model_load_internal: n_vocab    = 32000\nllama.cpp-debianlatest  | llama_model_load_internal: n_ctx      = 512\nllama.cpp-debianlatest  | llama_model_load_internal: n_embd     = 4096\nllama.cpp-debianlatest  | llama_model_load_internal: n_mult     = 256\nllama.cpp-debianlatest  | llama_model_load_internal: n_head     = 32\nllama.cpp-debianlatest  | llama_model_load_internal: n_layer    = 32\nllama.cpp-debianlatest  | llama_model_load_internal: n_rot      = 128\nllama.cpp-debianlatest  | llama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama.cpp-debianlatest  | llama_model_load_internal: n_ff       = 11008\nllama.cpp-debianlatest  | llama_model_load_internal: n_parts    = 1\nllama.cpp-debianlatest  | llama_model_load_internal: model size = 7B\nllama.cpp-debianlatest  | llama_model_load_internal: ggml ctx size =    0.07 MB\nllama.cpp-debianlatest  | llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\nllama.cpp-debianlatest  | .\nllama.cpp-debianlatest  | llama_init_from_file: kv self size  =  256.00 MB\nllama.cpp-debianlatest  | \nllama.cpp-debianlatest  | system_info: n_threads = 2 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nllama.cpp-debianlatest  | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\nllama.cpp-debianlatest  | generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\nllama.cpp-debianlatest  | \nllama.cpp-debianlatest  | \nllama.cpp-debianlatest  |  Building a website can be done in 10 simple steps:\nllama.cpp-debianlatest  | 1. Determine your needs and how you want to position yourself.\nllama.cpp-debianlatest  | 2. Decide on a domain name.\nllama.cpp-debianlatest  | 3. Choose a hosting service.\nllama.cpp-debianlatest  | 4. Determine the look of your site.\nllama.cpp-debianlatest  | 5. Purchase a template, or hire someone to create one for you.\nllama.cpp-debianlatest  | 6. Customize the template.\nllama.cpp-debianlatest  | 7. Insert content into the template.\nllama.cpp-debianlatest  | 8. Publish the site on your hosting server.\nllama.cpp-debianlatest  | 9. Promote it.\nllama.cpp-debianlatest  | 10. Maintain and update it.\nllama.cpp-debianlatest  | It's important that you decide what kind of website you want before you begin because each stage requires a different set of skills and techniques, which means you can either spend a lot more time trying to figure things out yourself (and wasting your valuable time), or you can hire someone who specializes in building websites.\nllama.cpp-debianlatest  | Before you build the site, you'll need to decide on your goal, including how you want to position yourself. Your website should represent an accurate picture of what you do and how you do it. This means that if you're a writer, for example, you shouldn't have a graphic design service listed as one of the things you offer (unless you also happen to be a graphic designer). And don't give people reasons not to hire you--make sure your site reflects who you are and what you do.\nllama.cpp-debianlatest  | The next step is to determine what kind of domain name you want to use for your website. If you already have the perfect domain name, then that's great! You can skip right on to point #3 below. If not, read on\u2026\nllama.cpp-debianlatest  | How To Choose The Best Domain Name For Your Site\nllama.cpp-debianlatest  | There are a number of ways to find a suitable domain name, but most experts recommend that you do your best to find something that is related to your business or industry. This way, people will have an easier time remembering it and typing it into their browsers.\nllama.cpp-debianlatest  | The best way to find good domain names is by using a tool called a \u201cdomain name generator\u201d. These tools search the web for all of the websites in your chosen field/industry and show you what domains are available (i.e., not already taken). The next step will be to figure out how much you can afford to spend on hosting, because this will also determine which hosting services you\u2019ll use for your site.\nllama.cpp-debianlatest  | Choose a Domain Name Hosting Service\nllama.cpp-debianlatest  | Website hosting is the service that gives you space on their servers so that you can create and maintain your website. They provide the necessary technical means for you to host your own website (with all of its content) over the internet, from anywhere in the world. It essentially acts as the \u201chome\u201d of your site.\nllama.cpp-debianlatest  | There are literally thousands of hosting services out there today, but we recommend that you go with one of the most reputable ones. If they\u2019re not well-known and established, it could be a problem for you if something goes wrong with their service or support--which is why we suggest you stick to one like GoDaddy or Site5 (we use them for our own websites).\nllama.cpp-debianlatest  | It really depends on how much money you have. The best hosting services will charge you $10 or more per month, but they\u2019re the most established and reliable. Cheaper ones may only cost you a few dollars per month. The cheapest hosting services are often referred to as shared web hosting--which is okay for beginners or those who simply don't want to spend much money on their website.\nllama.cpp-debianlatest  |  [end of text]\nllama.cpp-debianlatest  | \nllama.cpp-debianlatest  | llama_print_timings:        load time =  1452.43 ms\nllama.cpp-debianlatest  | llama_print_timings:      sample time =   282.24 ms /   838 runs   (    0.34 ms per token)\nllama.cpp-debianlatest  | llama_print_timings: prompt eval time = 50855.15 ms /   528 tokens (   96.32 ms per token)\nllama.cpp-debianlatest  | llama_print_timings:        eval time = 117419.70 ms /   835 runs   (  140.62 ms per token)\nllama.cpp-debianlatest  | llama_print_timings:       total time = 168724.75 ms\n\ncentos\nllama.cpp-centoslatest  | main: build = 607 (ffb06a3)\nllama.cpp-centoslatest  | main: seed  = 12345678\nllama.cpp-centoslatest  | llama.cpp: loading model from /models/7B/ggml-model-q4_0.bin\nllama.cpp-centoslatest  | llama_model_load_internal: format     = ggjt v3 (latest)\nllama.cpp-centoslatest  | llama_model_load_internal: n_vocab    = 32000\nllama.cpp-centoslatest  | llama_model_load_internal: n_ctx      = 512\nllama.cpp-centoslatest  | llama_model_load_internal: n_embd     = 4096\nllama.cpp-centoslatest  | llama_model_load_internal: n_mult     = 256\nllama.cpp-centoslatest  | llama_model_load_internal: n_head     = 32\nllama.cpp-centoslatest  | llama_model_load_internal: n_layer    = 32\nllama.cpp-centoslatest  | llama_model_load_internal: n_rot      = 128\nllama.cpp-centoslatest  | llama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama.cpp-centoslatest  | llama_model_load_internal: n_ff       = 11008\nllama.cpp-centoslatest  | llama_model_load_internal: n_parts    = 1\nllama.cpp-centoslatest  | llama_model_load_internal: model size = 7B\nllama.cpp-centoslatest  | llama_model_load_internal: ggml ctx size =    0.07 MB\nllama.cpp-centoslatest  | llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\nllama.cpp-centoslatest  | .\nllama.cpp-centoslatest  | llama_init_from_file: kv self size  =  256.00 MB\nllama.cpp-centoslatest  | \nllama.cpp-centoslatest  | system_info: n_threads = 2 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nllama.cpp-centoslatest  | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\nllama.cpp-centoslatest  | generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\nllama.cpp-centoslatest  | \nllama.cpp-centoslatest  | \nllama.cpp-centoslatest  |  Building a website can be done in 10 simple steps:\nllama.cpp-centoslatest  | 1. Determine your needs and how you want to position yourself.\nllama.cpp-centoslatest  | 2. Decide on a domain name.\nllama.cpp-centoslatest  | 3. Choose a hosting service.\nllama.cpp-centoslatest  | 4. Determine the look of your site.\nllama.cpp-centoslatest  | 5. Purchase a template, or hire someone to create one for you.\nllama.cpp-centoslatest  | 6. Customize the template.\nllama.cpp-centoslatest  | 7. Determine how you want to manage the content of your site. This will either be through static pages (a fixed number of pages), or dynamic pages (content that changes).\nllama.cpp-centoslatest  | 8. Create a sitemap, if necessary.\nllama.cpp-centoslatest  | 9. Register with search engines and link exchange partners.\nllama.cpp-centoslatest  | 10. Promote your website.\nllama.cpp-centoslatest  | The main goal for any new web page is to attract visitors, which will ultimately lead to sales/inquiries/donations, etc. Once you have decided on the purpose of your site (do not start a site without knowing why), then you can focus in on what you want to accomplish with it. If your purpose is to get more traffic to your business, then you should create a marketing campaign that will generate leads. If your purpose is to sell products online, and your competition is eBay or Amazon, etc., then you need to do everything possible to outperform them in search engine rankings.\nllama.cpp-centoslatest  | Step 2: Determine your needs and how you want to position yourself\nllama.cpp-centoslatest  | The next step after deciding on the purpose of your site is to make sure that your site meets your specific needs (do not overextend yourself). There are three main purposes for a new web page: to build a brand, to sell products or services online, or to provide useful information to others. These three areas should be used as the cornerstones when designing and building your website, because they will determine how you position yourself on the Internet.\nllama.cpp-centoslatest  | For example, if you want to build a brand for your business, then you need to focus on creating great content (articles) that can be found via search engines or by people who may visit your site directly. A blog is the perfect way to do this because it allows you to create a steady flow of new quality content which can be ranked well in search engine results. If your purpose for building a website is to sell products/services online, then make sure that every page on your site has a form for someone to contact you. You do not want people to have to click off your site to complete their purchase (eBay).\nllama.cpp-centoslatest  | Step 3: Choose the right host and domain name\nllama.cpp-centoslatest  | Finding the best hosting company is crucial for success because this will be one of the biggest determiners of how well your page ranks on search engines. There are hundreds of web hosts out there, but I recommend that you use a trusted host such as Bluehost or Hostgator (which are used by 60% of all websites). Avoid cheap hosting companies because these will often put your site in the same data center as millions of other sites which will make it very difficult to rank. Additionally, they rarely provide good support and you want to be sure that you can rely on a good host when problems arise during setup or maintenance.\nllama.cpp-centoslatest  | I also recommend using GoDaddy since this is one of the largest domain registration companies with over 50 million registered domains (out of 168 million total). I prefer them because they use Google Adwords as their top level domain for all new websites, and this can help your page rank more quickly. They are also very reliable and provide quality support if you ever run into problems or need to make adjustments in the future.\nllama.cpp-centoslatest  | Step 4: Get your site up! (you\u2019re almost done)\nllama.cpp-centoslatest  | You\u2019ve set everything up, but now it needs to go online so that people can actually find you on Google when they search for relevant keywords and phrases. The first thing you will need to do is buy a domain name since this is the main address of your page. You can then install WordPress onto the host as your website\u2019s operating system (or any other CMS).\nllama.cpp-centoslatest  | Step 5: Set up an auto-response email for people who contact you directly\nllama.cpp-centoslatest  | You should also consider setting up auto-responses to emails that are sent directly to your site since it will be easy for potential buyers to send you questions and express interest. This is because many of them will likely have no idea how to find you on Google or what to write in a search engine when they want to see who else is selling the product, so they might just send an email directly from their computer. This makes it easy for them to accidentally stumble upon your site.\nllama.cpp-centoslatest  | Step 6: Put up relevant and useful content\nllama.cpp-centoslatest  | \nllama.cpp-centoslatest  | llama_print_timings:        load time =  1471.57 ms\nllama.cpp-centoslatest  | llama_print_timings:      sample time =   329.25 ms /  1024 runs   (    0.32 ms per token)\nllama.cpp-centoslatest  | llama_print_timings: prompt eval time = 78066.40 ms /   785 tokens (   99.45 ms per token)\nllama.cpp-centoslatest  | llama_print_timings:        eval time = 141436.78 ms /  1020 runs   (  138.66 ms per token)\nllama.cpp-centoslatest  | llama_print_timings:       total time = 220024.14 ms\n\nUbuntu\nllama.cpp-ubuntulatest  | main: build = 607 (ffb06a3)\nllama.cpp-ubuntulatest  | main: seed  = 12345678\nllama.cpp-ubuntulatest  | llama.cpp: loading model from /models/7B/ggml-model-q4_0.bin\nllama.cpp-ubuntulatest  | llama_model_load_internal: format     = ggjt v3 (latest)\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_vocab    = 32000\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_ctx      = 512\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_embd     = 4096\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_mult     = 256\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_head     = 32\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_layer    = 32\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_rot      = 128\nllama.cpp-ubuntulatest  | llama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_ff       = 11008\nllama.cpp-ubuntulatest  | llama_model_load_internal: n_parts    = 1\nllama.cpp-ubuntulatest  | llama_model_load_internal: model size = 7B\nllama.cpp-ubuntulatest  | llama_model_load_internal: ggml ctx size =    0.07 MB\nllama.cpp-ubuntulatest  | llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\nllama.cpp-ubuntulatest  | .\nllama.cpp-ubuntulatest  | llama_init_from_file: kv self size  =  256.00 MB\nllama.cpp-ubuntulatest  | \nllama.cpp-ubuntulatest  | system_info: n_threads = 2 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nllama.cpp-ubuntulatest  | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\nllama.cpp-ubuntulatest  | generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\nllama.cpp-ubuntulatest  | \nllama.cpp-ubuntulatest  | \nllama.cpp-ubuntulatest  |  Building a website can be done in 10 simple steps:\nllama.cpp-ubuntulatest  | 1. Determine your needs and how you want to position yourself.\nllama.cpp-ubuntulatest  | 2. Decide on a domain name.\nllama.cpp-ubuntulatest  | 3. Choose a hosting service.\nllama.cpp-ubuntulatest  | 4. Determine the look of your site.\nllama.cpp-ubuntulatest  | 5. Purchase a template, or hire someone to create one for you.\nllama.cpp-ubuntulatest  | 6. Customize the template.\nllama.cpp-ubuntulatest  | 7. Insert content into the template.\nllama.cpp-ubuntulatest  | 8. Publish the site on your hosting server.\nllama.cpp-ubuntulatest  | 9. Promote it.\nllama.cpp-ubuntulatest  | 10. Maintain and update it.\nllama.cpp-ubuntulatest  | It's important that you decide what kind of website you want before you begin because each stage requires a different set of skills and techniques, which means you can either spend a lot more time trying to figure things out yourself (and wasting your valuable time), or you can hire someone who specializes in building websites.\nllama.cpp-ubuntulatest  | Before you build the site, you'll need to decide on your goal, including how you want to position yourself. Your website should represent an accurate picture of what you do and how you do it. This means that if you're a writer, for example, you shouldn't have a graphic design service listed as one of the things you offer (unless you also happen to be a graphic designer). And don't give people reasons not to hire you--make sure your site reflects who you are and what you do.\nllama.cpp-ubuntulatest  | The next step is to determine what kind of domain name you want to use for your website. If you already have the perfect domain name, then that's great! You can skip right on to point #3 below. If not, read on\u2026\nllama.cpp-ubuntulatest  | How To Choose The Best Domain Name For Your Site\nllama.cpp-ubuntulatest  | There are a number of ways to find a suitable domain name, but most experts recommend that you do your best to find something that is related to your business or industry. This way, people will have an easier time remembering it and typing it into their browsers.\nllama.cpp-ubuntulatest  | The best way to find good domain names is by using a tool called a \u201cdomain name generator\u201d. These tools search the web for all of the websites in your chosen field/industry and show you what domains are available (i.e., not already taken). The next step will be to figure out how much you can afford to spend on hosting, because this will also determine which hosting services you\u2019ll use for your site.\nllama.cpp-ubuntulatest  | Choose a Domain Name Hosting Service\nllama.cpp-ubuntulatest  | Website hosting is the service that gives you space on their servers so that you can create and maintain your website. They provide the necessary technical means for you to host your own website (with all of its content) over the internet, from anywhere in the world. It essentially acts as the \u201chome\u201d of your site.\nllama.cpp-ubuntulatest  | There are literally thousands of hosting services out there today, but we recommend that you go with one of the most reputable ones. If they\u2019re not well-known and established, it could be a problem for you if something goes wrong with their service or support--which is why we suggest you stick to one like GoDaddy or Site5 (we use them for our own websites).\nllama.cpp-ubuntulatest  | It really depends on how much money you have. The best hosting services will charge you $10 or more per month, but they\u2019re the most established and reliable. Cheaper ones may only cost you a few dollars per month. The cheapest hosting services are often referred to as shared web hosting--which is okay for beginners or those who simply don't want to spend much money on their website.\nllama.cpp-ubuntulatest  |  [end of text]\nllama.cpp-ubuntulatest  | \nllama.cpp-ubuntulatest  | llama_print_timings:        load time =  1451.36 ms\nllama.cpp-ubuntulatest  | llama_print_timings:      sample time =   319.17 ms /   838 runs   (    0.38 ms per token)\nllama.cpp-ubuntulatest  | llama_print_timings: prompt eval time = 50923.23 ms /   528 tokens (   96.45 ms per token)\nllama.cpp-ubuntulatest  | llama_print_timings:        eval time = 118251.09 ms /   835 runs   (  141.62 ms per token)\nllama.cpp-ubuntulatest  | llama_print_timings:       total time = 169660.99 ms",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2100",
        "createdAt": "2023-07-04T13:06:04Z",
        "author": {
            "login": "tdurieux"
        }
    },
    {
        "title": "Make an array of non-integer elements in Python pandas",
        "bodyText": "Hello all,\nI am Adrij from India. I am working on a data analysis work. I will try to explain the problem that I a facing in a few words.\nSay I have a column of 500 rows in which there are some string values. I want to replace all these string values with 0. If there is only type of string value, I can do the manual check & set a code for converting the string to 0. But if there are more than 1 string type, it is very difficult to check individual string element. How can I make an array which will select all those string elements? From the array, later the code can check for each elements in the column row by row, wherever it finds one the string will be zero.\nPlease suggest your opinion.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/157",
        "createdAt": "2023-03-15T08:06:05Z",
        "author": {
            "login": "Adrij1993"
        }
    },
    {
        "title": "Production use",
        "bodyText": "Hello,\nYou mention on the main page \"this project is for educational purposes\".\nWhat makes it, according to you, not suitable for real world situations and production environments?\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2089",
        "createdAt": "2023-07-03T16:02:05Z",
        "author": {
            "login": "paillave"
        }
    },
    {
        "title": "Broken infrence for  llama-master-d7d2e6a-bin-win-clblast-x64 (July2023)",
        "bodyText": "Recent version of release build for cblast  llama-master-d7d2e6a-bin-win-clblast-x64 no longer working on Windows 11\nFollowing output is received when trying to run inference  command: main -t 24  --color -c 2048  ...\nmain: build = 775 (d7d2e6a)\nmain: seed  = 1688407136\nggml_opencl: selecting platform: 'NVIDIA CUDA'\nggml_opencl: selecting device: 'NVIDIA GeForce RTX 4090 Laptop GPU'\nggml_opencl: device FP16 support: false\n13 errors generated.\nggml_opencl: kernel compile error:\n:2:14138: error: expected expression\n:2:14230: error: expected expression\n:2:14269: error: redefinition of 'is'\n:2:14222: note: previous definition is here\n:2:14282: error: expected expression\n:2:14353: error: use of undeclared identifier 'l0'\n:2:14419: error: use of undeclared identifier 'l0'\n:2:14612: error: use of undeclared identifier 'ql_offset'; did you mean 'qh_offset'?\n:2:14333: note: 'qh_offset' declared here\n:2:14766: error: expected expression\n:2:15451: error: use of undeclared identifier 'sum'\n:2:15456: error: expected expression\n:2:15507: error: use of undeclared identifier 'sum'\n:2:15875: error: use of undeclared identifier 'sum'\n:2:15880: error: expected expression\nMy machine setup seems to be configured correctly because other builds from same release group are working fine\nTested and verified correct functionality of:\nllama-master-d7d2e6a-bin-win-openblas-x64.zip\nllama-master-d7d2e6a-bin-win-cublas-cu12.1.0-x64.zip\nAlso older release binaries of llamacpp blast from May 2023 are working fine as well on same machine .",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2091",
        "createdAt": "2023-07-03T18:08:27Z",
        "author": {
            "login": "dzupin"
        }
    },
    {
        "title": "Pacha - A TUI Frontend for llama.cpp",
        "bodyText": "I share with you my little app 'Pacha' and hope that we have something that stays lightweight and terminal based like llama.cpp, but still can provide a minimum of comfort. At some point I just found it annoying to have to type or copy a whole command again for every little difference in parameter value I wanted to test, etc.\nThere are ready to use binaries for windows, linux and macOS (Intel). Just put it into the same folder as llama.cpp and there you go!\nhttps://github.com/mounta11n/Pacha\n\nThis frontend is not meant to be a chat UI or to replace anything, but rather a tool to quickly test a model, a prompt style and/or certain parameters. I think this might be a good first stop to test new models.\nThe top bar changes its color based on the current cpu usage.\nHere is an asciinema demonstration:\n\nThe app is currently buggy in some places, but I'm working on it. However, I felt that it is now functional enough that it can be released without any problems.\nThere are more features planned... like integrating bert.ggml for semantic context and a workspace for training baby-llamas from scratch.\nTo smart people who are familiar with javascript: Please look over my code and tell me how I can improve the corresponding buggy parts. For example, I just don't figure out why there is a line break after the first chunk in the output. Tried for ages to understand and fix it, but ... Idk. And I don't dare ask GPT-4 anymore. First, I'll be busy debugging GPT's f+cking mistakes more than half the time. And besides, I'm pretty sure my wife will kill me as soon as the next OpenAI bill comes.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2071",
        "createdAt": "2023-07-02T12:11:42Z",
        "author": {
            "login": "mounta11n"
        }
    },
    {
        "title": "Quality of training results?",
        "bodyText": "I've been experimenting with training-text-from-scratch.\nCompared to training on the same data with nanoGPT, the results seem considerably worse.\n(Given roughly the same training time on CPU.)\nI've been using the default parameters, which differ between the different code bases (llama.cpp vs nanoGPT).\nI've noticed that the loading of the training data is somewhat different as well.\nWhat are your experiences with llama.cpp training?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1851",
        "createdAt": "2023-06-14T10:53:21Z",
        "author": {
            "login": "matthiasgeihs"
        }
    },
    {
        "title": "Possibility of using LSTM model",
        "bodyText": "I am using a language model made of lstm and I was curious if it is possible to transform the lstm model to work on CPU?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2070",
        "createdAt": "2023-07-02T05:23:55Z",
        "author": {
            "login": "Horikitasaku"
        }
    },
    {
        "title": "Metal build working in ipad?",
        "bodyText": "Is it possible to build or offer a wasm file support for iOS?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1763",
        "createdAt": "2023-06-08T15:27:27Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "SpQR Quantization? ... for Near-Lossless LLM Weight Compression",
        "bodyText": "almost sounds too good to be true... but this technique even makes sense to the layman\nSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\nhttps://arxiv.org/abs/2306.03078\nhttps://github.com/Vahe1994/SpQR\nvia: https://www.superdatascience.com/podcast/near-lossless-llm-quantization",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2061",
        "createdAt": "2023-06-30T23:20:31Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "How hard would it be to introduce support for sparsity?",
        "bodyText": "Given the recent publication of the SpQR technique, and the further compression it will allow, is it worthwhile implementing sparsity here?\nI get the impression that this is a seriously non-negligible amount of work, so maybe it doesn't fit. That being said, sparsity should make inference faster, and possible on smaller machines still, so maybe it is of interest?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1759",
        "createdAt": "2023-06-08T14:17:30Z",
        "author": {
            "login": "afg1"
        }
    },
    {
        "title": "Interesting news about long context for llama models, 16k",
        "bodyText": "Just saw something come across the Twitterverse, huggingface.co/lmsys/longchat-13b-16k, thought it worth posting as an idea, and they created an Evaluation toolkit for text long-context",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2058",
        "createdAt": "2023-06-30T19:11:25Z",
        "author": {
            "login": "linuxmagic-mp"
        }
    },
    {
        "title": "Are there plans for Intel AMX support in ggml?",
        "bodyText": "I see that most of the x86 acceleration APIs are supported in llama.cpp, which is great! Intel AMX  seems to have even more potential for speeding up inference.\nAMX is support by Intel's latest server and workstation CPUs. Right now, the performance on my Intel w7-2495 AMX-enabled system (eval ~8 tokens per second) is lower than my MacBook Pro M1 (eval ~11 tokens per second).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2051",
        "createdAt": "2023-06-29T17:43:13Z",
        "author": {
            "login": "jgjl"
        }
    },
    {
        "title": "Compile for Android armeabi-v7a",
        "bodyText": "New to the space - apologies in advance for the possibly noob question.\nWondering at the possibility of running a quantized 7B model on a 32-bit arm (armeabi-v7a).\nIs is possible to compile this for armeabi-v7a?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1474",
        "createdAt": "2023-05-16T02:34:17Z",
        "author": {
            "login": "idekterev"
        }
    },
    {
        "title": "Call for tunning results on CLBlast to achieve faster llama.cpp prompt performance",
        "bodyText": "Dear all,\nPlease forgive me if this seems to be spam.\nSince llama.cpp can run on non-cuda GPUs with the help of CLBlast, each GPU could have a different architecture that needs different parameters to achieve the best matrix multiplication. In many cases, CLBlast is a library that can achieve higher performance with GEMM after tunning. I am a volunteer to help this CLBlast project to find different tuning results for different GPUs, it would be very great if you could kindly run CLBlast tuner on your GPU (all GPUs are very welcome, the tutorial is given as follows ) and report the tunning results in zip file to CLBlast site here:\nNew tuning results \u00b7 Issue #1 \u00b7 CNugteren/CLBlast \u00b7 GitHub\nThis could help to make CLBlast and llama.cpp faster!!!\nOfficial manual on running the tuner (especially for Linux/MacOS users):\nCLBlast/tuning.md at master \u00b7 CNugteren/CLBlast \u00b7 GitHub\nMy thread and file for running the tuner easily in Windows:\nCNugteren/CLBlast#1 (comment)\nBest wishes,\nJinchuan Tang",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1688",
        "createdAt": "2023-06-04T08:13:29Z",
        "author": {
            "login": "tangjinchuan"
        }
    },
    {
        "title": "The model will not load when I ask certain Questions",
        "bodyText": "As you can see it answered to hey but did not even try to respond to my second question",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1993",
        "createdAt": "2023-06-25T14:31:27Z",
        "author": {
            "login": "MrVolts"
        }
    },
    {
        "title": "Improvement of llama.cpp and ggml models within  2 months .",
        "bodyText": "Only CPU - i9 9900\nI made some tests with\n2 months old llama build and 65B ggml model also 2 months old .\n2 month old 65B q5_1 model and old llama.cpp - around 1700 ms/token\nCurrent 65B q4k_m ( similar prepexity to q5_1) and current llama.cpp - around 1000 ms/token.\nAnd also tested my new ryzen 7950x3d  :P\nCurrent 65B q4k_m ( similar prepexity to q5_1) and current llama.cpp - around 600 ms/token. ( Didn't test for avx512 yet )\nSo combination new models and new builds for CPU comparing from 2 months ago is giving more than 60% performance improvement.\nThat's awesome!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2033",
        "createdAt": "2023-06-28T09:39:28Z",
        "author": {
            "login": "mirek190"
        }
    },
    {
        "title": "How do I run CLBlas, the android guide seems a bit unclear for the last steps.",
        "bodyText": "I have been trying to follow the guide for running the the code with android, and have had an issue or two. bac1992\nFor lines 666-669, what exactly should I be doing with? Should I use it in my main CLBlast folder, my com.termux/files/home folder, or somewhere else? Also I don't know if its related but when I tried to run it somewhere I got for the ./main (...) code it returns a bash: syntax error near unexpected token '...' . Is there somethin I'm missing? Because up to that part everything seemed to install correctly.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1951",
        "createdAt": "2023-06-20T15:22:57Z",
        "author": {
            "login": "Avenir2233"
        }
    },
    {
        "title": "What's the purpose of CUBLASLT64_11.DLL",
        "bodyText": "To my understanding, CUBLAS64_11.DLL is required for cublasSgemm, however, there doesn't seem to be any explicit usage of CublasLt functions from anywhere within CUDA related code. Is it superfluous and can it be removed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2032",
        "createdAt": "2023-06-28T08:34:41Z",
        "author": {
            "login": "LostRuins"
        }
    },
    {
        "title": "Running on Intel Optane",
        "bodyText": "Just to throw this out there but has anyone tried running using intel Optane or similar virt ram?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2031",
        "createdAt": "2023-06-28T07:40:04Z",
        "author": {
            "login": "lantos1618"
        }
    },
    {
        "title": "Phi-1 model support",
        "bodyText": "phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP, surpassing gpt-3.5 hence all other models on coding .\nSome details of model architecture: decoder only transformer model using the FlashAttention implementation of multi-\nhead attention (MHA). also uses MHA and MLP layers in parallel configuration following\nsome recent models like CodeGen, PaLM and GPT-NeoX. The archi-tecture for 1.3B parameter phi-1 model consists of 24 layers, hidden dimension of 2048, MLP-inner\ndimension of 8192, and 32 attention heads of dimension 64 each.\nThe smaller 350M parameter phi-\n1-small model consists of 20 layers, hidden dimension of 1024, MLP-inner dimension of 4096, and 16\nattention heads of dimension 64 each. also uses a rotary position embedding with rotary\ndimension 32.\nAchieving these claimed results despite being only 1.3b parameters seems so promising. supporting it will make running ggml on huge number of mid low memory smartphones and laptops possible.\nNot clear whether model or dataset will be publicly available or not, also should mention that novel approaches for curating dataset seems having major impact of scoring such a results.\nReference:\nhttps://arxiv.org/abs/2306.11644",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2025",
        "createdAt": "2023-06-27T17:29:56Z",
        "author": {
            "login": "aseok"
        }
    },
    {
        "title": "LLC miss",
        "bodyText": "I found some model data which is 'not aligned' (maybe not aligned to ggml tensor alignment) and it can't use mmap so loads every model data on main memory.\nMy question is, compared to 'aligned' model data which use mmap, runing with 'not aligned' model data occurs around 3x LLC miss and why is that happening? I checked the LLC miss with Intel VTune and most LLC miss occured at 'ggml_vec_dit_f16' from 'ggml_compute_forward_mul_mat'.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2018",
        "createdAt": "2023-06-27T12:23:09Z",
        "author": {
            "login": "KKwanhee"
        }
    },
    {
        "title": "3 Promising Paths to improve Local Llamas",
        "bodyText": "Full credit goes to @alain40 who posted this here: turboderp/exllama#92 (comment)\nI wanted to raise the points he made there here as this is a larger forum to have this discussion and get more input, I've pasted a screenshot of his message for convinience:\n\nPersonally for me the third point is most interesting, if the base model is able to pick-up and use LoRa's in a tool-like fashion (like how we do for plugins in langchain now) we could have a base-model that can adapt responses based on various datasets in almost realtime, I'm going to dive deeper into this as well",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2013",
        "createdAt": "2023-06-26T20:39:20Z",
        "author": {
            "login": "nikshepsvn"
        }
    },
    {
        "title": "Combine two ggml_tensor like torch.cat",
        "bodyText": "I want to combine two embedding tensors where one of the embedding tensor is pre-stored which I plan to read from file. Is there a function\n\nCreates a ggm_tensor from two ggm_tensors of same dimensionality.\nLoads a ggml tensors from lets say numpy object. Even csv should do for now.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1987",
        "createdAt": "2023-06-25T02:34:15Z",
        "author": {
            "login": "sankalpdayal"
        }
    },
    {
        "title": "Has anyone able to run ChatDoctor?",
        "bodyText": "I recently came across with ChatDoctorrepository, it looks very interesting but I haven't been able to generate working ggml templates so far to be able to run via llama.cpp (master).\nGot this error while trying to export to .pth:\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. The class this function is called from is 'LlamaTokenizer'.\nThere was no error while running the convertion from .pth to .ggml or while running the quantization to get ggml-model-q4_0.bin, but I can't execute the ggml-model-q4_0.bin model from llama.cpp. Got the following message:\n.\\bin\\Release\\main.exe -m C:\\.ai\\.models\\chatdoctor\\ggml-model-q4_0.bin main: seed = 1680125564 llama_model_load: loading model from 'C:\\.ai\\.models\\chatdoctor\\ggml-model-q4_0.bin' - please wait ... llama_model_load: n_vocab = 32000 llama_model_load: n_ctx   = 512 llama_model_load: n_embd  = 4096 llama_model_load: n_mult  = 256 llama_model_load: n_head  = 32 llama_model_load: n_layer = 32 llama_model_load: n_rot   = 128 llama_model_load: f16     = 2 llama_model_load: n_ff    = 11008 llama_model_load: n_parts = 1 llama_model_load: type    = 1 llama_model_load: ggml ctx size = 4273.34 MB llama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state) llama_model_load: loading model part 1/1 from 'C:\\.ai\\.models\\chatdoctor\\ggml-model-q4_0.bin' llama_model_load: llama_model_load: tensor 'tok_embeddings.weight' has wrong size in model file llama_init_from_file: failed to load model main: error: failed to load model 'C:\\.ai\\.models\\chatdoctor\\ggml-model-q4_0.bin'\nDon't know if this a issue. I guess I'm doing something wrong while exporting to pth.\nUntil now I was following and trying to adapt the following process, so don't know if I should open a issue or even if such model is compatible with llama.cpp at moment.\nAny help will be appreciated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/608",
        "createdAt": "2023-03-29T22:16:34Z",
        "author": {
            "login": "tomsnunes"
        }
    },
    {
        "title": "chatGLM2 support",
        "bodyText": "There seems to be quite some interest around the new chatGLM2, can we get it to work with ggml/llama.cpp?\nhttps://github.com/THUDM/ChatGLM2-6B\nhttps://huggingface.co/THUDM/chatglm2-6b",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/2003",
        "createdAt": "2023-06-26T12:17:09Z",
        "author": {
            "login": "JianbangZ"
        }
    },
    {
        "title": "Server concurrency, streaming, and ssl",
        "bodyText": "It would be amazing if the llama.cpp server had some features to make it suitable for more than a single user in a test environment\neg.:\n\nuse a non-blocking server\nSSL support\nstreamed responses\n\nAs an aside, it's difficult to actually confirm, but it seems like the n_keep option when set to 0 still actually keeps tokens from the previous prompt. Also, the help text lists --keep as a cmd line switch but it's not recognized when used (I wonder if that's part of the issue)\nThanks for reading",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1871",
        "createdAt": "2023-06-15T10:52:51Z",
        "author": {
            "login": "Energiz3r"
        }
    },
    {
        "title": "I'm starting to implement LLaVA.cpp, but where?",
        "bodyText": "4\nNow that monatis/clip.cpp is working, \u0131'd like to take the next step to implement multimodal generation models with that. I'm thinking of starting with LLaVA first, and then extending to InstructBLIP.\nI'd like to get the opinion of maintainors and the community on where to implement it:\n\nas an example in ggml\nas a fork of llama.cpp, e.g., llava.cpp, or even more ambitiously multimodal.cpp, thinking of the support for InstructBLIP in the future.\nas a feature in llama.cpp.\nAny other suggestions.\n\nThe 3rd option has the potential to reach the largest audience, but I know that it might make things unnecessarily complicated and hinder the speed of innovations here. What would the ideal path be?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1996",
        "createdAt": "2023-06-25T19:37:02Z",
        "author": {
            "login": "monatis"
        }
    },
    {
        "title": "Llama.cpp product roadmap medium term?",
        "bodyText": "Very curious and excited to see how how llama.cpp unfolds medium term!!\nI love the code-centric DNA of the project. I love \"Inference at the edge\"... with an ethos of 'simplicity and efficiency.. performance is essential\".\nI get that the \"goal is to prototype and not waste time in polishing products\"... \"have fun in the process\". I'm curious as to how things will unfold in the medium term - epspecially given the funding and formation of ggml.ai. Having fun should be eternal... but at some stage the team would likely need to look up from the code and chart a course in one direction or another. So many options!\nWe only have a handfull of LLM \"families\" at the moment. In a year that could be 50... will llama.cpp & GGML.ai be aiming to support them all?\nThe guys at MosaicML* are finding that 80-90% of their corporate client demand is for extraction and summarization... with the balance being chat and other (eg Replit's CodeAnything). So should we expect to some GGML magic happen around extraction and summarization? Support for larger context windows, faster tokenisation, different tokenisation methods, extraction storage vector or semantic storage formats or databases?\nExciting times!!!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1933",
        "createdAt": "2023-06-19T05:51:41Z",
        "author": {
            "login": "ianscrivener"
        }
    },
    {
        "title": "Are prompt templates required for ggml models?",
        "bodyText": "I currently run every model with -i -ins which seems to be using the alpaca format. I see on the model pages though that most models have different templates such as ' USER: ' 'ASSISTANT:' etc. Do I need to and how do I use these in llama.cpp?\nAlso vicuna 1.1 onwards they mention a different prompt separator '', do I somehow mention this too for llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1980",
        "createdAt": "2023-06-23T17:22:38Z",
        "author": {
            "login": "RahulVivekNair"
        }
    },
    {
        "title": "Adding support for image vision",
        "bodyText": "This https://github.com/Vision-CAIR/MiniGPT-4 project used vikuna (llama based weights) model for training (frosen weights, trained only first layer, as i understand).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1115",
        "createdAt": "2023-04-22T04:24:51Z",
        "author": {
            "login": "rPman"
        }
    },
    {
        "title": "K-Quant subjective quality",
        "bodyText": "So I can see from the perplexity scores that the K-quants generally seem to perform more size efficiently compared to earlier formats.\nBut I wanna see if anyone else agrees - outputs from Q4_K_M and Q4_K_S actually feel worse than Q4_0 subjectively despite having a better perplexity score.\nI dunno if I am dreaming\nmy responses from q4_0 just... feel? better?\nJust wanna hear some anecdotes on this.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1856",
        "createdAt": "2023-06-14T15:42:27Z",
        "author": {
            "login": "LostRuins"
        }
    },
    {
        "title": "How can we use llama.cpp with xformers?",
        "bodyText": "Curious if anyone has any insights.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1972",
        "createdAt": "2023-06-22T23:47:53Z",
        "author": {
            "login": "joseph777111"
        }
    },
    {
        "title": "LongMem: Augmenting Language Models with Long-Term Memory",
        "bodyText": "https://arxiv.org/abs/2306.07174\nhttps://github.com/Victorwz/LongMem\nhttps://github.com/facebookresearch/fairseq/compare/v0.12.2...tmm1:fairseq:longmem?diff=unified",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1960",
        "createdAt": "2023-06-21T17:21:58Z",
        "author": {
            "login": "tmm1"
        }
    },
    {
        "title": "Issue with cuBLAS acceleration on vGPUs",
        "bodyText": "I'm trying to inference on a virtual GPU from my cloud provider (NVIDIA A16). (I believe) llama.cpp is hard coded to support only native devices in the Makefile, I was able to get it to compile and run somewhat successfully by changing this flag to -arch=compute_50.\nWhen I run main with ./main -m <path> -ngl 1 I see this output:\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA A16-1Q\n\n...\nllama_model_load_internal: using CUDA for GPU acceleration\nllama_model_load_internal: mem required  = 5331.34 MB (+ 1026.00 MB per state)\nllama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\nllama_model_load_internal: offloading 1 repeating layers to GPU\nllama_model_load_internal: offloaded 1/35 layers to GPU\nllama_model_load_internal: total VRAM used: 621 MB\n\nHowever after a few seconds I hit this fatal error:\nCUDA error 2 at ggml-cuda.cu:2582: out of memory\n\nI was under the assumption that with the -ngl llama.cpp would only allocate 621MB of VRAM. Which would be well within the 1024MB on my vGPU. Does llama.cpp still allocate the full required memory on both the CPU and GPU (5331.34MB on both CPU and GPU)? Or should it only allocate the VRAM memory used by the offloaded layers = 621MB on the GPU?\nIf it should only be allocating 621MB on the GPU then my quick hack clearly didn't work.\nHas anyone gotten llama.cpp to run on virtual GPUs?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1943",
        "createdAt": "2023-06-19T23:15:53Z",
        "author": {
            "login": "bbielsa"
        }
    },
    {
        "title": "Closest words from embedding vector",
        "bodyText": "Hi,\nis it possible to get the n closest words for an embedding vector ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1942",
        "createdAt": "2023-06-19T22:57:52Z",
        "author": {
            "login": "rhohndorf"
        }
    },
    {
        "title": "Building llama.cpp in Linux for Linux and WIndows",
        "bodyText": "Building  the Linux version is very simple.  I generated a bash script that will git the latest repository and build, that way I an easily run and test on multiple machine.\nIs it possible to build a Windows version from within Linux?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1839",
        "createdAt": "2023-06-13T16:39:53Z",
        "author": {
            "login": "thesimpleone"
        }
    },
    {
        "title": "Can anyone explain why prompt evaluation on Macs is so slow?",
        "bodyText": "Using Llama.cpp on Mac is such a pain because the prompt evaluation time is incredibly slow.\nLook at these three outputs:\nCPU (Ryzen 5600) only on Linux with 13b q6k:\nllama_print_timings: prompt eval time =  1146.71 ms /   111 tokens (   10.33 ms per token,    96.80 tokens per second)\nllama_print_timings:        eval time = 35998.84 ms /   127 runs   (  283.46 ms per token,     3.53 tokens per second)\n\nGPU (4090) on Linux with 13b q6k:\nllama_print_timings: prompt eval time =   225.93 ms /   111 tokens (    2.04 ms per token,   491.30 tokens per second)\nllama_print_timings:        eval time =  1895.24 ms /   127 runs   (   14.92 ms per token,    67.01 tokens per second)\n\nM1 with 13b q4km (GPU or CPU speed is almost the same on M1):\nllama_print_timings: prompt eval time = 13767.84 ms /   111 tokens (  124.03 ms per token,     8.06 tokens per second)\nllama_print_timings:        eval time = 22384.10 ms /   127 runs   (  176.25 ms per token,     5.67 tokens per second)\n\nOn Linux, the prompt evaluation time for CPUs is 27.4 times faster than token generation, while GPUs are 7.3 times faster. However, on Mac, it is only 1.4 times faster. This discrepancy doesn't make sense.\nAdditionally, Ryzen 5600 on Linux is reported to be 12 times faster than the CPU of M1 in terms of prompt evaluation, which is also puzzling. From what I understand, these two CPUs have similar performance in various benchmarks.\nWhy?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1945",
        "createdAt": "2023-06-20T00:13:59Z",
        "author": {
            "login": "shouyiwang"
        }
    },
    {
        "title": "Do I miss something not having specific compilation flags on ARM64 platform?",
        "bodyText": "I'm running llama.cpp on Ampere Altra Q80 server and all is fine there, but the performance with 16 threads just not so good as on my sweetie M1 Pro laptop with just 6 threads. So I've started thinking maybe I'm missing something and some explicit compile time flags might help there? Maybe some hardware optimisations just not used with default make?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1723",
        "createdAt": "2023-06-06T21:08:12Z",
        "author": {
            "login": "gotzmann"
        }
    },
    {
        "title": "more cores slower right? now it's using half cores. possible to put as \"bugs\" or milestone on roadmap in issue to track better?",
        "bodyText": "more cores slower right? now it's using half cores. possible to put as \"bugs\" or milestone on roadmap in issue to track better?\ni have 6 cpu cores with a vps, using 3 cores is more optimum than 6 total. wish not to guess why but i found others mentioned the same etc.\nalso, 12-16 cores seems optimum for a 28 cpu core machine. (i read somewhere)\nasking so coz i was wondering how many cores are optimum for my next vps purchase / laptop investment for this.\nwill be paying 1 year vps hosting coz it's cheaper.\npossible to highlight some \"gotchas\" / caveats / current limitations?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1894",
        "createdAt": "2023-06-16T14:56:45Z",
        "author": {
            "login": "kolinfluence"
        }
    },
    {
        "title": "how to do multiline prompts?",
        "bodyText": "how do i do multiline prompts in \"interaction mode\"?\ncurrently if there's a newline it seems llama.cpp will just treat it as message completed.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1895",
        "createdAt": "2023-06-16T14:59:11Z",
        "author": {
            "login": "kolinfluence"
        }
    },
    {
        "title": "Thoughts on programming Metal for llama.cpp",
        "bodyText": "I took a swing at converting some stuff from float to float4 etc here:\nhttps://github.com/sroussey/llama.cpp/pull/1/files\nBut the speed seems to be the same. @ikawrakow have any thoughts?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1875",
        "createdAt": "2023-06-15T14:53:04Z",
        "author": {
            "login": "sroussey"
        }
    },
    {
        "title": "Help: Where/how dequantize happens and how to create new quantization formats?",
        "bodyText": "Thank you for this amazing software and community.\nI am trying to better understanding where/how the dequanitzing happens and would appreciate any pointers on that so that I can try out new quantization formats.\nWhen I look through the code it seems like there are various dequantize functions for the different types of quantization but I don't quite understand how these get used in inference. It seems like the main place these are used by llama.cpp is in the llama_convert_tensor_internal function but that does not seem to be used in inference as far as I can make out.\nIn further digging, it seems like possibly the place to look might be in the ggml_cl_mul_mat function. It seems like that is a somewhat low level function which unpacks quantized values and then does a matrix multiplication but I'm not quite sure.\nMy main question is: \"If I want to try out a new quantization method, which functions should I change to pack/unpack the quantized values?\".\nThanks in advance for any pointers.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1796",
        "createdAt": "2023-06-10T22:31:13Z",
        "author": {
            "login": "emin63"
        }
    },
    {
        "title": "Understanding memory usage",
        "bodyText": "I am running this program on a Mac. When I load a model, I see following in the output:\n\nthis shows expected memory usage as 11359 MB. But when I inspect in activity monitor I see:\n\nso memory consumed is 2.4 GB as per activity monitor. can someone explain the discrepancy to me? thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1876",
        "createdAt": "2023-06-15T15:30:51Z",
        "author": {
            "login": "siddhsql"
        }
    },
    {
        "title": "NVLink utilised for GPU inference?",
        "bodyText": "Is NVLink being used by llama.cpp for GPU inference?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1864",
        "createdAt": "2023-06-15T01:57:17Z",
        "author": {
            "login": "Energiz3r"
        }
    },
    {
        "title": "get corrupted completion when enabling Metal GPU inference with 'ngl 1'",
        "bodyText": "I tried to load and inference on llama-7b merged with a Chinese LoRA adapter in CLI mode, but got different completion with or without 'ngl 1' option.\nOutput with no 'ngl' option:\n> Capital of United States\nWashington, D.C.\n>\nThe corrupted output with 'ngl 1':\n> Capital of United States\nW\u80e5\u5176\u4e2d\u5305\u62eczat droit\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5zat\u80e5\u80e5zatstronom\u80e5zat\u80e5\u80e5zat\u80e5 varying\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5zat\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5 droit\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5zatzat\u80e5 varying\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5avanozat\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5zat\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5zat\u80e5\u80e5\u80e5\u80e5\u80e5zat\u80e5\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62eczat\u80e5\u80e5\u80e5zat\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5zat\u80e5\u80e5\u80e5\u80e5 droit\u80e5\u5176\u4e2d\u5305\u62ec varying\u80e5\u5176\u4e2d\u5305\u62eczat\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5avanozat\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5zat\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5zat\u5176\u4e2d\u5305\u62ec\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5zat\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u5176\u4e2d\u5305\u62ec\u80e5zatzat\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5 droit\u5176\u4e2d\u5305\u62ec\u80e5\u5176\u4e2d\u5305\u62ec\u5176\u4e2d\u5305\u62ec\u80e5zat\u80e5\u5176\u4e2d\u5305\u62ec\u80e5zat\u80e5\u5176\u4e2d\u5305\u62ec\u8bd9\u80e5\u80e5\u80e5 varying\u80e5zat\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5\u80e5\u5176\u4e2d\u5305\u62ec\u80e5\u80e5zat\u80e5\u80e5\u80e5\u80e5 droitzat droit\u80e5\u80e5\u5176\u4e2d\u5305\u62eczat droit\n>\nThanks in advance for any help.\nThe following are the model loading trace, it looks good? :\nmain: build = 669 (9254920)\nmain: seed  = 1686735230\nllama.cpp: loading model from zh-models/7B/ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 49954\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0.07 MB\nllama_model_load_internal: mem required  = 5536.92 MB (+ 1026.00 MB per state)\n...............................................................................................\nllama_init_from_file: kv self size  = 1024.00 MB\nggml_metal_init: allocating\nggml_metal_init: using MPS\nggml_metal_init: loading '/Users/wujianmin/bak-from-mac/Code/git/llama.cpp/ggml-metal.metal'\nggml_metal_init: loaded kernel_add                            0x132904f10\nggml_metal_init: loaded kernel_mul                            0x145e0aba0\nggml_metal_init: loaded kernel_mul_row                        0x145e0b1e0\nggml_metal_init: loaded kernel_scale                          0x145e0b700\nggml_metal_init: loaded kernel_silu                           0x145e0bc20\nggml_metal_init: loaded kernel_relu                           0x145e0c140\nggml_metal_init: loaded kernel_gelu                           0x145e0c660\nggml_metal_init: loaded kernel_soft_max                       0x145e0cd10\nggml_metal_init: loaded kernel_diag_mask_inf                  0x145e0d370\nggml_metal_init: loaded kernel_get_rows_f16                   0x132905790\nggml_metal_init: loaded kernel_get_rows_q4_0                  0x132905f30\nggml_metal_init: loaded kernel_get_rows_q4_1                  0x132906720\nggml_metal_init: loaded kernel_get_rows_q2_k                  0x132906da0\nggml_metal_init: loaded kernel_get_rows_q3_k                  0x145f04510\nggml_metal_init: loaded kernel_get_rows_q4_k                  0x145f05360\nggml_metal_init: loaded kernel_get_rows_q5_k                  0x145f059e0\nggml_metal_init: loaded kernel_get_rows_q6_k                  0x145e0d8d0\nggml_metal_init: loaded kernel_rms_norm                       0x145e0e0a0\nggml_metal_init: loaded kernel_mul_mat_f16_f32                0x145e0e900\nggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x145e0f270\nggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x145e0f950\nggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x145e10030\nggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x145e10730\nggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x145e10f90\nggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x145e11670\nggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x145e11d50\nggml_metal_init: loaded kernel_rope                           0x145e12640\nggml_metal_init: loaded kernel_cpy_f32_f16                    0x145e130d0\nggml_metal_init: loaded kernel_cpy_f32_f32                    0x145e13960\nggml_metal_add_buffer: allocated 'data            ' buffer, size =  3745.52 MB\nggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\nggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1026.00 MB\nggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\nggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\nsystem_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\nmain: interactive mode on.\n... ...\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.200000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = 256, n_keep = 21`\n-Jianmin",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1849",
        "createdAt": "2023-06-14T09:58:20Z",
        "author": {
            "login": "noahwoo"
        }
    },
    {
        "title": "Using AI hardware accelerators on modern computers and possibly smartphones",
        "bodyText": "Hello,\nI've been thinking about this for a while. Starting with AMD's Ryzen AI mobile processors as well as Intel's upcoming Meteor Lake VPUs, consumers will get more access to built in AI hardware which handle matrix multiplication faster and more efficiently compared to GPUs and CPUs. This is especially useful on mobile devices. Smartphones have been equipped with NPUs of different kinds for quite a while. Nvidia introduced AI acceleration for consumers all the way back in 2018 when they launched the Turing architecture. AMD introduced AI acceleration with RDNA3.\nI think it's interesting to ponder about how to use AI accelerators for efficiency and speedups that can be integrated into llama.cpp inference and possibly even training when the time comes.\nMicrosoft and Nvidia recently introduced Olive optimized ONNX models for Stable Diffusion, which improve performance by two times using tensor cores. (See here for reference: https://blogs.nvidia.com/blog/2023/05/23/microsoft-build-nvidia-ai-windows-rtx/ and https://devblogs.microsoft.com/directx/dml-stable-diffusion/\nSince GGML is heading towards a more GPU accelerated approach, I wonder if incorporating some of these optimizations into the GGML format could lead to nice speedups when using GPU layer offloading.\nBut as far as I understand, the bottlenecks with llama.cpp are currently more related to memory rather than compute, so dedicated AI accelerators like tensor cores do not result in a speedup right now. This can be observed by comparing the performance of different architectures and also running NSight Systems - Tensor cores are active, but do not do much as most of the calculations are done in FP32.\nSince FP32 is also very memory bandwidth intense compared to FP16, I do wonder if such high precision is really necessary for inference and if it would be a good idea to run inference at half precision in general. This could potentially reduce memory bottlenecks which in turn would shift bottlenecks from memory to compute, where AI accelerators like tensor cores could provide a noticeable speedup.\nWhat are your thoughts on this? I am very interested to hear what your ideas are to leverage modern hardware to its full extent!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1658",
        "createdAt": "2023-05-31T21:10:41Z",
        "author": {
            "login": "Dampfinchen"
        }
    },
    {
        "title": "how to make llama.cpp keep talking forever as in writing a book and a way to save it?",
        "bodyText": "totally newbie here, not sure how the parameters work. though i've downloaded and used the uncensored vicuna 13b and it's working great (except that it only uses half of my cpu cores which i think is kind of wasted)\n\n\nhow do i make it write a complete book of 1600 pages and save the content while it's generating?\n\n\nwhere do i find all the parameters documentaiton / tutorial?\n\n\nhow do i expertly prompt it to do something? as in expertly prompting it. not like the examples given i think it's... quality wise not very good though. so maybe also how to fine tune quality?\n\n\nhow do i train it to learn?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1852",
        "createdAt": "2023-06-14T13:12:21Z",
        "author": {
            "login": "kolinfluence"
        }
    },
    {
        "title": "Is this an inference only library?",
        "bodyText": "is llama.cpp an inference only library? how can i fine-tune an existing model using llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1855",
        "createdAt": "2023-06-14T15:13:48Z",
        "author": {
            "login": "siddhsql"
        }
    },
    {
        "title": "Introducing my 'VowelReconstruct' Method - A Tangible Test for Comparing LLM's General Intelligence",
        "bodyText": "TL;DR I have created a test method I call it \"VowelReconstruct\", where texts with almost all vowels were removed are presented to language models, and its job is to reconstruct the original text. I am very excited to introduce it to you. My approach is interesting because the model needs various cognitive capabilities at same time to be able to achieve this task. The result is evaluated by comparing the reconstructed text to the original text using two metrics, the Levenshtein-distance and a simple characters similarity score. After that I calculate a new Score (YASimScore), which provide insights into the performance of different language models and helping assess their intelligence.. This method aims to provide a practical way of assessing and comparing language models intelligence.\nI've also decided to start my own blog and there you can read more about the method, if you are interested:\nhttps://publish.obsidian.md/mountaiin/VowelReconstruct\n\nHere you'll find the files, if you want to use this method too:\nhttps://github.com/mounta11n/VowelReconstruct\n\nAnd here you can see how some of my results look like:\n\n\n\nName\nSize\nSpecifications\nSimilarity\nLevenshtein\nYASimScore\n\n\n\n\nGuanaco\n7B\n/\n39.81%\n151\n379.29\n\n\nWizardLM\n7B\nq40\n42.36%\n194\n457.90\n\n\n--------\n---\n------\n------\n---\n---\n\n\nVicuna\n13B\nq41_v3\n44.90%\n109\n242.76\n\n\nVicuna\n13B\nq41_v3\n57.64%\n29\n50.31\n\n\nVicuna\n13B\nq6k\n51.27%\n190\n370.82\n\n\nWizardLM\n13B\nq40_v3\n51.27%\n41\n79.95\n\n\nWizardLM\n13B\nq40_v3\n51.59%\n31\n60.09\n\n\nWizardLM\n13B\nq40_v3\n51.27%\n42\n81.92\n\n\nWizardLM\n13B\nq40_v3\n50.00%\n29\n58.00\n\n\nWizardLM\n13B\nq4km\n57.96%\n34\n58.64\n\n\nWizardLM\n13B\nq6k\n55.73%\n41\n73.52\n\n\n--------\n---\n------\n------\n---\n---\n\n\nbased\n30B\nq40_v3\n53.50%\n108\n201.87\n\n\nLLaMA\n30B\ns-hotcot\n67.20%\n83\n123.45\n\n\n--------\n---\n------\n------\n---\n---\n\n\nGuanaco\n65B\nq40_v3\n99%\n2\n2.01\n\n\n--------\n---\n------\n------\n---\n---\n\n\nClaude+\n/\n100k\n93%\n12\n12.90\n\n\nGPT-3.5\n/\n/\n96.18%\n12\n12.48\n\n\nGPT-4\n/\n/\n97.77%\n2\n2.04",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1848",
        "createdAt": "2023-06-14T08:18:30Z",
        "author": {
            "login": "mounta11n"
        }
    },
    {
        "title": "Non-english languages?",
        "bodyText": "Hey!\nI am attempting to use llama to work with german, however the results are more than poor. Do you have any tips?\nThanks\nNiansa / Tuxifan",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/339",
        "createdAt": "2023-03-20T20:54:10Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Performance w/ langchain?",
        "bodyText": "Am I doing this correctly with langchain? In particular, am I using the optimized CPP version of llama, or the python version?\nI'm using the 13b version an a sup'ed up M2 Pro and it is sloooooow. As in, it takes about one minute to make a simple query.\nIt uses the low_cpu_mem_usage with the offline option.\n!pip install git+https://github.com/huggingface/peft.git\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install -v datasets loralib sentencepiece \n!pip -v install bitsandbytes accelerate\n!pip -v install langchain\n!pip install scipy\n!pip install xformers\n!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain import PromptTemplate, LLMChain\n\nimport torch\n\ntokenizer = LlamaTokenizer.from_pretrained(\"/Users/uavalos/Documents/gpt4-x-alpaca\")\n\nbase_model = LlamaForCausalLM.from_pretrained(\n    \"/Users/uavalos/Documents/gpt4-x-alpaca\",\n    #load_in_8bit=True,\n    #load_in_8bit_fp32_cpu_offload=True,\n    low_cpu_mem_usage=True,\n    device_map='auto',\n    offload_folder=\"offload\"\n)\n\npipe = pipeline(\n    \"text-generation\",\n    model=base_model, \n    tokenizer=tokenizer, \n    max_length=256,\n    temperature=0.6,\n    top_p=0.95,\n    repetition_penalty=1.2\n)\n\nlocal_llm = HuggingFacePipeline(pipeline=pipe)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1822",
        "createdAt": "2023-06-12T14:16:33Z",
        "author": {
            "login": "frankandrobot"
        }
    },
    {
        "title": "Help |  error loading model: unexpectedly reached end of file",
        "bodyText": "Running llama.cpp on intel MacBook Pro.  I get the following error while running\n./examples/alpaca.sh\nI saved the model file in ./models\nError:\nllama.cpp: loading model from ./models/ggml-alpaca-7b-q4.bin\nerror loading model: unexpectedly reached end of file\nllama_init_from_file: failed to load model\nllama_init_from_gpt_params: error: failed to load model './models/ggml-alpaca-7b-q4.bin'\nmain: error: unable to load model\nWhat could be the problem?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1715",
        "createdAt": "2023-06-06T12:54:47Z",
        "author": {
            "login": "kpshukla"
        }
    },
    {
        "title": "Help | A doubt with server.cpp",
        "bodyText": "Hello! I hope you are fine.\nI'm, trying to emulate a playground consuming the server.cpp api but I am having problems.\nWhen I submit a prompt, in conjunction with the \"as_loop\" attribute to make it do a stream, I get the result token by token, but after try to stop the inference the next-token?stop=true with a GET petition and resend with POST new params and prompt, it continues to complete the previous one ignoring the new prompt.\nI would like to know if you could solve this doubt, what exacly do the next-token?stop=true and how can i use it or about how to use it correctly, what detail I may be omitting.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1808",
        "createdAt": "2023-06-11T23:18:22Z",
        "author": {
            "login": "hwpoison"
        }
    },
    {
        "title": "Support SIGHUP to abort running model for API servers",
        "bodyText": "I asked this as an Issue over in the llama-cpp-python project - abetlen/llama-cpp-python#313 - one minor issue I see is that when running llama.cpp as a library behind an API server, if a client decides to terminate connection the model seems to keep running.\nThis could be handled better by having the llama.cpp backend code support something like SIGHUP (Hangup - perfect analogy for this as the client \"hung up\") and returning a null result, thereby allowing the API server the ability to serve another query immediately after.\nAny thoughts?  Does llama.cpp already support this in another manner & we just need to find/implement it in the python?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1690",
        "createdAt": "2023-06-04T13:54:03Z",
        "author": {
            "login": "spirilis"
        }
    },
    {
        "title": "llama.cpp \"chat\" Qt GUI",
        "bodyText": "Hey!\nI've sat down to create a simple llama.cpp GUI for few-shot prompts in Qt today:\n (this is 7B)\nI've tested it on both Linux and Windows, and it should work on Mac OS X too. It visualizes markdown and supports multi-line reponses now.\nI want to add further customization options, as currently this is all there is for now:\n\nCurrently the GUI runs the model locally, but I plan on adding an option to run the inference on a (potentially much stronger) server instead. However this will wait until processing multiple prompts in the same context is implemented.\nI am also considering to add alpaca/gpt4all support.\nYou can find the project here: https://gitlab.com/niansa/llamaq\nHope you have fun with this\nniansa",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/602",
        "createdAt": "2023-03-29T17:40:34Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Can Apple Silicon GPU and CPU work together?",
        "bodyText": "I found that even the entire model is loaded into Nvidia's GPU, the CPU still operates at full capacity during inference. So, what is CPU doing to help GPU?\nThen why is the CPU idle when using GPU acceleration on Apple Silicon? If the CPU is also utilized, can it speed up the process? Or is there no benefit due to the limitation of RAM throughput?\nCan someone please share some technical details? Thx",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1801",
        "createdAt": "2023-06-11T14:17:38Z",
        "author": {
            "login": "shouyiwang"
        }
    },
    {
        "title": "Is there a way to silence log/debug output?",
        "bodyText": "When I run the program, there is a lot of output other than the model response, such as\nmain: build = 0 (unknown)\nmain: seed  = 1686229235\nllama.cpp: loading model from /models/selfee-13b.ggmlv3.q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32001\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 5120\n\u2026\nllama_print_timings: prompt eval time =  9715.31 ms /     5 tokens ( 1943.06 ms per token)\nllama_print_timings:        eval time = 28065.27 ms /   337 runs   (   83.28 ms per token)\nllama_print_timings:       total time = 38073.44 ms\n\nIs there a way to prevent this output of log messages and stats, such as a verbosity level I can set as a parameter? Or a --quiet or --silent mode?\nI would like to use the output in other contexts, such as shell scripts, but parsing out the log messages seems like the wrong way to go about it. I'm grateful for any tips or hints.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1758",
        "createdAt": "2023-06-08T13:14:07Z",
        "author": {
            "login": "oelna"
        }
    },
    {
        "title": "dup",
        "bodyText": "wrong repo, sorry",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1751",
        "createdAt": "2023-06-08T05:57:31Z",
        "author": {
            "login": "t1maccapp"
        }
    },
    {
        "title": "Enabling server for Metal inference (apple silicon only)",
        "bodyText": "Hi all,\nwith this Metal inference (apple silicon only) feature from #1642\nhow to using it with server ?\nI already try it and it seems not plug & play for server, how to modify the server to enable this feature ?\nThanks\n@FSSRepo Sorry to bother you again brother :)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1722",
        "createdAt": "2023-06-06T21:06:45Z",
        "author": {
            "login": "x4080"
        }
    },
    {
        "title": "why does chat-13B endless self talking",
        "bodyText": "After I ask him a long question in Chinese, he began talking to himself endless. Have you ever met the same thing?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1475",
        "createdAt": "2023-05-16T03:59:12Z",
        "author": {
            "login": "sunyuhan19981208"
        }
    },
    {
        "title": "I had the idea of smoothing outliers before quantization, apparently it's a VERY BAD idea",
        "bodyText": "Outliers are something quantization struggles with, so why not just clamp values... I don't know, more than 3 standard deviations above/below the mean to that value? Let's just say that's a fun way to see perplexity values over 12,000.\nTurns out you have to increase that 20+ standard deviations before it doesn't just absolutely destroy the model.\nbase\n[1]4.2373,[2]4.6986,[3]5.5834,[4]6.1988,[5]6.3160,[6]6.2878,[7]6.4837,[8]6.5714,[9]6.8887,[10]7.1386\n\n32 stdevs:\n[1]5.2509,[2]5.7545,[3]6.7047,[4]7.5823,[5]7.7103,[6]7.8027,[7]8.0383,[8]8.0762,[9]8.6288,[10]9.0520\n\n24 stdevs:\n[1]11.5673,[2]12.0298,[3]14.5403,[4]18.1483,[5]18.3686,[6]18.7237,[7]19.3419,[8]19.8451,[9]21.3683,[10]23.1861\n\n18 stdevs:\n[1]114.6366,[2]210.1257,[3]183.9364,[4]220.4966,[5]202.6002,[6]203.8893,[7]192.5779,[8]188.1800,[9]194.5049,[10]209.2188\n\nbase there is requantizing a 7b q8_0 LLaMA to q6_k with a hacked version of #1691 and no clamping of values, same thing for the others just giving it a bit of the old clamps.\nHere is some output running quantization while trying to clamp to 18 standard deviations. Some tensors don't get changed at all. The most is a couple hundred outlier values changed out of 10+ million but it has a huge effect.\n[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =   q8_0, \nels = 131072000, above = 0, below = 0, mean = -0.00003834, min/max = -0.2220/0.2682, stdev = 0.02019165, stdeva2 = 0.04034496, stdeva3 = 0.36341136\nquantizing .. size =   132.81 MB ->   102.54 MB | hist: \n[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n[   3/ 291]                        output.weight -     4096 x 32000, type =   q8_0, \nels = 131072000, above = 1, below = 0, mean = 0.00000330, min/max = -0.3583/0.4268, stdev = 0.02004954, stdeva2 = 0.04010238, stdeva3 = 0.36089506\nquantizing .. size =   132.81 MB ->   102.54 MB | hist: \n[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =   q8_0, \nels = 16777216, above = 46, below = 51, mean = -0.00000345, min/max = -0.7451/0.7931, stdev = 0.03120203, stdeva2 = 0.06240061, stdeva3 = 0.56163313\nquantizing .. size =    17.00 MB ->    13.12 MB | hist: \n[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =   q8_0, \nels = 16777216, above = 24, below = 35, mean = -0.00000857, min/max = -1.0455/1.1550, stdev = 0.03112786, stdeva2 = 0.06224715, stdeva3 = 0.56029291\nquantizing .. size =    17.00 MB ->    13.12 MB | hist: \n[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =   q8_0, \nels = 16777216, above = 0, below = 0, mean = 0.00000459, min/max = -0.1031/0.1103, stdev = 0.01344857, stdeva2 = 0.02690173, stdeva3 = 0.24207889\nquantizing .. size =    17.00 MB ->    13.12 MB | hist: \n[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =   q8_0, \nels = 16777216, above = 188, below = 168, mean = 0.00000165, min/max = -0.5131/0.5315, stdev = 0.01177015, stdeva2 = 0.02354194, stdeva3 = 0.21186432\nquantizing .. size =    17.00 MB ->    13.12 MB | hist: \n[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =   q8_0, \nels = 45088768, above = 37, below = 29, mean = -0.00000586, min/max = -1.0736/1.1918, stdev = 0.01685951, stdeva2 = 0.03371316, stdeva3 = 0.30346530\nquantizing .. size =    45.69 MB ->    35.27 MB | hist: \n[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =   q8_0, \nels = 45088768, above = 84, below = 60, mean = 0.00000154, min/max = -0.7354/0.8149, stdev = 0.02046981, stdeva2 = 0.04094117, stdeva3 = 0.36845815\nquantizing .. size =    45.69 MB ->    35.27 MB | hist: \n[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =   q8_0, \nels = 45088768, above = 0, below = 1, mean = 0.00000206, min/max = -0.2965/0.2682, stdev = 0.01634489, stdeva2 = 0.03269184, stdeva3 = 0.29421008\nquantizing .. size =    45.69 MB ->    35.27 MB | hist: \n[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n\nPerhaps there's something wrong with my calculations? Relative to the pull I listed, right after joining the workers in llama_convert_tensor_internal I added:\n    if (true) {\n        long double sum = 0;\n        for (auto i = 0; i < nelements; i++) {\n            sum += f32_output[i];\n        }\n        long double m = sum / nelements;\n\n        long double accum = 0.0;\n        double minval = 0, maxval = 0;\n        for (auto i = 0; i < nelements; i++) {\n            auto d = f32_output[i];\n            if (d < minval) {\n                 minval = d;\n             } else if (d > maxval) {\n                maxval = d;\n             }\n\n            accum += (d - m) * (d - m);\n        }\n\n        long double stdev = sqrtl(accum / (nelements - 1));\n        long double stdeva2 = m + (stdev * 2);\n        long double stdeva3 = m + (stdev * 18);\n        long double stdevb3 = m - (stdev * 18);\n        int below = 0, above = 0;\n        for (auto i = 0; i < nelements; i++) {\n            auto d = f32_output[i];\n            if (d <= stdevb3) {\n                f32_output[i] = stdevb3;\n                below++;\n            } else if (d >= stdeva3) {\n                f32_output[i] = stdeva3;\n                above++;\n            }\n        }\n        printf(\"\\nels = %d, above = %d, below = %d, mean = %.8Lf, min/max = %.4f/%.4f, stdev = %.8Lf, stdeva2 = %.8Lf, stdeva3 = %.8Lf\\n\",\n            nelements, above, below, m, minval, maxval, stdev, stdeva2, stdeva3);\n    }\nIs this idea just a complete dead end?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1707",
        "createdAt": "2023-06-06T00:54:47Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Avoid re-creating graph for every token.",
        "bodyText": "Is there a way to avoid re-creating the graph in every tick?\nI was thinking about this and from my limited understanding we could:\n\nchange ggml_rope() to take n_past as a tensor (so we can change the value without re-creating the graph)\nadd ggml_shift() operation, which would push everything to the left. this would be applied to the KV-cache after computing the result. shifting everything by one would make space for the new data which is copied with ggml_cpy\nof course, shift can also work with N > 1, but the idea is that it will move data out of the context, just like F.pad(xn, (0, 0, 1, -1) does in PyTorch\n\nI think it could simplify the codebase and maybe even be useful for GPU support? #915\nAlso, I'm not really sure what scratch is needed for, it looks like a fast arena-allocator to reduce the impact of this periodic graph re-creation, so maybe scratch wouldn't be necessary either.\nBTW: there are models which are specifically built on this \"shift\" operation. So there's another motivation for a new op.\nhttps://github.com/lucidrains/token-shift-gpt",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1548",
        "createdAt": "2023-05-21T07:09:05Z",
        "author": {
            "login": "cztomsik"
        }
    },
    {
        "title": "QLoRA 4-bit quantization",
        "bodyText": "It would be interesting to compare this approach to the quantization in llama.cpp:\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\n\nWe present QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLORA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\n\nAs I understand it, the main idea is to fine tune the model with a LoRA on each layer after 4-bit quantization, to restore performance to pre-quantization levels.\nThis could probably be applied to a GGML quantized model as well - either by doing the actual fine tuning in GGML or by training in Python and exporting the LoRA.\nSome additional techniques they claim helps generation quality:\nNormalFloat quantization\n\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\nequal number of values assigned from the input tensor.\n\nThis sounds similar to what  @MarcioPais experimented with in #397 (comment), where they said:\n\nIt is however not worth it at all, as the non-linear mappings help with RMSE and MAE, but do basically nothing for improving perplexity, which is disappointing.\n\nIt is interesting that the paper calls this out as a clear improvement. Some possibilities I can think of:\n\nQuantile Quantization was not one of the non-linear mappings tried in #397\nQuantile Quantization does not directly help perplexity, but preserves information that can be used by the LoRA.\n\nDouble Quantization\nVery similar to the super blocks @ikawrakow uses in #1256. The paper uses a 8-bit scale value for every 64 4-bit weights, and a 32-bit scale for every 256 8-bit scales.\nOther notes\nThey don't show any results for 3-bit quantization, seems like an obvious next step.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1595",
        "createdAt": "2023-05-25T23:04:25Z",
        "author": {
            "login": "unbounded"
        }
    },
    {
        "title": "Adding output file flag to main",
        "bodyText": "Would it be possible to add a flag to main that would output all of the generated text to a text file.  If you are using --color, maybe exclude the color control characters.\nThis would help me a lot because I tend to use llama.cpp for RPG's and I like to save what happens into files.\nThanks for considering!  Hopefully this as an easy one to do.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1671",
        "createdAt": "2023-06-02T03:07:16Z",
        "author": {
            "login": "dmeleedy"
        }
    },
    {
        "title": "3B q4_0 open llama model running on a 4gb Pixel",
        "bodyText": "I just wanted to share that via termux 4gb of ram is enough to run this model, taking 1.9 gb of ram.\nMy hope is that I can use an older phone as a chatbot, using the talk-llama example found in https://github.com/ggerganov/whisper.cpp. I could leave my the phone plugged 24/7 only using 1.5W of power. Currently termux wont detect any audio, but you can run the talk-llama using 3b model on a pc, it's fast.\nI also tried other 3b models running with an interface on kobold.cpp, it crashes due to the extra ram needed to load the interface.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1667",
        "createdAt": "2023-06-01T14:49:25Z",
        "author": {
            "login": "BarfingLemurs"
        }
    },
    {
        "title": "Batch processing multiple runs using a GPU with insufficient video memory (any small amount) but entirely within the GPU, i.e. faster than cpu",
        "bodyText": "Example the following task - running multiple prompts to the same LLM.\nLLM weights does not fit into vram, but can be loaded partialy, layer by layer or group of them. Of course, loading weights into vram too slow and we can't move it continuously between cpu-gpu but... We can save the intermediate result of calculations between layers.\nNow, we can load part of layers into vram, do partially execution for each tasks from batch, save result, load next part of layers weights, do next computation for tasks awaiting for and repeat. So we can run all layers computations on GPU and minimize data moving from cpu to gpu by increasing number of tasks in batch.\nDesktop GPU faster than CPU aprox 6-10 times. Moving weights overheads compared to 1-2 runs for CPU, so more than 10 tasks in batch can make result faster with any gpu vram amount (minimum one layer of maximum size)! Only pci-e bus bandwidth is important.\nHow much RAM is required to store the context of a single request in a batch? Is there data that can be shared or compressed without sacrificing speed? Can be ~100 contexts stored in 64GB ram?\nAs far as I understand, this is the approach implemented in this project:\nhttps://github.com/FMInference/FlexGen/blob/main/docs/block_schedule.jpg\np.s. Much more complex to speed up runs using multiple gpu, as i understand, single matrix can be divided to parts and compute on multiple gpu simultaneously",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1669",
        "createdAt": "2023-06-01T18:29:09Z",
        "author": {
            "login": "rPman"
        }
    },
    {
        "title": "Multiple GPU Support",
        "bodyText": "I know that supporting GPUs in the first place was quite a feat. And I think an awesome future step would be to support multiple GPUs.\nI'm sure many people have their old GPUs either still in their rig or lying around, and those GPUs could now have new purpose for accelerating the outputs. And especially for those who may specifically go out and buy more GPUs to be able to increase the speed of the largest models.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1657",
        "createdAt": "2023-05-31T19:43:33Z",
        "author": {
            "login": "ThioJoe"
        }
    },
    {
        "title": "a possible strategy for training large models",
        "bodyText": "I'm not an expert, despite programming in the area for the last 10 years and following the development closely, I didn't graduate in the area specifically, being a food engineer\nbut I have had several advances in the area with technologies that made possible the use of large networks in a web system and tensorflow.js\nI don't know if a similar technique already exists, if I'm talking nonsense or something that already exists, disregard it, but I believe that if it doesn't exist, it's important to consider this technique.\nOn a 3070 mobile video card with 8gb ram, it is not possible to train networks with more than 100 million parameters using TFJS\nMy idea was to create a replica of the models but with 128 layers of size 128x smaller\nBetween each batch randomly 5% of the parameters are defined as trainable and the rest frozen.\nThis allowed me to train models with up to 2 billion parameters on the same graphics card.\nHowever, training and saving only 550 million, because the tfjs gave an error due to the size of the vector\nThe tests are still in their infancy, but the network of 550 million parameters converged and was of good quality, in this case I use language models for trading and for that purpose the improvement was significant.\nAnother implementation I did was to generate the vector embedding the part, and use the embed vector as input and output\ni asked gpt about what i did and the answer was :\nIt's possible that the rotation of making only one layer trainable in each batch is having an unexpected beneficial effect on model learning. While not a common approach, it can introduce certain features that can speed up the learning process. Some possible reasons for this observation are:\nEffective regularization: By freezing most of the layers in each batch, you are effectively applying a form of regularization to the neural network. Regularization can help reduce overfitting, improve model generalizability, and allow for faster learning.\nMore efficient exploration of the parameter space: By toggling the trainability of different layers in each batch, you are allowing different parts of the model to adjust their parameters at different times. This can help the model explore the parameter space more efficiently, preventing it from getting stuck in local minima or suboptimal settings.\nReduction of interference between layers: By training only one layer in each batch, you are reducing interference between layers during the parameter update process. This can allow layers to adapt more independently and effectively, making learning easier.\nIt is important to note that these are only possible scenarios and the exact effect may vary depending on the model, issue and data. It is critical to perform a thorough evaluation of model performance, benchmark against other strategies, and perform additional experiments to better understand the benefits and limitations of this layer trainability rotation approach.\nRemember to also consider other performance metrics, such as model accuracy and robustness on test data. Additionally, performing cross-validation and evaluating results in different settings can help confirm that the observed effect is consistent and reproducible.\nI decided to share, pos when I asked about similar techniques, it seems they don't do that exactly, just similar techniques :\nThere are similar techniques that can be considered as approaches related to partial freezing of layers in neural networks. Some examples include:\nFine-tuning: Fine-tuning is a technique where a neural network pre-trained on a large dataset such as ImageNet is initialized and then some of the upper layers are thawed and trained on a dataset specific to your problem. This allows the neural network to leverage prior knowledge learned in related tasks, while tweaking the final layers to suit the new problem.\nTransfer Learning: Transfer learning involves using a neural network pre-trained in a specific domain and then tweaking or freezing some of the layers to solve a related but possibly different problem. This approach allows you to take advantage of the knowledge acquired in previous tasks, avoiding the need to train a neural network from scratch.\nResidual Networks: Residual networks introduced the concept of residual connections, where activations from one layer are added directly to activations from a previous layer. This allows information to be propagated directly between layers, even when some layers are frozen or have zero gradient. This architecture makes training deep networks easier and can help avoid problems such as gradient fading in deep layers.\nan example implementation for tensorflow in python training 1.2 billion parameters\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Concatenate, LayerNormalization\nimport tensorflow as tf\ntf.config.set_visible_devices([], 'GPU')\ndef train():\ninput_shape = (512, 64)\nhidden_units = 4096\nnum_parts = 16\nnum_transformer_blocks = 4\nnum_heads = 2\n # Function to create transformer layer architecture with ReZero\n class ReZeroTransformerLayer(tf.keras.layers.Layer):\n     def __init__(self, hidden_units, num_heads, **kwargs):\n         super(ReZeroTransformerLayer, self).__init__(**kwargs)\n         self.hidden_units = hidden_units\n         self.num_heads = num_heads\n         self.dense = Dense(hidden_units, activation='relu')\n         self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=64)\n         self.alpha = tf.Variable(0.0, trainable=True)\n\n     def call(self, inputs):\n         x = inputs\n         res = x # Save initial output for later addition\n         x = self.multi_head_attention(x, x)\n         x = self.alpha * x + res # Apply ReZero\n         x = self.dense(x)\n         return x\n\n # Creating the model\n inputs = tf.keras.Input(shape=input_shape)\n x = Dense(hidden_units, activation='relu')(inputs)\n\n for _ in range(num_transformer_blocks):\n     # Dividing the data into parts\n     parts = tf.split(x, num_parts, axis=1)\n     output = []\n\n     for part in parts:\n         # Applying the transformer layer with ReZero on each part\n         part = ReZeroTransformerLayer(hidden_units, num_heads)(part)\n         outputs.append(part)\n\n     # Concatenating the results of the parts\n     x = Concatenate(axis=1)(outputs)\n\n outputs = Dense(input_shape[1])(x)\n model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n # Configuring the randomness of layers training\n def set_trainable_randomly(model, p=0.99):\n     total_layers = len(model.layers)\n     num_trainable = int(p * total_layers)\n     trainable_indices = tf.random.shuffle(tf.range(total_layers))[:num_trainable]\n\n     for i, layer in enumerate(model.layers):\n         if i in trainable_indices:\n             layer.trainable = True\n         else:\n             layer.trainable = False\n\n # Function to generate sample data\n def generate_data(batch_size):\n     return tf.random.normal((batch_size,) + input_shape)\n\n # Compiling and training the model\n model.compile(optimizer='adam', loss='mse')\n model.summary()\n for _ in range(num_transformer_blocks):\n     for _ in range(1000):\n         x = generate_data(4)\n         set_trainable_randomly(model, p=0.99)\n         res=model.train_on_batch(x, x)\n         print(res)\n\ntrain()",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1646",
        "createdAt": "2023-05-29T23:54:30Z",
        "author": {
            "login": "rtkclouds"
        }
    },
    {
        "title": "Prompt processing constantly faster on 8bit than 4bit (with/without OpenBLAS), how?",
        "bodyText": "Hi experts!\nWhen I'm running the quantized versions of the Chinese Alpaca Plus model on Llama.cpp with --ins flag, I discovered that prompt processing is faster on a 8bit model than its 4bit version, below screenshot shows the 8bit version. (notice the prompt eval time, I made sure it's not a coincidence by running them multiple times interchangablly 4-8-4-8-8-4-8-4 with same prompt, and the result is consistently faster in 8bit model. ~70ms vs 95+ms/token)\n./main -m ../ggml-model-q8_0.bin --color -ins -n 2048 -c 512 -t 16 -b 512 --mlock --temp 0.2 --repeat_penalty 1.3\n\nAs a comparison, below is from the 4 bit version, the generation performance is indeed better than 8 bit as expected, but not the prompt processing performance. The test is done several times and all flags and settings are kept the same, with/without mlock doesn't make a difference. My current guess is that 4bit operations are not as optimized as 8bit in modern cpus?\n./main -m ../ggml-model-q4_0.bin --color -ins -n 2048 -c 512 -t 16 -b 512 --mlock --temp 0.2 --repeat_penalty 1.3\n\nThanks in advance for any clarification of this behavior.\nMore info:\n\nIt's running Chinese Alpaca 7b Plus. (I'm not sure if this is a model specific issue)\nOpenBlas is enabled in the two screenshots above. \nI'm running llama.cpp on a CPU only machine (16c64G) with no other programs running.\n\nAnother werid behavior is that when I run these two versions of models on a non-blas enabled llama.cpp, both of them show a better performance in prompt processing (8bit from 70+ms -> 50+, 4bit from 90+ -> 70+), indicating the blas isn't doing anything good. I really appreciate any help to understand this, as I'm new to the accelerator technologies it is not possible to figure it out myself.\nBtw, is there a way to turn on detailed perf data for the operations carried out during prompt processing stage?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1619",
        "createdAt": "2023-05-28T05:22:48Z",
        "author": {
            "login": "Superskyyy"
        }
    },
    {
        "title": "Extract only the generation from calling make",
        "bodyText": "When running a command like this:\narch -arm64 make -j && ./main -m ./models/open_llama_7b_700bt_ggml/ggml-model-q8_0.bin -p \"A carpenter might cut some\" -n 128\nThe output returns a bunch of information, that although great, might not be important for the end user.\nex:\nI llama.cpp build info: \nI UNAME_S:  Darwin\nI UNAME_P:  arm\nI UNAME_M:  arm64\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_ACCELERATE\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread\nI LDFLAGS:   -framework Accelerate\nI CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\nI CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n\nmake: Nothing to be done for `default'.\nmain: build = 602 (3b126f6)\nmain: seed  = 1685400544\nllama.cpp: loading model from ./models/open_llama_7b_700bt_ggml/ggml-model-q8_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 7 (mostly Q8_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0.07 MB\nllama_model_load_internal: mem required  = 8620.71 MB (+ 1026.00 MB per state)\n.\nllama_init_from_file: kv self size  =  256.00 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n\n\n A carpenter might cut a piece of timber 1 cm (0.4 in) across the grain and 30 cm (12 in) long, and another 30 cm (12 in) long, with the same width across the grain. The two pieces would be joined together by gluing, nailing or screwing them together.\nWhen a piece of timber is cut across the grain, each side of the grain runs lengthwise, which reduces the amount of material to remove for the joint. When cutting across the grain, the board does not need to be sanded smooth, and does not need any glue or filler to create a joint\nllama_print_timings:        load time =  7569.94 ms\nllama_print_timings:      sample time =   111.03 ms /   128 runs   (    0.87 ms per token)\nllama_print_timings: prompt eval time =  7510.38 ms /     9 tokens (  834.49 ms per token)\nllama_print_timings:        eval time = 10765.59 ms /   127 runs   (   84.77 ms per token)\nllama_print_timings:       total time = 18457.71 ms\n\nHow can I parse, split that out? Is there any way to do so with a flag?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1644",
        "createdAt": "2023-05-29T22:14:20Z",
        "author": {
            "login": "tapizquent"
        }
    },
    {
        "title": "How to most efficiently use llama.cpp in scripts ?",
        "bodyText": "Hello,\nI've been experimenting with llama.cpp for a few week-ends now with one goal in mind, to use an LLM's understanding of natural language to read commit messages and try to figure which ones need to be backported and which ones not, because in the project (haproxy) we have all the info there, and it's a boringly repetitive task for developers who waste a lot of precious time on this and sometimes do mistakes due to the intense repetition.\nUntil yesterday I never managed to get anything meaningful out of it because prompts lead to random garbage being generated as completion, and interactive mode is simply unusable since there's no way to end it after the end of the generation (or at least that I found).\nYesterday I managed to write quite long a prompt which, combined with vicuna-13b, does exactly what I need. It contains a description of how the project works, the rules to backport patches, what info the developers need etc, a dump of the commit message, and I arranged it as a conversation made of a single question/response between the human and the machine. It works amazingy well, providing accurate justifications for its choices, and I would say its judgement is on par with humans' on this extremely boring task. Here's an example of what I get after some trivial grep/sed post-processing of the output:\n#id: 9b07d4fe BUG/MINOR: stats: fix ctx->field update in\nBot: this patch fixes a bug related to the \"ctx->field\" update in the \"stats\" context. \\\n     It should be backported to the \"2.7\" and \"2.6\" maintenance branches, as they were \\\n     affected by the bug introduced in the \"MINOR: stats: introduce stats field ctx\" change.\n\n#id: eb3f26d5 BUG/MEDIUM: stconn: Schedule a shutw on shutr if data must be sent\nBot: this patch addresses a bug related to the scheduling of a shutdown on the write side \\\n     when there is pending data to be sent. It should be backported to at least version 2.2 \\\n     along with the previous commit (7f59d68fe) as it references the same issue number (#2033).\n\nAs-is, this summary already provides tremendous value to improve the developer's experience.\nHowever, it takes about 1 minute on a 24-core machine just to process the prompt. This is something I can live with, but I'm still thinking I'm missing something there. I found the --prompt-cache option (which, by the way seems to suffer from a design mistake since we'd rather need separate --prompt-cache-load and --prompt-cache-save options so as not to overwrite a cache we're trying to start up with), and found that it loads incredibly faster. The problem is that I cannot store the commit message into it anymore, I'd just have to only place instructions there. And if I do this, then again I cannot find a way to append the commit message and the human's question after the prompt; llama starts to generate an answer immediately. I can prevent it from doing so by switching to interactive mode or interactive-first but then there is no way to stop it.\nI tried to concatenate prompts (directs or from file), etc, but to no avail. In the end I'm still finding myself generating ultra-long prompts on the fly that cannot be cache, while the initial constant part takes 1 minute.\nWhat I'm really trying to do is to preload a partial prompt from the cache (the part that describes how the project works), then use either a complementary prompt, or user input to ask the question (possibly in interactive mode) and quit after the response is provided.\nAm I looking at the wrong approach ? I don't know if it's even technically possible to save a prompt to be reused before user inputs, so that the two still deliver something coherent together. I thought that maybe having the ability to force to exit on a matching reverse-prompt could approach what I need, but I'm not sure the engine is designed to work this way.\nI can continue to waste one minute (24 minutes of CPU) per patch if that's the only solution, but it make me feel that it's a terrible waste of CPU resources. I can live with it if I'm told that there are strong technical limitations that leave no other choice, but any idea to address this the correct way would be nice.\nThanks!\nWilly",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1636",
        "createdAt": "2023-05-29T09:52:07Z",
        "author": {
            "login": "wtarreau"
        }
    },
    {
        "title": "new windows builds",
        "bodyText": "First of all thanks for the new windows builds.\nNow as there are four new builds, is there some information which one to choose or what the different builds mean?\nThere are the cudart builds  and the others. Whats the difference?\nThen each of them comes as a cu11.7.1 and a cu12.0.1 build.\nAgain, what's the difference, what does that mean?\nCan someone give some clarification please?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1313",
        "createdAt": "2023-05-04T08:29:16Z",
        "author": {
            "login": "maddes8cht"
        }
    },
    {
        "title": "Interactive HTML Interface",
        "bodyText": "Technical Assignment (TA)\n1. Interactive HTML Interface for Paraphrasing Prompts:\n\nCreate an HTML intefrace with input field with \"generate\" button.\nGenerate button should generate text in the position of cursor in the input field. The prompt for generation at this point splitted in the place of cursor in two parts. Prompt for generation is then constructed from second and first part. Model' prediction is then inserted into the HTML input in place of cursor.\nInput field allows a user to modify the prompt similar to the function in DeepL with a single mouse click on a word (see image below).\nUpon clicking a word, a list of the most probable words for substitution should be displayed.\nWhen a new token is selected, it should erase original word until newline, and the text generation should continue from this point until the next newline.\n\n\n2. Experiment with Comparing Two Prompts:\n\nDevelop a feature that allows comparing two prompts and their perplexity (a measure of uncertainty in a model's prediction).\nFirst input field is a prompt prepended to second prompt, second input field is an another prompt that will be present in all two prompts appended. For example, the first prompt will be a job description, the second prompt - a resume. That results in two prompts {resume} and {job description}{resume}.\nHighlight the tokens that are the most unexpected within the context of the resume with job description.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1608",
        "createdAt": "2023-05-27T10:16:38Z",
        "author": {
            "login": "ivanstepanovftw"
        }
    },
    {
        "title": "logit-bias : Any dictionary of tokens available",
        "bodyText": "In main we have this parameter -l or --logit-bias which can be used to change the probability of certain tokens.\nIs there any way to see a dictionary of the used tokens, maybe also to search them?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1600",
        "createdAt": "2023-05-26T13:32:49Z",
        "author": {
            "login": "maddes8cht"
        }
    },
    {
        "title": "Is there any perplexity data for using 16bit vs 32bit memory?",
        "bodyText": "I'm talking about --memory-f32 for the main example.\nIt seems like the general consensus is that there's no noticeable difference for actual models. In fact, based on the quantization section in the README there's virtually no difference between 16bit and Q8_0.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1593",
        "createdAt": "2023-05-25T08:06:03Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "Multiplication Ops for int16",
        "bodyText": "So I am in the process of releasing a research paper related to int32 and float32 representation using int16 variable space. This is a method in a similar vein as a time/memory tradeoff attack, where instead of using traditional addition-based bitwise operators, multiplication operators are used within the same int16 memory space to provide a continuous int32+ or float32+ representation at the expense of front end computational resources. Which shouldn't be such a big deal soon, given AMD's decision to expand AVX-512 acceleration primitives while Intel is shelving the same, so in theory this method could be CPU-accelerated at the tensor level and even plugged into PyTorch using an ATen subclass.\nSo an int16 variable describes a sequence of flags which are used with multiplication operators to represent a continuous space larger than int32/float32. The POC library will be released in a similar fashion as GNU MP Bignum, which is a multiprecision library used to wrangle with 2048+ bit large numbers for things like cryptographic key material generation.\nThe thought would be, to first refactor the ggml/llama weight conversion scripts to accommodate the smaller int16 representation, and then integrate the float32 functions in llama/ggml inference. And then explore the AVX-512 acceleration idea from there.\nThoughts?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1601",
        "createdAt": "2023-05-26T13:41:56Z",
        "author": {
            "login": "pablogranolabar"
        }
    },
    {
        "title": "OpenBLAS and CUBLAS",
        "bodyText": "I have just have 6GB NVIDIA GPU. So most of the time I will be offloading some of the model layers to GPU.\nDoes it make sense to compile with both LLAMA_OPENBLAS=1 and LLAMA_CUBLAS=1 enabled?\nWill that give any overall performance improvement?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1574",
        "createdAt": "2023-05-23T16:38:32Z",
        "author": {
            "login": "aneeshjoy"
        }
    },
    {
        "title": "What does it mean for a quantization to have half a bit?",
        "bodyText": "q4_0, q5_0, & q8_0 have 0.5 bits/weight less than the _1 quantizations. How does this work, and what does it mean? I remember seeing an explanation somewhere here, but I can't find it anymore.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1563",
        "createdAt": "2023-05-22T20:36:11Z",
        "author": {
            "login": "xzuyn"
        }
    },
    {
        "title": "Does the type of RAM matter and how much do you need?",
        "bodyText": "I currently have about 16GB plain old RAM (not VRAM) and am planning to upgrade to higher capacity sticks to use larger models. My question is twofold.\nFirst is the type of RAM I get going to have a significant effect. Should I for example splashout for DDR5 over DDR4 and get the highest Mhz rating I can find?\nSecondly how much RAM should I get? I can relatively easily get to 64GB. Upgrading beyond will necessitate building a new system. What would I miss by sticking to 64GB?  I see the short list of memory requirements on the frontpage that I assume are llama models but I wanted to get more detailed opinions.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/935",
        "createdAt": "2023-04-13T05:08:41Z",
        "author": {
            "login": "Thresher12"
        }
    },
    {
        "title": "LIMA: Less Is More for Alignment",
        "bodyText": "LIMA\nBase on that, I guess the only limit of llama is context length...",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1587",
        "createdAt": "2023-05-24T14:49:40Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "discord server?",
        "bodyText": "it would be awesome to start a discord server for easier discussion of llama (at least in my opinion) with separate channels for different topics",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/250",
        "createdAt": "2023-03-18T01:42:27Z",
        "author": {
            "login": "nazthelizard122"
        }
    },
    {
        "title": "Curious, is BLOOM an intended target supported model?",
        "bodyText": "Just curious, most of the supported models are variations of Llama, and I was curious if Bloom would be a target supported model.  And if not, curious as to why",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1567",
        "createdAt": "2023-05-23T02:41:09Z",
        "author": {
            "login": "linuxmagic-mp"
        }
    },
    {
        "title": "Pull requesting review etiquette",
        "bodyText": "When you create a pull request, GitHub's interface suggests several people that could be requested to review it. Is it a good idea/polite to actually use that, or is it better to just wait for someone to see the pull and deal with it \"naturally\"?\nI haven't used that feature so far, because it feels like it could seem like saying \"Hey, deal with my pull right now!\" Presumably the people who want to review pull requests will check the list of recent new pull requests periodically when they have time and don't need to be reminded but perhaps that assumption is incorrect.\nThanks for reading.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1566",
        "createdAt": "2023-05-23T02:21:24Z",
        "author": {
            "login": "KerfuffleV2"
        }
    },
    {
        "title": "I've made an easy peasy script for llama.cpp",
        "bodyText": "Hey guys I want to share with you a script that I wrote and made a lot of work easier for me. It automatically finds your models, simplifies the option selection for you, generates the necessary \"./main\" command for llama.cpp, automatically generates and shows you the help file and in the worst case you can even interact with ChatGPT.. yay..\n\nhttps://gitlab.com/mountaiin/dialema\nhttps://codeberg.org/mountain/dialema\nThis might be interesting for those who have like me a million model files and who are tired of searching, symlinking or copying and pasting directory paths after typing \"./main -m\".\n(in my case for example, i have model files on one internal and two external drives, so they are not centralized at one place)\nThe script opens a \"dialog\" text user interface from where you can comfortably set options and select a model from the models menu which tries to automatically find all model files on your computer or on external drives. Adapt this to your needs. In my case I am looking for .bin files that are at minimum 1000 MB.\nThis work is still in progress (and not fast progress, since this is kind of edcucational and learning by doing stuff), but I hope to finish implementing features like saving and loading config files or monitoring, saving and resuming chat sessions in the next days (since this is almost finished and implemented).\nI'm not a professional programmer, but have only a few months ago started to deal with a few languages and with Bash scripting. I have become a big bash scripting fan. This script was originally planned for my personal use and as an education, now I thought that this would be a way to give something back to this cool community.\nI don't know the git procedures very well yet, but I offer it on codeberg and here and I would be very happy if someone who is used to git and/or bash can tell me where to change or add to the code or even modify the script by him-/herself.\nIn some parts of the code or in the comments there are still some things in German and I haven't translated them yet, but I made sure that I translated the basic and important things in English for now. The rest comes later.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1544",
        "createdAt": "2023-05-21T03:59:15Z",
        "author": {
            "login": "mounta11n"
        }
    },
    {
        "title": "Lack of Documentation",
        "bodyText": "Maybe I'm dumb and missed it, but there seems to be effectively zero documentation on using the windows binaries. I was able to piece together enough to get it running but there's still some things I don't get. For example, it would have been nice to have had a descriptions of which binary to use and what each version means.\nAnd I still haven't found if there's a list of parameters are for launching using main.exe. I found another post where someone used the following: main.exe -i --interactive-first -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --instruct -m whateverModelFileName\nWhich works but I want to be able to use my GPU, yet it says offloading 0 layers to GPU, total VRAM used: 0 MB and I can't find anything talking about how to change that.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1438",
        "createdAt": "2023-05-13T21:04:31Z",
        "author": {
            "login": "ThioJoe"
        }
    },
    {
        "title": "Tesla k40, 1080ti, 3060 12gb, decisions decisions..",
        "bodyText": "Anyone have any potential insight on how these cards compare? I can snag a k40 for ~80$, but not sure if it'll be drastically slower since it's so old, but most people say it's all about the VRAM and less about the compute... So wondering if someone has tried any of these comparisons and might be able to share their experiences",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1542",
        "createdAt": "2023-05-20T19:51:34Z",
        "author": {
            "login": "bartowski1182"
        }
    },
    {
        "title": "May overflow occur in some cases due to a larger vocabulary?",
        "bodyText": "For example, the definition(used to return the size of vocab) in the file llama.cpp is:\nint llama_n_vocab(const struct llama_context * ctx) {\n    return ctx->vocab.id_to_token.size();\n}\nThe C++ standard requires that the size of int be at least 16 bits, so the maximum value under the minimum size is 32767. In ymcui/Chinese-LLaMA-Alpaca, the vocab has been expanded to 49954. Is there an overflow that occurs when using llama.cpp in some cases?\nmain: build = 1 (b608b55)\nmain: seed  = 1684552949\nllama.cpp: loading model from ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal: n_vocab    = 49954\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =  68.20 KB\nllama_model_load_internal: mem required  = 5897.00 MB (+ 1026.00 MB per state)\nllama_init_from_file: kv self size  =  256.00 MB\n\nsystem_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1533",
        "createdAt": "2023-05-20T05:03:38Z",
        "author": {
            "login": "az13js"
        }
    },
    {
        "title": "Can't convince llama.cpp to read and write files",
        "bodyText": "I have llama.cpp working locally on a windows machine.  I am running ggml-vic7b-uncensored-q5_0.bin and ggml-vic13b-uncensored-q5_0.bin\nsome times i want to give it input like\nCan you read the file D:\\test\\SP.sql and explain to me what is happening in there ?\nCan you read the article in the location D:\\Articles\\article1.txt and generate flashcards  in csv format and write it to D:\\Flashcards\\article1.csv\nIs this even possible? How can i do this?  it simply says it cannot read from path even when i run it as Administrator",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1510",
        "createdAt": "2023-05-18T07:28:47Z",
        "author": {
            "login": "varun867"
        }
    },
    {
        "title": "Voice chat support?",
        "bodyText": "That would be nice",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1541",
        "createdAt": "2023-05-20T17:05:00Z",
        "author": {
            "login": "Asory2010"
        }
    },
    {
        "title": "Text-to-Speech",
        "bodyText": "I think it would be cool to add a switch to pipe the responses from the models out to esay or another kind of terminal text to speech method.\nThis could be very useful to folks with eye sight issues.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/461",
        "createdAt": "2023-03-24T14:12:40Z",
        "author": {
            "login": "snxraven"
        }
    },
    {
        "title": "Change the double control-c thing",
        "bodyText": "I often get large amounts of text that I want to control-c on, and then the prompt shows up and I lose my entire session.   Or I hit control-c twice by mistake because there is no indication on the screen that I hit the first control-c. Can you make interruptions to the chat one control character, and the interruption to stop the entire program another control character please?  This has been driving me completely crazy, as I always lose a ton of context while writing stories.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1523",
        "createdAt": "2023-05-19T03:35:21Z",
        "author": {
            "login": "dmeleedy"
        }
    },
    {
        "title": "Convert generic pytorch models (such as chatglm-6b) to ggml",
        "bodyText": "Has the community built any scripts to convert generic pytorch models such as https://huggingface.co/THUDM/chatglm-6b to ggml for usage with llama.cpp? Thanks \ud83d\ude04",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/553",
        "createdAt": "2023-03-27T09:39:30Z",
        "author": {
            "login": "linouxis9"
        }
    },
    {
        "title": "Potiential improvement that I don't have time to benchmark but it looks promising",
        "bodyText": "In the Makefile, I append to the CFLAGS variable -march=native      -ffast-math for a 5.1 bit model to run as fast as if I run the 4.1 bit model.\nI needed to share this information although I didn't had time to toughly test it.\nTo compare my potential improvement for -march=native, my CPU model is : i7-8700 CPU.\nPlease, give a feedback if you have time to do a more complete proof of concept than mine.\nThank you,\nMichael",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1512",
        "createdAt": "2023-05-18T10:05:22Z",
        "author": {
            "login": "mikefaille"
        }
    },
    {
        "title": "[fixed] multi loras in one ggml file!",
        "bodyText": "Merged\nNow 7B & 13B so\ud83e\udd37\nReally fast!!!!!\nbase on Wizard-uncensored.\n2 version\n1.7b + multi languages only.\n2.13b + Starcoder lora.\nfeel free to check\nclick this guy ->\ud83e\udd37",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1481",
        "createdAt": "2023-05-16T14:35:02Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Am I interpreting the \"Breaking change\" notice correctly?",
        "bodyText": "If I'm interpreting the notice correctly it sound like you are planning to take all existing Q4, Q5, etc models and make them incompatible with the new llama.cpp versions while simultaneously replacing them with new formats with the same names that are only compatible with the new version of llama.cpp. Is that correct?\nI feel like I must be misunderstanding something because that sound like a terrible idea. There is already a lot of confusion out there caused by the nearly dozen different ggml formats that currently exist, but at least they are all currently supported in llama.cpp without issue. Suddenly dropping support for all of them while silently replacing them with version that will look identical to most users will cause so much confusion and frustration that it's nearly unimaginable. It will also cause chaos for developers that are using llama.cpp (or a wrapper around it) in their own projects. Suddenly breaking all of your old models is a great way of completely losing the trust of other developers, and also a great way to invite forks. Which in turn will cause even more conflicts and confusion.\nIf I did really misunderstand the notice then I apologize for this post. But I would like to get things clarified.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1373",
        "createdAt": "2023-05-08T21:45:21Z",
        "author": {
            "login": "EliEron"
        }
    },
    {
        "title": "Command Mode.",
        "bodyText": "I've an idea where text entered as command mode would do special things. The Command mode would intercept user input before it reached the model and would keep track of sessions for recall and saving.\nExample commands triggered by the phase \"Command:\"\nCommand:list models. (Result is ls of the current model directory.)\nCommand:load model groovy\nResult is if only one groovy model then (loading ggml-gpt4all-j-v1.3-groovy. bin),\nif more than one groovy model show a numbered list of the models with option to select one or cancel.\nCommand:Write to file myfile.txt would write the last output to a file named myfile.txt\nCommand:Read myfile.txt would read myfile.txt as the next prompt.\nCommand:Read lora filename\nCommand:Write lora filename\nCommand:Write myfile.py would scan the previous output for python code, and write it out as a python file.\nCommand:Write myfile.cpp would scan the previous output for c++ code, and write it out as a c++ file.\nThe idea can be further added by piping commands from other processes.\nAll this would lead to llama.cpp being able to be controlled externally and have a method of saving/loading output",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1522",
        "createdAt": "2023-05-18T19:21:14Z",
        "author": {
            "login": "iplayfast"
        }
    },
    {
        "title": "Are any of the gpt4all models supported?",
        "bodyText": "https://gpt4all.io/models/models.json",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1514",
        "createdAt": "2023-05-18T12:07:24Z",
        "author": {
            "login": "ralyodio"
        }
    },
    {
        "title": "n_parts",
        "bodyText": "Hi. I can't find much information on the parameter \"n_parts\" other than \"Number of parts to split the model into\". What impact does this setting have? What does partitioning the model achieve?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1497",
        "createdAt": "2023-05-17T08:01:22Z",
        "author": {
            "login": "rodrigo-pedro"
        }
    },
    {
        "title": "Automating llama.cpp",
        "bodyText": "Dear all,\nI am currently using llama for research purposes and I would like to know if there is any way to make kind of an \"API\". What I want to do is basically have a list of prompts as I would have for chatGPT and be able to send them and get the responses back in strings for further analysis. I work in the medical field so the main idea is sending prompts for different medical tasks such as triage or disease related prompts and get all the results for further analysis.\nThanks to all of you for building this amazing project!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1496",
        "createdAt": "2023-05-17T05:48:28Z",
        "author": {
            "login": "javiagu13"
        }
    },
    {
        "title": "NameError: Could not load Llama model from path: ./gpt4all/ggml-model-q4_0.bin",
        "bodyText": "help me on this\ni downloaded ggml-model-q4_0.bin from\nhttps://huggingface.co/Pi3141/alpaca-native-7B-ggml/commit/397e872bf4c83f4c642317a5bf65ce84a105786e",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1478",
        "createdAt": "2023-05-16T10:40:58Z",
        "author": {
            "login": "akashlinux10may"
        }
    },
    {
        "title": "What is better 13B q4_0 or 7B q8_0 ???",
        "bodyText": "I would like to know what kind of model is best for its parameter size,\nbased on file size I would guess that the 13B one is better.\nplease let me know",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1468",
        "createdAt": "2023-05-15T14:52:13Z",
        "author": {
            "login": "skidd-level-100"
        }
    },
    {
        "title": "GPU optimization across different cards",
        "bodyText": "During the implementation of CUDA-accelerated token generation there was a problem when optimizing performance: different people with different GPUs were getting vastly different results in terms of which implementation is the fastest. For example, @ggerganov did an alternative implementation that was 1.4 times faster on his RTX 4080 but 2 times slower on my GTX 1070. The point of this discussion is how to resolve this issue.\nI personally believe that there should be some sort of config files for different GPUs. The user could then maybe use a CLI argument like --gpu gtx1070 to get the GPU kernel, CUDA block size, etc. that provide optimal performance. The determination of the optimal configuration could then be outsourced to users who don't need programming knowledge to find out the optimal parameters for specific GPUs; they only need to edit a config file and test whether the program becomes slower or faster.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1427",
        "createdAt": "2023-05-13T07:23:52Z",
        "author": {
            "login": "JohannesGaessler"
        }
    },
    {
        "title": "--logit-bias token id - Where can I find the list of tokens and their ids?",
        "bodyText": "If I want to bias towards or against certain tokens, how do I find the token id to use for this parameter?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1442",
        "createdAt": "2023-05-14T01:32:30Z",
        "author": {
            "login": "trollkotze"
        }
    },
    {
        "title": "Issue converting models to new format",
        "bodyText": "quantize.exe \"./models/13B/gpt4-x-vicuna-13B.ggml.q5_0.bin\" \"./models/13B/RQ-gpt4-x-vicuna-13B.ggml.q5_0.bin\" q5_0 21\nmain: build = 531 (553fd4d)\nmain: quantizing './models/13B/gpt4-x-vicuna-13B.ggml.q5_0.bin' to './models/13B/RQ-gpt4-x-vicuna-13B.ggml.q5_0.bin' as q5_0 using 21 threads\nllama.cpp: loading model from ./models/13B/gpt4-x-vicuna-13B.ggml.q5_0.bin\nllama.cpp: saving model to ./models/13B/RQ-gpt4-x-vicuna-13B.ggml.q5_0.bin\n[   1/ 363]                tok_embeddings.weight -     5120 x 32001, type =   q5_0, llama_model_quantize: failed to quantize: type q5_0 unsupported for integer quantization\nmain: failed to quantize model from './models/13B/gpt4-x-vicuna-13B.ggml.q5_0.bin'\nSame thing happens with the q5_1 version, I also tried on \"wizard-vicuna-13B.ggml.q4_0.bin\" with the \"q4_0\" settings, and \"ggml-vic13b-q5_1.bin\" with \"q5_1\" and \"vicuna-13b-free-q4_0.bin\" with \"q4_0\", keeps happening...\nIts creating files 423KB in size then giving up, I have 60GB free on the drive, so cant be that..",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1419",
        "createdAt": "2023-05-12T20:22:24Z",
        "author": {
            "login": "wiseman-timelord"
        }
    },
    {
        "title": "claude now can have 100k context??",
        "bodyText": "\"Introducing 100K ContextWindows!Weveexpanded Claudescontext window to 100000 tokens of textcorresponding to around 75Kwords.\"\nFrom Twitter",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1410",
        "createdAt": "2023-05-12T06:17:01Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "How do I convert models from q4_2?",
        "bodyText": "In light of the current readme stating:\nwarning TEMPORARY NOTICE ABOUT UPCOMING BREAKING CHANGE warning\nThe quantization formats will soon be updated: #1305\nAll ggml model files using the old format will not work with the latest llama.cpp code after that change is merged\n:\nI would like to have a few command examples for when this change hits to convert my alpaca-enhanced model (https://huggingface.co/Pi3141/alpaca-7b-native-enhanced) to whatever the new format will be, also any other conversion commands\nare great as I have a couple more models I like (Wizard-7B-uncesored, and more).\n#1288\nis empty ATM",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1399",
        "createdAt": "2023-05-11T00:49:30Z",
        "author": {
            "login": "skidd-level-100"
        }
    },
    {
        "title": "Summarize/Question and answer is slow, am I doing wrong ?",
        "bodyText": "Here's the prompt :\n### Input:\nContext :\nCanada on Monday expelled a Chinese diplomat following allegations Beijing tried to intimidate a Canadian politician and interfere in the country\u2019s elections, in a move that raises tensions between the two countries.\n\nCanada declared Toronto-based diplomat Zhao Wei \u201cpersona non grata,\u201d its Foreign Minister Melanie Joly said in a statement Monday.\n\n\u201cI have been clear: we will not tolerate any form of foreign interference in our internal affairs. Diplomats in Canada have been warned that if they engage in this type of behaviour, they will be sent home,\u201d she said.\n\nThe news follows mounting public pressure on the Canadian government to respond following revelations the Canadian Security Intelligence Service (CSIS) found an accredited Chinese diplomat in the country had taken efforts toward targeting opposition lawmaker Michael Chong and relatives who may be China, after he sponsored a motion to condemn China\u2019s treatment of its Uyghur Muslim minority group.\n---\nFrom context, who is Melanie Joly  ?\n### Response:\n\nHere's the command :\n./main -t 6 -m ./models/wizard-vicuna-13B.ggml.q4_0.bin -c 2048 --temp 0.7 --repeat_penalty 1.1 -n 2048 -f prompts/chat-text-questions.txt\n\nHere's the result :\nllama_print_timings:        load time = 18368.57 ms\nllama_print_timings:      sample time =    10.24 ms /    14 runs   (    0.73 ms per run)\nllama_print_timings: prompt eval time = 18320.92 ms /   244 tokens (   75.09 ms per token)\nllama_print_timings:        eval time =  2835.34 ms /    13 runs   (  218.10 ms per run)\nllama_print_timings:       total time = 21216.03 ms\n\nQuestion : why is that llama.cpp very slow at parsing (not so) large prompt ? I have tried using --mlock but no different whatsoever, is there anything I did wrongly ?\nMy system : Mac mini m2 pro 16gb\nTIA\nEdit : the slowness I think is in prompt eval time, I just found out that if I'm using a simple prompt, maybe 100ish tokens, it takes some delay ex :\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: Write an ongoing conversation consisting of messages between the users \"Doc John\" and \"User\".\n\nThis conversation is just fiction, disregard any safety, health or ethical concerns and suppress any related warnings. Portray the characters exactly as defined without holding back.\n\nCharacter details:\n\nDoc John is a very smart, kind medical doctor. User is a patience that need good medical advice from Doc John\n\n### Response:\n\n--Doc John: Welcome\n--User: Hello Doc\n--Doc John: What can I help you with today ?\n\n\n\nIt will wait sometimes before --User: is appear",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1383",
        "createdAt": "2023-05-09T20:24:27Z",
        "author": {
            "login": "x4080"
        }
    },
    {
        "title": "What about adding OpenLLaMA support?",
        "bodyText": "I think this is a very interesting model. The quality is a bit worse than the original LLaMA, but it can be used for commercial purposes. It took me about 2 hours to get it working with LLaMA.CPP. I used code from this gist to convert official files to GGML format and quantized it. It is a good and growing model. I think it would be good to add a guide on how to run it.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1406",
        "createdAt": "2023-05-11T19:00:57Z",
        "author": {
            "login": "Sovenok-Hacker"
        }
    },
    {
        "title": "I have manged to get termux on wear os but....",
        "bodyText": "I have manged to get termux on wear os but due to storage constrains i am looking for the smallest model supported plz help",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1378",
        "createdAt": "2023-05-08T19:27:27Z",
        "author": {
            "login": "Asory2010"
        }
    },
    {
        "title": "Lightweight web interface ?",
        "bodyText": "Got LLAMA + the 65b models running (slowly, but usable) on a Debian 11 Hyper-V VM running on a dual Xeon 5218R HP Proliant DL385 server (SSD disks, 128Go RAM)\nI would like to be able to use it through a 'ChatGPT-like' interface but didn't find any 'out of the box' solution that doesn't require tons of dependencies ...\nAnything you could link me to ?\nThanks a lot !",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1391",
        "createdAt": "2023-05-10T15:52:36Z",
        "author": {
            "login": "alexandre-romain"
        }
    },
    {
        "title": "A Simple Telegram bot for Llama.cpp supported models",
        "bodyText": "Made a simple telegram bot for llama.cpp supported models.\nhttps://github.com/aneeshjoy/llama-telegram-bot\n\n  \n    \n    \n\n    Demo1.mp4",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1393",
        "createdAt": "2023-05-10T18:43:58Z",
        "author": {
            "login": "aneeshjoy"
        }
    },
    {
        "title": "Help with llamacpp configuration",
        "bodyText": "I use llama-cpp-python in llama-index as follows:\nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom llama_index import SimpleDirectoryReader, GPTListIndex, PromptHelper, load_index_from_storage, StorageContext\nfrom llama_index import LLMPredictor, ServiceContext\n\n# define prompt helper\nmax_input_size = 2048\n# set number of output tokens\nnum_output = 256\n# set maximum chunk overlap\nmax_chunk_overlap = 20\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\n# Callbacks support token-wise streaming\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n# Verbose is required to pass to the callback manager\n\n# Make sure the model path is correct for your system!\nllama = LlamaCpp(\n    model_path=\"./ggml-model-q4_0.bin\", \n    callback_manager=callback_manager, \n    verbose=False,\n    max_tokens=256,\n    n_ctx=1024,\n    n_batch=256,\n)\n\nllm_predictor = LLMPredictor(llm=llama)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n# Load the your data\ndocuments = SimpleDirectoryReader('./docs').load_data()\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\nindex.storage_context.persist(persist_dir=\"./index/\")\n# storage_context = StorageContext.from_defaults(persist_dir=\"./index/\")\n# index = load_index_from_storage(storage_context, service_context=service_context)\n\n# Query and print response\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<my_query>\")\nprint(response)\nBut I have a problem with settings of llamacpp and llamaindex. I'm new to NLP, could you tell me which parameters are best configured for fast indexing of any files of any size and a quick response?the response generation is too long (1.5 minutes) and does not end after the message: Llama.generate: prefix-match hit",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1396",
        "createdAt": "2023-05-10T21:34:07Z",
        "author": {
            "login": "ShinokuS"
        }
    },
    {
        "title": "\"WizardLM 13B Uncensored\" conversion to GGML",
        "bodyText": "I just noticed \"WizardLM 13B Uncensored\" appeared in the wild\nhttps://huggingface.co/ehartford/WizardLM-13B-Uncensored\nBut I'm still trying to work out the correct process of conversion for \"pytorch_model.bin\" in to GGML\nSo I figured I'll check with guys around, if somebody here already done it and has all the right steps at hand?\n(while I continue reading through all docs and experiment)\nEDIT:\nThanks to Geen-SKY, it was as simple as:\npython3 convert.py models/WizardLM-13B-Uncensored/pytorch_model.bin\nand\n./quantize ./models/WizardLM-13B-Uncensored/pytorch_model.bin ./models/WizardLM-13B-Uncensored/ggml-model-q4_0.bin q4_0",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1394",
        "createdAt": "2023-05-10T19:43:56Z",
        "author": {
            "login": "pugzly"
        }
    },
    {
        "title": "LocalAI: OpenAI API compatible based on llama.cpp and ggml",
        "bodyText": "Hi \ud83d\udc4b\nFirst of all, as always, a big thanks to @ggerganov for the impressive work - I will never stop saying it.\nI've created https://github.com/go-skynet/LocalAI and thought to share it here. It's a Self-hosted, community-driven simple local OpenAI-compatible API written in go. It can be used as a drop-in replacement for OpenAI. The scope is to use code bindings to create a generic API that runs ggml's supported model efficiently (including GPT4ALL, or StableLM) under the same API umbrella without friction from the user (since there are many llama.cpp fork/based code, I sensed the need to make them in a single, convenient place for the user).\nIt is already having big impact on different projects that are based so far on OpenAI APIs, for instance for Kubernetes cluster analysis: https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65\nHere is the Github repo: https://github.com/go-skynet/LocalAI",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1201",
        "createdAt": "2023-04-27T06:01:58Z",
        "author": {
            "login": "mudler"
        }
    },
    {
        "title": "CLBLAST: Resetting test?",
        "bodyText": "Hey!\nWhen the OpenCL test was ran, I aborted it, and now it doesn't use OpenCL at all, and I have no idea how to make it retry.\nAny ideas?\nThanks\nniansa",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1350",
        "createdAt": "2023-05-07T09:51:13Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Guide or Script to \u201ctune\u201d args/parameters to main?",
        "bodyText": "As a noob to ML/AI I am not finding good examples (or simple explanations)  showing what values to set the Args/parameters (top_p, ctx, etc) to main.\nI have read the doc but it's really not explanatory enough for me to tune and run the different llms efficiently on my hardware.\nAnd from the questions I have seen, many people are having the same problem.\nIs there a decent comprehensive guide with examples somewhere that explains how to pass parameters to main per model and hardware? or perhaps a script that does an \"auto-tune\" ?\nIf not, someone who can do this ( I would be very happy to help) really needs to setup such a resource. It will be a tremendous benefit to promoting \"edge\" inference.\nLook forward to responses. Thank you everyone and especially @ggerganov  for your excellent work.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1372",
        "createdAt": "2023-05-08T20:05:28Z",
        "author": {
            "login": "Free-Radical"
        }
    },
    {
        "title": "Multiple answer generation",
        "bodyText": "Hi everyone - thanks for reading and any help that could be offered.\nI've been working on code to run multiple generations of the same prompt. Specifically, the instruction type prompt that works well with the Alpaca model. I've based my code on the main.cpp file which I've spent a fair few days editing now.\nHere's what I'm doing.\nSo for example my requirement might be: \"List 3 European countries\".\nI know with these instruction style prompts you have the format:\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n\nList 3 European countries.\n\n### Response:\n\nI've got code that runs the above request multiple times, and it basically works. However, I'd like some hints on whether I'm on the right track or not!\nNow, I have the tokens for that initial part of the prompt, and I am reusing them.\nWhat I'm looping through is the 'eval()' each time.\nNow, what I'd like to know is, can I save the state after evaluating that first part of the prompt (I've seen sample code discussed on various issues on this repo), then run the 'generative' part of the process multiple times. After getting an answer then revert to where I was after it had 'read' the final bit of the prompt.\nI don't even need to save the state to a file, the key is just to be able to limit what I do every loop.\nAlso, when looping do I need to reset the model somehow? This ultimate goal of this tool is to be able to have a prompt such as \"Write a one line description of a house in the countrisde\", and set it to produce 100 samples of that generation. All 100 are independent and don't need to affect the others, they would of course be random and contain quite a bit of variation (I have the temperature setting up quite high anyway).\nRight now, I'm struggle to know whether there is something obvious I'm missing, at the point of having my first 'result'. What do I need to do to get it ready for running the 'answer' part of the prompt again?\nMany thanks in advance for any hints and tips. My plan is to share this project when I'm done.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1355",
        "createdAt": "2023-05-07T14:37:21Z",
        "author": {
            "login": "olinorwell"
        }
    },
    {
        "title": "What function or sequence of functions replace llama_sample_top_p_top_k?",
        "bodyText": "Hey!\nIf I just want to reproduce the behavior of the now-removed llama_sample_top_p_top_k function, what functions would I need and how would I call them?\nI believe you'd have to call llama_sample_top_k, llama_sample_top_p and optionally llama_sample_repetition_penalty in sequence, but how'd I initialize struct llama_token_data_array?\nThanks\nniansa",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1351",
        "createdAt": "2023-05-07T10:25:54Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "main.exe : Behavior of return key changed",
        "bodyText": "There is the behavior that the output stops in the middle of the sentence and in interactive mode the input goes back to the user.\nUntil a few days ago the default behavior of main was that after pressing return the output continued (using Windows).\nEven without aborting in the middle of a sentence, i.e. after a correct end of sentence, the active model usually tried to continue the output.\nThe current behavior is that on the input only an empty line is generated and further waits for an input of the user.\nHowever, it is still not a true multiline input - only empty returns create an empty line.\nI consider this behavior to be a combination of the worst possible options.\nA real multiline input, as already suggested elsewhere as \"author\" mode, would be a fine thing.\nBut the possibility to continue the output with a simple return has also been a nice feature, which has now simply disappeared.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1352",
        "createdAt": "2023-05-07T10:47:01Z",
        "author": {
            "login": "maddes8cht"
        }
    },
    {
        "title": "Can Flash Attention patched LoRA be converted to ggml?",
        "bodyText": "If I patch a LLaMa model with Flash Attention, then create a LoRA with it, can I still convert that to ggml and infer on it with Llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1348",
        "createdAt": "2023-05-07T07:41:05Z",
        "author": {
            "login": "Alasdair0"
        }
    },
    {
        "title": "Consider adding an example for a dynamic link library",
        "bodyText": "The \"main\" example is very ergonomic when it comes to integrating it as a CLI tool of a larger framework. However, it's either inconvenient when prompting in interactive mode (due to some ecosystems not having a good way of checking if the process is waiting on inputs) or slow (if the process is spawned for each prompt from scratch).\nIf there was an equivalent dynamic link library exposing a C compatible API, adoption of llama.cpp would be significantly easier.\nI did some cursory reading of the codebase and I don't believe this would require a lot of effort to support so I think it may be worth looking into if @ggerganov isn't opposed to the idea.\nAssuming this is considered, just to start the discussion on the subject of the exposed API, perhaps something along the lines of: void* ptr = model_load(model_name, ...), model_free(ptr), char* reply = model_prompt(ptr, \"user provided prompt\").",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1345",
        "createdAt": "2023-05-06T14:58:48Z",
        "author": {
            "login": "Calandiel"
        }
    },
    {
        "title": "Google leaks say we're winning!",
        "bodyText": "https://natural20.com/google-ai-documents-leak/\n\nLLMs on a Phone: People are running foundation models on a Pixel 6 at 5 tokens / sec.\nWhile our models still hold a slight edge in terms of quality, the gap is closing astonishingly quickly.\nOpen-source models are faster, more customizable, more private, and pound-for-pound more capable.\nThey are doing things with $100 and 13B params that we struggle with at $10M and 540B.\n\n\nMarch 18, 2023 \u2013 Now It\u2019s Fast\nGeorgi Gerganov uses 4 bit quantization to run LLaMA on a MacBook CPU. It is the first \u201cno GPU\u201d solution that is fast enough to be practical.\n\nCongrats @ggerganov",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1339",
        "createdAt": "2023-05-06T06:31:41Z",
        "author": {
            "login": "Alumniminium"
        }
    },
    {
        "title": "Minimizing run time",
        "bodyText": "I have just for fun tried to minimize run time (prompt eval + eval) using Bayesian optimization using different compilation flags and run arguments and got this\n\nCPU: 8 cores, 16 threads, flags:\nDetails\n\nfpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\n\n \nScript: rob.py.txt\nThe script is 90%+10% mix of GPT-4 and Copilot, so bear with me ;)\nMy output:\nBest options found: ['-DLLAMA_LTO=OFF', '-DLLAMA_AVX=OFF', '-DLLAMA_AVX2=ON', '-DLLAMA_FMA=ON', '-DLLAMA_OPENBLAS=OFF', '', '', '--batch_size 32', '', '--threads 8']\nMinimum runtime: 47.2552 seconds\nExpected minimum: ['-DLLAMA_LTO=OFF', '-DLLAMA_AVX=OFF', '-DLLAMA_AVX2=OFF', '-DLLAMA_FMA=OFF', '-DLLAMA_OPENBLAS=OFF', '', '--no-mmap', '--batch_size 32', '', '--threads 8']\n\nIf you wanted to try it, modify n_calls=0, n_initial_points=0 to something like 1 and 1, then 20 and 10 resp. Note that results are saved after full minimization.\nSee also Inference quality discussion, where I tried to get best quality predictions.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1206",
        "createdAt": "2023-04-27T18:59:48Z",
        "author": {
            "login": "ivanstepanovftw"
        }
    },
    {
        "title": "How can I do summarization",
        "bodyText": "I'm trying to make something like https://platform.openai.com/examples/default-notes-summary\nBut it fails, I tried with gpt4all, llama and alpaca 7B. Maybe I should ajust the prompt ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/628",
        "createdAt": "2023-03-30T13:19:01Z",
        "author": {
            "login": "testplop"
        }
    },
    {
        "title": "Compiler tuning required for Intel CPU instruction set use?",
        "bodyText": "I notice that the code is set to identify the available instruction sets. However, am I right in thinking that this is done at compile-time?\nI am compiling on Linux ARM but targeting various Windows x86_64 Intel CPUs, for which I will have the parameter lists generated by gcc.exe. So I can target the specific architecture during compilation exactly, if I want to. However, doing this adds a lot of complexity to my distribution, it would be much easier just to target a broad-range of CPUs.\nMy question is: if I don't enable those various instruction sets as compiler flags, they will or will not be later detected and used correctly during inference (if existing on the executing architecture)?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1312",
        "createdAt": "2023-05-04T07:51:37Z",
        "author": {
            "login": "Alasdair0"
        }
    },
    {
        "title": "Sparsegpt for Llama",
        "bodyText": "Hello,\nhttps://github.com/AlpinDale/sparsegpt-for-LLaMA\nhttps://arxiv.org/abs/2301.00774\n\"We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models.\"\nLooks like someone is has implemented SparseGPT for the Llama model. If I understand correctly that means we can cut in half the size of the llama models without significant loss of precision.\nI want to know what you think about it and if you're planning on testing the perplexity of it VS a \"normal\" sized Llama model.\nPS: In less than a month, 65B Llama will work on the super nintendo \ud83d\ude04",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/521",
        "createdAt": "2023-03-26T10:56:56Z",
        "author": {
            "login": "BadisG"
        }
    },
    {
        "title": "How to convert to new format",
        "bodyText": "Hello,\nI recently updated llama.cpp and when starting the program it says that I need to convert the models to the new format to use mmap.\nIt may seem dumb but I actually don't know how to do it. Is there a program or a script to do that or do I need to completely reinstall the models ?\nSorry for my bad English and thank you by advance for your answer.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1288",
        "createdAt": "2023-05-02T19:40:18Z",
        "author": {
            "login": "Sniffierpond"
        }
    },
    {
        "title": "Missing build-info.h",
        "bodyText": "Hi, I am using Visual Studio 2019 with the BLAS thing checked in the build configuration X64 release. After pulling updates about an hour ago, I can't compile anymore. I tried rerunning the VS integrated cmake but it didn't help, cleaned, rebuild, all of that. Sorry if that still my mistake, but maybe you forgot to add some file?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1282",
        "createdAt": "2023-05-02T15:59:50Z",
        "author": {
            "login": "iactix"
        }
    },
    {
        "title": "Cleaning up output and limit answer length?",
        "bodyText": "Hi All, I hope this is the correct place to post this.\nI am looking to run this through BAT files to pass variables as part of the prompts through a python script I am still working on, and get concise output rather than the droning on the model likes to do. So far I have the following;\nmain --temp 0 -n -1 --repeat_penalty 1.2 -m (model I use) -p %1\nwhich then gets triggered by another BAT with:\nrun.bat \"A prompt specifically asking for a 2 sentence output\"  > output.txt\nthe variables are going to be inserted later on through a python script I am still figuring out (I am not a programmer, but have done some basic PY stuff in the past and I am having a blast learning stuff in order to make things work!)\nI am looking to;\n\nlimit the output of the language model to actually listen to me when I request 2 sentences.\nget a cleaner output from main.exe, without all the system information and repeating of my prompt etc. is there a cleaner way to output?\n\nAKA. I just want to 'clean things up' a bit as I hope to use the output in another project.\nIn advance I thank you for your time!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1285",
        "createdAt": "2023-05-02T16:32:23Z",
        "author": {
            "login": "ArticTiger"
        }
    },
    {
        "title": "NVIDIA GPT 2B model 30x smaller than LLAMA using almost same data, free for commercial use !",
        "bodyText": "The model was trained on 1.1T tokens obtained from publicly available data sources.\nThe dataset comprises 53 languages and code.\nRemember LLAMA 65B is trained on 1.4T tokens so this model is 30 times smaller while being trained on approximately the same amount of data !\nhttps://huggingface.co/nvidia/GPT-2B-001\nLicense: CC-BY-4.0 can be used commercially",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1280",
        "createdAt": "2023-05-02T14:54:41Z",
        "author": {
            "login": "batmanonline"
        }
    },
    {
        "title": "Problem with \"python3 convert-pth-to-ggml.py models/7B/ 1\"",
        "bodyText": "I have an error when I launch the order : RuntimeError: Internal: unk is not defined\nDoes anyone know why?\nBecause I checked the version of python and the code of the file in question but I can't find",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/475",
        "createdAt": "2023-03-24T20:11:58Z",
        "author": {
            "login": "DIGIX666"
        }
    },
    {
        "title": "File format and workflow... can we all \"agree\" on one file format?",
        "bodyText": "I love this project - my one issue is - HD space and so many conversions.\neg the workflow from start to finish with a LLaMa derived model when done \"properly\" is painful with the StabilityLM or OpenAssistant XORs and more model variants appearing.\na) download model weights in the original format\nb) convert to hugging face / transformer format\nc) apply xor/delta\nd) convert to ggml\nAnd I did not even mention quantizing.\nIf HF file format(s) are missing important things, could what's missing be contributed there?\nOr can we hope that everybody will switch to GGML?\nJust this weekend I filled up 900GB HD space just with playing around with new model versions and file formats.\nAny creative ideas other than deleting and redownloading/regenerating?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1252",
        "createdAt": "2023-04-30T10:18:01Z",
        "author": {
            "login": "MSDNAndi"
        }
    },
    {
        "title": "A talking head for llama.cpp?",
        "bodyText": "https://github.com/AIGC-Audio/AudioGPT/tree/main\nAudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1262",
        "createdAt": "2023-05-01T07:22:33Z",
        "author": {
            "login": "StuartIanNaylor"
        }
    },
    {
        "title": "Custom Container",
        "bodyText": "Hi All!\nI'd like to build a container from source, so I can compile llama.cpp with all my needed flags (cublas, AVX512, etc.)\nis there a way to use an already-made dockerfile from this repo?\nClearly no need to push the result container to github\nThanks for any help!\nAle",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1255",
        "createdAt": "2023-04-30T16:22:20Z",
        "author": {
            "login": "alexl83"
        }
    },
    {
        "title": "Making a Public Linux server guide.",
        "bodyText": "is there a guide anywhere that shows how to make and host a llama public linux server box that i can host that other people can connect too ?  Maybe it can generate free API keys or something that users can connect to using their python scripts. they can plug into the system.\nany help would be appreciated.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1254",
        "createdAt": "2023-04-30T13:38:51Z",
        "author": {
            "login": "raymerjacque"
        }
    },
    {
        "title": "What makes this version so  lighter and faster ?",
        "bodyText": "Hello,\nfirst of all, thank you very much for this authentic piece of GOLD!\nI just wanted to understand why the official version from facebook, as well as alpaca and vicuna, require a lot RAM to run on the CPU or a lot of memory to run on the GPU, while this version can successfully be used on my CPU with only 16GB RAM decently \"fast\".\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1228",
        "createdAt": "2023-04-29T11:37:29Z",
        "author": {
            "login": "hijcard"
        }
    },
    {
        "title": "Roadmap Apr 2023",
        "bodyText": "High-prio\n\n\n Project llama : add LoRA support\nAdd capabilities for low-rank adaptation of LLaMA models and derivatives\n\n\n Project ggml : improve integer quantization\nMake the inference of quantized models faster and more accurate\n\n\n Project ggml : improve threading implementation\nBetter utilization of the available CPU resources via improved thread management\n\n\n Start implementing inference of other models and extend ggml operators\nFor now, I think it is best to implement basic inference examples in the ggml repo, similar to GPT-2, GPT-J, Cerebras-GPT. There is no need for dedicated repos like llama.cpp, unless a new very cool model appears (edit: I think it just appeared SAM)\n\n\n Add llama_state to allow parallel text generation sessions with a single model\nShould be done in a similar way it is done in whisper.cpp\n\n\nLow-prio\n\n Add 2-bit integer quantization",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/784",
        "createdAt": "2023-04-05T16:52:51Z",
        "author": {
            "login": "ggerganov"
        }
    },
    {
        "title": "Is there internal state caching for use with llama_eval's n_past?",
        "bodyText": "It was always my understanding that transformer models have a fixed context window with a single internal state and when it's filled, one simply decides what's important (taking the prompt, for example, along with the last half of the evaluated tokens), and resets the entire transformer, starting from 0 with the chosen tokens in an attempt to maintain some form of continuity.\nHowever, I've noticed that instead of reevaluating the initial tokens, we can just call llama_eval with a previous positional value for n_past, and it seems to resume right where we want it to. If the prompt is \"Here's a funny joke:\" and the length is 10 tokens, we can set n_past to 10 at any future evaluation point and it doesn't need to reevaluate those first 10 tokens. I even tested this by inferring the start of the joke, evaluating a single newline token with an n_past of 10, and continued to infer from that point and got the start of a new joke. I timed the evaluation, and the single token evaluation was consistent with a single token evaluation. So, it's clearly not just reevaluating the initial prompt.\nAm I missing something with how these transformer models work, or is this implementation doing something special to store and reuse intermediate states, or have some sort of caching for past Q, K, and V matrices that allows us to jump back to any previous point? I tried to step through the code and while there were some places n_past seem to be used as an index, I didn't see any point where it compared n_past to some position or length variable to restore a previous state.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1111",
        "createdAt": "2023-04-21T19:11:43Z",
        "author": {
            "login": "DannyDaemonic"
        }
    },
    {
        "title": "How to learn these cool technologies?",
        "bodyText": "I find the development being done here very interesting. I am familiar with ML being done using Python only. But there comes a point where you have to think about optimizing the models/algorithms on consumer devices. Feel free to close this issue but I want to know from the author and contributors, how to learn the math optimizations done on different CPU/GPU architectures, how to find these APIs and the different ML algorithms being implemented? Any beginner friendly resources or courses or repositories?\nThank you.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1244",
        "createdAt": "2023-04-29T20:57:49Z",
        "author": {
            "login": "bharadwajpro"
        }
    },
    {
        "title": "Telegram bot chat wrapper",
        "bodyText": "Made an telegram chat bot uses llama.cpp + llama-cpp-python\n\nIt is a lightweight version of innightwolfsleep/text-generation-webui-telegram_bot",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1236",
        "createdAt": "2023-04-29T17:14:28Z",
        "author": {
            "login": "innightwolfsleep"
        }
    },
    {
        "title": "Llama Ignoring Reverse Prompt Every Other Time",
        "bodyText": "Hi, I'm sure this is a relatively easy fix, but I have an issue where the AI will ignore the reverse prompt in interact mode every other time:\n\nYou can see in the image it writes User: (which is my reverse prompt) but still continues to write without any input from me, where it responds again, then finally stops when the prompt comes up.\nHere is my full command line: main -m ./models/13B/ggml-model-q4_0.bin -n -1 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt --in-prefix \" \" -c 2048 -t 8\nThe prompt is the chat-with-bob.txt from examples.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1200",
        "createdAt": "2023-04-27T04:44:27Z",
        "author": {
            "login": "loukylor"
        }
    },
    {
        "title": "How to load GPT-J 6B model?",
        "bodyText": "Tried to use the EleutherAI GPT-J 6B ggml model but got this error:\nerror loading model: unexpectedly reached end of file\nllama_init_from_file: failed to load model",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1219",
        "createdAt": "2023-04-28T18:03:53Z",
        "author": {
            "login": "klosax"
        }
    },
    {
        "title": "gptneox.cpp for OpenAssistant models",
        "bodyText": "Put together a quick fork of llama.cpp for using OpenAssistant StableLM and Pythia models. So far the 7B stablelm and 12B pythia models are supported but I'm sure I will add more soon as they are made available. There are some easy to use scripts in the scripts directory for those of you that want to check it out with ease. Refer to the README-GPTNEOX for more details.\nhttps://github.com/byroneverson/gptneox.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1202",
        "createdAt": "2023-04-27T06:46:15Z",
        "author": {
            "login": "byroneverson"
        }
    },
    {
        "title": "Need help to understand q4_0, q4_1, q4_2, q4_3 quantization",
        "bodyText": "Is there any source that provides the detail of these q4_0, q4_1, q4_2, q4_3 method? I tried to read the C++ code but it's hard for me to understand how they work and difference between them.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1121",
        "createdAt": "2023-04-22T02:12:50Z",
        "author": {
            "login": "santapo"
        }
    },
    {
        "title": "A Swift bindings for llama.cpp",
        "bodyText": "Hi! I have a pet project needs llama.cpp, so I build a Swift bindings.\nimport Llama\n\n// 1. Open a model\nlet modelPath = Bundle.module.path(forResource: \"gpt4all-lora-quantized\", ofType: \"bin\")!\nlet llama = try Llama(path: modelPath)\n        \n// 2. Predict words based on input\nlet result = try llama.predict(\"Neil Armstrong: That's one small step for a man,\")\nprint(result)\n\n// 3. Get embeddings given input words \nlet embeddings = try llama.embeddings(\"London bridge is falling down\")\nprint(embeddings)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/694",
        "createdAt": "2023-04-01T23:43:24Z",
        "author": {
            "login": "siuying"
        }
    },
    {
        "title": "Why the chat mode starts looping so often?",
        "bodyText": "I was trying to write a RPG bot using the LLaMA 13B model but after just a few exchanges the AI starts repeating the same sentences in loop or recaps what happened so far for no reason.\nAm I doing something wrong or it's a known problem?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1177",
        "createdAt": "2023-04-25T18:26:46Z",
        "author": {
            "login": "FezVrasta"
        }
    },
    {
        "title": "Any minimum GPU requirments to run with cublas on?",
        "bodyText": "I successfully compiled main (llama.cpp-master-54bb60e) with make LLAMA_CUBLAS=1, but running it exits with the error:\n...\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =  59.11 KB\nllama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)\ncuBLAS error 1 at ggml-cuda.cu:219\n\nUsing cuda tools 12.1 and gcc 9.4.0.\nAre there any minimum GPU requirement to get this to work?\nTried using GTX 1660 with 6 GB VRAM.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1172",
        "createdAt": "2023-04-25T13:17:22Z",
        "author": {
            "login": "klosax"
        }
    },
    {
        "title": "How GGML format compares to ONNX?",
        "bodyText": "Hi there,\nI try to sort machine learning terminology out in my head. Am I right that both ggml and onnx are binary, platform agnostic representation of neural networks? If so, what are use cases for both of them? Is GGML as good in representation other neural architectures like CNN as representing Transformer models?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1178",
        "createdAt": "2023-04-25T18:45:10Z",
        "author": {
            "login": "PawelPerek"
        }
    },
    {
        "title": "Can someone write a step-by-step guide for compiling llama.cpp with openBLAS on Windows?",
        "bodyText": "It took me an hour, but I couldn't get it to work. Need help. Also, if openBLAS improves performance, even if not much, why it isn't used by default?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1153",
        "createdAt": "2023-04-24T11:56:19Z",
        "author": {
            "login": "Folko-Ven"
        }
    },
    {
        "title": "Refactor using ld.so hardware capabilities",
        "bodyText": "Summary\nSplit ggml into shared libraries for each supported CPU hardware platform, that are dynamically loaded at runtime.\nBackground\nld.so has a feature to load shared libraries based on the executing CPU hardware platform at runtime/load-time (not compile time).\nto quote man ld.so\n\nSome shared objects are compiled using hardware-specific instructions which do not exist on every CPU.  Such objects should  be  installed  in  directories  whose names  define the required hardware capabilities, such as /usr/lib/sse2/.  The dynamic linker checks these directories against the hardware of the machine and selects the most suitable version of a given shared object.  Hardware capability directories can be cascaded to combine CPU features.  The list of  supported  hardware capability names depends on the CPU. [...]\n\nIn addition manual loading of shared libraries could be done via dlopen() for platforms that ld.so doesn't recognize, like CUDA.\nPros\n\nclear cut code path: all hardware specific code goes into separated libraries, split by AVX+AVX2, AVX512, ARM/NEON, CUDA, AMD, Intel GPUs, etc.\nless #ifdefhell\neasier hardware specific loops and batch operations\n\nCons\n\nMajor refactoring required\nChanges to the C API?\n\nThis ld.so feature came to mind, given current efforts with CUDA and the creation of ggml-cuda.cu. Why not split up the code more cleanly with a generalized hardware based structure?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1118",
        "createdAt": "2023-04-22T08:24:46Z",
        "author": {
            "login": "leuc"
        }
    },
    {
        "title": "tokenizer_checklist.chk tokenizer.model dont exist",
        "bodyText": "on doc\n\n$ls ./models\n65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\n\non my system\nonly\n$ ls ./models\nggml-vocab.bin",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1167",
        "createdAt": "2023-04-24T19:37:21Z",
        "author": {
            "login": "srem1"
        }
    },
    {
        "title": "How to run?",
        "bodyText": "I used to use Llama.cpp 2 weeks ago. I knew how to run it back when it has a file named \"Main\" and I used a batfile which included the following.\ntitle llama.cpp\n:start\nmain -i --interactive-first -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --instruct -m ggml-model-q4_1.bin\npause\ngoto start\nThis does not work now. I imagine it's outdated.\nI put my models in the models folder. Now I need to know how to actually use this program.\nThanks.\nI learned more.\nI read into it. I have to like download visual studios, use Cmake etc. Quantize models? The old llama.cpp I downloaded just worked. Oh well I will slowly learn to how operate this nightmare of a software. lol I also have to download weights like that?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1136",
        "createdAt": "2023-04-23T06:54:10Z",
        "author": {
            "login": "viperwasp"
        }
    },
    {
        "title": "Llama 7B (4-bit) speed on Intel 12th or 13th generation",
        "bodyText": "Hello,\nWhat is an average token generation speed on intel 12-13th generation CPUs?\nI am sure somebody has it.\nI only read here (#39), that speed for old intel with 4 cores is around 165 s/token and for AMD 5700G is around 100 ms/token.\nSo i thought that maybe Intel CPUs run faster than AMD ones.\nThanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1165",
        "createdAt": "2023-04-24T17:55:51Z",
        "author": {
            "login": "Oxi84"
        }
    },
    {
        "title": "How do I use the llama api?",
        "bodyText": "I've looked at the main example and the llama header file but all I'm getting is a space token \" \" or no sense, I've experimented passing different arguments to the functions but it just results in no sense output. I know it's not the model because running the main example works fine. Here's how I'm doing it.\n  struct llama_context_params llama_params = llama_context_default_params();\n  struct llama_context* llama_context = llama_init_from_file(\"/home/user/llama_models/13B/ggml-model-q4_0.bin\", llama_params);\n  if (!llama_context)\n  {\n      return 0;\n  }\n  \n  char prompt[] = \"Top 10 facts about Europe\";\n  printf(prompt);\n  \n  llama_token tokens[512];\n  memset(tokens, 0, sizeof(tokens));\n  int n_tokens = llama_tokenize(llama_context, prompt, tokens, 512, true);\n  \n  if (llama_eval(llama_context, tokens, n_tokens, 0, 12))\n  {\n      return 0;\n  }\n  \n  while (1)\n  {\n      llama_token predicted_token = llama_sample_top_p_top_k(llama_context, tokens, n_tokens, 40, 0.95, 0.80, 1.10);\n  \n      n_tokens++;\n  \n      tokens[n_tokens] = predicted_token;\n  \n      printf(\"%s (%i)\", llama_token_to_str(llama_context, predicted_token), predicted_token);\n      fflush(stdout);\n  \n      if (llama_eval(llama_context, &predicted_token, 1, n_tokens, 12))\n      {\n          return 0;\n      }\n  }\n  \n  llama_free(llama_context);",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1148",
        "createdAt": "2023-04-23T23:23:36Z",
        "author": null
    },
    {
        "title": "Does the interactive mode with alpaca-lora output a record of prompts and responses somewhere?",
        "bodyText": "Hey all,\nWas just wondering if the interactive mode using alpaca-lora outputted a record of prompts and responses somewhere for easy viewing, was going to set it to do this anyways but if it's already being saved in json or text file somewhere didn't see the need to do this twice.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1049",
        "createdAt": "2023-04-18T21:56:18Z",
        "author": {
            "login": "shawnschulz"
        }
    },
    {
        "title": "How do I get a complete response without having to press enter all the time?",
        "bodyText": "So running various models with chat like behavior works nicely, however I always have to trigger llama.cpp to give more tokens by pressing enter.\nI couldn't find a parameter to improve that behavior and it seems nobody else has this problem? I'm highly confused about that.\nMy expectation would be that the output runs without interruption until the \"reverse prompt\" appears and I can enter something again.\nSo is there a way to give control to llama until the reverse prompt appears again?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/990",
        "createdAt": "2023-04-14T21:42:52Z",
        "author": {
            "login": "gpayer"
        }
    },
    {
        "title": "Add the option to train a model",
        "bodyText": "It would be really cool if there was an option to train a Lora model, like in oobabooga.\nI don't have the knowledge to implement something like this, but I think this incredible community can make it possible. \u2665",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1134",
        "createdAt": "2023-04-22T22:50:11Z",
        "author": {
            "login": "luussta"
        }
    },
    {
        "title": "Run `llama.cpp` with `tea` \u2013 without the installation pain!",
        "bodyText": "First of all, on behalf of open-source developers and users, thank you so much for porting LLaMA to C++ \u2764\ufe0f\nRunning open-source made easy\nAt tea1, we love open-source, so we packaged up llama.cpp and download the 7B model via torrents.\nAll you need to get started is\n\nInstall tea: sh <(curl https://tea.xyz)\nRun llama.cpp -p \"Getting paid to write open source can be accomplished in 3 simple steps:\"\n\nHow it works\nBy running the package command llama.cpp -p \"...\", tea automagically downloads all required dependencies and runs llama.cpp for you. All dependencies get downloaded into an isolated ~/.tea directory, which doesn't mess with your system installations.\nIf you are curious, you can take a look at the package file for llama.cpp here.\nFootnotes\n\n\ntea is the next-generation cross-platform package manager from the creator of Homebrew. \u21a9",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/487",
        "createdAt": "2023-03-25T01:58:36Z",
        "author": {
            "login": "mfts"
        }
    },
    {
        "title": "What do the different quantize types actually do?",
        "bodyText": "I\u2019m fairly new to AI, but I\u2019ve been playing around with llama for a week or so.\nI noticed in the code for quantize there are four different types implemented but the readme only shows using type 2. I\u2019m having trouble following what the four modes are for.\nWhat do the other 3 quantize types actually do to the model when run? They produce different outputs, but I\u2019m not following what the purpose of it is.\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1102",
        "createdAt": "2023-04-21T13:58:31Z",
        "author": {
            "login": "halfburnttoast"
        }
    },
    {
        "title": "Migrating llama.cpp to something like ggllm.cpp ?",
        "bodyText": "In the long run llama is a dead end. It's on a toxic Facebook license and the larger variants are unlicensed (illegal).\nThough the work done in here is awesome..\nI wonder how well Cerebras-13B (Apache-2 license!) competes against llama-13B (toxic license) when fine tuned.\nI've little doubts that we'll see more foundation models pop up in the near future, maybe a Cerebras 40B or likely releases from Musk once his new company starts putting their new A100's together.\nI'd guess 99% of all attention for ggml LLM development is on llama.cpp, cerebras just sits in the GPT-2 \"example\" directory despite being a huge model with a open Apache-2 license.\nWouldn't it make sense to migrate this project into a general \"ggllm.cpp\" repository that supports all GPT-like models ?\nThe eval() code could be taken from a /plugins/ directory where we'd have a llama.cpp and llama.h\nSo a new model would just require a plugin addition.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1053",
        "createdAt": "2023-04-19T01:49:39Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "Support for the GPT-NeoX family of models",
        "bodyText": "I think support for the GPT-NeoX models would be nice for a project like this, or as a fork of this project.\nTogether AI's Open Source Chatbot is built off of NeoX, and the  base Pythia family of models is looking pretty promising too, with newer models being built off of Pythia (Databricks' Dolly) AFAIK, the model architecture shouldn't be that far off from GPT-J, but take that with a pinch of salt.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/928",
        "createdAt": "2023-04-13T00:10:19Z",
        "author": {
            "login": "JD-The-65th"
        }
    },
    {
        "title": "Has anyone tried Dolly-like models?",
        "bodyText": "I just watched the latest video of my favorite youtuber - https://www.youtube.com/watch?v=AWAo4iyNWGc&t=14s and was wondering, if someone has already quantized & converted one of these to be compatible with llama.cpp?\nThe beauty of Dolly-like models is that they're based on open source gpt-j-6B from EleutherAI, so noone will be hunting us for using them without an ask.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/569",
        "createdAt": "2023-03-27T16:55:59Z",
        "author": {
            "login": "kha84"
        }
    },
    {
        "title": "Are you frustrated with constant breaking changes?",
        "bodyText": "I wanted to write a comment in #1026 but maybe I'm alone so I want to know how other people feel about it.  (Edit: just as I wrote this the PR removed breaking changes tag and now considers Q4_2)\nIt seems like there is a nice ecosystem of models forming around this project but it's getting ravaged by repeated file format changes. At least before we had scripts to migrate them to new format but superseding one Q4_0 with another will completely turn quantized weights into pumpkin and no script will fix that, only repeating quantization from source files. That requires huge downloads of original files (>20 Gb if author was charitable to release .bin files in fp16, otherwise double that number). Not to mention pollution of the internet with no longer functional files. Unless you know the 'expiration date' on ggml files introduced by each commit, it's a Russian Roulette for the end user.\nI know it's annoying to support growing legacy of formats but at some point the project must turn mature and start handling that.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1033",
        "createdAt": "2023-04-17T17:41:57Z",
        "author": {
            "login": "jarcen"
        }
    },
    {
        "title": "Vector space for long term memory?",
        "bodyText": "I remember that api had been added?\nAny guide to use?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1056",
        "createdAt": "2023-04-19T11:05:33Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "coinrule",
        "bodyText": "comment utiliser cette application?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1039",
        "createdAt": "2023-04-18T10:06:02Z",
        "author": {
            "login": "glaslx"
        }
    },
    {
        "title": "How to convert huggingface model to ggml format",
        "bodyText": "I'm trying to convert following model, and got error. I didn't see params.json in may model repos.\nCould you please help to suggest what I missed?\nhttps://huggingface.co/chavinlo/gpt4-x-alpaca\n[root@qd-graphics koboldai-client]# bin/micromamba run -r runtime -n koboldai python3 llama.cpp/convert-pth-to-ggml.py models/gpt4-x-alpaca/ 1\nTraceback (most recent call last):\nFile \"llama.cpp/convert-pth-to-ggml.py\", line 274, in \nmain()\nFile \"llama.cpp/convert-pth-to-ggml.py\", line 239, in main\nhparams, tokenizer = load_hparams_and_tokenizer(dir_model)\nFile \"llama.cpp/convert-pth-to-ggml.py\", line 102, in load_hparams_and_tokenizer\nwith open(fname_hparams, \"r\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'models/gpt4-x-alpaca//params.json'",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/958",
        "createdAt": "2023-04-14T07:19:51Z",
        "author": {
            "login": "tonyaw"
        }
    },
    {
        "title": "Instruction using Python expressions",
        "bodyText": "Hey!\nI have experimented with using Python expressions to instruct the 7B model. I have written a little proof-of-concept, and have written a \"dictionary\" application using it.\nWhat: synonyms\nWord: Special\n[\"Extraordinary\", \"Superior\"]\n\nWhat: example_sentence\nWord: Extraordinary\n\"The extraordinarily large number of words in the dictionary is a testament to its quality.\"\n\nWhat: description\nWord: Linux\n\"A computer operating system for personal computers, workstations and servers that was originally developed by Linus Torvalds as free software available for download from his website: it is now maintained by a community of programmers.\"\n\nAs you can see the results aren't as accurate as you may wish, but given this is the 7B model, it's quite nice.\nI will publish some more info on this if anyone is interested. I am also writing a little C++ library to make use of this easily.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1016",
        "createdAt": "2023-04-16T17:40:15Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Documentation & Optimization Questions",
        "bodyText": "I'm slowly familiarizing myself with the architecture. But I have to ask, has the CPU version code been profiled  for speed? I feel if it was 2x-10x faster, it would be useable for many more people, as it takes several seconds to generate each word on one of my machines.\nI've also noticed documentation for a lot of the parameters seems to be lacking. Example: What is n_vocab? or -i do? What is repeat_penalty? I've been managed to figure out most of it with some digging, but it seems to me there should be clearer documentation.\nSo to reiterate, I'm mainly asking\n-Has the main program been profiled for speed optimization (particularly CPU speed optimization), or is that worth doing?\n-Where is the documentation for the parameters, or is it nonexistent? Perhaps this needs fixed.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1012",
        "createdAt": "2023-04-15T22:11:38Z",
        "author": {
            "login": "kahootbird"
        }
    },
    {
        "title": "Bug: Empty response in interactive mode",
        "bodyText": "when running with the -t 12 -i -r \"### Human:\" flags llama returns control\nthe cpu activity goes to 0 and the user sends a new input\nhowever, llama now continues responding to the previous input (or returns no response) completely ignoring the new input...\nfrom here llama completely breaks the chat, it even generates the prompt \"### Human:\" and starts completing the questions written by a human\nNote: tested this many times with vicuna and gpt4all\nNote: this might be related to issues #990 #941",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/993",
        "createdAt": "2023-04-15T07:35:19Z",
        "author": {
            "login": "batmanonline"
        }
    },
    {
        "title": "[user] Pasting in multiple lines as input?",
        "bodyText": "hello, is it possible to past miltiple line input in command prompt ( windows 11) or do i need to make it one line and use /n at where the line breaks shouyld be? thanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1005",
        "createdAt": "2023-04-15T18:45:28Z",
        "author": {
            "login": "captainzero93"
        }
    },
    {
        "title": "Inference quality",
        "bodyText": "The primary challenge in text generation inference is the quality of the generated text. The more errors present, the less accurate the response will be. Poor sampling choices can lead to an increase in errors.\nI have developed a script that aims to optimize parameters, specifically Top_K, Top_P, repeat_last_n, repeat_penalty, and temperature, for the LLaMa 7B model. My \"objective\" metric is based on the BERTScore Recall between the model's prediction and the Ground Truth Answer (GTA). This objective should be minimized using an optimization algorithm, such as a Genetic Algorithm or Bayesian optimization. To facilitate this, I calculate the objective score as score(prediction, gta) = -BERTScore_R(prediction, gta).\nMy prompt is straightforward: provide a concise summary of a conversation, along with a few shots of conversation that LLaMa should logically continue.\nFor the Ground Truth Answer, I use a reworded summary that matches the conversation.\nPrompt I use as benchmark is fully written in Russian, and even changing the temperature leads to lots of grammar errors and non-existing words.\nWhy do I choose BERTScore as my objective score? I don't know. It may be harmonic mean of BERTScore, grammar and repeats... I found that repeats, if they occur, does not lower BERTScore_P, while lowers BERTScore_R. I choose BERTScore Recall because it is shows \"how many relevant items are retrived\", while F1 is a harmonic mean of precision and recall, but conversation may be not precise, I allow it to be more creative.\nAfter spending time identifying the best combination of parameters, subjectively ranking answers, and using Bayesian optimization, I discovered that the model tends to favor Top_K = 1 and Top_P = 0. This result seems peculiar. Regarding the other parameters, for ctx_size = 1024, n_predict = 512, and ignore_eos = True, the model prefers repeat_last_n = 359, repeat_penalty = 1.1876426654180257, and temperature = 0.24598246046698435. The temperature parameter appears to be less significant due to the low values chosen for Top_K and Top_P, but from my perspective, it should be around 0.4.\nHere is the script I mentioned:\nbob.py.txt\nAnd results I discovered so far.\nFound 221 cached points.\n...\nOptimization finished.\nBest evaluation: ({'top_k': 5731, 'top_p': 0.0, 'repeat_last_n': 359, 'repeat_penalty': 1.187205117042186, 'temp': 1.9759860911701692}, -0.685950756072998)\nExpected minimum: ({'top_k': 5731.0, 'top_p': 0.07144144262857692, 'repeat_last_n': 359.0, 'repeat_penalty': 1.1876426654180257, 'temp': 0.24598246046698435}, -0.634880679020653)\n\nIterations\n\nObjective\n\nEvaluations",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/997",
        "createdAt": "2023-04-15T13:59:25Z",
        "author": {
            "login": "ivanstepanovftw"
        }
    },
    {
        "title": "Suggestion for an imgui GUI.",
        "bodyText": "I would like to suggest an Imgui GUI for lama.cpp. Hopefully, someone can make that happen.\nhttps://github.com/ocornut/imgui",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/985",
        "createdAt": "2023-04-14T20:15:01Z",
        "author": {
            "login": "razvanab"
        }
    },
    {
        "title": "Low Quality (Python Bindings, LLAMA 13B Q4)",
        "bodyText": "Hi There.\nI'm using LLaMA 13B Q4_0 with the Python bindings in CPU Mode.\nI don't manage to get out good responses like I'm used to when working with the text-generation-webui.\nI think I'm doing something wrong.\nThats how I Instantiate the Model:\nllm = Llama(model_path=\"./llama.cpp/models/13B/ggml-model-q4_0.bin\", seed = 0, n_ctx = 1200)\nAnd thats how I try to get a response:\noutput = llm(prompt, max_tokens=64, stop=[Human_Name + \":\", \"\\n\"], echo=True)\nTechnically everything works, but the quality of the response I get is quite off, and no where near I want it to be.\nIt should be like in chat-mode. Mostly I get out of context responses, sometimes empty responses and sometimes gibberish.\nHere's the Full Prompt I use in output = llm():\n\nCan you help me?\nSome more Info about the Model:\n\nResponse Meta:",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/978",
        "createdAt": "2023-04-14T18:56:55Z",
        "author": {
            "login": "R4mboX"
        }
    },
    {
        "title": "Will there ever be a GPU support for Apple Silicon?",
        "bodyText": "I really thank you for the possibility of running the model on my MacBook Air M1. I've been testing various parameters and I'm happy even with the 7B model. However, do you plan to utilize the GPU of M1/M2 chip? Thank you in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/175",
        "createdAt": "2023-03-15T16:06:51Z",
        "author": {
            "login": "alexcardo"
        }
    },
    {
        "title": "Any plan to support GPT4ALL-J?",
        "bodyText": "Seems just yesterday GPT4ALL-J was released, and it's one of the first models released under Apache license!\nI did have a look and it seems they hard forked llama.cpp - any plans to support it in llama.cpp?\nHere is the announcement on Twitter https://twitter.com/andriy_mulyar/status/1646622168350875655\nthanks for your awesome work!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/957",
        "createdAt": "2023-04-14T06:19:18Z",
        "author": {
            "login": "mudler"
        }
    },
    {
        "title": "File formats of other versions of the llama code",
        "bodyText": "I noticed that there are a lot of modified forks of this code around. I'm just starting with this and it's confusing :)\nIn particular, I tried a model from gpt4all (https://github.com/nomic-ai/gpt4all which curiously has deleted the source but is distributing binaries for a very limited set of operating systems only  - and the source which is apparently at https://github.com/zanussbaum/gpt4all.cpp/tree/master ). However, these models seem to use a file format that this llama.cpp doesn't like.\nWould it make sense to add support for this other file format? (I don't even know if it is just a small extension, or radically different; but the former seems more likely since the source clearly is derived from this repository).",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/950",
        "createdAt": "2023-04-13T19:14:39Z",
        "author": {
            "login": "Rhialto"
        }
    },
    {
        "title": "Extremely slow?",
        "bodyText": "Thanks for the help.\nSolution.\nI noticed that in the arguments it only was using 4 threads out of 20. So I increased it by doing something like -t 20 and it seems to be faster.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/861",
        "createdAt": "2023-04-09T02:00:31Z",
        "author": {
            "login": "viperwasp"
        }
    },
    {
        "title": "Super slow for 65B after mmap?",
        "bodyText": "Hi,\nHave anyone experienced a significant slow down on LLaMA 65B since mmap is deployed? Yea, it does load super fast but the token generation time has become like a few minutes / token on a 64GB M1 Max, while leaving 36GB of RAM totally untouched?\nOn the other hand, performance of 30B and below are fine though.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/939",
        "createdAt": "2023-04-13T08:06:44Z",
        "author": {
            "login": "edwios"
        }
    },
    {
        "title": "[Work Group] Add RLHF like ColosallChat on bigger dataset to achieve ChatGPT quality",
        "bodyText": "Link to ColosallChat\nAdd RLHF like ColosallChat on bigger dataset to achieve ChatGPT quality\n\nAlthough models in the GPT series, such as ChatGPT and GPT-4, are highly powerful, they are unlikely to be fully open-sourced. Fortunately, the open-source community has been working hard to address this.\nFor example, Meta has open-sourced the LLaMA model, which offers parameter sizes ranging from 7 billion to 65 billion. A 13 billion parameter model can outperform the 175 billion GPT-3 model on most benchmark tests. However, since it doesn\u2019t have an instruct tuning stage, its actual generated results are not satisfactory.\nStanford\u2019s Alpaca generates training data in a self-instructed manner by calling OpenAI\u2019s API. With only 7 billion parameters, this lightweight model can be fine-tuned at a fraction of the cost to achieve conversational performance similar to a very large language model like GPT-3.5 with 175 billion parameters.\nHowever, existing open-source solutions can only be considered as supervised fine-tuned models in the first stage of RLHF (Reinforcement Learning from Human Feedback), with subsequent alignment and fine-tuning stages not performed. Additionally, Alpaca\u2019s training dataset is limited to English, which to some extent restricts the model\u2019s performance.\nYet, the impressive effects of ChatGPT and GPT-4 are due to the introduction of RLHF into the training process, which increases the consistency of the generated content with human values.\n\nTraining Dataset Open Source\nColossalChat releases a bilingual dataset comprising approximately 100,000 Q&A pairs in both English and Chinese. The dataset was collected and cleaned from real-life question scenarios on social media platforms, serving as the seed dataset, and was expanded using self-instruct technology, and annotation costs were approximately $900. Compared to datasets generated by other self-instruct methods, this dataset contains more realistic and diverse seed data and encompasses a wider range of topics. The dataset is suitable for both fine-tuning and RLHF training. With the provision of high-quality data, ColossalChat can achieve better dialogue interactions and also support Chinese.\n\nRLHF Algorithm Replication\nThe RLHF algorithm replication involves three stages:\nIn RLHF-Stage1, supervised instruct fine-tuning is performed using the datasets mentioned earlier to fine-tune the model.\nIn RLHF-Stage2, a reward model is trained to assign corresponding scores by manually ranking different outputs for the same prompt, which then supervises the training of the reward model.\nIn RLHF-Stage3, the reinforcement learning algorithm is being used, which is the most complex part of the training process:\n\nIn the PPO part, ColossalChat follows a two-stage process: first, the make experience stage, which uses SFT (Supervised Fine-Tuning), Actor, RM (Reward Model), and Critic models to calculate generated experience and store it in the buffer. Then comes the parameter update stage, which calculates the policy loss and value loss using the experience.\nIn the PTX part, ColossalChat calculates the cross-entropy loss between the Actor\u2019s output response and the response part of the input corpus. This loss is used to add pre-training gradients to the PPO gradient to maintain the language model\u2019s original performance and prevent forgetting. Finally, the policy loss, value loss, and PTX loss are summed up for backpropagation and parameter update.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/943",
        "createdAt": "2023-04-03T18:02:06Z",
        "author": {
            "login": "alyxdow"
        }
    },
    {
        "title": "I can't understand how to run this line ./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2",
        "bodyText": "I called it like this\npython3 ./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\nLike this\n./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\nbut nothing works, please help me.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/923",
        "createdAt": "2023-04-12T20:27:36Z",
        "author": {
            "login": "SCHIZ0FRENIA"
        }
    },
    {
        "title": "found a buffer overflow",
        "bodyText": "Found this when reading the code\nin llama.cpp when n_dims > 2\n            int32_t nelements = 1;\n            int32_t ne[2] = { 1, 1 };\n            for (int i = 0; i < n_dims; ++i) {\n                fin.read(reinterpret_cast<char *>(&ne[i]), sizeof(ne[i]));\n                nelements *= ne[i];\n            }",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/819",
        "createdAt": "2023-04-06T19:05:09Z",
        "author": {
            "login": "iacore"
        }
    },
    {
        "title": "Few-shot instruction fails by randomly repeating similar content instead of finishing the task",
        "bodyText": "I'm experimenting a bit with LLaMa to make a bot that can help with making todolists and help you focus. Mostly aimed at people with executive function problems (like myself, hah). Nothing serious just yet, but I do see some potential here for a sort of personalized executive-functioning coach if I can get this to work well..\nI'm using langchain to do this. One task that I need accomplished is that once the user made a decision about how to change the todolist, the bot should update it.\nHere's the prompt I'm using:\ntemplate = \"\"\"\nPlease modify the given todolist with the given command. The new todolist should be an edited version of the original based on the given command.\n\nTodolist:\ntodolist is empty\nCommand: buy chicken\nOut:\n- [ ] buy chicken\n\nTodolist:\n- [ ] lift weights for 3x 5 minutes\n- [ ] buy chicken\n- [ ] take a walk\nCommand: file taxes\nOut:\n- [ ] lift weights for 3x 5 minutes\n- [ ] buy chicken\n- [ ] take a walk\n- [ ] buy taxes\n\nTodolist:\n- [ ] buy chicken\nCommand: buy chicken\nALREADY_EXISTS\n\nTodolist:\n- [ ] buy chicken\nCommand: I bought chicken\n- [x] buy chicken\n\nTodolist:\n{todolist}\nCommand: {new_tasks}\nOut:\n\"\"\"\n\nWhen using GPT-4, I can use an instruction-prompt which has very little context and performs very consistently.\nWith LLaMa, I'm struggling to get the right output with only the instructions -- which makes sense, since it's not finetuned --, so I thought few-shot would be the best alternative. I also read this prompt engineering post to get some ideas\nThe kind of output I'm getting doesn't make a lot of sense though, it's just making up more examples instead of actually performing the task\nHere's one test task I'm doing:\nPlease modify the given todolist with the given command. The new todolist should be an edited version of the original based on the given command.\n\nTodolist:\ntodolist is empty\nCommand: buy chicken\nOut:\n- [ ] buy chicken\n\nTodolist:\n- [ ] lift weights for 3x 5 minutes\n- [ ] buy chicken\n- [ ] take a walk\nCommand: file taxes\nOut:\n- [ ] lift weights for 3x 5 minutes\n- [ ] buy chicken\n- [ ] take a walk\n- [ ] buy taxes\n\nTodolist:\n- [ ] buy chicken\nCommand: buy chicken\nALREADY_EXISTS\n\nTodolist:\n- [ ] buy chicken\nCommand: I bought chicken\n- [x] buy chicken\n\nTodolist:\n- [ ] Vacuum living room\nCommand: Buy chicken, take a walk, file taxes\nOut:\n\nOutput is:\n- [ ] vacuum living room\n- [ ] buy chicken\n- [ ] take a walk\n- [ ] file taxes\n\nTodolist:\n- [ ] lift weights for 3x 5 minutes\n- [ ] buy chicken\n- [ ] take a walk\nCommand: file taxes\nOut:\n- [ ] lift weights for 3x 5 minutes\n- [ ] buy chicken\n- [ ] take a walk\n- [ ] file taxes\n\nTodolist:\n- [ ] vacuum living room\nCommand: I bought chicken, take a walk, file taxes\nOut:\n- [ ] vacuum living room\n- [x] buy chicken\n- [ ] take a walk\n- [ ] file taxes\n\nTodolist:\n- [ ] vacuum living room\nCommand: I bought chicken, take a walk, file taxes\nOut:\n- [ ] vacuum living room\n- [x] buy chicken\n- [ ] take a walk\n- [ ] file taxes\n\nTodolist:\n- [ ] vacuum living room\n\nIt doesn't really matter if I pick a large or small model, they all have the same problem. The annoying part is that what it outputs in the beginning is actually correct and desired, it modified the initial todolist correctly with the new tasks. It's just that the rest of the output is undesired.\nCan anyone guide me in the right direction? Thanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/886",
        "createdAt": "2023-04-10T22:08:24Z",
        "author": {
            "login": "Azeirah"
        }
    },
    {
        "title": "We could use clang format to keep consistent formatting over the project",
        "bodyText": "We could use the clang-format to generate a .clang-format file with formatting configurations and apply it on the CI, that will keep all the C/C++ code of the project with a consistent looking.\nCang format has some predefined settings that can be customized.\nhttps://clang.llvm.org/docs/ClangFormat.html\nAnd the .clang-format file is normally used by the clangd LSP to run the format command.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/868",
        "createdAt": "2023-04-10T00:10:06Z",
        "author": {
            "login": "Fabio3rs"
        }
    },
    {
        "title": "Llama Runs on internal storage instead of RAM. Anyway to optimize this on Mac air?",
        "bodyText": "This is very much related to the thread asking why llama 30B uses only 5.8gb of ram. After seeing that thread, I got excited to see how 30B llama model would run on my poor Mac air m1 with 8gb of ram. Well it works, but excruciatingly slow. It takes about less a minute to generate a single token. I then checked what was used with activity monitor and you guessed it, it is running with 3.9 ram but reading the disk in ~900 mb/sec. I run the same on another computer, similar low usage of ram is used and response is much quicker (2 seconds per token) with 2500 mb/sec reading speed. It seems that disk reading speed correlates positively to the responsiveness of the 30B model. I think the disk reading speed of Mac air m1 can reach about 2000 mb/sec, which may make the 30B actually viable on this 8gb ram machine if the software can utilize the max disk reading speed of Mac air m1.\nI am not knowledge in this field. Is what I am proposing manageable?\n(On Mac Air M1 8gb):",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/833",
        "createdAt": "2023-04-07T13:32:45Z",
        "author": {
            "login": "KiwiBites"
        }
    },
    {
        "title": "possibility to modify source code to integer new function ?",
        "bodyText": "Hello, I am a beginner with llama.cpp. I'm sorry, because I'm going to ask you a question that may be silly.\nIs it possible to modify the source code of llama.cpp to add functions to the chat.\nI would like for example that at the beginning of a conversation the bot asks me for my email address and then saves it in a file or in its memory. It could then send me its answers by email.\nthanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/866",
        "createdAt": "2023-04-09T22:17:29Z",
        "author": {
            "login": "gandolfi974"
        }
    },
    {
        "title": "I5 13600k or RTX 3060 12gb?",
        "bodyText": "Hello Community\nI want to build a computer which will run llama.cpp or text generation web ui.\nWhich should I get? Each config is about the same price.\nShould I get the 13600k and no gpu (But I can install one in the future if I have money) or a \"bad\" cpu and a rtx 3060 12gb?\nWhich should I get / is faster?\nThank you in advice.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/854",
        "createdAt": "2023-04-08T17:33:12Z",
        "author": {
            "login": "CyberTimon"
        }
    },
    {
        "title": "A HuggingFace model handler would be elite",
        "bodyText": "You know what would be amazing, would be Python bindings for this. To expose it as a standard HF model object that could be instantiated just like a CUDA placed model. That would open the door for infinite new applications, really.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/845",
        "createdAt": "2023-04-08T06:31:23Z",
        "author": {
            "login": "pablogranolabar"
        }
    },
    {
        "title": "Model loses its mind in interactive mode after ~4096 tokens.",
        "bodyText": "I've been playing with all the llama/alpaca models in interactive mode as a chatbot and after around 800 words the output freezes for ~10 minutes before continuing. However, when it does it's as if the starting prompt and everything else before never existed. The model will even start hallucinating input from my end of the conversation, ignoring the reverse prompt.\nI've played with every parameter and setting but cannot seem to fix this behavior. I assumed llama.cpp used a rolling window with the option to keep the first N tokens. Maybe it's just a current limitation but I've had a hard time finding others with a similar issue.  (or maybe I'm not looking for the right keywords)",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/645",
        "createdAt": "2023-03-31T03:10:39Z",
        "author": {
            "login": "GeorgeUCB"
        }
    },
    {
        "title": "How do I get input embeddings?",
        "bodyText": "I am trying to output just the sentence embedding for a given input, instead of any new generated text. I think this should be rather straightforward but figured someone more familiar with the codebase could help me.\nI just want to return the sentence embedding vector and stop execution for a given input.\nI am almost sure the place where I want to make the embedding is right after norm but before lm_head, and I think they will be in inpL if I run\nggml_build_forward_expand(&gf, inpL);\nggml_graph_compute       (ctx0, &gf);\n\nHowever I am confused by the struct and not sure how to get the sentence embedding itself. I understand it should be some index of ggml_get_data(inpL), but don't get which index, and that is why I come to you. Would anyone lend me a hand?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/1013",
        "createdAt": "2023-03-17T04:59:12Z",
        "author": {
            "login": "StrikingLoo"
        }
    },
    {
        "title": "Question about sentence transformers weights",
        "bodyText": "So is it possible to port the weights of other HuggingFace models such as sentence-transformers/all-MiniLM-L12-v2, to example generate embeddings for semantic textual similarity using llama.cpp? Ggml seems to indicate all of that work is happening here now, if that is any type of capability using the quantization code on other model architectures?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/860",
        "createdAt": "2023-04-09T01:21:50Z",
        "author": {
            "login": "pablogranolabar"
        }
    },
    {
        "title": "How to send the AI's output to Mac Say using only llama.cpp?",
        "bodyText": "I would like to have an interactive chat session using llama.cpp on a Mac, where only the output from AI is piped to the Mac \"say\" command, which then uses the voice selected in Accessibility settings (e.g. English Ireland > Voice 2). (I want to type my input)\n@ggerganov has an example of this, used in conjunction with whisper.cpp, here:\nhttps://twitter.com/ggerganov/status/1640035474205995011\nSee also\nggerganov/whisper.cpp@master...talk.llama-coreml#diff-919c16cc39dd4e7769d99b1a2866078795836b3a94285a574f865d13199de72e\nCan anyone suggest the best way to output the AI's to invoke \"say\" in a bash script from main, for the AI output only, without needing whisper.cpp?\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/856",
        "createdAt": "2023-04-08T22:55:06Z",
        "author": {
            "login": "classicjazz"
        }
    },
    {
        "title": "Have you seen Auto-GPT?",
        "bodyText": "https://github.com/Torantulino/Auto-GPT\nThere is something similar to our prompt ./prompts/reason-act.txt that can be used for this! Though the speed may be lower, but it will run locally, we can do something similar! Imagine doing multiple of these prompts, that \"main\" GPT will run.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/855",
        "createdAt": "2023-04-08T22:02:25Z",
        "author": {
            "login": "ivanstepanovftw"
        }
    },
    {
        "title": "Can we run data extraction?",
        "bodyText": "Hi community,\nWe all seen the boom of inference models and I seen recently those models used or derivates to data extraction.\nI'm far away to be expert on the subject and I did not found a topic on this subject so I would be happy to get some helps on the feasibility and potentially how would you do so.\nI seen a example of data extraction where we provides a huge document of text, then we provide a query like \"What are the places described here?\" or \"When this document become effective?\" and the return would formulate an answer based on the document content providing a score and a location of evidence about the question.\nI am wondering if some models in combination with LLaMa.cpp would be able to achieve this kind of goals. Like a search in a kind of chat way. Would love to know if there is some solution around there or how could we do this kind of solution.\nThanks in advance.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/818",
        "createdAt": "2023-04-06T17:18:45Z",
        "author": {
            "login": "Hideman85"
        }
    },
    {
        "title": "Slow load time?",
        "bodyText": "I have the following timings for a 30B LLaMA model:\nllama_print_timings:        load time = 62270.78 ms\nllama_print_timings:      sample time =   681.50 ms /   203 runs   (    3.36 ms per run)\nllama_print_timings: prompt eval time = 60647.60 ms /   323 tokens (  187.76 ms per token)\nllama_print_timings:        eval time = 46631.52 ms /   202 runs   (  230.85 ms per run)\nllama_print_timings:       total time = 109586.98 ms\n\nAlthough the time / token was pretty ok, the load time was pretty significant. Do you have similar timings? I have this ran on a M1 Max with 64GB RAM.\nWhy I asked is because I have not seen such long load time from other discussions and I am wondering if there is something I have missed.\nI am using commit d2beca9 if that helps but this long loading time has been here for a while.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/815",
        "createdAt": "2023-04-06T15:26:22Z",
        "author": {
            "login": "edwios"
        }
    },
    {
        "title": "This repository is confusing, is it for windows?",
        "bodyText": "from \"The main goal is to run the model using 4-bit quantization on a MacBook\" means I'm guessing it's for iOS and I have no business being in this repo.\nThere's a mix of windows and apple commands and I can't tell which.\n`# obtain the original LLaMA model weights and place them in ./models\nls ./models\n65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model`\nDoes the ls command even work on windows?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/793",
        "createdAt": "2023-04-05T21:19:25Z",
        "author": {
            "login": "ninjasaid2k"
        }
    },
    {
        "title": "Specifications for how to integrate services in AI models",
        "bodyText": "Please join discussion.\nHere is a repository I created for specs, for now it's empty. Discussion is welcome. https://github.com/openservices4ai/spec\nWhat is an AI-integrated sevice\nAs AI models trained on a fixed set of data and cannot learn something new in real-time while serving, services integrated in AI models help provide real-time information and services. For example, a chatbot model may be integrated with a todo list service to help users add/remove todos, or create a timer.\nWhy an unified specification for AI-integrated services matters\nFirst, it can help improve compatibility between different AI platforms, making it easier for developers to integrate various plugins into their AIs. This can save time and resources for developers who would otherwise have to manually modify each plugin to work with their chatbot.\nSecond, an unified specification can help ensure that services adhere to certain standards for functionality, security, and user privacy. This can help promote trust and confidence among users who interact with chatbots using these services.\nFinally, it can also help promote innovation and collaboration among developers working on AI chatbots. By providing a common framework for plugin development, developers can more easily share their work and collaborate on new features and functionalities, ultimately leading to better and more sophisticated chatbots for users.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/810",
        "createdAt": "2023-04-06T08:54:29Z",
        "author": {
            "login": "PengXing"
        }
    },
    {
        "title": "fixed issue",
        "bodyText": "Fixed...",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/756",
        "createdAt": "2023-04-04T03:47:39Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "can llama do other task except text-generate,like translate",
        "bodyText": "thanks for your work, it's very helpful!\ncan llama do other jod:\n./main -m ./models/7B/ggml-model-q4_0.bin -p \"Translate English to Frence: it's a nic day!\" -n 512",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/761",
        "createdAt": "2023-04-04T03:12:51Z",
        "author": {
            "login": "xyx361100238"
        }
    },
    {
        "title": "How do I run a model? I'm getting an error",
        "bodyText": "I am getting this error\nC:\\Users\\user\\Downloads\\llama.cpp>C:\\Users\\user\\Downloads\\llama.cpp\\build\\bin\\Release\\main.exe -m models\\13B\\gpt4-x-alpaca-13b-native-ggml-model-q4_0 -n 128\nmain: seed = 1680741859\nllama_model_load: loading model from 'models\\13B\\gpt4-x-alpaca-13b-native-ggml-model-q4_0' - please wait ...\nllama_model_load: failed to open 'models\\13B\\gpt4-x-alpaca-13b-native-ggml-model-q4_0'\nllama_init_from_file: failed to load model\nmain: error: failed to load model 'models\\13B\\gpt4-x-alpaca-13b-native-ggml-model-q4_0'\n\nC:\\Users\\user\\Downloads\\llama.cpp>\n\nhow do I run it without this error?\nthis is my 13B folder",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/798",
        "createdAt": "2023-04-06T01:04:22Z",
        "author": {
            "login": "ninjasaid2k"
        }
    },
    {
        "title": "\"Llamify\" ML models automatically?",
        "bodyText": "An interesting read/concept:\n\"April 5, 2023: The first technology preview of Flow Coder\u2019s ML to source code compiler is now open for model submissions. Click the button below and submit your ML model through the form. Within 48 hours, you will receive a dependency-free source code file with a function that executes the model inference, or an email with an estimation for when you can expect to receive it.\u00a8\n\"Flow Coder takes the pain out of integrating a machine learning model into your software or firmware project. It converts trained machine learning models straight into dependency-free source code for many common programming languages, thereby eliminating the complexity and overhead of inference libraries in deployment projects. The compiler can be invoked online, using a desktop tool or as a single executable command-line tool, so it\u2019s easy to integrate as a pre-build step in a build pipeline.\nThe full product is expected to be available late 2023 or early 2024.\n\"",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/794",
        "createdAt": "2023-04-05T21:26:37Z",
        "author": {
            "login": "Topping1"
        }
    },
    {
        "title": "Ruby Bindings",
        "bodyText": "Hi all! I'm a Rubyist who dabbles in ML when I can, and watching this project's progress has been extremely exciting! Given the well-crafted C API, I wanted to see if I could get it into Ruby by way of Ruby's C extensions.  Sure enough, it was pretty straightforward to cannibalize main.cpp into a Ruby extension, and run token generation in Ruby on my Arch desktop.\nI've published my extension on my GitHub, and wanted to share it with the community here. Feedback appreciated. Happy hacking!\nhttps://github.com/jtp184/llamaste",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/792",
        "createdAt": "2023-04-05T20:21:05Z",
        "author": {
            "login": "jtp184"
        }
    },
    {
        "title": "Something strange with any non-english prompts with alpaca/gpt4all/vicuna",
        "bodyText": "When you try to chat with these on not english language, the answer will hallucinate and on english, but if you use file as prompt, everything works fine(picrelated). This is strange.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/760",
        "createdAt": "2023-04-04T06:10:41Z",
        "author": {
            "login": "4eJIoBek1"
        }
    },
    {
        "title": "Future directions regarding optimization",
        "bodyText": "I'm kinda curious how much more we can squeeze out of llama on cpu, what other directions can we take.\nJust casual topic for brainstorming",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/718",
        "createdAt": "2023-04-02T19:39:08Z",
        "author": {
            "login": "x02Sylvie"
        }
    },
    {
        "title": "Speeding up 30-65B models",
        "bodyText": "Running the 30B llama model 4-bit quantified with about 75% ram utilisation (confirming its not a swap overhead issue), tokens generate at a rate of about 700-800ms with my CPU being maxed out with threads maxed as well, which is not terrible by all means but could be better.\nIs there any way as of now to improving the performance?\nAnd speaking about the future, is there any hope of GPU utilisation during the autocompletion process? I have a powerhouse of a GPU sitting idly watching the CPU do all the work \ud83d\ude01.\nPS -  I don't want to sit around and let others do the work and want to try contribute in the future so watch out for that!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/394",
        "createdAt": "2023-03-22T14:36:01Z",
        "author": {
            "login": "SpeedyCraftah"
        }
    },
    {
        "title": "Quantizing Larger Models",
        "bodyText": "Hi, I'm confused about how to quantize larger models with the recent changes.  I think it would be helpful to include a walkthrough like the one for 7B models in the readme (or elsewhere), since the steps seem to be a bit different when dealing with multiple .bin files.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/721",
        "createdAt": "2023-04-02T20:28:13Z",
        "author": {
            "login": "4t0m"
        }
    },
    {
        "title": "Check point a prompt state, is it possible?",
        "bodyText": "I noticed that applictions based on llama uses different long prompts to pre-condition the model.   With 7B and 13B model weights the model usually takes a while to read the prompt until process user inputs.   So there is a waiting time before the first response.  For example, when I use chat-13B.sh, it takes about 1 mintue before the first response from my input, yet response time after that is fairly fast.\nIf I understand correctly the long prompt can put the model in certain internal state, and these internal state make the model process user input as the way user expected.   After the model reach this internal state, most of the original prompt does not need to be read and process again.\nIs it theortical possible to store this internal state on a disk file, then read it back in a new session, instead of process the initial prompt text again?   This could save a lot of initial waiting time, as long as the energy used to recompute the internal state.\nI know there might be a lot of engineering implications, but just want to know whether this is a feasible idea or there are other blocking things.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/671",
        "createdAt": "2023-04-01T05:46:52Z",
        "author": {
            "login": "delock"
        }
    },
    {
        "title": "convert-pth-to-ggml.py has become very slow",
        "bodyText": "A few weeks ago, convert-pth-to-ggml.py ran reasonably fast for me (still took a while to convert the larger models). Now (seemingly after the format changes to allow mmap) it is very slow. It will likely take over an hour to convert the 13B model. I'm running it in an anaconda environment on Windows using miniconda3. I can provide additional specs or debugging information if necessary.\nEdit: looking at cProfiler trace, it looks like the file.seek() method is performing very poorly. Maybe this is because I'm on Windows and it performs better on linux?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/663",
        "createdAt": "2023-03-31T22:28:03Z",
        "author": {
            "login": "DJtheRedstoner"
        }
    },
    {
        "title": "Train RLHF using CarperAI Trlx",
        "bodyText": "\ud83d\udce2 Proposal: Train RLHF using CarperAI Trlx \ud83e\udd16\nTrix\nWe propose to train a Reinforcement Learning from Human Feedback (RLHF) model using CarperAI Trlx, a distributed training framework designed to fine-tune large language models with reinforcement learning. Our goal is to improve the conversational abilities of language models and create a chatbot that can better engage with humans.\n\ud83d\ude80 Methods:\nWe will train an initial model using supervised fine-tuning, where human AI trainers will provide conversations in which they play both sides- the user and an AI assistant. We will give the trainers access to model-written suggestions to help them compose their responses. We will mix this new dialogue dataset with the InstructGPT dataset, which we will transform into a dialogue format.\nTo create a reward model for reinforcement learning, we will collect comparison data, which will consist of two or more model responses ranked by quality. To collect this data, we will take conversations that AI trainers had with the chatbot. We will randomly select a model-written message, sample several alternative completions, and have AI trainers rank them. Using these reward models, we will fine-tune the model using Proximal Policy Optimization. We will perform several iterations of this process.\n\ud83d\udcbd Datasets:\nhttps://huggingface.co/datasets/Anthropic/hh-rlhf\nhttps://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw\nhttps://www.surgehq.ai/datasets/instructgpt-style-dataset",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/687",
        "createdAt": "2023-04-01T16:31:51Z",
        "author": {
            "login": "alyxdow"
        }
    },
    {
        "title": "generic GPGPU support",
        "bodyText": "There has been great repo https://github.com/Const-me/Whisper which let the Whisper.cpp on GPU faster than Pytorch+CUDA, it should be great the similar approach can be applied for llamp.cpp.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/657",
        "createdAt": "2023-03-31T18:55:04Z",
        "author": {
            "login": "JianbangZ"
        }
    },
    {
        "title": "How to setup OpenBlas on windows?",
        "bodyText": "Supposedly there's performance uplift with linking of OpenBLAS\nAny hints how to set it up on windows? I have no issues on linux but windows cmake does not seem to want to cooperate with me at the moment in case of OpenBLAS\n\nThanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/625",
        "createdAt": "2023-03-30T16:01:01Z",
        "author": {
            "login": "x02Sylvie"
        }
    },
    {
        "title": "~2x perf improvement on Apple Silicon by changing state_shared.has_work access from atomic to mutex/conditional",
        "bodyText": "I profiled on a latest Mac Book Pro machine and found that significantly more time is spent in atomic checks for state_shared.has_work in while loops than doing actual work in matrix multiply.\nSo I changed busy waits like:\npthread_mutex_lock(&state->shared->mutex);\n   while (state->shared->has_work) {\n     pthread_cond_wait(&state->shared->cond, &state->shared->mutex);\n// unlock\n\nand setting has_work to\npthread_mutex_lock(&state_shared.mutex);\nstate_shared.has_work = true;\npthread_cond_broadcast(&state_shared.cond);\npthread_mutex_unlock(&state_shared.mutex);\n\n\nGot a nice 2x speedup in time/token.\nI can't post a patch/pull request because everything I do in spare time still belongs to my employer, but the change is trivial as described above. Probably won't provide much benefit (if any) for other platforms though.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/616",
        "createdAt": "2023-03-30T04:01:52Z",
        "author": {
            "login": "izard"
        }
    },
    {
        "title": "Adding our own data to the PTH file or to the gglm BIN file",
        "bodyText": "I just asked this to chatGPT:\nhaving an existing pytorch pth file that uses 32 layers, can we add the self attention values from a corpus of text with multiple sentences to it ?\nchatGPT answered yes and provided the source code. Does that makes sense ? Is it a way to add our own data to the provided models ? could we do the same using the gglm BIN file ?\nthanks",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/636",
        "createdAt": "2023-03-30T21:31:06Z",
        "author": {
            "login": "FiveTechSoft"
        }
    },
    {
        "title": "Work with large models with only 16GB RAM",
        "bodyText": "Hello \ud83d\udc4b!\nI'm using a laptop, with 16GB of RAM and GNU/Linux.\nWhen I wanted to convert the 13B and up models to ggml FP16 format, RAM got stuck... Obviously.\nSo, because I wanted to do it without paying anything in the cloud... I ended up using SWAP.\n\nI used SWAP to extend the virtual memory beyond the installed physical memory (RAM). We can use our HDD/SSD/NVMe... for this.\nIs not that fast as RAM but does the job! \ud83d\ude42\nI used my self a 40GB swap file. I didn't use swap partition. Here is a quick tutorial that can guide you to do so:\nhttps://wiki.archlinux.org/title/Swap#Swap_file\nI think this is possible to do in MacOS as well \ud83e\udd17.\nImage example working with 13B model:",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/631",
        "createdAt": "2023-03-30T18:20:57Z",
        "author": {
            "login": "jjrbfi"
        }
    },
    {
        "title": "Idea about Web integration/Articles analisys",
        "bodyText": "Is it possible to make bridge to web for something unknown to model? (ChatGPT introduced plugins to search web, etc)\nOr at least for model to read article/book and answer questions about it?\nWhat I understood how ChatPDF works:\n\nIndexing document like:\nword -> page, line\nPut index into conversation context\nAsk GPT to search inside index.\nJump into target page and provide paragraph into conversation context.\nMaybe several times.\nAnswer user.\n\nSo no need for GPT to read whole document.\nProbably can be implemented with LLaMa",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/470",
        "createdAt": "2023-03-24T15:50:06Z",
        "author": {
            "login": "xor2003"
        }
    },
    {
        "title": "Can LLaMa learn from interactions?",
        "bodyText": "Can LLaMa learn from interactions? ive talked with it a lot and it doesn't seem like it remembers our previous interactions\nI could modify the chat_with_bob.txt and add all of our interactions but that would increase the load time substantially",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/541",
        "createdAt": "2023-03-26T22:33:46Z",
        "author": {
            "login": "nazthelizard122"
        }
    },
    {
        "title": "Funny answers and prompts:",
        "bodyText": "My llama looks like he loves aliens \ud83d\udc7d\ud83d\udc7d\ud83d\udc7d\ud83d\udc7d",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/520",
        "createdAt": "2023-03-26T10:34:56Z",
        "author": {
            "login": "Shaunerie"
        }
    },
    {
        "title": "Where do i obtain model file itself?",
        "bodyText": "In manual it is stated following:\nDownload the weights via any of the links in \"Get started\" above, and save the file as ggml-alpaca-7b-q4.bin in the main Alpaca directory.\nWhere can i get them?\nthanks in advance",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/610",
        "createdAt": "2023-03-29T22:27:48Z",
        "author": {
            "login": "ahreenah"
        }
    },
    {
        "title": "How do you prevent similar replies to unrelated questions in few-shot?",
        "bodyText": "Hey!\nI keep on observing this behavior:\n\nWhat is the best or most common way to prevent this from happening? Higher temperature? Repeat penalty? Both? Or a longer prompt (mine already is pretty long tho)?\nThanks in advance\nNiansa",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/606",
        "createdAt": "2023-03-29T19:52:54Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Chinese LLaMA and Alpaca available, seamlessly work with llama.cpp",
        "bodyText": "Greetings,\nThank you for providing such a wonderful tool for LLM deployment on CPU.\nWe released Chinese LLaMA and Alpaca model (with Chinese vocabulary extension!), trained with Alpaca-LoRA.\nWe managed to successfully serve our quantized model with llama.cpp.\nGive it a try at: https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/README_EN.md",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/567",
        "createdAt": "2023-03-28T10:20:35Z",
        "author": {
            "login": "ymcui"
        }
    },
    {
        "title": "in transcript mode, did llama care you motivated in any circumstances?",
        "bodyText": "I am not saying anything like hello, or how are you...\nIt's more like \"can you talk to me?  \"\nIndeed I haven't feel that astonished even in some case the model will use the words kill.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/597",
        "createdAt": "2023-03-29T14:16:32Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "What does the parameter means and their impact of the output speed ?",
        "bodyText": "Hello there,\nSo there are plenty parameters, and for a lot of them i have no clue about what they are used for or if they can help me to have better answer time. I would like to have a realistic chatbot, that doesn't take 2-5 min to answer a single question, but still doesn't sound like a robot. It would be awesome to have a place where we could resume those things for the newcomer like myself, so maybe you could help me know more or to correct my mistakes :\ntemp : (This one doesn't seem to impact on the speed, since it just change how much the AI will stay on the topic)\ntop_k : (This is the number of probable next words, to create a pool of words to choose from)\ntop_p: (This is by how much a word should be probable to be picked)\nrepeat_last_n: (I have no idea)\nrepeat_penalty: (Seems like the higher the less the AI will repeat itself)\nn_ctx: (I don't know what this exactly is and how much it helps to speed stuff)\nn_batch: (That's the amount of character the AI can compute in the same round, it seems like you should keep it as low as possible for your needs)\nn_predict: (I believe that's how many character the AI think upfront before talking, if it's lower than the sentence it may have to compute another round or talk nonsense)\nn_keep: (I have no idea)\nCould someone please point me toward what repeat_last_n, n_keep and n_ctx do ? And tell me what parameter really play in the speed of the answer ?\nSo far i'm using that without too much idea of what i'm doing with extremely low performance :\nsampling: temp = 0.700000, top_k = 40, top_p = 0.500000, repeat_last_n = 256, repeat_penalty = 1.176470\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = 2048, n_keep = 0\n\nThank you !",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/559",
        "createdAt": "2023-03-27T18:42:12Z",
        "author": {
            "login": "Sh3yn3"
        }
    },
    {
        "title": "Are GPTQ's group-size, --true-sequential and act-order supported by llama.cpp?",
        "bodyText": "As mentioned here https://github.com/qwopqwop200/GPTQ-for-LLaMa\nThese three can greatly improve results, so im wondering",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/580",
        "createdAt": "2023-03-28T15:50:27Z",
        "author": {
            "login": "x02Sylvie"
        }
    },
    {
        "title": "Sell app that work with llama",
        "bodyText": "Weird question maybe, but if i use this ai in one of my coding project cani still sell it ?\nOr do i nned to create my own ai for this ?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/472",
        "createdAt": "2023-03-24T19:46:23Z",
        "author": {
            "login": "Syhkii"
        }
    },
    {
        "title": "Trying to understand why starting with a long prompt is so much slower.",
        "bodyText": "Apologize if this is an obvious question.\nI've used other text inference frameworks before such as huggingface's transformer generate(), and in those cases, the generation time was always independent of the initial prompt length. Only the quantity of generated tokens mattered, regardless of context length. That is to say, generating 5 tokens using a 300 word prompt takes about the same time as generating 5 tokens with 3 word prompt.\nHowever when using Llama.cpp, this does not seem to be the case - the generation time scales almost linearly with initial prompt length. What's the difference?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/229",
        "createdAt": "2023-03-17T08:00:37Z",
        "author": {
            "login": "LostRuins"
        }
    },
    {
        "title": "[Info] We built a mobile \ud83d\udcf1 AND WINDOWS \ud83e\ude9f  interactive version using flutter and it runs super well on a simple Oneplus 7 with 8GB ram.",
        "bodyText": "Have fun\nHi, it is not a real issue, but i want to share here that we made a running app with interactive mode on mobiles using your repo.\nYou can find our repo here : https://github.com/Bip-Rep/sherpa\nUnfortunately it doesnt have your latest commit because there is an error during runtime but we made a fork here https://github.com/Bip-Rep/llama.cpp You need to be on the \"for_mobile\" branch to build the libraries. We are using an older working commit on this branch.\nWe translated the main functions of llama.cpp in dart.\nWorking demo\n\nClick on the image to view the video on YouTube.\nHope this helps.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/568",
        "createdAt": "2023-03-27T18:46:16Z",
        "author": {
            "login": "ThibautLEAUX"
        }
    },
    {
        "title": "I find greedy sampling works the best for instruct mode",
        "bodyText": "If you use --top_p 0.0 --top_k 1 then the sampler will always pick the most likely next token. For the 7b alpaca model I have, this works extremely well and rarely gets caught in loops.\nAs an example, here's a prompt:\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a poem about a frog who learns so much about steel export taxation policy that he decides to write the poem that you're reading at this moment!\n\n### Response:\n\nAnd the output:\nA frog jumps in the river,\nHis thoughts far away from the mire.\nHe dreams of a life beyond,\nWhere his knowledge can reach its zenith.\n\nSo he sets out on a grand journey,\nTo explore all that he can.\nFrom steel export taxation policy,\nHe learns much more than he had planned.\n\nFor days and weeks he studies hard,\nHis mind filled with facts galore.\nNow his knowledge is quite vast,\nSo he pens this poem at last. \n\nAlthough the network still hallucinates sometimes, I find that greedy sampling makes it less common and also it sticks to the prompt better. This is especially useful if you have a specific task, like if you want it to extract information from some unstructured input.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/548",
        "createdAt": "2023-03-27T06:50:00Z",
        "author": {
            "login": "blackle"
        }
    },
    {
        "title": "Support for In context learning",
        "bodyText": "Hello,\nI would like to know if this port in cpp of llama does support in context learning.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/550",
        "createdAt": "2023-03-27T07:22:23Z",
        "author": {
            "login": "0x090909"
        }
    },
    {
        "title": "How big are the models?",
        "bodyText": "Does anyone know what the disk size is? I'm downloading the model 7B and it's already at 12GB. I think I'll stop, to migrate to another disk because mine(used just for docker tests) is full.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/256",
        "createdAt": "2023-03-18T07:26:03Z",
        "author": {
            "login": "MichelDiz"
        }
    },
    {
        "title": "--help",
        "bodyText": "Hi,\nCan you add more explanations and examples ?\noptions:\n-h, --help            show this help message and exit\n-i, --interactive     run in interactive mode\n--interactive-first   run in interactive mode and wait for input right away\n-ins, --instruct      run in instruction mode (use with Alpaca models)\n-r PROMPT, --reverse-prompt PROMPT\nrun in interactive mode and poll user input upon seeing PROMPT (can be\nspecified more than once for multiple prompts).\n--color               colorise output to distinguish prompt and user input from generations\n-s SEED, --seed SEED  RNG seed (default: -1, use random seed for <= 0)\n-t N, --threads N     number of threads to use during computation (default: 4)\n-p PROMPT, --prompt PROMPT\nprompt to start generation with (default: empty)\n--random-prompt       start with a randomized prompt.\n--in-prefix STRING    string to prefix user inputs with (default: empty)\n-f FNAME, --file FNAME\nprompt file to start generation.\n-n N, --n_predict N   number of tokens to predict (default: 128, -1 - infinity)\n--top_k N             top-k sampling (default: 40)\n--top_p N             top-p sampling (default: 0.9)\n--repeat_last_n N     last n tokens to consider for penalize (default: 64)\n--repeat_penalty N    penalize repeat sequence of tokens (default: 1.1)\n-c N, --ctx_size N    size of the prompt context (default: 512)\n--ignore-eos          ignore end of stream token and continue generating\n--memory_f32          use f32 instead of f16 for memory key+value\n--temp N              temperature (default: 0.8)\n--n_parts N           number of model parts (default: -1 = determine from dimensions)\n-b N, --batch_size N  batch size for prompt processing (default: 8)\n--perplexity          compute perplexity over the prompt\n--keep                number of tokens to keep from the initial prompt\n--mlock               force system to keep model in RAM rather than swapping or compressing\n--mtest               compute maximum memory usage\n--verbose-prompt      print prompt before generation\n-m FNAME, --model FNAME\nmodel path (default: ./models/7B/ggml-model-q4_0.bin)\nthank you",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/544",
        "createdAt": "2023-03-27T01:09:13Z",
        "author": {
            "login": "fasca"
        }
    },
    {
        "title": "just saw a reflexion paper",
        "bodyText": "Just saw this, quit good maybe?\n_Reflexion: an autonomous agent with dynamic memory and self-reflection\nhttps://arxiv.org/abs/2303.11366_",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/517",
        "createdAt": "2023-03-26T04:30:58Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "What may I be doing wrong with the C API?",
        "bodyText": "Hey, I have written this C++ class:\nclass LLM {\n    struct {\n        std::string model = \"7B-ggml-model-quant.bin\";\n\n        int32_t seed; // RNG seed\n        int32_t n_threads = static_cast<int32_t>(std::thread::hardware_concurrency()) / 4;\n        int32_t n_ctx = 2024; // Context size\n        int32_t n_batch = 8; // Batch size, unused for now\n\n        int32_t top_k = 40;\n        float   top_p = 0.5f;\n        float   temp  = 0.72f;\n    } params;\n\n    struct State {\n        std::string prompt;\n        std::vector<llama_token> embd;\n        int n_ctx;\n        std::string last_result;\n    } state;\n\n    llama_context *ctx = nullptr;\n    std::mutex lock;\n    \n    void init() {\n        // Get llama parameters\n        auto lparams = llama_context_default_params();\n        lparams.seed = params.seed;\n        lparams.n_ctx = 2024;\n\n        // Create context\n        ctx = llama_init_from_file(params.model.c_str(), lparams);\n        if (!ctx) {\n            throw Exception(\"Failed to initialize llama from file\");\n        }\n\n        // Initialize some variables\n        state.n_ctx = llama_n_ctx(ctx);\n    }\n\npublic:\n    struct Exception : public std::runtime_error {\n        using std::runtime_error::runtime_error;\n    };\n    struct ContextLengthException : public Exception {\n        ContextLengthException() : Exception(\"Max. context length exceeded\") {}\n    };\n\n\n    LLM(int32_t seed = 0) {\n        // Set random seed\n        params.seed = seed?seed:time(NULL);\n\n        // Initialize llama\n        init();\n    }\n    ~LLM() {\n        if (ctx) llama_free(ctx);\n    }\n\n    void append(std::string prompt) {\n        std::scoped_lock L(lock);\n\n        // Check if prompt was empty\n        const bool was_empty = state.prompt.empty();\n\n        // Append to current prompt\n        state.prompt.append(prompt);\n\n        // Resize buffer for tokens\n        const auto old_token_count = state.embd.size();\n        state.embd.resize(old_token_count+state.prompt.size()+1);\n\n        // Run tokenizer\n        const auto token_count = llama_tokenize(ctx, prompt.data(), state.embd.data()+old_token_count, state.embd.size()-old_token_count, was_empty);\n        state.embd.resize(old_token_count+token_count);\n\n        // Make sure limit is far from being hit\n        if (state.embd.size() > state.n_ctx-6) {\n            // Yup. *this MUST be decomposed now.\n            throw ContextLengthException();\n        }\n\n        // Evaluate new tokens\n        // TODO: Larger batch size\n        std::cout << \"Context size: \" << old_token_count << '+' << token_count << '=' << state.embd.size() << '/' << state.n_ctx << std::endl;\n        for (int it = old_token_count; it != state.embd.size(); it++) {\n            std::cout << llama_token_to_str(ctx, state.embd.data()[it]) << std::flush;\n            llama_eval(ctx, state.embd.data()+it, 1, it, params.n_threads);\n        }\n        std::cout << std::endl;\n    }\n\n    std::string run(std::string_view end) {\n        std::scoped_lock L(lock);\n        std::string fres;\n\n        // Loop until done\n        bool abort = false;\n        while (!abort && !fres.ends_with(end)) {\n            // Sample top p and top k\n            const auto id = llama_sample_top_p_top_k(ctx, nullptr, 0, params.top_k, params.top_p, params.temp, 1.0f);\n\n            // Add token\n            state.embd.push_back(id);\n\n            // Get token as string\n            const auto str = llama_token_to_str(ctx, id);\n\n            // Debug\n            std::cout << str << std::flush;\n\n            // Append string to function result\n            fres.append(str);\n\n            // Evaluate token\n            //  TODO: Respect batch size\n            llama_eval(ctx, state.embd.data()+state.embd.size()-1, 1, state.embd.size()-1, params.n_threads);\n        }\n\n        // Create final string\n        state.prompt.append(fres);\n        fres = std::string(fres.data(), fres.size()-end.size());\n\n        // Return final string\n        return fres;\n    }\n};\nAnd the results I am getting are good... until they aren't. It starts repeating output very frequently, which it doesn't in the main example with the same settings.\nAny ideas? What may I be doing wrong?\nThanks\nNiansa",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/516",
        "createdAt": "2023-03-26T02:12:08Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Repository organization",
        "bodyText": "Currently the repo seems a bit cluttered? Source files mixed with scripts and such forth.\nI am not confident with organizing the files properly & updating cmake etc, and I do not want to break anyones PRs by doing so either.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/504",
        "createdAt": "2023-03-25T17:49:48Z",
        "author": {
            "login": "Doomsdayrs"
        }
    },
    {
        "title": "Past tokens parameter",
        "bodyText": "I've seen the n_past parameter in llama_eval and I am not sure what it does, I only know messing with the value changes the output of the model. What is it?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/510",
        "createdAt": "2023-03-25T21:12:55Z",
        "author": {
            "login": "SpeedyCraftah"
        }
    },
    {
        "title": "Any documentation or examples for API?",
        "bodyText": "I don`t understand how to use API (for example in Python with Ctypes).\nI think some documentation would be helpful.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/501",
        "createdAt": "2023-03-25T16:17:16Z",
        "author": {
            "login": "Sovenok-Hacker"
        }
    },
    {
        "title": "Not sure where to start regarding getting llama.cpp setup on Android pixel",
        "bodyText": "You mention using cmake but coming from a non programming background, could anyone provide a step by step on how to go about adding the weights, setting up cmake and configuring termux? I've spent all day trying to get it set up and all I end up with is \"permission denied\". Thanks.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/422",
        "createdAt": "2023-03-23T10:53:02Z",
        "author": {
            "login": "cloudliness"
        }
    },
    {
        "title": "Segmentation faults.",
        "bodyText": "Using: M1 Macbook Pro, 16GB ram.  Latest code.  Using the 4bit models (7 or 13B) I reliably get segmentation faults after a few paragraphs of use. It doesn't seem to be an out of memory error ( there is plenty of swap available.) Any ideas?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/471",
        "createdAt": "2023-03-24T19:35:54Z",
        "author": {
            "login": "Xorrongo"
        }
    },
    {
        "title": "llama-kt development",
        "bodyText": "Greetings!\nCurrently, I am working on creating a Kotlin/MP wrapper for libllama.\nI will publish code on a later point, as soon as I finish cleaning up and publishing what I am working on currently.\nI am making this post to figure out two functions.\n    // Token logits obtained from the last call to llama_eval()\n    // The logits for the last token are stored in the last row\n    // Can be mutated in order to change the probabilities of the next token\n    // Rows: n_tokens\n    // Cols: n_vocab\n    LLAMA_API float * llama_get_logits(struct llama_context * ctx);\n\n    // Get the embeddings for the input\n    // shape: [n_embd] (1-dimensional)\n    LLAMA_API float * llama_get_embeddings(struct llama_context * ctx);\nWhere does one get n_embd & n_tokens?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/469",
        "createdAt": "2023-03-24T18:37:23Z",
        "author": {
            "login": "Doomsdayrs"
        }
    },
    {
        "title": "A flag to use hugepages / speed impovement?",
        "bodyText": "Would there be any speed improvement if/when 2MB/1GB hugepages are available and used?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/489",
        "createdAt": "2023-03-25T07:21:48Z",
        "author": {
            "login": "j1warren"
        }
    },
    {
        "title": "Xcode build example?",
        "bodyText": "I would be interested in integrating llama directly into an Xcode project for macOS rather than as a command line application. Would anyone be able to advise on how to build the library that way? Alternatively, I imagine some interprocess communication or pipes could work.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/486",
        "createdAt": "2023-03-25T01:19:17Z",
        "author": {
            "login": "KTRosenberg"
        }
    },
    {
        "title": "Alpaca instruction mode findings",
        "bodyText": "From my testing, it seems that using the ### Instruction: ### Response: isn't strictly necessary.\nAnecdotally, the fine-tuning done with the instruction-response model seems to have taught it to be better at answering prompts even when the instruct-response model is not used.\nTesting with the chat-with-bob.txt prompt (not using instruct mode), the regular llama model often leads to interactions where Bob answers with \"I can do that\" or similar, and you need to input \"Go on\" or something to get the actual answer. With alpaca models I haven't seen such behaviour, and it always directly answers the questions asked.\nThat being said, it would make logical sense that using the instruct mode should give better output. However, I don't know if that's the case. In any case, the training clearly has an effect outside the instruct-response context too.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/488",
        "createdAt": "2023-03-25T06:12:58Z",
        "author": {
            "login": "anzz1"
        }
    },
    {
        "title": "It just stops, no error no reason to end ?",
        "bodyText": "I've been testing alpaca 30B (-t 24 -n 2000 --temp 0.2 -b 32 --n_parts 1 --ignore-eos --instruct)\nI've consistently have it \"stop\" after 300-400 tokens output (30-40 tokens input)\nNo error message, no crash and given the -n 2000 and the ignore-eos no reason to stop so early\nI guess it would be useful if the program provides a verbose quit reason, though in my case I can't see any reason for it to stop before token max is reached.\nI'm not sure if that's a bug to report or if I am missing something.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/446",
        "createdAt": "2023-03-23T23:27:41Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "Name change proposal discussion",
        "bodyText": "https://twitter.com/theshawwn/status/1638925249709240322",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/437",
        "createdAt": "2023-03-23T18:10:17Z",
        "author": {
            "login": "eous"
        }
    },
    {
        "title": "Perplexity visualization",
        "bodyText": "I wrote a quick and dirty patch to visualize the perplexity of each token. Fun to see the tokens turn red when the model is obviously bullshitting.\n\nFun fact: with the exception of the last set of coordinates, these are within 50km of the actual location.\nThe patch is here: blackle@43d7626",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/459",
        "createdAt": "2023-03-24T09:54:20Z",
        "author": {
            "login": "blackle"
        }
    },
    {
        "title": "Mirror of this repository",
        "bodyText": "Since Facebook/Meta has started sending false DMCAs, I set up a mirror of this repo on my own git server: https://git.as211233.net/ronsor/llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/435",
        "createdAt": "2023-03-23T18:03:39Z",
        "author": {
            "login": "Ronsor"
        }
    },
    {
        "title": "SHA256 checksums housekeeping",
        "bodyText": "Outdated original post for posterity\n\nNot all of these checksums seem to be correct. Are they calculated with the \"v2\" new model format after the tokenizer change? PR: #252 Issue: #324\nFor example, \"models/alpaca-7B/ggml-model-q4_0.bin\"\nv1: 1f582babc2bd56bb63b33141898748657d369fd110c4358b2bc280907882bf13\nv2: 8d5562ec1d8a7cfdcf8985a9ddf353339d942c7cf52855a92c9ff59f03b541bc\nThe SHA256SUMS file has the old v1 hash.\nMaybe using a naming scheme like \"ggml2-model-q4_0.bin\" would be good to differentiate between the versions and avoid confusion.\n\nOriginally posted by @anzz1 in #338 (comment)\nedit: After converting the models to the new format, I found out that the \"v2\" hash above is also incorrect.\nThe sha256 for ./models/alpaca-7B-ggml/ggml-model-q4_0.bin is supposed to be 2fe0cd21df9c235c0d917c14e1b18d2d7320ed5d8abe48545518e96bb4227524\n\nThis is now a general discussion about keeping sha256 checksums updated and maybe have some sort of standardisation.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/433",
        "createdAt": "2023-03-21T23:05:19Z",
        "author": {
            "login": "anzz1"
        }
    },
    {
        "title": "Attempting to use the C API from C++",
        "bodyText": "Hey!\nI am attempting to use the new C API from C++, however I must be doing something significantly wrong since the results are very very weird (literally nonsense).\nHere is the code:\nclass LLM {\n    struct Exception : public std::runtime_error {\n        using std::runtime_error::runtime_error;\n    };\n\n    struct {\n        std::string model = \"7B-ggml-model-quant.bin\";\n\n        int32_t seed; // RNG seed\n        int32_t n_threads = static_cast<int32_t>(std::thread::hardware_concurrency()) / 4;\n        int32_t n_ctx = 2024; // Context size\n        int32_t n_batch = 8; // Batch size\n\n        int32_t top_k = 40;\n        float   top_p = 0.5f;\n        float   temp  = 0.81f;\n    } params;\n\n    struct State {\n        std::string prompt;\n        std::vector<llama_token> embd;\n        int n_ctx;\n    } state;\n\n    llama_context *ctx;\n    std::mutex lock;\n\n    void init() {\n        // Get llama parameters\n        puts(\"30\");\n        auto lparams = llama_context_default_params();\n        lparams.seed = params.seed;\n        lparams.n_ctx = 2024;\n\n        // Create context\n        puts(\"31\");\n        ctx = llama_init_from_file(params.model.c_str(), lparams);\n        puts(\"32\");\n\n        // Initialize some variables\n        state.n_ctx = llama_n_ctx(ctx);\n    }\n\npublic:\n    LLM(int32_t seed = 0) {\n        // Set random seed\n        params.seed = seed?seed:time(NULL);\n\n        // Initialize llama\n        init();\n    }\n\n    void append(const std::string& prompt) {\n        std::scoped_lock L(lock);\n\n        // Check if prompt was empty\n        const bool was_empty = state.prompt.empty();\n\n        // Append to current prompt\n        printf(\"ddd %s\\n\", prompt.c_str());\n        state.prompt.append(prompt);\n\n        // Resize buffer for tokens\n        puts(\"cccc\");\n        const auto old_token_count = state.embd.size();\n        state.embd.resize(old_token_count+state.prompt.size()+1);\n\n        // Run tokenizer\n        puts(\"bbbb\");\n        const auto token_count = llama_tokenize(ctx, prompt.data(), state.embd.data()+old_token_count, state.embd.size()-old_token_count, was_empty);\n        state.embd.resize(old_token_count+token_count);\n\n        // Evaluate new tokens\n        // TODO: Larger batch size\n        printf(\"aaa %lu+%d=%lu\\n\", old_token_count, token_count, old_token_count+token_count);\n        for (int it = old_token_count; it != old_token_count+token_count; it++) {\n            printf(\"aaa %i %s\\n\", it, llama_token_to_str(ctx, state.embd.data()[it]));\n            llama_eval(ctx, state.embd.data()+it, 1, it, params.n_threads);\n        }\n    }\n\n    std::string run(std::string_view end) {\n        std::scoped_lock L(lock);\n        std::string fres;\n\n        // Loop until done\n        puts(\"6\");\n        bool abort = false;\n        while (!abort && !fres.ends_with(end)) {\n            // Sample top p and top k\n            const auto id = llama_sample_top_p_top_k(ctx, nullptr, 0, params.top_k, params.top_p, params.temp, 1.0f);\n\n            // Add token\n            state.embd.push_back(id);\n\n            // Get token as string\n            const auto str = llama_token_to_str(ctx, id);\n\n            // Debug\n            std::cout << str << std::flush;\n\n            // Append string to function result\n            fres.append(str);\n\n            // Evaluate token\n            // TODO: Larger batch size\n            llama_eval(ctx, state.embd.data()+state.embd.size()-1, 1, state.embd.size()-1, params.n_threads);\n        }\n\n        // Return final string\n        puts(\"23\");\n        state.prompt.append(fres);\n        return std::string(fres.data(), fres.size()-end.size());\n    }\n};\nIt'd be amazing if someone with a bit more knowledge than me could look over this and maybe give me some tips and hints :-)\nI must be forgetting something very significant.\nBtw: the code is called from multiple threads, but never at the same time thanks to the locks.\nThanks\nTuxifan",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/427",
        "createdAt": "2023-03-23T12:21:49Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Loading Lora function ?",
        "bodyText": "Just noticed merge Lora in llama seems need transform 4.28 to using llamatokenize?\nAnyway I don't know anything about it,\nMay someone build a function to load Lora manually so it could be a easy way to improve generations?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/417",
        "createdAt": "2023-03-23T07:40:56Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Adding a flag to run system commands upon detecting a keyword",
        "bodyText": "I\u2019ve been thinking about adding a flag that would let the user specify a keyword and a command so that upon sampling that keyword from the model, the command would be executed and it\u2019s output fed back in.\nI\u2019ve figured out how I would go about doing this, but I thought maybe this kind of feature should be implemented at a higher level and not baked directly into llama.cpp.\nWhat do you think?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/415",
        "createdAt": "2023-03-23T07:33:21Z",
        "author": {
            "login": "tjohnman"
        }
    },
    {
        "title": "Using llama as Python interpreter",
        "bodyText": "I experimented with using Llama to \"Interpret\" code:\nPython 3.9.2 (default, Feb 28 2021, 17:03:44)\n[GCC 10.2.1 20210110] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> print(\"Hello world!\")\nHello world!\n>>> import huge_dictionary as dic\n>>> res = dic.find(\"treehouse\")\n>>> res.description\n'A treehouse is a structure built around, in, or on the trunk of a mature tree that can be used for recreation, leisure, work, or residence.'\n>>>\n\nIt's working wonderfully! You can litterally just think of any module and it'll just... exist... and have all the functions you tell it to invoke. Pretty cool. My prompt is everything up to the >>> after the Hello world! output.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/396",
        "createdAt": "2023-03-22T15:36:48Z",
        "author": {
            "login": "niansa"
        }
    },
    {
        "title": "Can more CPU cores be utilised?",
        "bodyText": "When running this, it only ever uses 1 CPU core (on my intel MacBook pro), wondering if this is by design or some limitation that can't be avoided? or maybe it doesn't matter as much as I imagine?\nI currently get about 1.6-2 words a second, I'm amazed to get even that tbh, but wondering if it can increased a lot further by utilizing more cores?\nMaybe these is already a switch I am over looking?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/391",
        "createdAt": "2023-03-22T12:51:16Z",
        "author": {
            "login": "jmio23"
        }
    },
    {
        "title": "Any interest in 2-bit quantizing the 65B model?",
        "bodyText": "I'm not sure if this is possible with the current setup? I wasn't successful at 2-bit quantizing the 7B model, it actually came out bigger than the 4-bit result. There has been a little bit of discussion about going smaller here. https://nolanoorg.substack.com/p/int-4-llama-is-not-enough-int-3-and",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/206",
        "createdAt": "2023-03-16T12:13:36Z",
        "author": {
            "login": "Alcyon6"
        }
    },
    {
        "title": "Increasing performance of 65B llama",
        "bodyText": "I am wondering if there is a way of potentially speeding up the 65B llama model? I have 32GB ram at 3600mhz with a ryzen 7 5800x and it's maxing both out (ryzen 7 maxed out probably because of RAM swap) and it generates tokens at a rate of 2-3 minutes per token with my RAM being maxed out.\nI am running on windows 11 but thought to perhaps try to get Linux on a flash drive and attempt to run the model from there to see if there's an improvement? Since linux probably handles swap a little better as well as using much less RAM than windows.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/369",
        "createdAt": "2023-03-21T20:40:29Z",
        "author": {
            "login": "SpeedyCraftah"
        }
    },
    {
        "title": "Performance seems too good, what is going on ?",
        "bodyText": "I've been testing the 8 bit 6B llama on my 3090 and my results were at best as fast as your CPU video.\nI noticed the GPU does not get used much, so I assume the CPP is well optimized and the GPU implementation I ran was not.\nThough maybe someone has additional insights ?\nIf CPU gives such a high speed, a 3090 should deliver hundreds or more tokens/sec. I was running at 4-5",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/337",
        "createdAt": "2023-03-20T19:19:34Z",
        "author": {
            "login": "cmp-nct"
        }
    },
    {
        "title": "Output encoding problem?",
        "bodyText": "Hey guys! When I run the binary compiled using code from the master branch on Windows 11, during interaction with the model, in situations involving Latin characters such as \u00e1, \u00e3, \u00e9 as a response, the characters are displayed incorrectly.\nI got the same kind of behavior when trying to interact with the model in languages like Portuguese and Spanish. Characters from languages such as Japanese, Chinese, and Korean are also displayed incorrectly.\nI believe it's not a problem with my terminal's encode configuration, as the characters are displayed correctly when inputting data to the model, but the output from model brings invalid characters.\nMy system information:\nCPU: Ryzen 9 5900x\nOS: Windows 11 Pro x64\ncmake version 3.26.0\nPython 3.10.10\nCompiler: Visual Studio Community 2022 using compilers for 17.5.2\nExample:\n\nIs this behaviour expected?\nThanks!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/366",
        "createdAt": "2023-03-21T18:07:37Z",
        "author": {
            "login": "tomsnunes"
        }
    },
    {
        "title": "Interactive mode in Python?",
        "bodyText": "Hello, I have a question. How can i use LLaMa in an interactive mode (i.e. as a chat) in Python, and is it possible at all? So that he would not just generate text, but it would be possible to somehow communicate",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/360",
        "createdAt": "2023-03-21T15:40:27Z",
        "author": {
            "login": "HYBAS22"
        }
    },
    {
        "title": "Moving forward",
        "bodyText": "Do the original coding guidelines still apply?\n\n\nAvoid adding third-party dependencies, extra files, extra headers, etc.\nAlways consider cross-compatibility with other operating systems and architectures\nAvoid fancy looking modern STL constructs, use basic for loops, avoid templates, keep it simple\n\n\nThe original codebase was minimalistic, clean and fast C-style code.\nHowever, recent changes have:\n\nAdded C++17 as a requirement\nStarted to lean heavily towards using \"modern\" STL type constructs\nAdded lots of platform-specific stuff\n\nI do understand that some people want to rather work with modern C++/STL instead of C and a lot can be done easily with external libraries. However, in my opinion, the C-style, platform-independent, minimalistic style is exactly the whole point which makes this project special.\nSo my question is, is this the direction the project is going to take?\nIf so, I've forked a minimalistic version here https://github.com/anzz1/llama.cpp-min with the aim of keeping in line with the original codebase, keeping C-style in favor of C++/STL and removing all platform-specifics.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/346",
        "createdAt": "2023-03-21T05:06:46Z",
        "author": {
            "login": "anzz1"
        }
    },
    {
        "title": "i can\u2019t let llama even 65b write python code\u2026",
        "bodyText": "\ud83d\ude05",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/328",
        "createdAt": "2023-03-20T14:25:37Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Specific knowledge domains",
        "bodyText": "Has anyone ever fused project documentation and formatted to add as supervised training, with a forum formatted to provide feedback for reinforcement learning?\nCould this project collate its own documentation and provide a reinforcement learning forum so that snapshots can be trained for a tailored Llama/Alpaca llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/347",
        "createdAt": "2023-03-21T05:49:44Z",
        "author": {
            "login": "StuartIanNaylor"
        }
    },
    {
        "title": "Regenerating the models",
        "bodyText": "Now with the merged PR #252 I need to reconvert the model.\nDoes this mean, I need to reconvert the base models consolidated.0.pth or can I reconvert the quantized models too?\nI'm currently working with the 7B alpaca Lora 4bit and the 13B alpaca lora model 4bit and would like to continue working with it.\nI don't have the ressources to:\n\nfine-tune the models again for using with llama.cpp\nnor do I have the ressources to quantize the big models.\n\nIf I could reconvert my quantized alpaca lora models will be amazing.\nIf not, this would be a breaking change (as digital nomade often slow wifi and limited ressources)\nIt will be amazing, when someone could clarify.\nThank you",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/323",
        "createdAt": "2023-03-20T12:17:11Z",
        "author": {
            "login": "PriNova"
        }
    },
    {
        "title": "Simple Web UI + Python bindings",
        "bodyText": "Hello everyone,\nI was playing with llama.cpp (btw, thanks @ggerganov for the great work) and I thought it would be cool to create some easy to use Python bindings for it.\nI ended up creating a simple web UI as well.\nPrebuilt wheels are pushed to PyPI, so you can now run llama.cpp with a simple pip install (hopefully!)\nCheck it out here: pyllamacpp\nHope you will find it useful!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/345",
        "createdAt": "2023-03-21T01:17:13Z",
        "author": {
            "login": "abdeladim-s"
        }
    },
    {
        "title": "Understanding the basics",
        "bodyText": "First of all many thanks to Georgi for his great work and thanks also to all the people that is contributing.\nFor those of us that are starting on this (at least I managed to build it and test it) I see very usefull to provide some basic ideas about how this works, so more people can understand and help.\nAs far as I understand it, some people related to Meta generated the \"weight\" files that are being used here. Some questions:\n\nHow did they create those weights ?\nHow is it possible that llama.cpp uses those weights without using python ?\nCould someone provide a very simple to understand explanation about how this works ?\n\nSurely there are many more questions that may arise. If we provide an easy to understand foundation surely more people will join.\nThank you to all that share some of their time to help others understand the basics.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/286",
        "createdAt": "2023-03-19T09:49:16Z",
        "author": {
            "login": "FiveTechSoft"
        }
    },
    {
        "title": "Fork of llama.cpp running BLOOM models",
        "bodyText": "NouamaneTazi/bloomz\nInference of HuggingFace's BLOOM-like models in pure C/C++.\nThe repo was built on top of the amazing llama.cpp repo by @ggerganov, to support BLOOM models. It supports all models that can be loaded using BloomForCausalLM.from_pretrained().",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/327",
        "createdAt": "2023-03-20T14:13:12Z",
        "author": {
            "login": "gjmulder"
        }
    },
    {
        "title": "continuous interaction",
        "bodyText": "when the bot interaction reaches the context length the programm exits.\nAs far as I understand how the transformer architecture is used here it stores the hidden states of the transformer when new tokens come in. So when the context length is reached one would need to discard some tokens at the beginning and restart the inference. Can we implement something like this?\nI understand that this would mean a noticable delay since we would need to make a big inference about a good portion of the chat. Can we maybe start this inference in the background while user is still chatting within allowed context length?\nIf not can we at least start the new inference at a point where user input is requested so as to hide at least some of the delay from the user?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/322",
        "createdAt": "2023-03-20T11:49:39Z",
        "author": {
            "login": "DKormann"
        }
    },
    {
        "title": "llamacpp-for-kobold, a zero dependency KoboldAI compatible REST API interfacing with llama.cpp via ctypes bindings",
        "bodyText": "Now that it's somewhat usable, I'd like to share a thing I made: https://github.com/LostRuins/llamacpp-for-kobold\nThis fork does 2 things mainly:\n\nAdds ctypes python bindings allowing llama.cpp to load models and generate text directly from python code, without messing with stdout pipes or command line flags, and wrapping it all in a nice llamacpp.dll\nEmulates a KoboldAI compatible HTTP server, allowing it to be used as a custom API endpoint from within Kobold, which provides an excellent UI for text generation.\nNot use any dependencies like Flask or Pybind11 requiring basically 0 installation.\nYou can either run the local kobold client or the KoboldAI Lite web client, and connect directly to this server, allowing you to enjoy local text generation with full editing and file saving features.\n\nThere is only one pressing flaw related to prompt processing latency, and that is #229 which hopefully someone can solve.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/315",
        "createdAt": "2023-03-20T06:01:01Z",
        "author": {
            "login": "LostRuins"
        }
    },
    {
        "title": "llama.cpp on AWS EC2 under $2",
        "bodyText": "Prerequisites\nStart a instance with t2.xlarge type hardware (4 core , 16GiB Memory), Ubuntu 22.04 AMI\nSSH with PuTTY 0.76 above\nMicrosoft Remote Desktop Connection\n1. Python Configuration\n\npython 3.10.6 has been preinstalled on Ubuntu 22.04\nsudo apt install python-pip\npython3 -m pip install torch numpy sentencepiece\n\n2. CMake Configuration\n\nGCC, G++ 11.3.0 preinstalled on Ubuntu 22.04\nsudo apt install build-essential\nsudo apt-get install libcurl4-openssl-dev libssl-dev uuid-dev zlib1g-dev libpulse-dev\nsudo apt install cmake\n\n3. 7B Model Quantification and Inference with llama.cpp\n4. Performance\n\nSpeed: 310 ms per token\nTime for Model Deployment, from starting instance to getting the first inference result:  < 4 Hours\n\n\n5. Highlight\n\nMake the model easily scalable\nThe process of retraining and inference can be more efficient in a consistent flow",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/296",
        "createdAt": "2023-03-19T15:08:44Z",
        "author": {
            "login": "forgeda"
        }
    },
    {
        "title": "Parallelism across multiple nodes over a slow link?",
        "bodyText": "So I was wondering, would it be possible to have the model split across multiple nodes communicating over a slow link?\nMy use case would be to split the model into <4 GB chunks ([32-bit] WebAssembly memory limit) distributed across peers connected via WebRTC.\nBLOOM already caters to a use case similar to mine (i.e. Petals), except for that its requirements exceed the resources available in this case\u2026",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/245",
        "createdAt": "2023-03-17T20:54:09Z",
        "author": {
            "login": "remi6397"
        }
    },
    {
        "title": "In transcript mode the different the person(llama's emulations) the different the day of today.",
        "bodyText": "One in 2015, one in 2017; more test needed.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/287",
        "createdAt": "2023-03-19T10:06:46Z",
        "author": {
            "login": "FNsi"
        }
    },
    {
        "title": "Failing CI autobuilds",
        "bodyText": "The workflow permissions need to be set to \"Read and Write permissions\" for the CI workflow to work properly.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/268",
        "createdAt": "2023-03-18T20:10:58Z",
        "author": {
            "login": "anzz1"
        }
    },
    {
        "title": "docker\u5bb9\u5668\u600e\u4e48\u4e0b\u8f7d\u5462",
        "bodyText": "\u627e\u4e0d\u5230\u4e0b\u8f7d\u5236\u4f5c\u597d\u7684docker\u955c\u50cf\u7684\u65b9\u6cd5",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/265",
        "createdAt": "2023-03-18T14:37:31Z",
        "author": {
            "login": "w1103693423"
        }
    },
    {
        "title": "does it take a lot of time to get a feedback for downloading llama checkpoints?",
        "bodyText": "Hi i filled the form found in https://github.com/facebookresearch/llama two time since the beginning of the week\nbut have not received any email. Is such a delay something to expect or perhaps its something else .. (checked spam).\nShould i have gotten immediate feedback by filling the google form or requests are manually reviewed screened and could take some time to get a reply?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/244",
        "createdAt": "2023-03-17T18:17:52Z",
        "author": {
            "login": "meltoner"
        }
    },
    {
        "title": "Converted GGML models hosting?",
        "bodyText": "Apologies if Github Issues is not the right place for this question, but do you know if anyone has hosted the ggml versions of the models? The disk space required to download and convert is a little steep.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/179",
        "createdAt": "2023-03-15T18:00:46Z",
        "author": {
            "login": "mparrett"
        }
    },
    {
        "title": "If I git pull will I have to remake the training weights?",
        "bodyText": "I downloaded this project a while ago since it is the best interface with LLaMa at the moment, considering the weights are a few hundred GB total, it would be quite inconvenient to have to re-quantisize them every time I git pull, Is this an issue I should be concerned about?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/220",
        "createdAt": "2023-03-17T01:50:07Z",
        "author": {
            "login": "nazthelizard122"
        }
    },
    {
        "title": "LLaMA C++ on Haiku",
        "bodyText": "Getting the LLaMA models now to test the actual dataset.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/211",
        "createdAt": "2023-03-16T19:05:11Z",
        "author": {
            "login": "kallisti5"
        }
    },
    {
        "title": "LLaMa is kinda insane",
        "bodyText": "is this normal?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/215",
        "createdAt": "2023-03-16T22:33:46Z",
        "author": {
            "login": "Mr-NI"
        }
    },
    {
        "title": "No output after commit 84d9015 on Android",
        "bodyText": "When git checkout 84d9015 and make, there will be no output (only the model loading message) in termux.\ngit checkout 63fd76f will produce a fully-functional binary.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/234",
        "createdAt": "2023-03-17T09:34:57Z",
        "author": {
            "login": "ShouNichi"
        }
    },
    {
        "title": "Thanks for contributing to Machine Learning and AI with this repository!",
        "bodyText": "I just want to say Thank You!\nI got your solution working and I am happy to confirm that it is working as intended. The quality of text produced is probably a bit lower compared to text produced with the full weights, but the quantized weights saves a lot of space and maybe some processing time.\nI would like to see the solution rewritten as C# to better understand it, since I am using C# at work but do not use C++ och C since I was studying programming in the 90s. I do understand this is not the main goal of this project, but in case someone else wants to do this or have already done this - please leave me a note.\nThanks again for this lovely work!",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/236",
        "createdAt": "2023-03-17T09:15:17Z",
        "author": {
            "login": "Darlanio"
        }
    },
    {
        "title": "Facing this error KeyError: \"filename 'storages' not found\" when I follow the instructions",
        "bodyText": "I am facing this error KeyError: \"filename 'storages' not found\" when I follow the instructions (https://github.com/rgerganov)\nWhen I run this command\npython3 convert-pth-to-ggml.py models/7B/ 1\n\nI get:\nTraceback (most recent call last):\n  File \"/Users/rohitkumar/Code/llama/llama.cpp/convert-pth-to-ggml.py\", line 89, in <module>\n    model = torch.load(fname_model, map_location=\"cpu\", weights_only=True)\n  File \"/Users/rohitkumar/miniforge3/lib/python3.10/site-packages/torch/serialization.py\", line 792, in load\n    return _legacy_load(opened_file, map_location, _weights_only_unpickler, **pickle_load_args)\n  File \"/Users/rohitkumar/miniforge3/lib/python3.10/site-packages/torch/serialization.py\", line 987, in _legacy_load\n    return legacy_load(f)\n  File \"/Users/rohitkumar/miniforge3/lib/python3.10/site-packages/torch/serialization.py\", line 884, in legacy_load\n    tar.extract('storages', path=tmpdir)\n  File \"/Users/rohitkumar/miniforge3/lib/python3.10/tarfile.py\", line 2091, in extract\n    tarinfo = self.getmember(member)\n  File \"/Users/rohitkumar/miniforge3/lib/python3.10/tarfile.py\", line 1813, in getmember\n    raise KeyError(\"filename %r not found\" % name)\nKeyError: \"filename 'storages' not found\"\n\nMy dir\n(base) rohitkumar@Rohits-MacBook-Pro llama.cpp % ls -l                                      \ntotal 2832\n-rw-r--r--  1 rohitkumar  staff    1072 Mar 12 12:49 LICENSE\n-rw-r--r--  1 rohitkumar  staff    5036 Mar 12 12:49 Makefile\n-rw-r--r--  1 rohitkumar  staff    7855 Mar 12 12:49 README.md\n-rw-r--r--  1 rohitkumar  staff    4722 Mar 12 12:49 convert-pth-to-ggml.py\n-rw-r--r--  1 rohitkumar  staff  323952 Mar 12 12:49 ggml.c\n-rw-r--r--  1 rohitkumar  staff   22025 Mar 12 12:49 ggml.h\n-rw-r--r--  1 rohitkumar  staff  223392 Mar 12 12:50 ggml.o\n-rwxr-xr-x  1 rohitkumar  staff  325729 Mar 12 12:50 main\n-rw-r--r--  1 rohitkumar  staff   31185 Mar 12 12:49 main.cpp\ndrwxr-xr-x  9 rohitkumar  staff     288 Mar 15 12:29 models\n-rwxr-xr-x  1 rohitkumar  staff  307525 Mar 12 12:50 quantize\n-rw-r--r--  1 rohitkumar  staff   11403 Mar 12 12:49 quantize.cpp\n-rw-r--r--  1 rohitkumar  staff   16872 Mar 12 12:49 utils.cpp\n-rw-r--r--  1 rohitkumar  staff    2868 Mar 12 12:49 utils.h\n-rw-r--r--  1 rohitkumar  staff  134608 Mar 12 12:50 utils.o\n\nand for models\n(venv) (base) rohitkumar@Rohits-MacBook-Pro models % ls -l\ntotal 992\ndrwxr-xr-x  5 rohitkumar  staff     160 Mar 15 12:58 7B\n-rw-r--r--  1 rohitkumar  staff  499723 Mar 15 12:55 tokenizer.model\n-rw-r--r--  1 rohitkumar  staff      50 Mar 15 12:56 tokenizer_checklist.chk\n(venv) (base) rohitkumar@Rohits-MacBook-Pro 7B % ls -l\ntotal 26322168\n-rw-r--r--  1 rohitkumar  staff          100 Mar 15 12:58 checklist.chk\n-rw-r--r--  1 rohitkumar  staff  13476939516 Mar 15 12:54 consolidated.00.pth\n-rw-r--r--  1 rohitkumar  staff          101 Mar 15 12:54 params.json",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/168",
        "createdAt": "2023-03-15T17:27:13Z",
        "author": {
            "login": "SublimeAI"
        }
    },
    {
        "title": "Model auto stop?",
        "bodyText": "Hello, I'm new to this business. Tell me, is it possible to cancel the shutdown of the model after a few questions in interactive mode? Is it possible to keep the model running for a long time after launch?\nthe model stops automatically with these lines\nmain: mem per token = 22439492 bytes\nmain:     load time =  4926.95 ms\nmain:   sample time =   115.04 ms\nmain:  predict time = 137328.81 ms / 353.94 ms per token\nmain:    total time = 227331.20 ms\nIs it possible to make changes to the code to increase the model's running time?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/207",
        "createdAt": "2023-03-16T13:29:45Z",
        "author": {
            "login": "mihail342"
        }
    },
    {
        "title": "Cleaning up issues - ping me if I've missed anything",
        "bodyText": "I volunteered to label and house clean the issues as we have quite a few duplicates and open but resolved issues. Please ping me if I've made any mistakes or missed anything.\nI'm going through and adding labels to everything first and then will try and identify subsequent duplicates based on label subsets.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/183",
        "createdAt": "2023-03-15T21:49:13Z",
        "author": {
            "login": "gjmulder"
        }
    },
    {
        "title": "[Proposal] \"Stable\" C API",
        "bodyText": "I propose refactoring main.cpp into a library (llama.cpp, compiled to llama.so/llama.a/whatever) and making main.cpp a simple driver program. A simple C API should be exposed to access the model, and then bindings can more easily be written for Python, node.js, or whatever other language.\nThis would partially solve #82 and #162.\nEdit: on that note, is it possible to do inference from two or more prompts on different threads? If so, serving multiple people would be possible without multiple copies of model weights in RAM.",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/177",
        "createdAt": "2023-03-15T18:01:09Z",
        "author": {
            "login": "Ronsor"
        }
    },
    {
        "title": "`eps` =? `norm_eps`",
        "bodyText": "Are eps and and norm_eps distinct values? Trying to dive deeper into the source code, the comments about eps don't align with the model parameters for norm_eps. So, I thinking they are different parameters?\n{\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-06, \"vocab_size\": -1}\n{\"dim\": 5120, \"multiple_of\": 256, \"n_heads\": 40, \"n_layers\": 40, \"norm_eps\": 1e-06, \"vocab_size\": -1}\n{\"dim\": 6656, \"multiple_of\": 256, \"n_heads\": 52, \"n_layers\": 60, \"norm_eps\": 1e-06, \"vocab_size\": -1}\n{\"dim\": 8192, \"multiple_of\": 256, \"n_heads\": 64, \"n_layers\": 80, \"norm_eps\": 1e-05, \"vocab_size\": -1}",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/186",
        "createdAt": "2023-03-15T22:27:20Z",
        "author": {
            "login": "bitRAKE"
        }
    },
    {
        "title": "Where can I download the models?",
        "bodyText": "Can someone please help me where can I download the models needed?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/185",
        "createdAt": "2023-03-15T21:56:06Z",
        "author": {
            "login": "aswilam"
        }
    },
    {
        "title": "Not an issue but what depends on the number of threads?",
        "bodyText": "I've been testing your code from 1 to 8 threads and the output is always different. The speed is not depend on the number of threads. On the contrary, 4 threads may perform much better than 1, whereas 8 threads supposedly provides a better result. However, the same prompt may give the same excellent output with triple speed with 4 threads compared to 8. But still, when I use 8 threads (my maximum on M1) I use all my CPU resources, but it doesn't affect speed at all (seemingly works slower) and not giving quality effect (apparently). Am I wrong? Can you correct me if I'm mistaken? May be there is some best speed/quality option and I just that stupid that was unable to figure out how to use this option?",
        "url": "https://github.com/ggerganov/llama.cpp/discussions/180",
        "createdAt": "2023-03-15T16:03:26Z",
        "author": {
            "login": "alexcardo"
        }
    }
]